% docx2tex 1.6 --- ``The last thing your Word file sees before the recycle bin.'' 
% 
% docx2tex is Open Source and  
% you can download it on GitHub: 
% https://github.com/transpect/docx2tex 
%  
\newcommand{\LangUsed}{0}	% 0=English, 1=Italian				%Baldo
\newif\ifLangEnglish
\LangEnglishtrue			% de-comment if English


\documentclass[fontsize=12pt, paper=a4, pagesize, DIV=calc, twoside]{scrbook} 	%Baldo
\usepackage[OT6,T2A,T1]{fontenc}	% Baldo: % Armenian, Russian, English
\usepackage[utf8]{inputenc} 
\usepackage{graphicx}
%\usepackage{hyperref}
\usepackage[bookmarksopen,bookmarksdepth=2,breaklinks=true]{hyperref}														%Baldo for long links
 
\usepackage{multirow} 
\usepackage{tabularx} 
\usepackage{color} 
\usepackage{textcomp} 
\usepackage{tipa}
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage{amsfonts} 
\usepackage{amsxtra} 
\usepackage{wasysym} 
\usepackage{isomath} 
\usepackage{mathtools} 
\usepackage{txfonts} 
\usepackage{upgreek} 
\usepackage{enumerate} 
\usepackage{tensor} 
\usepackage{pifont} 
\usepackage{ulem} 
\usepackage{xfrac} 
\usepackage{soul}
\usepackage{arydshln} 

\ifLangEnglish
	\usepackage[russian,italian,english]{babel}	% Baldo 	Note for English invert with Italian
\else
	\usepackage[russian,english,italian]{babel}	% Baldo 	Note for English invert with Italian
\fi

% \usepackage{animate}						% Baldo

\definecolor{color-1}{rgb}{1,1,1}
\definecolor{color-2}{rgb}{0.18,0.33,0.59}
\definecolor{color-3}{rgb}{0.33,0.1,0.55}
\definecolor{color-4}{rgb}{0.2,0.2,1}
\definecolor{color-5}{rgb}{0.8,0,1}
\definecolor{color-6}{rgb}{0,0.44,0.75}
\definecolor{color-7}{rgb}{0.12,0.31,0.47}
\definecolor{color-8}{rgb}{0,0,1}
\definecolor{color-9}{rgb}{0.33,0.51,0.21}
\definecolor{color-10}{rgb}{0,0,0.6}
\definecolor{color-11}{rgb}{0.58,0.31,0.45}
\definecolor{color-12}{rgb}{1,0,0}
\definecolor{color-13}{rgb}{0.6,0,0}
\definecolor{color-14}{rgb}{0,0.4,0}
\definecolor{color-15}{rgb}{0.27,0.45,0.77}
\definecolor{color-16}{rgb}{0.6,0.2,1}
\definecolor{color-17}{rgb}{0.2,0.8,0}
\definecolor{color-18}{rgb}{0,0,0.93}
\definecolor{color-19}{rgb}{0.8,0,0}
\definecolor{color-20}{rgb}{0,0.6,0}
\definecolor{color-21}{rgb}{1,0,1}
\definecolor{color-22}{rgb}{0.2,0,0.2}
\definecolor{color-23}{rgb}{0.2,0.2,0.2}
\definecolor{color-24}{rgb}{0,0.4,0.13}
\definecolor{color-25}{rgb}{0.2,0.2,0.2}

% ================= Baldo ===================
%\usepackage{lmodern}
%% \usepackage{mathptmx}					% Change font
%\setkomafont{disposition}{\bfseries}	% Change font
%\usepackage{microtype}					% Load the microtype package for better text justification.
%\frenchspacing

\hyphenation{s-cri-pt}			% I know, it's an error
\usepackage[showframe]{geometry}	% for debug
\usepackage{url}
\Urlmuskip = 0mu plus 1mu\relax 	%for URLs too long
\def\UrlBreaks{\do\/\do-}
\urlstyle{rm}
%%\renewcommand{\UrlFont}{\ttfamily\small}



\newcommand{\QRCodeQuality}{H}	% QR Codes quality L, M, Q, H

% --- No new page for Chapters - Start
\usepackage{etoolbox}
\makeatletter
\patchcmd{\scr@startchapter}{\if@openright\cleardoublepage\else\clearpage\fi}{}{}{}
\makeatother
% --- No new page for Chapters - E


% --- Images Start ---
\newcommand{\InsImage}[2]{	%\InsImage{Size}{Image}
	{
		\noindent
		\includegraphics[width=#1\linewidth]{#2}
}
}

\newcommand{\InsImageLink}[3]{	%\InsImageLink{Size}{Image}{Link}
	\noindent
	\ifx&#3&%
		\includegraphics[width=#1\linewidth]{#2} 
	\else%
%		\href{\ifstrequal{#3}{*}{https://terpconnect.umd.edu/~toh/spectrum/#2}{#3}}{\includegraphics[width=#1\linewidth]{#2}}

		\ifstrequal{#3}{*}%
		{
			\href{https://terpconnect.umd.edu/~toh/spectrum/#2}
		}%
		{
			\href{#3}
		}%
		{
			\includegraphics[width=#1\linewidth]{#2} 
		}
	\fi
}


\newcommand{\InsImageInline}[3] { 	% \InsImageInline{0.5}{l}{image9.png}
\begin{wrapfigure}{#2}
{#1\textwidth}
%\begin{framed}
\centering
\vspace{-13pt}
	\noindent
\includegraphics[width=#1\textwidth]{#3}
%\vspace{-30pt}
%\caption{This is a comment!}
%\end{framed}
\end{wrapfigure}	
}


%	Parameter	Position
%	h		Place the float here, i.e., approximately at the same point it occurs in the source text (however, not exactly at the spot)
%	t		Position at the top of the page.
%	b		Position at the bottom of the page.
%	p		Put on a special page for floats only.
%	!		Override internal parameters LaTeX uses for determining "good" float positions.
%	H		Places the float at precisely the location in the LATEX code. Requires the float package, though may cause problems occasionally. This is somewhat equivalent to h!.
\newcommand{\InsImageEx}[5]{  %	\InsImageEx{Name}{Size}{Position}{LabeL}{Caption}
	\begin{figure}[#3]
		\centering
		\noindent
		\includegraphics[width=#2\linewidth]{#1}
		\ifx&#5&%
			 % #1 is empty
		\else
			\caption{#5}
		\fi
		\label{#4}
	\end{figure}
}

\usepackage{qrcode}

\usepackage{setspace}
\usepackage{graphbox}
\usepackage{wrapfig, framed, caption}		% Wrapped figures
\usepackage{etoolbox}

%-- DropCap START---
%\usepackage{ebgaramond}
\usepackage{lettrine}
\usepackage{xstring}

\makeatletter
\def\setIn#1{\@setIn#1\@nil}
\def\@setIn#1#2\@nil
{%
	% \lettrine[nindent=0em, lines=4]{#1}{\,#2}%
	\lettrine[nindent=0pt,findent=2pt]{#1}{\,#2}%
}%

%\makeatother
% \makeatletter
% \let\ltx@@chapter\@chapter
%% \def\@chapter[#1]#2 #3 {\ltx@@chapter[#1]{#2}\lettrine[nindent=0em]{\StrLeft{#3}{1}}{\@gobble#3}\ }
% \def\@chapter[#1]#2 #3 {\ltx@@chapter[#1]{#2}\setIn{#3}\ }
% \makeatother


%-- DropCap END ---

% see https://terpconnect.umd.edu/~toh/spectrum/SPECTRUM.zip
\graphicspath{	{F:/Dati/OmegaT/A Pragmatic Introduction to Signal Processing/Org/20210413/SPECTRUM}
				{F:/Dati/OmegaT/A Pragmatic Introduction to Signal Processing/Org/GifPngImages}
				{./IntroToSignalProcessing2021.docx.tmp/word/media}
				{./\jobname.docx.tmp/word/media}
		}

\DeclareUnicodeCharacter{2055}{*}			% Baldo
\DeclareUnicodeCharacter{0190}{$\mathcal{E}$}	% Baldo % LATIN CAPITAL LETTER OPEN E (u+2107 or rather u+0190)

\newcommand{\armenian}{\fontencoding{OT6}\fontfamily{cmr}\selectfont}
\DeclareTextFontCommand{\textarmenian}{\armenian}
%\newcommand{\DifferentialD}{\mathrm{d}} % or just d % declare "\DifferentialD" --> "d"	%Unicode U+2146
\newcommand{\DifferentialD}{d}			 % or just d % declare "\DifferentialD" --> "d"	%Unicode U+2146

%\renewcommand{\topfraction}{0.9} 
%\renewcommand{\blankpage}{\thispagestyle{empty} \quad \newpage} 
\def\blankpage{%
      \clearpage%
      \thispagestyle{empty}%
      \addtocounter{page}{-1}%
      \null%
      \clearpage}

\newcommand{\theauthor}{Tom O'Haver}
\ifLangEnglish
	\newcommand{\thetitle}{Pragmatic Introduction to Signal~Processing}
	\newcommand{\thesubtitle}{Applications in scientific measurement}
	\newcommand{\theversion}{March 2021 edition


	\title{\thetitle} \author{\theautho}
}
\else
	\newcommand{\thetitle}{Introduzione Pratica al Signal~Processing}
\newcommand{\thesubtitle}{Applicazioni nelle misure scientifiche}
\newcommand{\theversion}{Edizione di aprile 2021

\title{\thetitle} \author{\theautho}

	(Trad. Ita: 10 aprile 2021
 
%	file \jobname
	
%	{./\jobname.docx.tmp/word/media}
}
\fi


% ===Baldo END====================
\begin{document}

% \ifnum \LangUsed=0 {Inglese} \else {Italiano}\fi.\\ \LangUsed

%\ifLangEnglish
%	English
%\else
%	Italiano
%\fi


% ===Baldo START===================
\thispagestyle{empty}

\begin{flushright}

\vspace*{2.0in} 

\begin{center}

\begin{spacing}{3} {\huge \thetitle}\\ {\Large \thesubtitle} \end{spacing}

\vspace*{1.0in} 

\theversion

\vspace{2.0in} 

\ifLangEnglish
	\resizebox{1\linewidth}{!}{An illustrated handbook with free software and spreadsheet templates to download.}
\else
	\resizebox{1\linewidth}{!}{Un manuale illustrato con software gratuito e fogli di calcolo da scaricare.}
\fi


\vfill

\end{center}


\end{flushright}

%--verso------------------------------------------------------
\blankpage

%\blankpage

%--title page--------------------------------------------------
\pagebreak

\thispagestyle{empty} 

\begin{flushright}

\vspace*{1.0in} 

\begin{center}

\begin{spacing}{3} {\huge \thetitle}\\ {\Large \thesubtitle} \end{spacing} 

\vspace{0.25in} 

\theversion

\vspace{.5in} 

\ifLangEnglish
	A retirement project by
\else
Un progetto di
\fi

{\Large \theauthor\\ } 

\ifLangEnglish
	Professor Emeritus
	
	Department of Chemistry and Biochemistry
	
	The University of Maryland at College Park
\else
Professore Emerito

Dipartimento di Chimica e Biochimica

The University of Maryland at College Park
\fi



\vspace{0.5in} 

%{\Large Kindle Direct Publishing}

%{\small Naples, Florida}


orcid.org/0000-0001-9045-1603
\end{center}

\begin{center}

%\InsImage{0.3}{image2.png}

% \InsImage{0.3}{image2.png}		% email

\hypersetup{hidelinks}		% Hide boxes around links

\qrcode[level=\QRCodeQuality, height=4cm]{BEGIN:VCARD
VERSION:3.0
N:Tom O'Haver
ORG:University of Maryland
EMAIL;TYPE=INTERNET:toh@umd.edu
URL:https://terpconnect.umd.edu/~toh/
ADR:;;College Pary;Maryland;USA
END:VCARD}
\end{center}

\ifLangEnglish
\else
\begin{center}
Traduzione italiana: Baldassarre Cesarano
\end{center}
\fi



\vfill

\end{flushright} 

\pagebreak

% Links and QRCodes

\newcommand{\QRCodeResize}{1\linewidth - 3cm}

\begin{center}
\ifLangEnglish
	{\Large{\textbf{Online access to the latest versions}}}
\else
{\Large{\textbf{Accesso online alle ultime versioni}}}
\fi

\end{center}

\small{
\begin{center}
\ifLangEnglish
	\textbf{Book in PDF format:}\\
\else
	\textbf{Libro in inglese in formato PDF:}\\
\fi
	\begin{tabular}{cl}
%		\url{https://bit.ly/3mGm3fj}
		\resizebox{\QRCodeResize}{!}{\url{https://terpconnect.umd.edu/~toh/spectrum/IntroToSignalProcessing2021.pdf}}
			&	\qrcode[level=\QRCodeQuality, height=2cm]{https://terpconnect.umd.edu/~toh/spectrum/IntroToSignalProcessing2021.pdf}
	\end{tabular}
\end{center}

\ifLangEnglish
\else
\begin{center}
		\textbf{Libro in italiano in formato PDF:}\\
	\begin{tabular}{rl}
		\resizebox{\QRCodeResize}{!}{\url{https://github.com/BravoBaldo/IntroToSignalProcessing_it}}	&	\qrcode[level=\QRCodeQuality, height=2cm]{https://github.com/BravoBaldo/IntroToSignalProcessing_it}
	\end{tabular}
\end{center}
\fi


\begin{center}
\ifLangEnglish
	\textbf{Web address:}\\
\else
	\textbf{Indirizzo Web:}\\
\fi
	\begin{tabular}{rl}
		% http://bit.ly/1NLOlLR
		\resizebox{\QRCodeResize}{!}{\url{https://terpconnect.umd.edu/~toh/spectrum/index.html}}	&	\qrcode[level=\QRCodeQuality, height=2cm]{https://terpconnect.umd.edu/~toh/spectrum/index.html}
	\end{tabular}
\end{center}


\begin{center}
\ifLangEnglish
	\textbf{Interactive Matlab Tools:}\\
\else
	\textbf{Tool Interattivi Matlab:}\\
\fi
	\begin{tabular}{rl}
		% http://bit.ly/1r7oN7b
		\resizebox{\QRCodeResize}{!}{\url{terpconnect.umd.edu/~toh/spectrum/SignalProcessingTools.html}}	&	\qrcode[level=\QRCodeQuality, height=2cm]{https://terpconnect.umd.edu/~toh/spectrum/SignalProcessingTools.html}
	\end{tabular}
\end{center}

\begin{center}
\ifLangEnglish
	\textbf{Software download links:}\\
\else
	\textbf{Download del software:}\\
\fi
	\begin{tabular}{rl}
		% http://tinyurl.com/cey8rwh
		\resizebox{\QRCodeResize}{!}{\url{https://terpconnect.umd.edu/~toh/spectrum/functions.html}}			&	\qrcode[level=\QRCodeQuality, height=2cm]{https://terpconnect.umd.edu/~toh/spectrum/functions.html}
	\end{tabular}
\end{center}

\begin{center}
\ifLangEnglish
	\textbf{Animated examples:}\\
\else
	\textbf{Esempi animati:}\\
\fi
	\begin{tabular}{rl}
		\resizebox{1\linewidth - 3cm}{!}{\url{https://terpconnect.umd.edu/~toh/spectrum/ToolsZoo.html}}	&	\qrcode[level=\QRCodeQuality, height=2cm]{https://terpconnect.umd.edu/~toh/spectrum/ToolsZoo.html}
	\end{tabular}
\end{center}
    }

%\vfill

\begin{center}
\ifLangEnglish
\textit{If you are reading this book on an Internet-connected computer or tablet, you can tap, click or Ctrl-Click on any of the page numbers in the text to jump directly to that page. If you this read this book within Microsoft Word 365, the GIF animations will run automatically; in the PDF version, you must click on links to view them. You can also click on the https addresses, on the names of downloadable software or on graphics to view, enlarge, or download those items.}
\else
\textit{Leggendo questo libro su un computer o un tablet connessi a Internet, si pu\`{o} toccare [Tap], o un Ctrl-Click su qualsiasi numero di pagina nel testo per saltare direttamente a quella pagina. Se si legge questo libro con Microsoft Word 365, le animazioni GIF verranno eseguite automaticamente; nella versione PDF, \`{e} necessario fare clic sui collegamenti per visualizzarli. \`{E} inoltre possibile cliccare sugli indirizzi https, sui nomi dei software scaricabili o sulle figure per visualizzare, ingrandire o scaricare tali elementi.}
\fi
\end{center}

\begin{center}
\ifLangEnglish
	Have a question or suggestion? E-mail me at toh@umd.edu
\else
Ci sono domande o suggerimenti? Scrivere all'indirizzo toh@umd.edu
\fi
\end{center}

\begin{center}
\ifLangEnglish
	Join our Facebook group: ``Pragmatic Signal Processing''
\else
C'\`{e} un gruppo Facebook a cui unirsi: ``Pragmatic Signal Processing''
\fi
\end{center}

\pagebreak

\ifLangEnglish
\textbf{Acknowledgements}.  Thanks to M. Farooq Wahab for his many contributions and for many fruitful discussions, to Baldassarre Cesarano for his close reading and typographical correction of this text, to Dr. Raphael Atti\'{e} of NASA/Goddard Space Flight Center for corrections, to Diederick of The University of Hong Kong for code contributions, to Yuri Kalambet of Ampersand, Ltd., and to the many email correspondents who have made suggestions, asked questions, caught errors, and have shown me new types of data and new applications that have broadened the scope of this work.
\else
\textbf{Ringraziamenti}.  Grazie a M. Farooq Wahab per i suoi numerosi contributi e per molti fruttuosi discussioni, a Baldassarre Cesarano per la sua attenta lettura e correzione di questo testo, al Dr. Raphael Atti\'{e}, del NASA/Goddard Space Flight Center, per le correzioni, a Diederick dell'Universit\`{a} di Hong Kong per i contributi al codice, a Yuri Kalambet della Ampersand, Ltd., e ai molti corrispondenti via e-mail che hanno fornito suggerimenti, fatto domande, rilevato errori e mi hanno mostrato nuovi tipi di dati e nuove applicazioni che hanno ampliato la portata di questo lavoro.
\fi

% ===Baldo END====================


%\textbf{Tabella dei Contenuti}
%\textbf{Table of Contents}

\tableofcontents

\pagebreak

\chapter{ Introduction\label{ref-0001}}

The interfacing of measurement instrumentation to small computers for the purpose of online data acquisition has now become standard practice in the modern science laboratory. Computers are used for data acquisition, data, and storage, using digital computer-based numerical methods. Techniques are available that can transform signals into more useful forms, detect and measure peaks, reduce noise, improve the resolution of overlapping peaks, compensate for instrumental artifacts, test hypotheses, optimize measurement strategies, diagnose measurement difficulties, and decompose complex signals into their component parts. These techniques can often make difficult measurements easier by extracting more information from the available data. Many of these techniques employ laborious mathematical procedures that were not even practical before the advent of computerized instrumentation. It is important to appreciate the abilities, as well as the limitations, of these techniques. In recent decades, computer storage and digital processing has become far less costly and literally millions of times more capable, reducing the cost of raw data and making complex computer-based signal processing techniques both more practical and necessary. Approximations and shortcuts that were once necessitated by mathematical convenience are no longer needed (pages \pageref{ref-0183}, \pageref{ref-0258}, \pageref{ref-0335}). And it is not just the growth of computers: there are now new materials, new instruments, new fabrication techniques, new automation capabilities. We have lasers, fiber optics, superconductors, supermagnets, holograms, quantum technology, nanotechnology, and more. Sensors are now smaller, cheaper, and faster than ever before; we can measure over a wider range of speeds, temperatures, pressures, and locations. There are new kinds of data that we never had before. As Erik Brynjolfsson and Andrew McAfee wrote in \textit{The Second Machine Age} (W. W. Norton, 2014): "...many types of raw data are getting dramatically cheaper, and as data get cheaper, the bottleneck increasingly is the ability to interpret and use data".

This book covers only basic topics related to one-dimensional time-series signals, not two-dimensional data such as images. It uses a pragmatic approach and is limited to mathematics only up to the most elementary aspects of calculus, statistics, and matrix math. If you are math-phobic, just look through this book; you will not see any solid pages of math, but you will see \textit{lots} of figures. Data processing without math? Not really! Math is essential, just as it is for the technology of cell phones, GPS, digital photography, the Web, and computer games. However, you can get started using these tools without understanding all the underlying math and software details. Seeing it work makes it more likely that you will want to understand \textit{how} it works. Nevertheless, in the end, it is not enough just to know how to operate the software, any more than knowing how to use a word processor or a MIDI sequencer makes you a good author or musician. I get you \textit{started} with things that work; it is up to you to decide if a deep dive into advanced topics becomes necessary for your purposes. 

Why do I title this document "signal processing" rather than "data processing"? By "signal" I mean the continuous x,y numerical ``time-series'' data recorded by scientific instruments, where x may be time or another quantity like energy or wavelength, as in the various forms of spectroscopy. "Data" is a more general term that includes categorical data as well. In other words, I am oriented to data that you would plot in a spreadsheet using the scatter chart type rather than bar or pie charts. 

Some of the examples come from my own areas of research in analytical chemistry, but these techniques have been used in a wide range of application areas. Over 500 journal papers, theses, and patents have cited my software, covering fields from academia, industry, environmental, medical, engineering, earth science, space, military, financial, agriculture, communications, and even music and speech science (page \pageref{ref-0535}). Suggestions and experimental data sent by hundreds of readers from their own work have helped shape my writing and software development. Much effort has gone into making this document concise and understandable; it has been \href{https://terpconnect.umd.edu/~toh/spectrum/index.html\#comments}{highly praised by many readers}.

At the present time, this work does not cover image processing, pattern recognition, or factor analysis. For these advanced topics and for a more rigorous treatment of the underlying mathematics of the topics I do cover, refer to the extensive literature on signal processing and on statistics and chemometrics.

This book had its origin in one of the experiments in a course called "\href{https://terpconnect.umd.edu/~toh/Chem498C/}{Electronics and Computer Interfacing for Chemists}" that I developed and taught at the University of Maryland in the '80s and '90s. The first Web-based version went up in 1995. Subsequently, I have revised and greatly expanded it based on feedback from users. It is still a work in progress and, as such, will always benefit from feedback from readers and users.

This tutorial makes considerable use of \href{https://en.wikipedia.org/wiki/MATLAB}{Matlab}\index{Matlab}, a high-performance commercial and proprietary numerical computing environment and "fourth-generation" programming language that is widely used in research (references 14, 17, 19, 20 on page \pageref{ref-0533}), and Octave, a free Matlab alternative that runs almost all of the programs and examples in this tutorial (page \pageref{ref-0018}). There is a good reason why Matlab is so massively popular in science and engineering; it is powerful, fast, and relatively easy to learn. A very important aspect of Matlab is the concept of \textit{functions}, which are self-contained modules of code that accomplish a specific task. Functions usually "take in" data, process it, and "return" a result. (A trivial example is \textit{a}=sqrt(\textit{b}), which takes the value of \textit{b}, computes its square root, and assigns it to the variable \textit{a}). Once a function is written, it can be used repeatedly. Functions can be "called" from the inside of other functions. Matlab \textit{comes with built-in functions for doing data processing tasks} like matrix math, filtering, Fourier transforms, convolution and deconvolution, multilinear regression, and optimization. \textit{You can write your own custom functions} and download powerful toolboxes and free user-contributed functions. Matlab can interface to C, C++, Java, Fortran, and Python, and it is extensible to symbolic computing and model-based design for dynamic and embedded systems. There are many code examples in this text that you can Copy and Paste (or drag and drop) into the Matlab/Octave command line to run immediately or to modify as you like.

Some of the illustrations in this book were produced on my old 90s-era freeware signal-processing application for Macintosh OS8, called \href{https://terpconnect.umd.edu/~toh/spectrum/SPECTRUM.html}{S.P.E.C.T.R.U.M}. (\textbf{S}ignal \textbf{P}rocessing for \textbf{E}xperimental \textbf{C}hemistry \textbf{T}eaching and \textbf{R}esearch / \textbf{U}niversity of \textbf{M}aryland). See page \pageref{ref-0481}.

Most of the techniques covered in this work can also be performed in spreadsheets such as Microsoft \textit{Excel} or OpenOffice \textit{Calc} (11, 22, 23). \textit{Octave} and the OpenOffice \textit{Calc} (or LibreOffice \textit{Calc}) spreadsheet program can be downloaded without cost from their respective web sites (\url{https://sourceforge.net/projects/octave/} and \url{https://www.libreoffice.org/} ).

You can download all of the Matlab/Octave scripts and functions, the SPECTRUM program, and the spreadsheets used here from \url{http://tinyurl.com/cey8rwh} at no cost; they have received \href{https://terpconnect.umd.edu/~toh/spectrum/SignalProcessingTools.html\#comments}{extraordinarily positive feedback from users}. If you try to run one of my scripts or functions and it gives you a "missing function" error, look for the missing item from \url{http://tinyurl.com/cey8rwh}, download it into your Matlab/Octave path, and try again.

If you do not know Matlab, read page \pageref{ref-0012} and following for a quick start-up. It is not general-purpose language, like C++ or Python; rather, it is specifically suited to numerical methods, matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages - essentially the needs of \href{https://www.amazon.com/Scientific-Computing-Scientists-Engineers-Textbook-ebook/dp/B0138NP7GM}{numerical computing by scientists and engineers}. Matlab and Octave are more \href{https://www.computerhope.com/jargon/l/looslang.htm}{loosely typed} and are less well-structured in a formal sense than other languages, and it tends to be more favored by scientists and engineers and less well-liked by computer scientists and professional programmers. To get a general-purpose language like Python up to where Matlab \textit{starts} requires the installation of many add-on ``\href{https://packaging.python.org/tutorials/installing-packages/}{packages}''.

There are several versions of Matlab, including stand-alone low-cost student and home versions, fully functional versions that run \href{https://www.mathworks.com/products/matlab-online.html}{in a web browser} (see graphic below), and apps that run \href{https://itunes.apple.com/us/app/matlab-mobile/id370976661?mt=8}{on iPads and iPhones}. See \href{https://www.mathworks.com/pricing-licensing.html\%20}{https://www.mathworks.com/pricing-licensing.html} for prices and restrictions in their use. 

\InsImage{1.0}{MatlabOnlineChrome.png}\textit{Figure 1. Matlab Online running my interactive peak fitter (ipf.m) in Chrome on a Windows PC\label{ref-0002}}



There are alternatives to Matlab,\index{alternatives to MATLAB} in particular, \textit{Octave}, which is essentially a Matlab clone, but there is also Scilab, FreeMat, Julia, and Sage, which are mostly or somewhat compatible with the MATLAB language. For a discussion of other possibilities, see \url{http://www.dspguru.com/dsp/links/matlab-clones}. 

If you are reading this book \textit{online}, on an Internet-connected computer, you can \textbf{Ctrl-Click} on any of the http Web addresses or on the names of downloadable software or animations to view or download that item. For a complete list of all my software, see page \pageref{ref-0497} or \url{http://tinyurl.com/cey8rwh}.

\chapter{Signal arithmetic\index{Signal arithmetic}\label{ref-0003}\label{ref-0004}}

The most basic signal processing operations are those that involve simple signal arithmetic\index{signal arithmetic}: point-by-point addition, subtraction, multiplication, or division of two signals or of one signal and a constant. Despite their mathematical simplicity, these operations can be very useful. For example, in the left part of the figure below (Window 1) the top curve is the optical absorption spectrum\index{absorption spectrum} of an extract of a sample of oil shale, a kind of rock that is a source of petroleum.


\begin{center}
\InsImage{1.0}{Figure1.GIF.png}\label{ref-0005}
\end{center}



\begin{center}
\textit{A simple point-by-point subtraction of two signals allows the background (bottom curve on the left) to be subtracted from a complex sample (top curve on the left), resulting in a clearer picture of what is really in the sample (right). (X-axis = wavelength in nm; Y-axis = absorbance).}
\end{center}


This optical spectrum exhibits two absorption bands, at 515 nm and 550 nm. These peaks are due to a class of molecular fossils of chlorophyll called \textit{porphyrins}, which are used as ``geomarkers'' in oil exploration. These bands are superimposed on a background absorption caused by the extracting solvents and by non-porphyrin compounds in the shale. The bottom curve is the spectrum of an extract of a non-porphyrin-bearing shale, showing only the background absorption. To obtain the spectrum of the shale extract without the background, the background (bottom curve) is simply subtracted from the sample spectrum (top curve). The difference is shown in the right in Window 2 (note the change in the Y-axis scale). In this case, the removal of the background is not perfect, because the background spectrum is measured on a separate shale sample. However, it works well enough that you can see the two bands more clearly and it is easier to measure precisely their absorbances and wavelengths. (Thanks to Prof. David Freeman of the Univ. of Maryland for the spectra of oil shale extracts).\label{ref-0006}

In this example and the one below, I am assuming that the two signals in Window 1 have the \textit{same x-axis values} - in other words, that both spectra have been digitized at the same set of wavelengths. Subtracting or dividing two spectra would not be valid if two spectra were digitized over different wavelength ranges or with different intervals between adjacent points. The x-axis values must match up point for point. In practice, this is very often the case with data sets acquired within one experiment on one instrument, but you must be careful if you change the instrument’s settings or if you combine data from two experiments or two instruments. It is possible to use the mathematical technique of \href{http://en.wikipedia.org/wiki/Interpolation}{\textit{interpolation}} to change the sampling rate (x-axis interval) or to equalize unequally spaced x-axis intervals of signals; the results are usually only approximate but often close enough in practice. Excel can perform the calculations using the \href{https://exceloffthegrid.com/interpolate-values-using-the-forecast-function/}{forecast} function. Matlab and Octave have built-in functions for interpolation, including \href{https://www.mathworks.com/help/matlab/ref/interp1.html}{interp1.m}, see \href{https://terpconnect.umd.edu/~toh/spectrum/CompareInterp1andSpline.m}{example1} (\href{https://terpconnect.umd.edu/~toh/spectrum/CompareInterp1andSpline.png}{graphic}) and \href{https://terpconnect.umd.edu/~toh/spectrum/CompareInterpolationMethods2.m}{example2} (\href{https://terpconnect.umd.edu/~toh/spectrum/CompareInterpolationMethods2.png}{graphic}). 

Sometimes one needs to know whether two signals have the same shape, for example in comparing the signal of an unknown to a stored reference signal. Most likely the amplitudes of the two signals, will be different. Therefore, a direct overlay or subtraction of the two signals will not be useful. One possibility is to compute the point-by-point ratio of the two signals; if they have the same shape, the ratio will be a constant. For example, examine this figure:

\InsImage{1.0}{Figure2.GIF.png}
\begin{center}
\textit{Do the two signals on the left have the same shape? They certainly do not look the same, but that may simply be because one is much weaker than the other one. The ratio\label{ref-0007} of the two signals, shown in the right part (Window 2), is relatively constant from 300 to 440 nm, with a value of 10 +/- 0.2. This means that the shape of these two signals is very nearly identical over this x-axis range.}
\end{center}


The left part (Window 1) shows two superimposed signals, one of which is much weaker than the other. But do they have the same shape? It is hard to tell. The ratio of the two signals, shown in the right part (Window 2), is relatively constant from x=300 to 440, with a value of 10 +/- 0.2. This means that the shape of these two signals is the same, within about +/-2 \%, over this x-axis range, and that top curve is very about 10 times more intense than the bottom one. Above x=440 the ratio is not even approximately constant; this is caused by \textit{noise}, which is the subject of the \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html}{next section} (page \pageref{ref-0020}).

A \textbf{division by zero error} will be caused by \textit{even a single zero} in the denominator vector, but that can usually be avoided by applying a small amount of \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothing }(page \pageref{ref-0045}) of the denominator, by adding a positive number to the denominator, or by using the Matlab/Octave function \href{https://terpconnect.umd.edu/~toh/spectrum/rmz.m}{rmz.m} (\textbf{r}e\textbf{m}ove \textbf{z}eros) which replaces zeros with the nearest non-zero numbers. The related function \href{https://terpconnect.umd.edu/~toh/spectrum/rmnan.m}{rmnan.m} removes NaNs (``Not a Number'') and Infs (``Infinite'') from vectors, replacing with neighboring real finite numbers. 

\textbf{On-line} \textbf{calculations and plotting.} \href{http://www.wolframalpha.com/}{\textit{Wolfram Alpha}\index{\uline{\textit{Wolfram Alpha}}}} is a Web site and a \href{http://products.wolframalpha.com/mobile/}{smartphone app} that is an extremely useful computational tool and information source, including capabilities for symbolic mathematics, \href{https://www.wolframalpha.com/examples/mathematics/plotting-and-graphics/}{plotting}, vector and \href{http://www.wolframalpha.com/input/?i=matrix}{matrix manipulations}, \href{http://www.wolframalpha.com/input/?i=statistics}{statistics and data analysis}, and many \href{http://www.wolframalpha.com/examples/}{other topics}. \href{http://statpages.org/index.html}{\textit{Statpages.org}} can perform a huge range of statistical calculations and tests. There are several Web sites that specialize in plotting data, including \href{https://plotly.com/matlab/}{Plotly} and \href{http://itools.subhashbose.com/grapher/}{Grapher}. All of these require a reliable Internet connection, and they can be useful when you are working on a mobile device or computer that does not have the required software installed. In the PDF version of this book, you can \textbf{Ctrl-Click} on these links to open them in your browser.

\section{Signal arithmetic in \href{https://terpconnect.umd.edu/~toh/spectrum/SPECTRUM.html}{SPECTRUM\index{SPECTRUM}} \label{ref-0008}}

\InsImageInline{0.5}{l}{image9.png}SPECTRUM is a simple 90s era freeware signal-processing application for Macintosh OS8 that includes the following signal arithmetic operations: addition and multiplication with constant; addition, subtraction, multiplication, and division of two signals, normalization, and a large number of other basic operations (common and natural log and antilog, reciprocal, square root, absolute value\index{absolute value}, standard deviation, etc.) in the \href{https://terpconnect.umd.edu/~toh/spectrum/~toh/spectrum/SPECTRUMReferenceManual.pdf\#page=9}{\textbf{Math}}\href{http://www.wam.umd.edu/~toh/spectrum/SPECTRUMReferenceManual.pdf\#page=9}{ menu}. It also performs many other signal processing operations described in this paper, including smoothing, differentiation, peak sharpening, interpolation, fused peak area measurement, Fourier transformation, Fourier convolution and deconvolution, and polynomial curve fitting. It runs only on Macintosh OS 8.1 and earlier but can be made to work on Windows PCs and various specific \href{http://en.wikipedia.org/wiki/Linux}{Linux} distributions using the \href{http://en.wikipedia.org/wiki/Executor\_\%28software\%29}{Executor emulator.} No native PC version is available or planned. 

\section{Signal arithmetic\index{Spreadsheets!Signal arithmetic} in Spreadsheets\label{ref-0009}\label{ref-0010}\label{ref-0011}}

\InsImageInline{0.5}{l}{PeakDetectionSpreadsheet.png}
Popular \textbf{spreadsheets,} such as \href{http://www.microsoftstore.com/store/msstore/pd/Excel-Home-and-Student-2010/productID.216446900/vip.true}{\textit{Excel}\index{\textit{Excel}}}\href{http://www.microsoftstore.com/store/msstore/pd/Excel-Home-and-Student-2010/productID.216446900/vip.true}{ }or \href{http://en.wikipedia.org/wiki/OpenOffice.org\_Calc}{\textit{Open Office Calc}}, are aimed mainly at business and financial applications, but still have built-in functions for many common math operations, named variables, x,y plotting, text formatting, matrix math, etc. Cells can contain numerical values, text, mathematical expression, or references to other cells. You can represent a spectrum as a row or column of cells. You can represent a set of spectra as a rectangular block of cells. You can assign your own names to individual cells or to ranges of cells, and then refer to them in mathematical expression by name. You can copy mathematical expressions across a range of cells, with the cell references changing or not as desired. You can make plots of various types (including the all-important \textit{x-y} or \textit{scatter} graph) by menu selection. For a nice video demonstration, see this YouTube video: \url{http://www.youtube.com/watch?v=nTlkkbQWpVk}. Both \textit{Excel} and \textit{Calc} offer a form design capability with a full set of user interface objects such as buttons, menus, sliders, and text boxes; you can use these to create attractive graphical user interfaces for end-user applications, such as ones I have created for teaching analytical chemistry courses on \url{http://terpconnect.umd.edu/~toh/models/}. The latest versions of both Excel (\textit{Excel} 2013) and OpenOffice \textit{Calc} (3.4.1) can open and save either spreadsheet file formats (.xls and .ods, respectively). Simple spreadsheets in either format are compatible with the other program. However, there are small differences in the way that certain operations are interpreted, and for that reason I supply most of my spreadsheets in .xls (for \textit{Excel}) \textit{and} in .ods (for \textit{Calc}) formats. See "\href{http://office.microsoft.com/en-us/excel-help/differences-between-the-opendocument-spreadsheet-ods-format-and-the-excel-xlsx-format-HA010355787.aspx}{Differences between the Open-Document Spreadsheet (.ods) format and the Excel (.xlsx) format}". Basically, \textit{Calc} can do most everything \textit{Excel} can do, but \textit{Calc} is free to download and is more Windows-standard in terms of look-and-feel. Excel is more "Microsoft-y" and for some operations is faster than \textit{Calc}. If you have access to \textit{Excel}, I recommend using that.

If you are working on a tablet or smartphone, you could use the \href{http://www.macworld.com/article/2139403/excel-for-ipad-review-the-best-spreadsheet-app-for-the-ipad.html}{Excel mobile app}, \href{http://www.apple.com/ios/numbers/?cid=wwa-us-kwg-features-com}{Numbers }for iPad, or several other \href{http://www.searchenginejournal.com/5-awesome-spreadsheet-apps-for-the-iphone/}{mobile spreadsheet}s. These apps can do basic tasks but do not have the fancier capabilities of the desktop computer versions. By saving their data in the "cloud" (e.g., iCloud or SkyDrive), these apps automatically sync changes in both directions between mobile devices and desktop computers, making them useful for field data entry.

\section{Signal arithmetic in Matlab\index{Matlab}\label{ref-0012}\label{ref-0013}\label{ref-0014}}

\InsImageInline{0.5}{l}{Matlab.gif.png}
~\href{https://www.mathworks.com/products.html?s\_tid=gn\_ps}{\textit{Matlab}} is a "multi-paradigm numerical computing environment and fourth-generation programming language" (\href{https://en.wikipedia.org/wiki/MATLAB}{Wikipedia}). In Matlab (and in its GNU clone \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{\textit{Octave}}), a single variable can represent either a single "scalar" value, a \textit{vector} of values (such as a spectrum or a chromatogram), a \textit{matrix} (a rectangular array of values, such as a set of spectra), or a set of \textit{multiple} matrices. \textit{All the standard math operations and functions adjust to match}. This greatly facilitates mathematical operations on signal waveforms. For example, if you have signal amplitudes in the variable \texttt{\textbf{y}}, you can plot it just by typing "\texttt{\textbf{plot(y)}}". And if you also have a vector \texttt{t} of the same length containing the times at which each value of \texttt{\textbf{y}} was obtained, you can plot \texttt{\textbf{y}} vs \texttt{\textbf{t}} by typing "\texttt{\textbf{plot(t,y)}}". Two signals \textbf{y} and \textbf{z} can be plotted on the same time axis for comparison by typing "\texttt{\textbf{plot(t,y,t,z)}}". (Matlab automatically assigns different colors to each line, but you can control the color and line style yourself by adding additional symbols; for example, "\texttt{\textbf{plot(x,y,'r.',x,z,'b-')}}" will plot \textbf{y} vs x with red dots and \textbf{z} vs x with a blue line. You can divide up one figure window into multiple smaller plots by placing \texttt{\textbf{subplot(m,n,p)}} before the plot command to plot in the p\raisebox{4pt}{th} section of an m-by-n grid of plots. (If you are reading this online, you can click \href{https://terpconnect.umd.edu/~toh/spectrum/QuadFitToGaussian.png}{here} \href{https://terpconnect.umd.edu/~toh/spectrum/QuadFitToGaussian.png}{for an example} of a 2x2 subplot. You can also select, copy and paste, or select, drag and drop, any of the single-line or multi-line code examples into the Matlab or Octave editor or directly into the command line and press \textbf{Enter} to execute it immediately). In Matlab, type "help plot" for more plotting options. 

For \textbf{publication-quality} graphs, click on a Figure window, then click \textbf{File {\textgreater} Export setup}, choose the size, resolution, color, fonts, etc, then click \textbf{Export} and select the file format (e.g., TIF, eps, etc.). You can also use \href{http://masumhabib.com/blog/plotpub-publication-quality-graph-v2-0-released/}{PlotPub }, a downloadable library that is free, \href{https://terpconnect.umd.edu/~toh/spectrum/InbuiltPlotVersusPlotPub.txt}{easy to use}, allows great flexibility in choosing graph details, and creates \href{https://terpconnect.umd.edu/~toh/spectrum/LastPeakTwoGaussiansPlotpub.png}{great-looking graphs} within Matlab that can be exported in EPS, PDF, PNG and TIFF with adjustable resolution. Here is an example (\href{https://terpconnect.umd.edu/~toh/spectrum/GaussVsExpGauss.m}{script}, \href{https://terpconnect.umd.edu/~toh/spectrum/PLotPub.png}{graphic}).

The function \texttt{\textbf{max(y)}} returns the maximum value of \texttt{\textbf{y}} and \texttt{\textbf{min(y)}} returns the minimum. Individual elements in a vector are referred to by \textit{index number}; for example, \texttt{\textbf{t(10)}} is the 10th element in vector \textbf{t}, and \texttt{\textbf{t(10:20)}} is the vector of values of \textbf{t} from the 10th to the 20th entries. You can find the index number of the entry closest to a given value in a vector by using my \href{https://terpconnect.umd.edu/~toh/spectrum/val2ind.m}{val2ind.m} function. For example, \texttt{t(val2ind(y,max(y)))} returns the time of the maximum y, and \texttt{t(val2ind(t,550): val2ind(t,560))} is the vector of values of \textbf{t} between 550 and 560 (assuming \textbf{t} contains values within that range). The \textit{units} of the time data in the \textbf{t} vector could be anything - microseconds, milliseconds, hours, any time units. 

A Matlab variable can also be a \textit{matrix}, a set of vectors of the same length combined into a rectangular array. For example, intensity readings of 10 different optical spectra, each taken at the same set of 100 wavelengths, could be combined into the 10x100 matrix S. \texttt{S(3,:)} would be the third of those spectra and \texttt{S(5,40)} would be the intensity at the 40th wavelength of the 5th spectrum. The Matlab scripts \href{https://terpconnect.umd.edu/~toh/spectrum/plotting.m}{plotting.m} (left) and \href{https://terpconnect.umd.edu/~toh/spectrum/plotting2.m}{plotting2.m} (right) show how to plot multiple signals using matrices and subplots. 

\InsImage{0.5}{image10.png}  \InsImage{0.5}{image11.png}\label{ref-0015}

The subtraction of two signals \texttt{\textbf{a}} and \texttt{\textbf{b}}, as on page \pageref{ref-0006}, can be performed simply by writing \texttt{\textbf{a-b}}. To plot the difference, you would write "\texttt{plot(a-b)}". Likewise, to plot the ratio of two signals, as on page \pageref{ref-0007}, you would write "\texttt{plot(a./b)}". So, "./" means divide point-by-point and ".*" means multiply point-by-point. The * by itself means matrix multiplication, which you can use to perform repeated multiplications without using loops. For example, if x is a vector.

  \texttt{A=[1:100]'*x}\texttt{;} 

creates a matrix \textbf{A} in which each column is x multiplied by the numbers 1, 2,...100. It is equivalent to, but more compact and faster than, writing a "for'' loop like this: 

\texttt{for n=1:100;}

  \texttt{A(:,n)=n.*x;}

\texttt{end}

See \href{https://terpconnect.umd.edu/~toh/spectrum/TimeTrial.txt}{TimeTrial.txt} for details. It will help if you pre-allocate memory space for the \textbf{A} matrix by adding the statement \texttt{A=zeros(100,100)} before the loop. Even then, the matrix notation is faster than the loop. 

In Matlab/Octave, "/" is not the same as "\textbackslash ". Typing "b\textbackslash a" will compute the "\href{http://www.mathworks.com/help/matlab/ref/arithmeticoperators.html}{matrix left divide}", in effect the weighted average ratio of the amplitudes of the two vectors (a type of least-squares best-fit solution). The point here is that \textit{Matlab does not require you to deal with vectors and matrices as collections of numbers}; it knows when you are dealing with matrices, or when the result of a calculation will be a matrix, and it adjusts calculations accordingly. See \url{https://www.mathworks.com/help/matlab/matlab_prog/array-vs-matrix-operations.html}. 

Probably the most common errors you'll make in Matlab/Octave are punctuation errors, such as mixing up periods, commas, colons, and semicolons, or parentheses, square brackets, and curly brackets; type "\href{https://terpconnect.umd.edu/~toh/spectrum/help\_punct.txt}{help punct}" at the Matlab prompt and \textit{read the help file} until you fall asleep. \textit{Little things can mean a lot} in Matlab. Another common error is getting the rows and columns of vectors and matrices mixed up. (Full disclosure: I \textit{still} make all these kinds of mistakes all the time). Click for \href{https://terpconnect.umd.edu/~toh/spectrum/RowsAndColumns.txt}{text file that gives examples} of common vector and matrix operations and errors in Matlab and Octave. If you are new to this, I recommend that you read this file and play around with the examples there. Writing Matlab is a trial-and-error process, with the emphasis on \textit{error}. Start simple, get it to work, then add to it in steps.

\InsImage{1.0}{image12.png} \textit{There are many code examples in this text that you can Copy and Paste and modify into the Matlab/ Octave command line}, which is a great way to learn. In the PDF version of this book, you can select, copy and paste, or select, drag and drop, any of the single-line or multi-line code examples into the Matlab or Octave editor or directly into the command line and press \textbf{Enter} to execute it immediately). This is especially convenient if you run Matlab and read my web site or book on the same computer; position the windows so that Matlab shares the screen with this website (e.g. Matlab on the left and web browser on the right \href{https://terpconnect.umd.edu/~toh/spectrum/CopyPasteintoMatlab.png}{as shown above}). Or, even better, you could have \textit{two} monitors hooked to your desktop computer configured to expand the desktop horizontally. 

Hint: \textit{If you try to run one of my scripts or functions and it gives you a "missing function" error, look for the missing item from} \textit{\url{http://tinyurl.com/cey8rwh}}\textit{, download it into your path, and try again.}

One thing that you will notice about Matlab is that the \textit{very first time} you execute a script or function, and \textit{only} the first time, there is a small delay before execution, while Matlab compiles the code into binary machine language. However, that only happens the \textit{first} time; after that, the execution starts instantly. (For the fastest execution, the separately available ``\href{http://www.mathworks.com/products/matlab-compiler-sdk/index.html}{Matlab Compiler}'' lets you share programs as \textit{stand-alone} applications, separate from the Matlab environment. ``\href{http://www.mathworks.com/products/matlab-compiler-sdk/index.html}{Matlab Compiler SDK}'' lets you build C/C++ shared libraries, Microsoft .NET assemblies, Java classes, and Python packages from Matlab programs). You can even do some \textit{real-time} plotting in Matlab/Octave; see page \pageref{ref-0418}.

\textbf{Putting data into Matlab/Octave}\index{\textbf{Getting data into Matlab/Octave}}\textbf{.} You can \href{http://www.mathworks.com/help/matlab/import\_export/recommended-methods-for-importing-data.html}{import your own data into Matlab} or Octave by using the ``load'' command. Data can be imported from plain text files (.txt), CSV files (comma separated values), from several image and sound formats, or from spreadsheets. Matlab also has a convenient \textit{Import Wizard} (click \textbf{File {\textgreater} Import Data}) that gives you a preview into the data file, parses the data file looking for columns and rows of numeric data and their labels, and gives you a chance to select and re-label the desired variables and to choose to import them as vectors, matrices, or tables. Very useful.

\InsImage{1.0}{ImportDataWizard.png}It is even possible to import approximate data from graphical line plots or \textit{printed} graphs by using the built-in "ginput" function that obtains numerical data from the coordinates of mouse clicks, or by using applications such as ``\href{https://datathief.org/}{\textit{Data Thief}}'' or \href{http://www.mathworks.com/matlabcentral/fileexchange/11077-figure-digitizer}{\textit{Figure Digitizer}} in the Matlab File Exchange.

Matlab R2013a or newer can even \href{http://blogs.mathworks.com/pick/2013/08/09/reading-from-sensors-on-your-mobile-phone/}{read the sensors on your iPhone or Android phone} via Wi-Fi. To read the outputs of older analog instruments, you need an \href{https://www.google.com/\#q=analog-to-digital+converter,}{analog-to-digital converter}, an \href{https://www.google.com/\#q=Arduino+microcontroller+board}{Arduino microcontroller board}, or a \href{https://www.google.com/\#q=USB+voltmeter}{USB voltmeter}. 

\textbf{Matlab Versions}\index{\textbf{Matlab Versions}}. The standard \textit{commercial} version of Matlab is expensive (over \$2000) but there are \textit{student} and \textit{home} versions that cost much less (as little as \$49 for a basic student version) and that have all the capabilities to perform any of the methods detailed in this book \textit{at comparable execution speeds}. \InsImageInline{0.8}{l}{image14.png}There is also \href{https://www.mathworks.com/products/matlab-online.html}{\textit{Matlab Online}}, which \textit{runs in a common web browser} (see the graphic on \pageref{ref-0002}). You do not even need a regular computer: there is a free \href{https://itunes.apple.com/us/app/matlab-mobile/id370976661?mt=8}{Matlab Mobile} app that runs a Matlab interface on Internet-connected iPhones and iPads (illustrated on the right). This requires only a basic student license and uses all the standard functions, plus any of my functions or scripts, or any of your own that you have that you have previously up-loaded to your account on the \href{https://www.mathworks.com/cloud.html}{Matlab cloud}. All these versions have computational speeds that are mostly within a factor of 2 of each other, as shown by \href{https://terpconnect.umd.edu/~toh/spectrum/TimeTrial.txt}{TimeTrial.txt}. See \url{https://www.mathworks.com/pricing-licensing.html}\uline{\textcolor{color-2}{.}}

\section{\href{http://en.wikipedia.org/wiki/GNU\_Octave}{\label{ref-0016}\label{ref-0017}GNU Octave} \label{ref-0018}}

\begin{center}\InsImage{1.0}{Octave.gif.png}\end{center}

Octave\index{Octave} is a \href{https://cvw.cac.cornell.edu/matlab/octave}{free alternative to Matlab} that is "\href{http://en.wikipedia.org/wiki/GNU\_Octave\#MATLAB\_compatibility}{mostly compatible}". \href{http://www.dspguru.com/dsp/links/matlab-clones}{DspGURU }says that Octave is ``...a mature high-quality Matlab clone. It has the highest degree of Matlab compatibility of all the clones.'' Everything I said above about Matlab also works in Octave. In fact, \textit{the most recent versions of almost all my Matlab functions, scripts, demos, and examples in this document will work in the latest version of Octave without change.} The exceptions are my keystroke-operated interactive functions \textit{iPeak} (page \pageref{ref-0320}), \textit{iSignal} (page \pageref{ref-0433}), and \textit{ipf.m} (page \pageref{ref-0461}); they work only in Matlab. If you plan to use Octave, make sure you get the current versions; many of them were updated for Octave compatibility in 2015 and this is an ongoing project. There is an \href{http://wiki.octave.org/FAQ\#Porting\_programs\_from\_Matlab\_to\_Octave}{FAQ} that may help in \href{http://wiki.octave.org/FAQ\#Porting\_programs\_from\_Matlab\_to\_Octave}{porting Matlab programs to Octave}. See ``\href{https://www.google.com/search?q=Key+Differences+Between+Octave+\%26+Matlab&oq=Key+Differences+Between+Octave+\%26+Matlab&aqs=chrome..69i57j33i10i22i29i30.1334j0j4&sourceid=chrome&ie=UTF-8}{Differences Between Octave \& Matlab}''. There are Windows, Mac, and Unix versions of Octave. The Windows version can be downloaded from \href{http://sourceforge.net/projects/octave/files/Octave\%20Windows\%20binaries/Octave\%203.6.1\%20for\%20Windows\%20MinGW\%20installer/}{Octave Forge}. There is lots of help online: Google "\href{https://www.google.com/search?q=GNU+Octave&aq=f&oq=GNU+Octave&sugexp=chrome,mod=0&sourceid=chrome&ie=UTF-8}{GNU Octave}" or see the \href{http://www.youtube.com/results?search\_query=GNU+octave&oq=GNU+octave&gs\_l=youtube.3..0l2j0i5.18053.19469.0.20453.4.4.0.0.0.0.52.167.4.4.0...0.0...1ac.1.UrtIQXZNZoQ}{YouTube videos} for help. For signal processing applications specifically, Google "\href{https://www.google.com/search?q=signal+processing+octave&ie=utf-8&oe=utf-8&aq=t&rls=org.mozilla:en-US:unofficial&client=seamonkey-a}{signal processing octave}". 

\href{https://www.gnu.org/software/octave/news/release/2019/03/01/octave-5.1-released.html}{Octave Version 6.2.0} now available for \href{http://www.gnu.org/software/octave/download.html}{download}.\href{http://wiki.octave.org/Octave\_for\_Windows}{ }Documentation is online; see \url{https://www.octave.org}. Almost all my scripts and functions run on Octave. However, it is still computationally about 5 times slower on average than the latest Matlab version, depending on the task (specific comparisons for several different signal processing tasks are in \href{https://terpconnect.umd.edu/~toh/spectrum/TimeTrial.txt}{TimeTrial.txt}). Bottom line: Matlab is better, but if you cannot afford Matlab, Octave provides most of the functionality for 0\% of the cost. Note: the older \href{http://wiki.octave.org/Rasperry\_Pi}{Octave 3.6 can even run on a Raspberry Pi} \textcolor{color-3}{(a low-cost single-board computer).}

\section{Spreadsheet or Matlab/Octave\index{Spreadsheet or Matlab/Octave}? \label{ref-0019}}

For signal processing, Matlab/Octave is faster and more powerful than using a spreadsheet, but it is safe to say that spreadsheets are more commonly installed on science workers' computers than Matlab or Octave. For one thing, spreadsheets are cheaper, are easier to get started with, and they offer flexible presentation and user interface design. Spreadsheets are better for manual data entry; you can easily deploy them on portable devices such as smartphones and tablets (e.g. using \textit{Google Sheets}, \href{https://www.apple.com/mac/numbers/}{i}\href{https://www.apple.com/mac/numbers/}{\textit{Cloud Numbers}} or the \href{https://itunes.apple.com/us/app/microsoft-excel-for-ipad/id586683407?mt=8}{\textit{Excel} app}). \textit{Spreadsheets are concrete and more low-level, showing every single value explicitly in a cell.} In contrast, \textit{Matlab}/\textit{Octave} is more high level and abstract, because a single variable can be a number, a vector, or a matrix, and each punctuation or function can do so much. This is very powerful, but it is difficult to master \textit{at first}. One advantage of Matlab and Octave is that their functions and script files (``m-files'') are just plain text files with an ``.m'' extension, so \textit{those files can be opened and inspected using any text editor, even on devices that do not have Matlab or Octave installed}, which facilitates the translation of its scripts and functions into other languages. In addition, user-defined functions can call other built-in or user-defined functions, which in turn can call other functions, and so on, allowing you to \textit{build up very complex high-level functions in layers}. Fortunately, Matlab can easily analyze Excel ``.xls'' and ``.xlsx'' files and import the rows and columns into vector/matrix variables. 

Using the analogy of electronic circuits, spreadsheets are like \textit{discrete component} electronics, where every resistor, capacitor, inductor, and transistor is a discrete, macroscopic entity that you can see and manipulate directly. A function-based programming language like Matlab/Octave is more like \textit{micro-electronics}, where the functions (the "m-files" that begin with "function...") are the "chips", which condense complex operations into one package \textit{with documented input and output pins} (the function's input and output \textit{arguments}) that you can connect to other functions, but which \textit{hide the internal details} (unless you care to look at the code, which you always can do). A good example is the "555 timer", an 8-pin timer, pulse generator and oscillator chip introduced in 1972, which is still in use today and has become the \href{https://en.wikipedia.org/wiki/555\_timer\_IC}{most popular integrated circuit ever manufactured}. Almost all electronics is now done with chips, because \textit{it is easier to understand the relatively small number of inputs and outputs} of a chip than to deal with the greater number of internal components. Much of the Matlab/Octave is written in Matlab/Octave itself, using more basic functions to build more complex ones. You can write new functions of your own that essentially extend the language in whatever direction you need (page \pageref{ref-0042}).

The bottom line is that spreadsheets are easier at first, but, in my experience, eventually the Matlab/ Octave approach is faster, can handle much larger data sets, and is ultimately more productive. This is demonstrated by the comparison of both platforms for \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html\#spreadsheets}{ multicomponent spectroscop}y, covered on page \pageref{ref-0243} (\href{https://terpconnect.umd.edu/~toh/spectrum/RegressionDemo.xls}{RegressionDemo.}\uline{xls} versus the Matlab/Octave \href{https://terpconnect.umd.edu/~toh/spectrum/CLS.m}{CLS.m}). Even more dramatic are the different approaches to finding and measuring peaks in signals, which is covered in the section beginning on page \pageref{ref-0294} (i.e. a \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionAndMeasurement.xls}{250Kbyte spreadsheet} versus a \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaks.m}{7Kbyte Matlab script} that does the same thing but is \textit{50 times faster}). If you have large quantities of data and you need to run it through a multi-step customized process automatically, hands-off, and as quickly as possible, then Matlab is a great way to go\textit{.} It is much easier to write a script in Matlab that will \textit{automate the hands-off processing of volumes of data} stored in separate data files on your computer, as shown by the example on page \pageref{ref-0413}. 

Both spreadsheets and Matlab/Octave programs have a huge advantage over commercial end-user programs and compiled freeware programs such as SPECTRUM (page \pageref{ref-0481}); you can \textit{inspect and modify them} to customize the routines for specific needs. Simple changes are easy to make with little or no knowledge of programming. For example, you could easily change the labels, titles, colors, or line style of the graphs - in Matlab or Octave programs, search for "title(", "label(" or "plot(". My code contains \textit{comments that indicate places where you can make specific changes}: just use \textbf{Find...} to search for the word ``change''. \textit{I invite you to modify my scripts and functions as you wish.} The \href{https://terpconnect.umd.edu/~toh/spectrum/license.txt}{software license} embedded within the comments of all my Matlab/Octave code is very liberal. \label{ref-0020}

\chapter{Signals and noise\index{noise}\label{ref-0021}\label{ref-0022}\label{ref-0023}\label{ref-0024}}

Experimental measurements are never perfect, even with sophisticated modern instruments. Two main types of measurement errors are recognized: (a) \textit{systematic error,} in which every measurement is consistently less than or greater than the correct value by a certain percentage or amount, and (b) \textit{random error,} in which there are unpredictable variations in the measured signal from moment to moment or from measurement to measurement. This latter type of error is often called \textit{noise}, by analogy to acoustic noise. There are many sources of noise in physical measurements, such as building vibrations, air currents, electric power fluctuations, stray radiation from nearby electrical equipment, static electricity, interference from radio and TV transmissions, turbulence in the flow of gases or liquids, random thermal motion of molecules, background radiation from natural radioactive elements, ``cosmic rays'' from outer space (seriously), the basic quantum nature of matter and energy itself, and \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#Digitization}{digitization noise} (the rounding of numbers to a fixed number of digits). Then, of course, there is the ever-present "human error", which can be a major factor anytime humans are involved in operating, adjusting, recording, calibrating, or controlling instruments and in preparing samples for measurement. If random error is present, then a set of repeat measurements will yield results that are not all the same but rather vary or scatter around some \href{https://en.wikipedia.org/wiki/Average}{average value}, which is the sum of the values divided by the number of data values ``d'': \texttt{sum(d)./length(d)} in Matlab/Octave notation. The most common way to measure the amount of variation or dispersion of a set of data values is to compute the \href{https://en.wikipedia.org/wiki/Standard\_deviation}{\textit{standard deviation}}, which is the square root of the sum of the squares of the deviations from the average divided by one less than the number of data points: \texttt{sqrt(sum((d-mean(d)).\textasciicircum{}2)./ (length(d)-1))}. In Matlab/Octave notation, this is most easily calculated by the built-in function \texttt{std(d), where d is the data vector}. A basic fact of random variables is that when they combine, you must calculate the results \textit{statistically}. For example, when two random variables are added, the standard deviation of the sum is the ``quadratic sum'' (the square root of the sum of the squares) of the standard deviations of the individual variables, as demonstrated by the series of Matlab/Octave commands \href{https://terpconnect.umd.edu/~toh/spectrum/RandomNoisesAddQuadratically.txt}{at this link}. Try it.

The term ‘signal’ has two meanings. In the more general sense, it can mean the \textit{entire} data recording, including the noise and other artifacts, as in the ``raw signal'' before processing is applied. But it can also mean only the \textit{desirable} or \textit{important} part of the data, the \textit{true} \textit{underlying signal} that you seek to measure, as in the expression ``signal-to-noise ratio''. A fundamental problem in signal measurement is distinguishing the true underlying signal from the noise. For example, suppose you want to measure the average of the signal over a certain time or the height of a peak or the area under a peak that occurs in the data. In the absorption spectrum\index{absorption spectrum} in the right-hand half of the figure on page \pageref{ref-0005}, the ``important'' parts of the data are probably the absorption peaks located at 520 and 550 nm. The height or the position of either of those peaks might be considered the signal, depending on the application. In this example, the height of the largest peak is about 0.08 absorbance units. But how to measure the noise? In the exceptional case that you have a physical system \textit{and} a measuring instrument which are \textit{both} completely stable (\textit{except} for the random noise), an easy way to isolate and measure the noise is to \textit{record two signals m1 and m2 of the same physical system}. If you subtract those two recordings, the signal part will cancel out. Then the standard deviation of the noise in the original signals is given by sqrt((std(m1-m2)\textsuperscript{2})/2), where ``sqrt'' is the square root and ``std'' is the standard deviation. (The simple derivation of this expression is based on the \href{https://terpconnect.umd.edu/~toh/spectrum/Derivation.txt}{rules for mathematical error propagation} and is worked out in \url{https://terpconnect.umd.edu/~toh/spectrum/Derivation.txt}). The Matlab/Octave script ``\href{https://terpconnect.umd.edu/~toh/spectrum/SubtractTwoMeasurements.m}{SubtractTwoMeasurements.m}'' (\uline{right}) demonstrates this process quantitatively \InsImageInline{0.5}{l}{image15.png}and graphically.\label{ref-0025}

But suppose that the measurements are not that reproducible or that you had only \textit{one} recording of that spectrum and no other data? In that case, you could try to estimate the noise in that single recording, based on the \textit{assumption} that the visible \textit{short-term fluctuations} in the signal - the little random wiggles superimposed on the smooth signal - are noise and not part of the true underlying signal. That depends on some knowledge of the origin of the signal and the possible forms it might take. The examples in the previous section are the absorption spectra of liquid solutions over the wavelength range of 450 nm to 700 nm (page \pageref{ref-0005}). These solutions ordinarily exhibit broad smooth peaks with a width of the order of 10 to 100 nm, so those little wiggles must be \textit{noise}. In this case, those fluctuations have a standard deviation of about 0.001. Often the best way to measure the noise is to locate a region of the signal on the baseline where the signal is flat and to compute the standard deviation in that region. This is easy to do with a computer if the signal is digitized. The important thing is that you must know enough about the measurement and the data it generates to recognize the kind of signals that is likely to generate, so you have some hope of knowing which is the \textit{signal} and which is the \textit{noise}.

It is important to appreciate that the standard deviations calculated of a small set of measurements can be much higher or much lower than the actual standard deviation of a larger number of measurements. For example, the Matlab/Octave function \texttt{randn(1,}\texttt{\textit{n}}\texttt{)}, where \textit{n} is an integer, returns \textit{n} random numbers that have \textit{on average} a mean of zero and a standard deviation of 1.00 if \textit{n} is large. But if \textit{n} is small, the standard deviations will be different each time you evaluate that function; for example, if n=5, the standard deviation \texttt{std(}\texttt{randn(1,5)}\texttt{)} might vary randomly from 0.5 to 2 or even more. This is the \href{https://en.wikipedia.org/wiki/Law\_of\_large\_numbers}{Law of Large Numbers}; it is the unavoidable nature of small sets of random numbers that their standard deviation is only a \textit{very rough approximation} to the real underlying ``population'' standard deviation.

A quick but approximate way to estimate the amplitude of noise visually is the \textit{peak-to-peak} range, which is the difference between the highest and the lowest values in a region where the signal is flat. The peak-to-peak range of \textit{n}=100 normally-distributed random numbers is about 5 time the standard deviation, as can be proved by running this line of Matlab/Octave code several times: \texttt{n=100; rn=randn(1,n);(max(rn)-min(rn))/std(rn)}\texttt{\textit{.}} For example, the data on the right half of the figure on the next page has a peak in the center with a height of about 1.0. The peak-to-peak noise on the baseline is also about 1.0, so the standard deviation of the noise is about 1/5th of that, or 0.2. \textit{However, that ratio varies with the logarithm of n} and is closer to 3 when \textit{n} = 10 and to 9 when \textit{n} = 100000. In contrast, the standard deviation becomes closer and closer to the true value as \textit{n} increases. It is better to compute the standard deviation if possible. 

In addition to the \textit{standard} deviation, it is also possible (but not usual) to measure the \textit{mean absolute} deviation ("mad"). The standard deviation is larger than the mean absolute deviation because the standard deviation weights the large deviation more heavily. For a normally distributed random variable, the mean absolute deviation is on average 80\% of the standard deviation: mad=0.8*std.

\InsImageInline{0.5}{l}{image16.png}The \textit{quality} of a signal is often expressed quantitatively as the \textit{signal-to-noise} \textit{ratio} (S/N ratio or SNR), which is the ratio of the true underlying signal amplitude (e.g., the average amplitude or the peak height) to the standard deviation of the noise. Thus, the S/N ratio of the spectrum in the figure on page \pageref{ref-0005} is about 0.08/0.001 = 80, and the signal on page \pageref{ref-0031} has an S/N ratio of 1.0/0.2 = 5. So, we would say that the quality of the first one is better because it has a greater S/N ratio. Measuring the S/N ratio is much easier if the noise can be measured separately, in the absence of a signal. Depending on the type of experiment, it may be possible to acquire readings of the noise alone, for example on a segment of the baseline before or after the occurrence of the signal. However, if the magnitude of the noise depends on the level of the signal, then the experimenter must try to produce a constant signal level to allow measurement of the noise on the signal. In some cases, you can use ``least-squares curve fitting'' (page \pageref{ref-0258}) to model the shape of the signal accurately by means of a mathematical function (such as a \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html}{polynomial }or the weighted sum of a number of simple \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{peak shape} functions). The noise can then be isolated by subtracting the model from the un-smoothed experimental signal. For example, the graph shown on the left shows a complex experimental signal that never goes all the way to the baseline to allow a simple noise measurement. But the signal can be approximated by fitting a model (red line) consisting of 5 overlapping smooth Gaussian peak functions. The difference between the raw data and the model, shown at the bottom, is a good measure of the random noise in the data. If possible, however, it is usually better to determine the standard deviation of repeated measurements of the thing that you want to measure (e.g., the peak heights or areas, for example), rather than trying to estimate the noise from a single recording of the data. \label{ref-0026}\label{ref-0027}

\section{\textbf{Detection limit}\label{ref-0028}\label{ref-0029}}

\InsImageInline{0.5}{l}{RectSNRAtTheDetectionLimit.png}The "detection limit" is defined as the smallest signal that you can reliably detect in the presence of noise. In quantitative analysis, it is usually defined as the concentration that produces the smallest detectable signal (Reference 92). A signal that is below the detection limit cannot be reliably detected; that is, if the measurement is repeated, the signal will often be "lost in the noise" and reported as zero. A signal above the detection limit will be reliably detected and will seldom or never reported as zero. The most common definition of signal-to-noise ratio for reliable detection is 3. This is illustrated in the figure on the left (created by the Matlab/Octave script\href{https://terpconnect.umd.edu/~toh/spectrum/SNRdemo.m}{ SNRdemo.m}). This figure shows a noisy signal in the form of a rectangular pulse. We define the "signal" as the average signal magnitude during the pulse, indicated by the red line, which is about 3. We define the "noise" as the standard deviation of the random noise on the baseline before and after the pulse, which is about 1.0, roughly 1/5 of the peak-to-peak baseline noise (black lines). The signal-to-noise ratio (SNR) in this case is about 3, which is a common definition of detection limit. This means that signals lower than this should be reported as "undetectable". 

But there is a problem. The signal here is clearly detectable by eye; in fact, it should be possible to visually detect lower signals than this. How can this be? The answer is "averaging". When you look at this signal, you are \textit{unconsciously estimating the average of the data points} on the signal pulse and on the baseline, and your detection ability in enhanced by this visual averaging. Without that averaging, looking only at \textit{individual} data points in the signal, only about half those individual points would meet the SNR=3 criterion. You can see in the graphic above that several points on the signal peak are actually \textit{lower} that some of the data points on the baseline. But this is not a problem in practice, because any properly written software will include averaging that duplicates the visual averaging that we all do.

In the script \href{https://terpconnect.umd.edu/~toh/spectrum/SNRdemo.m}{SNRdemo.m}, the number of points averaged is controlled by the variable "AveragePoints" in line 7. If you set that to 5, the result (shown on the left) shows that all the signal \InsImageInline{0.5}{l}{RectSNR3.png}points (each of which is now the average of 5 raw data points) are above the highest baseline points. This graphic more closely represents how we judge a signal like that in the previous graphic, which has a clear separation of signal and baseline. The SNR of the peak has improved from 3.1 to 7.7 and \textit{the detection limit will be correspondingly reduced}. As a rule of thumb, for the most common type of noise, the noise decreases by the roughly the square root of the number of points averaged (sqrt(5)=2.2). Higher values will further improve the SNR and reduce the relative standard deviation of the average signal, but the \textit{response time} \textendash{} which is the time it takes for the signal to reach the average value - will become slower and slower as the number of points averaged increases. This is shown by this \href{https://terpconnect.umd.edu/~toh/spectrum/RectSNR100pnts.png}{graphic with 100 points averaged}. With a much lower signal equal to 1.0, the raw signal is \href{https://terpconnect.umd.edu/~toh/spectrum/RectSNR1.png}{not reliably detectable visually}, but with a 100 point average, the \href{https://terpconnect.umd.edu/~toh/spectrum/RectSNR1avg100.png}{signal precision is good}; digital averaging beats visual averaging in this case. Similar behavior would be observed if the signal were a rounded peak rather than a rectangle.

In \href{https://terpconnect.umd.edu/~toh/spectrum/SNRdemo.m}{SNRdemo.m}, the noise is constant and independent of the signal amplitude, which is commonly the case. In the variant \href{https://terpconnect.umd.edu/~toh/spectrum/SNRdemoHetero.m}{SNRdemoHetero.m}, the noise in the signal is directly proportional to the signal level or to its square root, and as a result the detection limit depends on the constant baseline noise (\href{https://terpconnect.umd.edu/~toh/spectrum/RectSNRhetero.png}{graphic}). See page \pageref{ref-0036}. In the variant \href{https://terpconnect.umd.edu/~toh/spectrum/SNRdemoArea.m}{SNRdemoArea.m}, it is the peak \textit{area} that is measured rather than the peak height, which results in the SNR improved by the square root of the width of the peak (\href{https://terpconnect.umd.edu/~toh/spectrum/RectSNRarea.png}{graphic}).

An example of a practical application of a signal like this would be to turn on a warning light or buzzer if the signal ever exceeds a threshold value of 1.5, for the signal illustrated in the figures above. This would not work if you used the raw unaveraged signal in the previous figure; there is no threshold value that would never be exceeded by the baseline but always exceeded by the signal. Only the \textit{averaged} signal would reliably turn on the alarm above the threshold of 1.5 and never activate it below 1.5.

You will also hear the term ``Limit of determination'', which is the lowest signal or concentration that achieves a minimum acceptable precision, defined as the relative standard deviation of the signal amplitude. The limit of determination is defined at much higher signal-to-noise ratio, say 10 or 20, depending on the requirements of your applications.

Averaging such as done here is the simplest form of ``smoothing'', which is covered in the next chapter (page \pageref{ref-0048}).

\section{Ensemble averaging\index{Ensemble averaging}\label{ref-0030}}

One key thing that really distinguishes signal from noise is that random noise is not the same from one measurement of the signal to the next, whereas the genuine signal is (ideally) reproducible. So, if the signal can be measured more than once, use can be made of this fact by measuring the signal repeatedly, as fast as is practical, and \textit{adding up} all the measurements point-by-point, then dividing by the number of signals averaged. This is called \textit{ensemble averaging}, and it is one of the most powerful methods for improving signals, when it can be applied. For this to work properly, the noise must be random, and the signal must occur at the same time in each repeat. Look at the example this figure. 


\begin{center}
\InsImage{1.0}{Figure3.GIF.png}\label{ref-0031}
\end{center}



\begin{center}
\textit{Window 1 (left) is a single measurement of a very noisy signal. There is a broad peak near the center of this signal, but it is difficult to measure its position, width, and height accurately because the S/N ratio is very poor. Window 2 (right) is the average of 9 repeated measurements of this signal, clearly showing the peak emerging from the noise. The expected improvement in S/N ratio is 3 (the square root of 9). Often it is possible to average hundreds of measurements, resulting in much more substantial improvement. The S/N ratio in the resulting average signal in this example is about 5.}
\end{center}


You can reduce digitization noise by ensemble averaging, \textit{but only if small amounts of random noise are present in, or added to, the signal}; see page \pageref{ref-0366}. The Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/EnsembleAverageDemo.m}{EnsembleAverageDemo.m} demonstrates the technique graphically. (If you are reading this online, \href{https://terpconnect.umd.edu/~toh/spectrum/EnsembleAverageDemo.png}{click for graphic}\uline{)}. Other examples are shown in the video animation at these links, \href{https://terpconnect.umd.edu/~toh/spectrum/EnsembleAverage1.wmv}{EnsembleAverage1.wmv} or \href{https://terpconnect.umd.edu/~toh/spectrum/EnsembleAverageDemo.gif}{EnsembleAverageDemo.gif}, which shows the ensemble averaging of 1000 repeats of a signal, improving the S/N ratio by about 30 times. 

\textbf{Visual animation of ensemble averaging.} This 17-second video (\href{https://terpconnect.umd.edu/~toh/spectrum/EnsembleAverage1.wmv}{EnsembleAverage1.wmv}) demonstrates the ensemble averaging of 1000 repeats of a signal with a very poor S/N ratio. The signal itself consists of three peaks located at x = 50, 100, and 150, with peak heights 1, 2, and 3 units. These signal peaks are buried in random noise whose standard deviation is 10. Thus, the S/N ratio of the smallest peaks is 0.1, which is far too low to even \textit{see} a signal, much less measure it. The video shows the accumulating average signal as 1000 measurements of the signal are performed. At the end of the run, the noise is reduced (on average) by the square root of 1000 (about 32), so that the S/N ratio of the smallest peaks ends up being about 3, just enough to detect the presence of a peak reliably. If you are reading this online, click \href{https://terpconnect.umd.edu/~toh/spectrum/EnsembleAverage1.wmv}{here }to download a brief video (2 MBytes) in WMV format. 

\section{Frequency distribution of random noise\index{Frequency distribution of random noise} \label{ref-0032}\label{ref-0033}}

\InsImageInline{0.5}{l}{image19.png}Sometimes the signal and the noise can be partly distinguished based on \href{https://terpconnect.umd.edu/~toh/spectrum/HarmonicAnalysis.html}{frequency components}: for example, the signal may contain mostly low-frequency components and the noise may be located at higher frequencies or spread out over a much wider frequency range. This is the basis of \href{https://terpconnect.umd.edu/~toh/spectrum/FourierFilter.html}{filtering} and \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothing} (page \pageref{ref-0045}). In the figure above, the peak itself contains mostly low-frequency components, whereas the noise is (apparently) random and distributed over a much wider frequency range. The frequency of noise is characterized by its \href{http://en.wikipedia.org/wiki/Frequency\_spectrum}{frequency spectrum}, often described in terms of \href{http://en.wikipedia.org/wiki/Colors\_of\_noise}{noise color}. \href{http://en.wikipedia.org/wiki/White\_noise}{\textit{White noise}} is random and has \href{https://terpconnect.umd.edu/~toh/spectrum/WhiteNoiseSpectrum.png}{equal power over the range of frequencies}. It derives its name from \textit{white light}, which has equal brightness at all wavelengths in the visible region. The noise in the previous example signals and in the left half of the figure on the right is \textit{white}. In the acoustical domain, white noise sounds like a \textit{hiss}. In measurement science, white noise is very common. For example, quantization noise, \href{http://en.wikipedia.org/wiki/Johnson–Nyquist\_noise}{Johnson-Nyquist} (thermal) noise, \href{http://en.wikipedia.org/wiki/Photon\_noise}{photon noise}, and the noise made by \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#G}{single-point spikes} all have white frequency distributions, and all have in common their origin in discrete quantized instantaneous events, such as the flow of individual electrons or photons. 

\InsImageInline{0.5}{l}{image20.png}A noise that has a more low-frequency-weighted character, that is, that has more power at low frequencies than at high frequencies, is often called "\href{http://en.wikipedia.org/wiki/Pink\_noise}{pink noise}". In the acoustical domain, pink noise sounds more like a \textit{roar}. (A commonly-encountered sub-species of pink noise is "\href{https://www.google.com/search?ix=aca&sourceid=chrome&ie=UTF-8&q=1\%2Ff+noise}{1/f noise}", where the noise power is inversely proportional to frequency, illustrated in the upper right quadrant of the figure on the right). Pink noise is more troublesome that white noise because a \textit{given standard deviation of pink noise has a greater effect on the accuracy of most measurements than the same} \textit{standard deviation of white noise} (as demonstrated by the Matlab/Octave function \href{https://terpconnect.umd.edu/~toh/spectrum/noisetest.m}{noisetest.m}, which generated the figure on the right). Moreover, the application of \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothing} and low-pass \href{https://terpconnect.umd.edu/~toh/spectrum/FourierFilter.html}{filtering} (page \pageref{ref-0045}) to reduce noise is more effective for white noise than for pink noise. When pink noise is present, it is sometimes beneficial to apply modulation techniques, for example, \href{http://en.wikipedia.org/wiki/Optical\_chopper}{optical chopping} or \href{https://www.google.com/search?aq=f&ix=aca&sourceid=chrome&ie=UTF-8&q=wavelength+modulation}{wavelength modulation} in optical measurements, to convert direct-current (DC) signals into alternating current (AC) signals, thereby increasing the frequency of the signal to a frequency region where the noise is lower. In such cases, it is common to use a \href{http://terpconnect.umd.edu/~toh/models/lockin.html}{lock-in amplifier}, or the digital equivalent thereof, to measure the amplitude of the signal. Another type of low-frequency weighted noise is \href{https://en.wikipedia.org/wiki/Brownian\_motion}{\textit{Brownian} noise}, named after the botanist Robert Brown. It is also called "red noise", by analogy to pink noise, or "\href{https://en.wikipedia.org/wiki/Random\_walk}{random walk}", which has a noise power that is\href{https://terpconnect.umd.edu/~toh/spectrum/RandomWalkFrequencySpectrum.png}{ inversely proportional to the \textit{square} of frequency}. This type of noise is not uncommon in experimental signals and can seriously interfere with accurate signal measurements. See page \pageref{ref-0379}: \textit{Random walks and baseline correction}. 

Conversely, noise that has more power at \textit{high} frequencies is called \href{http://www.livescience.com/38583-what-is-blue-noise.html}{``blue'' noise}. This type of noise is less commonly encountered in experimental work, but it can occur in processed signals that have been subject to some sort of \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html}{differentiation }process (page \pageref{ref-0081}) or that have been \href{https://terpconnect.umd.edu/~toh/spectrum/Deconvolution.html}{deconvoluted }from some blurring or broadening process (page \pageref{ref-0152}). Blue noise is \textit{easier} to reduce by smoothing (page \pageref{ref-0032}), and it has less effect on least-squares fits than the equivalent amount of white noise. 

\section{Dependence on signal amplitude\index{Dependence on signal amplitude}\label{ref-0034}\label{ref-0035}\label{ref-0036}}

\InsImageInline{0.5}{l}{image21.png}Noise can also be characterized by the way it varies with the signal amplitude. Constant ``background'' noise is independent of the signal amplitude. Or the noise may increase with signal amplitude, which is a behavior that is often observed in emission spectroscopy, \href{https://www.chem.agilent.com/Library/technicaloverviews/Public/5990-7651EN.pdf}{mass spectroscopy} and in the \href{http://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#F}{frequency spectra of signals}. The fancy names for these two types of behaviors is \textit{homoscedastic} and \textit{heteroscedastic}, respectively. One way to observe this is to select a segment of signal over which the signal amplitude varies widely, fit the signal to a \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html}{polynomial }or to a multiple peak model, and observe how the residuals vary with signal amplitude. \href{https://terpconnect.umd.edu/~toh/spectrum/PropNoise.png}{The example} on the left is a real experimental signal showing the residuals from a curve-fitting operation (page \pageref{ref-0258}) that reveals the noise increasing with signal amplitude. In other cases, the noise is almost independent of the signal amplitude. 

Often, there is a mix of noises with different behaviors; in \href{http://www.agilent.com/labs/features/2011\_101\_spectroscopy.pdf}{optical spectroscopy}, three fundamental types of noise are recognized, based on their origin and on how they vary with light intensity: \textit{photon noise}, \textit{detector noise}, and \textit{flicker (fluctuation) noise}. Photon noise (often the limiting noise in instruments that use photo-multiplier detectors) is \textit{white} and is proportional to the \textit{square root} of light intensity, and therefore the SNR is proportional to the square root of light intensity and directly proportional to the monochromator slit-width. Detector noise (often the limiting noise in instruments that use solid-state photodiode detectors) is \textit{independent} of the light intensity and therefore the detector SNR is directly proportional to the light intensity and to the square of the monochromator slit width. Flicker noise, caused by light source instability, vibration, sample cell positioning errors, sample turbulence, light scattering by suspended particles, dust, bubbles, etc., is directly proportional to the light intensity (and is usually \textit{pink} rather than \textit{white}), so the flicker S/N ratio is not decreased by increasing the slit width. In practice, the total noise observed is likely to be some contribution of all three types of amplitude dependence, as well as a mixture of white and pink noises. 

Only in a very few special cases is it possible to eliminate noise completely, so usually, you must be satisfied by increasing the S/N ratio as much as possible. The key in any experimental system is to understand the possible sources of noise, break down the system into its parts and measure the noise generated by each part separately, then seek to reduce or compensate for as much of each noise source as possible. For example, in optical spectroscopy, source flicker noise can often be reduced or eliminated by using in \href{http://en.wikipedia.org/wiki/Feedback}{feedback stabilization}, choosing a better light source, using an \href{http://en.wikipedia.org/wiki/Internal\_standard}{internal}\href{http://en.wikipedia.org/wiki/Internal\_standard}{ standard}, or specialized instrument designs such as \href{https://terpconnect.umd.edu/~toh/models/UVVisSNR.html}{double-beam}, \href{https://terpconnect.umd.edu/~toh/models/DualWave1.html}{dual-wavelength}, \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html}{derivative}, and \href{https://terpconnect.umd.edu/~toh/modspec.html}{wavelength modulation} (page \pageref{ref-0384}). The effect of photon noise and detector noise can be reduced by increasing the light intensity at the detector or increasing the spectrometer slit width, and electronic noise can sometimes be reduced by cooling or upgrading the detector and/or electronics. \href{http://en.wikipedia.org/wiki/Fixed-pattern\_noise}{Fixed pattern noise} in array detectors can be corrected in software. Only \textit{photon noise} can be predicted from first principles (e.g. as is done in these spreadsheets that simulate the signal-to-noise behavior \uline{\textcolor{color-2}{of}} \href{https://terpconnect.umd.edu/~toh/models/UVVisSNR.html}{ultraviolet-visible spectrophotometry}, \href{https://terpconnect.umd.edu/~toh/models/FluorescenceSNR.html}{fluorescence spectroscopy}, and \href{https://terpconnect.umd.edu/~toh/models/AES.html}{atomic emission spectroscopy}). 

\section{The probability distribution of random noise\index{Probability distribution of random noise} \label{ref-0037}}

\InsImageInline{0.5}{l}{CentralLimitDemo.gif.png}Another property that distinguishes random noise is its \href{http://en.wikipedia.org/wiki/Probability\_distribution}{probability distribution}, the function that describes the probability of a random variable falling within a certain range of values. In physical measurements, the most common distribution is called \href{http://en.wikipedia.org/wiki/Normal\_distribution}{\textit{normal curve}} (also called as a ``bell'' or ``haystack'' curve) and is described by a \href{http://en.wikipedia.org/wiki/Gaussian\_function}{\textit{Gaussian}}\href{http://en.wikipedia.org/wiki/Gaussian\_function}{ }function, \href{http://www.wolframalpha.com/input/?i=plot+y\%3Dexp\%28-\%28\%28x-mu\%29\%2F\%28sigma\%2F\%282*sqrt\%28ln\%282\%29\%29\%29\%29\%29\%5e2\%29+for+mu\%3D0\%2Csigma\%3D1}{y=e\textasciicircum{}(-(x-}\href{http://www.wolframalpha.com/input/?i=plot+y\%3Dexp\%28-\%28\%28x-mu\%29\%2F\%28sigma\%2F\%282*sqrt\%28ln\%282\%29\%29\%29\%29\%29\%5e2\%29+for+mu\%3D0\%2Csigma\%3D1}{\textit{mu}}\href{http://www.wolframalpha.com/input/?i=plot+y\%3Dexp\%28-\%28\%28x-mu\%29\%2F\%28sigma\%2F\%282*sqrt\%28ln\%282\%29\%29\%29\%29\%29\%5e2\%29+for+mu\%3D0\%2Csigma\%3D1}{)\textasciicircum{}2 / (2*}\href{http://www.wolframalpha.com/input/?i=plot+y\%3Dexp\%28-\%28\%28x-mu\%29\%2F\%28sigma\%2F\%282*sqrt\%28ln\%282\%29\%29\%29\%29\%29\%5e2\%29+for+mu\%3D0\%2Csigma\%3D1}{\textit{sigma}}\href{http://www.wolframalpha.com/input/?i=plot+y\%3Dexp\%28-\%28\%28x-mu\%29\%2F\%28sigma\%2F\%282*sqrt\%28ln\%282\%29\%29\%29\%29\%29\%5e2\%29+for+mu\%3D0\%2Csigma\%3D1}{\textasciicircum{}2)) / (sqrt(2*}\href{http://www.wolframalpha.com/input/?i=plot+y\%3Dexp\%28-\%28\%28x-mu\%29\%2F\%28sigma\%2F\%282*sqrt\%28ln\%282\%29\%29\%29\%29\%29\%5e2\%29+for+mu\%3D0\%2Csigma\%3D1}{\textit{mu}}\href{http://www.wolframalpha.com/input/?i=plot+y\%3Dexp\%28-\%28\%28x-mu\%29\%2F\%28sigma\%2F\%282*sqrt\%28ln\%282\%29\%29\%29\%29\%29\%5e2\%29+for+mu\%3D0\%2Csigma\%3D1}{)}\href{http://www.wolframalpha.com/input/?i=plot+y\%3Dexp\%28-\%28\%28x-mu\%29\%2F\%28sigma\%2F\%282*sqrt\%28ln\%282\%29\%29\%29\%29\%29\%5e2\%29+for+mu\%3D0\%2Csigma\%3D1}{\textit{*sigma}}\href{http://www.wolframalpha.com/input/?i=plot+y\%3Dexp\%28-\%28\%28x-mu\%29\%2F\%28sigma\%2F\%282*sqrt\%28ln\%282\%29\%29\%29\%29\%29\%5e2\%29+for+mu\%3D0\%2Csigma\%3D1}{)}, where \textit{mu} is the mean (average) value and \textit{sigma (}${\upsigma}$) is the standard deviation. In this distribution, the most common noise errors are small (that is, close to the \textit{mean}) and the errors become less common the greater their deviation from the mean. So why is this distribution so common? The noise observed in physical measurements is often the balanced sum of many unobserved random events, each of which has some unknown probability distribution related to, for example, the kinetic properties of gases or liquids or to the quantum mechanical description of fundamental particles such as photons or electrons. But when many such events combine to form the overall variability of an observed quantity, the resulting probability distribution is almost always \textit{normal}, that is, described by a Gaussian function. This common observation is summed up in the \href{http://en.wikipedia.org/wiki/Normal\_distribution\#Central\_limit\_theorem}{\textit{Central Limit Theorem}}.

A simulation can demonstrate how this behavior arises naturally. In the example on the left, we start with a set of 100,000 \textit{uniformly distributed} random numbers that have an equal chance of having any value between certain limits - between 0 and +1 in this case (like the "rand" function in most spreadsheets and in Matlab/Octave). The graph in the upper left of the figure shows the probability distribution, called a ``\href{http://en.wikipedia.org/wiki/Histogram}{histogram}'', of that random variable. Next, we combine two sets of such independent, uniformly distributed random variables (changing the signs so that the average remains centered at zero). The result (shown in the graph in the upper right in the figure) has a \textit{triangular} distribution between -1 and +1, with the highest point at zero, because there are many ways for the difference between two random numbers to be small, but only one way for the difference to be 1 or to -1 (that happens only if one number is exactly zero \textit{and} the other is exactly 1). Next, we combine \textit{four} independent random variables (lower left); the resulting distribution has a total range of -2 to +2, but it is even \textit{less} likely that the result be near 2 or -2 and many \textit{more} ways for the result to be small, so the distribution is narrower and more rounded, and is already starting to be visually close to a normal Gaussian distribution (generated by using the ``randn'' function and shown for reference in the lower right). If we combine ever more independent uniform random variables, the combined probability distribution becomes closer and closer to Gaussian (shown for comparison in the bottom right). \textit{The emerging Gaussian distribution that we observe here is not forced by prior assumption; rather, it arises naturally}. (You can download a Matlab script for this simulation from \href{https://terpconnect.umd.edu/~toh/spectrum/CentralLimitDemo.m}{http://terpconnect.umd.edu/\textasciitilde{}toh/spectrum/CentralLimitDemo.m}\uline{)}. 

Remarkably, \textit{the distributions of individual events hardly matter at all}. You could modify the individual distributions in this simulation by substituting the \textit{rand} function by modified versions such as sqrt(rand), sin(rand), rand\textasciicircum{}2, log(rand), etc., to obtain other \textit{radically non-normal} individual distributions. But it seems that no matter what the distribution of the single random variable might be, by the time you combine even as few as four of them, the resulting distribution is already visually close to normal. Real-world macroscopic observations are often the result of \textit{thousands} or \textit{millions} of individual microscopic events, so whatever the probability distributions of the \textit{individual} events, the \textit{combined} macroscopic observations approach a normal distribution essentially perfectly. It is on this common adherence to normal distributions that the common statistical procedures are based; the use of the \href{http://en.wikipedia.org/wiki/Mean}{mean}, \href{http://en.wikipedia.org/wiki/Standard\_deviation}{standard deviation} ${\upsigma}$, \href{http://en.wikipedia.org/wiki/Least\_squares}{least-squares} fits, \href{http://en.wikipedia.org/wiki/Confidence\_limits}{confidence intervals}, etc., are all based on the \textit{assumption} of a normal distribution. 

Even so, experimental errors and noise are not \textit{always} normal; sometimes there are very large errors that fall well beyond the ``normal'' range. They are called ``outliers'' and they can have a very large effect on the standard deviation. In such cases, it is possible to use the ``\href{https://en.wikipedia.org/wiki/Interquartile\_range}{interquartile range}'' (IQR), defined as the difference between the upper and lower quartiles, instead of the standard deviation, because \textit{the interquartile range is not affected by a few outliers}. For a \textit{normal} distribution, the interquartile range is equal to 1.34896 times the standard deviation. A quick way to check the distribution of a large set of random numbers is to compute both the standard deviation and the interquartile range; if they are roughly equal, the distribution is probably normal; if the standard deviation is \textit{much} larger, the data set probably contains outliers and the standard deviation \textit{without} the outliers can be better estimated by dividing the interquartile range by 1.34896. 

\textbf{The importance of the normal distribution} is that if you know the standard deviation ``${\upsigma}$'' of some measured value, then you can predict the likelihood that your result might be in error by a certain amount. About 68\% of values drawn from a normal distribution are within one ${\upsigma}$ away from the mean; 95\% of the values lie within 2${\upsigma}$, and 99.7\% are within 3${\upsigma}$. This is known as the \href{https://en.wikipedia.org/wiki/Normal\_distribution\#Standard\_deviation\_and\_coverage}{3-sigma rule}. But the real practical problem is this: \textit{standard deviations are hard to measure accurately unless you have large numbers of samples.} See ``\textit{The Law of Large Numbers''} (\uline{page} \pageref{ref-0425}\uline{).}\label{ref-0038}

The three characteristics of noise discussed in the paragraphs above - the frequency distribution, the amplitude distribution, and the signal dependence - are mutually independent; a noise may in principle have any combination of those properties. 

\href{https://terpconnect.umd.edu/~toh/spectrum/SPECTRUM.html}{\textbf{SPECTRUM},} the legacy Macintosh freeware signal-processing application, includes several functions for measuring signals and noise in the \href{https://terpconnect.umd.edu/~toh/spectrum/~toh/spectrum/SPECTRUMReferenceManual.pdf\#page=9}{\textbf{Math}}\href{http://www.wam.umd.edu/~toh/spectrum/SPECTRUMReferenceManual.pdf\#page=9}{ }and \href{https://terpconnect.umd.edu/~toh/spectrum/~toh/spectrum/SPECTRUMReferenceManual.pdf\#page=9}{\textbf{Window}} pull-down menus, plus a signal-generator that can be used to generate artificial signals with Gaussian and Lorentzian bands, sine waves, and normally-distributed random noise in the \textbf{New} command in the \href{https://terpconnect.umd.edu/~toh/spectrum/~toh/spectrum/SPECTRUMReferenceManual.pdf}{\textbf{File}}\href{http://www.wam.umd.edu/~toh/spectrum/SPECTRUMReferenceManual.pdf}{ Menu}. See page \pageref{ref-0482}.

\section{Spreadsheets\label{ref-0039}}

Popular spreadsheets\textbf{,} such as \href{http://www.microsoftstore.com/store/msstore/pd/Excel-Home-and-Student-2010/productID.216446900/vip.true}{\textit{Excel}}\href{http://www.microsoftstore.com/store/msstore/pd/Excel-Home-and-Student-2010/productID.216446900/vip.true}{ }or \href{http://en.wikipedia.org/wiki/OpenOffice.org\_Calc}{\textit{Open Office Calc}}, have built-in functions that can be used for calculating, measuring and plotting signals and noise. For example, the cell formula for one point on a \textbf{Gaussian} peak is 

\begin{center}\texttt{\textbf{amplitude*EXP(-1*((x-position)/(0.60056120439323*width))\textasciicircum{}2)}}, \end{center}where 'amplitude' is the maximum peak height, 'position' is the location of the maximum on the x-axis, 'width' is the full width at half-maximum (FWHM) of the peak (which is equal to \textit{sigma} times 2.355), and 'x' is the value of the independent variable at that point. The cell formula for a \textbf{Lorentzian} peak is 

\begin{center}\texttt{\textbf{amplitude/(1+((x-position)/(0.5*width))\textasciicircum{}2)}}.\end{center}

 Other useful functions include AVERAGE, MAX, MIN, STDEV, VAR, RAND, and QUARTILE. Most spreadsheets have only a uniformly-distributed random number function (RAND) and not a \textit{normally-distributed} random number function, but it is much more realistic to simulate errors that are \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#PDF}{normally distributed}. But do not worry, you can use the Central Limit Theorem to create approximately normally distributed random numbers by combining several RAND functions, for example, the odd-looking expression \textbf{SQRT(3)*(RAND()-RAND()+RAND()-RAND())} creates nearly normal random numbers with a mean of zero, a standard deviation very close to 1, and a maximum range of ${\pm}$4. I use this trick in \href{http://terpconnect.umd.edu/~toh/models/}{spreadsheet models that simulate the operation of analytical instruments}. (The expression 

\begin{center}\texttt{\textbf{SQRT(2)*(}} \textbf{RAND}\texttt{\textbf{()-}}\textbf{RAND}\texttt{\textbf{()+}}\textbf{RAND}\texttt{\textbf{()-}}\textbf{RAND}\texttt{\textbf{()+}}\textbf{RAND}\texttt{\textbf{()-}}\textbf{RAND}\texttt{\textbf{())}}\end{center}

 works similarly but has a larger maximum range). To create random numbers with a standard deviation other than 1, simply \textit{multiply} by that number. To create random numbers with an average other than zero, simply \textit{add} that number. The \textit{interquartile range} (IQR) can be calculated in a spreadsheet by subtracting the third quartile from the first (e.g., \textbf{QUARTILE(B7: B504,3) - QUARTILE(B7: B504,1)}). 

The spreadsheets \href{https://terpconnect.umd.edu/~toh/spectrum/RandomNumbers.xls}{RandomNumbers.xls}, for Excel, and \href{https://terpconnect.umd.edu/~toh/spectrum/RandomNumbers.ods}{RandomNumbers.ods,} for OpenOffice, (\href{https://terpconnect.umd.edu/~toh/spectrum/RandomNumbers.png}{screen} image below), and the Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/RANDtoRANDN.m}{RANDtoRANDN.m}, all demonstrate these facts. The same technique is used in the spreadsheet \href{https://terpconnect.umd.edu/~toh/spectrum/SimulatedSignal6Gaussian.xlsx}{SimulatedSignal6Gaussian.xlsx}, which computes and plots a simulated signal consisting of up to 6 overlapping Gaussian bands plus random white noise. \label{ref-0040}

\InsImage{0.8}{image23.png}\section{\href{http://en.wikipedia.org/wiki/MATLAB}{\label{ref-0041}Matlab} and \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{Octave}}

\href{http://en.wikipedia.org/wiki/MATLAB}{\textbf{Matlab}} and \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{\textbf{Octave}} have built-in functions that can be used for calculating, measuring and plotting signals and noise, including \href{https://terpconnect.umd.edu/~toh/spectrum/mean.txt}{mean}, \href{https://terpconnect.umd.edu/~toh/spectrum/max.txt}{max}, \href{https://terpconnect.umd.edu/~toh/spectrum/min.txt}{min}, \href{https://terpconnect.umd.edu/~toh/spectrum/std.txt}{std}, \href{https://terpconnect.umd.edu/~toh/spectrum/kurtosis.txt}{kurtosis}, \href{https://terpconnect.umd.edu/~toh/spectrum/skewness.txt}{skewness}, \href{https://terpconnect.umd.edu/~toh/spectrum/plot.txt}{plot}, \href{https://terpconnect.umd.edu/~toh/spectrum/hist.txt}{hist}, \href{https://terpconnect.umd.edu/~toh/spectrum/rand.txt}{rand}, and \href{https://terpconnect.umd.edu/~toh/spectrum/randn.txt}{randn}. Just type "help" and the function name at the command prompt, e.g., "help mean". \textit{Most of these functions apply to vectors and matrices as well as scalar variables}. For example, if you have a series of results in a vector variable 'y', \texttt{mean(y)} returns the average and \texttt{std(y)} returns the \href{https://en.wikipedia.org/wiki/Standard\_deviation}{standard deviation} of all the values in \texttt{y}. For vectors, \texttt{std} computes \texttt{sqrt(mean(y.\textasciicircum{}2))}. You can subtract a scalar number from a vector (for example, \texttt{\textbf{v}} \texttt{=} \texttt{\textbf{v}}\texttt{-min(}\texttt{\textbf{v}}\texttt{)} sets the lowest value of vector \textbf{v} to zero). If you have a set of signals in the rows of a matrix \textbf{S}, where each column represents the value of each signal at the same value of the independent variable (e.g., time), you can compute the ensemble average of those signals just by typing "\texttt{mean(}\texttt{\textbf{S}}\texttt{)}", which computes the mean of each column of \textbf{S}. Note that function and variable names are case-sensitive. (You can open the code for any function by selecting its name and selecting ``open..'').

As an example of the "randn" function in Matlab/Octave, it is used here to generate 100 normally-distributed random numbers, then the "hist" function computes the "histogram" (probability distribution) of those random numbers, then my \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{\textbf{peakfit.m}} function (\href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{download link}) fits a Gaussian function (\href{https://terpconnect.umd.edu/~toh/spectrum/RANDNGaussianFit.png}{plotted with a red line}) to that distribution. 

\InsImageInline{0.5}{l}{image24.png}  \texttt{[N,X]=hist(randn(size(1:100)));}

 \texttt{peakfit([X;N])}\texttt{;} 

If you change the 100 to 1000 or to a higher number, the distribution becomes \textit{closer and} \href{https://terpconnect.umd.edu/~toh/spectrum/RANDNGaussianFit1000.png}{\textit{closer} to a perfect Gaussian} and its peak falls closer to 0.00. Here is \href{https://terpconnect.umd.edu/~toh/spectrum/Histogram1000.mp4}{an MP4 animation} that demonstrates the gradual emergence of a Gaussian normal distribution as the number of ``randn'' samples increases from 2 to 1000. Note how many samples it takes before the normal distribution is well-formed. The "randn" function is useful in signal processing for predicting the uncertainty of measurements in the presence of random noise, for example by using the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Monte}{Monte Carlo} or the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#bootstrap}{bootstrap }methods that will be described in a \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Reliability}{later section} (pages \pageref{ref-0209}, \pageref{ref-0213}). (Note: In the PDF version of this book, you can select, copy, and paste, or select, drag, and drop, any of the single-line or multi-line code examples into the Matlab or Octave editor or directly into the command line and press \textbf{Enter} to execute it immediately. 

\section{The difference between scripts and functions\label{ref-0042}\label{ref-0043}}

If you find that you are writing the same series of Matlab commands repeatedly, consider writing a script or a function that will save your code to the computer so you can use it again easily without the danger of typographical errors or clumsy copying and pasting. It is extremely handy to \href{http://www.ugrad.cs.ubc.ca/~cs302/MatlabGuide/node11.html}{create your own user-defined scripts and functions} in Matlab or Octave to automate commonly used algorithms. 

Scripts and functions are just simple text files saved with the ".m" file extension to the file name. The difference between a script and a function is that a function definition begins with the word 'function'; a script is just any list of Matlab commands and statements. For a \textit{script}, all the variables defined and used are listed in the workspace window and shared with other scripts. For a \textit{function}, on the other hand, the variables are \textit{internal and private to that function}; values can be passed \textit{to} the function through the \textit{input} variables (also called \textit{arguments}), and values can be passed \textit{from} the function through the \textit{output} variables, which are both defined in the first line of the function definition. 


\begin{center}
\texttt{[output variables] = FunctionName(input variables)}
\end{center}


That means that functions are a great way to package chunks of code that perform useful operations in a form that can be used as components in \textit{other} scripts and functions \textit{without worrying that the internal variable names within the function will conflict and cause errors}. When you write a function, it is saved to the computer and can be called again on that computer, just like the built-in functions that came with Matlab. For an example of a very simple function, look at the code for \href{https://terpconnect.umd.edu/~toh/spectrum/rsd.m}{rsd.m}.

function relstddev=rsd(x)

\% Relative standard deviation of vector x

relstddev=std(x)./mean(x);

Scripts and functions can call other functions; scripts must have those functions in the Matlab path; functions, on the other hand, \textit{can have all their required sub-functions defined within the main function itself and thus can be self-contained}. If you write a script or function that calls one or more of your custom functions, and you send it to someone else, be sure to include all the custom functions that it calls. (It is best to make all your functions \textit{self-contained} with all required sub-functions included).

If you run one of my scripts and get an error message that says, "\texttt{Undefined function...}", you need to download the specified function from \url{http://tinyurl.com/cey8rwh} and place it in the Matlab/Octave path. Note: in Matlab R2016b or later, you CAN include functions within scripts; just place them at the end of the script and add an additional ``end'' statement to each function added. (see \url{https://www.mathworks.com/help/matlab/matlab_prog/local-functions-in-scripts.html}. 

For writing or editing scripts and functions, Matlab and the latest version of Octave have an internal editor. For an explanation of a function and a simple worked example, type ``help function'' at the command prompt. When you are writing your own functions or scripts, you should always add lots of "comment lines" (beginning with the character \%) that explain what is going on. \textit{You will be glad you did later}. The first group of comment lines, up to the first blank line that does not begin with a \%, are used as the "help file" for that script or function. Typing "help \_\_\_'', where \_\_\_ is the name of the function, displays the comment lines for that function or script in the command window, just as it does for the built-in functions and scripts. This will make your scripts and functions much easier to understand and use, both by other people and by yourself in the future. \textit{Resist the temptation to skip this}. As you develop custom functions for your own work, you will be developing a ``toolkit'' that will become very useful to your students or co-workers, or even to yourself in the future, \textit{if you use comments liberally}. 

Here is a very handy helper: when you type a function name into the Matlab editor, if you \textit{pause for a moment} after typing the open parenthesis immediately after the function name, Matlab will display a pop-up listing all the possible input variables as a reminder. \textit{This works even for downloaded functions and for any new functions that you yourself create}! It is especially handy when the function has so many possible input variables that it is hard to remember all of them. The popup \textit{stays on the screen as you type}, highlighting each variable in turn, to remind you where you are:

\InsImage{1.0}{FunctionPrompt.gif.png}This feature is easily overlooked, but it is very handy. Clicking on the little ``\uline{\textcolor{color-4}{More Help...''}} link on the right displays the help for that function in a separate window. Note: Octave does not have this feature.

\section{\href{https://terpconnect.umd.edu/~toh/spectrum/functions.html}{\label{ref-0044}User-defined functions} related to signals and noise.}

Here are some examples of my\href{https://terpconnect.umd.edu/~toh/spectrum/functions.html}{ user-defined functions} related to signals and noise for you to download and use. These are not built-in functions; you must download them and place them in the path.

\textbf{Data plotting}: \href{https://terpconnect.umd.edu/~toh/spectrum/plotit.m}{plotit.m}, an easy-to-use function for plotting and fitting x,y data in matrices or in separate vectors. For handling very large signals more easily, \href{https://terpconnect.umd.edu/~toh/spectrum/plotxrange.m}{plotxrange.m} (\texttt{[xx,yy,irange] = plotxrange(x,y,x1,x2})) extracts and plots values of vectors x,y only for x values between specified values of x; \href{https://terpconnect.umd.edu/~toh/spectrum/segplot.m}{segplot.m} (\texttt{[s,xx,yy] = segplot(x,y,NumSegs,seg})) divides signals into "NumSegs" equal-length segments and plots segments marked by vertical lines, each labeled with a small segment number at the bottom, and returns a vector 's' of segment indexes and the subset xx,yy, of values in the segment number 'seg'.

\textbf{Peak shapes}. Several functions for peak shapes commonly encountered in analytical chemistry such as \href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{Gaussian}, \href{https://terpconnect.umd.edu/~toh/spectrum/lorentzian.m}{Lorentzian}, \href{https://terpconnect.umd.edu/~toh/spectrum/lognormal.m}{lognormal}, \href{https://terpconnect.umd.edu/~toh/spectrum/pearson.m}{Pearson 5}, \href{https://terpconnect.umd.edu/~toh/spectrum/expgaussian.m}{exponentially-broadened Gaussian}, \href{https://terpconnect.umd.edu/~toh/spectrum/explorentzian.m}{exponentially-broadened Lorentzian}, \href{https://terpconnect.umd.edu/~toh/spectrum/exppulse.m}{exponential pulse}, \href{https://terpconnect.umd.edu/~toh/spectrum/sigmoid.m}{sigmoid}, \href{https://terpconnect.umd.edu/~toh/spectrum/GL.m}{Gaussian/Lorentzian blend}, \href{https://terpconnect.umd.edu/~toh/spectrum/BiGaussian.m}{bifurcated Gaussian}, \href{https://terpconnect.umd.edu/~toh/spectrum/BiLorentzian.m}{bifurcated Lorentzian}), \href{https://terpconnect.umd.edu/~toh/spectrum/voigt.m}{Voigt profile}, \href{https://terpconnect.umd.edu/~toh/spectrum/triangle.m}{triangular} and others. See page \pageref{ref-0501}.

\href{https://terpconnect.umd.edu/~toh/spectrum/peakfunction.m}{peakfunction.m}, a function that generates any of those peak types specified by number. 

\href{https://terpconnect.umd.edu/~toh/spectrum/ShapeDemo.m}{ShapeDemo} demonstrates the 12 basic peak shapes \href{https://terpconnect.umd.edu/~toh/spectrum/ShapeDemo.png}{graphically}, showing the variable-shape peaks as multiple lines. (graphic on page \pageref{ref-0469})

\textbf{Noise generators}. There are several functions for simulating different types of random noise (\href{https://terpconnect.umd.edu/~toh/spectrum/whitenoise.m}{white noise}, \href{https://terpconnect.umd.edu/~toh/spectrum/pinknoise.m}{pink noise}, \href{https://terpconnect.umd.edu/~toh/spectrum/bluenoise.m}{blue noise}, \href{https://terpconnect.umd.edu/~toh/spectrum/propnoise.m}{proportional noise}, and \href{https://terpconnect.umd.edu/~toh/spectrum/sqrtnoise.m}{square root noise}).

\textbf{Miscellaneous functions:}

\href{https://terpconnect.umd.edu/~toh/spectrum/stdev.m}{stdev.m}, a standard deviation function that works in both Matlab and in Octave (the built-in std.m function behaves differently in Matlab and Octave); \href{https://terpconnect.umd.edu/~toh/spectrum/rsd.m}{rsd.m}, the \textit{relative} standard deviation. 

\href{https://terpconnect.umd.edu/~toh/spectrum/PercentDifference.m}{PercentDifference.m}, simply calculates the percent difference between two variables.

\href{http://terpconnect.umd.edu/~toh/spectrum/IQrange.m}{IQrange.m} computes the interquartile range (explained above).

\href{https://terpconnect.umd.edu/~toh/spectrum/halfwidth.m}{halfwidth.m} for measuring the full width at half maximum of smooth peaks. 

\href{https://terpconnect.umd.edu/~toh/spectrum/ExpBroaden.m}{E}\href{https://terpconnect.umd.edu/~toh/spectrum/ExpBroaden.m}{xpBroaden.m} applies exponential broadening to any time-series vector.

\href{https://terpconnect.umd.edu/~toh/spectrum/rmnan.m}{rmnan.m} removes "not-a-number" entries from vectors, which is useful for cleaning up real data files; \href{https://terpconnect.umd.edu/~toh/spectrum/rmz.m}{rmz.m} removes zeros from vectors, replacing with nearest non-zero numbers.

\href{https://terpconnect.umd.edu/~toh/spectrum/val2ind.m}{val2ind.m} returns the index and the value of the element of vector x that is closest to a particular value. This is a simple function that is more useful than you might imagine. Search this document for ``val2ind'' to find several examples of the practical use of this function. 

These functions are useful in modeling and simulating analytical signals and testing measurement techniques. In the PDF version of this book, you can click or ctrl-click on these links to inspect the code or you can right-click and select "Save link as..." to download them to your computer. Once you have downloaded those functions and placed them in the "path", you can use them just like any other built-in function. For example, you can plot a simulated Gaussian peak with white noise by typing \texttt{x=[1:256]; y=gaussian(x,128,64) + whitenoise(x); plot(x,y)}\texttt{.} The script \href{https://terpconnect.umd.edu/~toh/spectrum/plotting.m}{plotting.m}, shown in the figure on page \pageref{ref-0015}, uses the \href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian.m} function to demonstrate the distinction between the \textit{height}, \textit{position}, and \textit{width} of a Gaussian curve. The script \href{https://terpconnect.umd.edu/~toh/spectrum/SignalGenerator.m}{SignalGenerator.m} calls several of these downloadable functions to create and plot a realistic computer-generated signal with multiple peaks on a variable baseline plus variable random noise; you might try to modify the variables in the indicated places to make it look like your type of data. All these functions will work in the latest version of \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{Octave} without change. For a complete list of my downloadable functions and scripts developed for this project, see page \pageref{ref-0498} or on the Web at \url{http://tinyurl.com/cey8rwh}.

The Matlab/Octave function \href{https://terpconnect.umd.edu/~toh/spectrum/noisetest.m}{noisetest.m} demonstrates the appearance and effect of different noise types. It plots Gaussian peaks with four different types of added noise: constant white noise, constant pink (1/f) noise, proportional white noise, and square root white noise, then fits a Gaussian to each noisy data set and computes the average and the standard deviation of the peak height, position, width, and area for each noise type. Type "help noisetest" at the command prompt. My Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/SubtractTwoMeasurements.m}{SubtractTwoMeasurements.m} (page \pageref{ref-0025}) demonstrates the technique of subtracting two separate measurements of a waveform to extract the random noise (but it works only if the signal is stable, except for the noise). 

\href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{\textbf{\textit{iSignal}}} (page \pageref{ref-0433}) is one of a group of multi-purpose downloadable Matlab modules I have developed that combine many of the techniques covered here; \textit{iSignal} can plot signals with pan and zoom controls, measure signal and noise amplitudes in selected regions of the signal and compute the S/N ratio of peaks. It is operated by simple key presses. Other capabilities of iSignal include smoothing (page \pageref{ref-0045}), differentiation, peak sharpening, and least-squares peak measurement, etc. 

Others in the group include \textit{iPeak}, page \pageref{ref-0321}, which focuses on peak detection, and \textit{ipf.m}, page \pageref{ref-0465}, which focuses on iterative curve fitting. These functions are ideal for initial explorations of complex signal because they make it easy to select operations and adjust the controls by simple key presses. These interactive modules work even if you run \href{https://www.mathworks.com/products/matlab-online.html}{Matlab Online in a web browser}, but they do not work on \href{https://itunes.apple.com/us/app/matlab-mobile/id370976661?mt=8}{Matlab Mobile} nor, at the time of this writing, in Octave.

For signals that contain repetitive waveform patterns occurring in one continuous signal, with nominally the same shape except for noise, the interactive peak detector function \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#ipeak}{\textit{iPeak} }(page \pageref{ref-0320}), has an \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#EnsembleAveraging}{ensemble averaging function} (\textbf{Shift-E}) can compute the average of all the repeating waveforms. It works by detecting a single reference peak in each repeat waveform to synchronize the repeats (and therefore does not require that the repeats be equally spaced or synchronized to an external reference signal). To use this function, first adjust the peak detection controls to detect \textit{only one peak in each repeat pattern}, zoom in to isolate any one of those repeat patterns, and then press \textbf{Shift-E}. The average waveform is displayed in Figure 2 and saved as ``EnsembleAverage.mat'' in the current directory. See \href{https://terpconnect.umd.edu/~toh/spectrum/iPeakEnsembleAverageDemo.m}{iPeakEnsembleAverageDemo.m} for a demonstration. See page \pageref{ref-0393}: \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#SNR}{\textit{Measuring the Signal-to-Noise Ratio of Complex Signals}} for more examples of the signal-to-ratio in Matlab/Octave computations. \label{ref-0045}\label{ref-0046}\label{ref-0047}

\chapter{Smoothing\label{ref-0048}\label{ref-0049}}

In many experiments in science, the true signal amplitudes (y-axis values) change rather smoothly as a function of the x-axis values, whereas many kinds of noise are seen as rapid, random changes in amplitude from point to point within the signal. In the latter situation it may be useful in some cases to attempt to reduce the noise by a process called \textit{smoothing}. In smoothing, the data points of a signal are modified so that individual points that are \textit{higher} than the immediately adjacent points (presumably because of noise) are reduced, and points that are \textit{lower} than the adjacent points are increased. This naturally leads to a smoother signal (and a slower step response to signal changes). If the true underlying signal is smooth, then the true signal will not be much distorted by smoothing, but the high-frequency noise will be reduced. In terms of the \href{https://terpconnect.umd.edu/~toh/spectrum/HarmonicAnalysis.html\#smoothing}{frequency components} of a signal, a smoothing operation acts as a \href{https://en.wikipedia.org/wiki/Low-pass\_filter}{low-pass filter}, reducing the high-frequency components and passing the low-frequency components with little change. If the signal and the noise is measured for all frequencies, then the \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#SNR}{signal-to-noise ratio} will be improved by smoothing, by an amount that depends on the \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#frequency}{frequency distribution} of the noise. (Smoothing can be contrasted to \textit{wavelet denoising}, pages \pageref{ref-0170} and \pageref{ref-0080}, which also reduces noise but does not necessarily make the signal completely smooth). 

\section{ Smoothing algorithms\label{ref-0050}\label{ref-0051}}

The simplest smoothing algorithms are based on the "\textit{shift and multiply}" technique, in which a group of adjacent points in the original data is multiplied point-by-point by a set of numbers (coefficients) that defines the smooth shape, the products are added up and divided by the sum of the coefficients, which becomes one point of smoothed data, then the set of coefficients is shifted one point along the original data and the process is repeated. The \href{http://www.analog.com/media/en/technical-documentation/dsp-book/dsp\_book\_Ch15.pdf}{simplest smoothing algorithm} is the \textit{rectangular boxcar} or \textit{unweighted sliding-average smooth}; it simply replaces each point in the signal with the average of \textit{m} adjacent points, where \textit{m} is a positive integer called the \textit{smooth width}. For example, for a 3-point smooth (\textit{m} = 3): 

  \begin{center}$S_{j} = \frac{Y_{j-1} + Y_{j} + Y_{j+1}}{3}$\end{center} 

This is evaluated for j = 2 to \textit{n}-1, where S\raisebox{-4pt}{j} is the j\raisebox{4pt}{th} point in the smoothed signal, Y\raisebox{-4pt}{j} is the j\raisebox{4pt}{th} point in the original signal, and \textit{n} is the total number of points in the signal. Most spreadsheets and programming languages have a ``mean'' or ``average'' function which can do this work quickly, so S\textsubscript{j}=mean(y\textsubscript{j-w/2}:y\textsubscript{j+w/2}). Similar smooth operations can be constructed for any desired smooth width, \textit{m}. Usually \textit{m} is an odd number. If the noise in the data is "white noise" (that is, evenly distributed over all frequencies) and its standard deviation is \textit{D}, then the standard deviation of the noise remaining in the signal after the first pass of an unweighted sliding-average smooth will be approximately D over the square root of \textit{m} (\textit{D}/sqrt(\textit{m})), where \textit{m} is the smooth width. Despite its simplicity, \href{https://www.dspguide.com/ch15/2.htm}{this smooth is actually optimum} for the common problem of reducing white noise while keeping the \textit{sharpest step response}. The response to a step change is, in fact, \textit{linear}, so this filter has the advantage of responding completely with no residual effect within its \textit{response time}, which is equal to the smooth width divided by the sampling rate. Smoothing can be performed either \textit{during} data acquisition, by programming the digitizer to measure and to average multiple readings and save only the average, or \textit{after} data acquisition ("post-run"), by storing all the acquired data in memory and smoothing the stored data. The latter requires more memory but is more flexible.

The \textit{triangular smooth} is like the rectangular smooth, above, except that it implements a \textit{weighted} smoothing function. For a 5-point smooth (\textit{m} = 5):



\begin{center}$S_{j} = \frac{Y_{j-2} + 2Y_{j-1} + 3Y_{j} + 2Y_{j+1} + Y_{j+2}}{9}$\end{center}

for j = 3 to n-2, and similarly for other smooth widths (see the spreadsheet \href{https://terpconnect.umd.edu/~toh/spectrum/UnitGainSmooths.xls}{UnitGainSmooths.xls}). In both of these cases, the integer in the denominator is the \textit{sum of the coefficients} in the numerator, which results in a ``unit-gain'' smooth that has no effect on the signal where it is a straight line and which preserves the \href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html}{area under peaks}. 

It is often useful to apply a smoothing operation more than once, that is, to smooth an already smoothed signal, to build longer and more complicated smooths. For example, the 5-point triangular smooth above is equivalent to two passes of a 3-point rectangular smooth. \textit{Three} passes of a 3-point rectangular smooth result in a 7-point \textit{haystack} smooth, also called a p-spline, for which the coefficients are in the ratio 1:3:6:7:6:3:1. The general rule is that \textit{n} passes of a \textit{w}-width smooth results in a combined smooth width of \textit{n}*\textit{w}-\textit{n}+1. For example, 3 passes of a 17-point smooth results in a 49-point smooth. These multi-pass smooths are more effective at reducing high-frequency noise in the signal than a rectangular smooth, but they exhibit a slower step response.

In all these smooths, the width of the smooth \textit{m} is chosen to be an odd integer, so that the smooth coefficients are symmetrically balanced around the central point, which is important because it \textit{preserves the x-axis position of peaks} \textit{and other features} in the smoothed signal. (This is especially critical for analytical and spectroscopic applications because the peak positions are often important measurement objectives.

Note that we are assuming here that the x-axis interval of the signal is uniform, that is, that the difference between the x-axis values of adjacent points is the same throughout the signal. This is also assumed in many of the other signal-processing techniques described in this book, and it is a very common (but not necessary) characteristic of signals that are acquired by automated and computerized equipment. 

The \href{http://en.wikipedia.org/wiki/Savitzky\%96Golay\_smoothing\_filter}{\textit{Savitzky-Golay}} smooth is based on the least-squares fitting of polynomials to segments of the data. The algorithm is discussed on \href{https://en.wikipedia.org/wiki/Savitzky\%E2\%80\%93Golay\_filter}{Wikipedia}. Compared to the sliding-average smooths of the same width, the Savitzky-Golay smooth is less effective at reducing noise, but more effective at retaining the shape of the original signal. It is capable of \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html}{differentiation} as well as smoothing. The algorithm is more complex, and the computational times may be greater than the smooth types discussed above, but with modern computers, the difference is usually not significant. \href{https://www.google.com/search?sourceid=chrome&ie=UTF-8&q=Savitzky-Golay+smooth+code}{Code in various languages is widely available online}. See \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothingComparison.html}{page} \pageref{ref-0075}.

The shape of any smoothing algorithm can be determined by applying that smooth to a \textit{delta function}, a signal consisting of all zeros except for one point, as demonstrated by the simple Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/DeltaTest.m}{DeltaTest.m}. The result is called the \textit{impulse response function}. 

\section{Noise reduction\label{ref-0052}}

 Smoothing usually reduces the noise in a signal. If the noise is "white" (that is, evenly distributed over all frequencies) and its standard deviation is \textit{D}, then the standard deviation of the noise remaining in the signal after one pass of a rectangular smooth will be approximately \textit{D}/sqrt(\textit{m}), where \textit{m} is the smooth width. If a triangular smooth is used instead, the noise will be slightly less, about \textit{D}*0.8/sqrt(\textit{m}). Smoothing operations can be applied more than once: that is, a previously smoothed signal can be smoothed again. In some cases, this can be useful if there is a great deal of \textit{high}-frequency noise in the signal. However, the noise reduction for \textit{white} noise is less in each successive smooth. For example, \textit{three} passes of a rectangular smooth reduce white noise by a factor of approximately \textit{D}*0.7/sqrt(\textit{m}), only a slight improvement over two passes. For a spreadsheet demonstration, see \href{https://terpconnect.umd.edu/~toh/spectrum/VariableSmoothNoiseReduction.xlsx}{VariableSmoothNoiseReduction.xlsx}.

\section{Effect of the frequency distribution of noise\label{ref-0053}\label{ref-0054}\label{ref-0055}\label{ref-0056}}

\InsImage{1.0}{NoiseColorTest2small.png}The frequency distribution of noise, designated by \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#Frequency}{noise ``color}'' (page \pageref{ref-0020}), substantially affects the ability of smoothing to reduce noise. The Matlab/Octave function ``\href{https://terpconnect.umd.edu/~toh/spectrum/NoiseColorTest.m}{NoiseColorTest.m}'' compares the effect of a 20-point boxcar (unweighted sliding average) smooth on the standard deviation of white, pink, red, and blue noise, all of which have an \href{https://terpconnect.umd.edu/~toh/spectrum/NoiseColorTest1.png}{original unsmoothed standard deviation of 1.0}. Because smoothing is a low-pass filter process, it affects low-frequency (pink and red) noise \textit{less}, and effects high-frequency (blue and violet) noise \textit{more}, than it does white noise.


\begin{table}
\begin{tabularx}{\textwidth}{|
p{\dimexpr 0.823\linewidth-2\tabcolsep-2\arrayrulewidth}|
p{\dimexpr 0.177\linewidth-2\tabcolsep-\arrayrulewidth}|} \hline 
\centering\arraybackslash{}Original unsmoothed noise & \centering\arraybackslash{}1 \\\hline 
\centering\arraybackslash{}Smoothed \textbf{white} noise & \centering\arraybackslash{}0.1 \\\hline 
\centering\arraybackslash{}Smoothed \textbf{pink} noise & \centering\arraybackslash{}0.55 \\\hline 
\centering\arraybackslash{}Smoothed \textbf{blue} noise & \centering\arraybackslash{}0.01 \\\hline 
\centering\arraybackslash{}Smoothed \textbf{red} (\href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#RandomWalk}{random walk}) noise & \centering\arraybackslash{}0.98 \\\hline 
\end{tabularx}
\end{table}
Note that the computation of standard deviation is independent of the order of the data and thus of its frequency distribution; sorting a set of data does not change its standard deviation. The standard deviation of a sine wave is independent of its frequency. Smoothing, however, changes both the frequency distribution and standard deviation of a data set.

\section{End effects and the lost points problem\label{ref-0057}}

In the equations above, the 3-point rectangular smooth is defined only for j = 2 to n-1. There is not enough data in the signal to define a complete 3-point smooth for the first point in the signal (j = 1) or for the last point (j = n), because there are no data points before the first point or after the last point. (Similarly, a 5-point smooth is defined only for j = 3 to n-2, and therefore a smooth cannot be calculated for the first two points or for the last two points). In general, for an \textit{m}-width smooth, there will be (\textit{m}-1)/2 points at the beginning of the signal and (\textit{m}-1)/2 points at the end of the signal for which a complete \textit{m}-width smooth cannot be calculated the usual way. What to do? There are two approaches. One is to accept the loss of points and trim off those points or replace them with zeros in the smooth signal. (That's the approach taken in most of the figures in this paper). The other approach is to use \textit{progressively smaller smooths} at the ends of the signal, for example to use smooth widths of 2, 3, 5, 7... points for signal points 1, 2, 3, and 4..., and for points n, n-1, n-2, n-3..., respectively. The latter approach may be preferable if the edges of the signal contain critical information, but it increases execution time. The Matlab/Octave \href{https://terpconnect.umd.edu/~toh/spectrum/fastsmooth.m}{\textit{fastsmooth}} function (page \pageref{ref-0507}) can utilize either of these two methods.

\section{Examples of smoothing\label{ref-0058}}

\InsImageInline{0.5}{l}{Figure4.GIF.png}The figure on the left shows a simple example of smoothing. The left half of this signal is a noisy peak. The right half is the same peak after undergoing a triangular smoothing algorithm. The noise is greatly reduced while the peak itself is hardly changed. The reduced noise allows the signal characteristics (peak position, height, width, area, etc.) to be measured more accurately by visual inspection.

 \textit{The left half of this signal is a noisy peak. The right half is the same peak after undergoing a} \textbf{\textit{smoothing}} \textit{algorithm. The noise is greatly reduced while the peak itself is hardly changed, making it easier to measure the peak position, height, and width directly by graphical or visual estimation (but it does not improve measurements made by least-squares methods;} \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#NOT\_smooth}{see below}\textit{).} 

The larger the smooth width, the greater the noise reduction, but also the greater the possibility that the signal will be \textit{distorted} by the smoothing operation. The optimum choice of smooth width depends upon the width and shape of the signal and the digitization interval. For peak-type signals, the critical factor is the \textit{smooth ratio}, the ratio between the smooth width \textit{m} and the number of points in the half-width of the peak. In general, increasing the smoothing ratio improves the signal-to-noise ratio but causes a reduction in amplitude and an increase in the width of the peak. Be aware that the smooth width can be expressed in two different ways: (a) as the number of data points or (b) as the x-axis interval (for spectroscopic data usually in nm or in frequency units). The two are simply related: the number of data points is simply the x-axis interval times the increment between adjacent x-axis values. The \textit{smooth ratio} is the same in either case.

\InsImage{0.5}{s7s25s51.GIF.png}\InsImage{0.5}{s72551.GIF.png}The figures here show examples of the effect of three different smooth widths on noisy Gaussian-shaped peaks. In the figure on the left, the peak has a true height of 2.0 and there are 80 points in the half-width of the peak. The red line is the original unsmoothed peak. The three superimposed green lines are the results of smoothing this peak with a triangular smooth of width (from top to bottom) 7, 25, and 51 points. Because the peak width is 80 points, the \textit{smooth ratios} of these three smooths are 7/80 = 0.09, 25/80 = 0.31, and 51/80 = 0.64, respectively. As the smooth width increases, the noise is progressively reduced but the peak height also is reduced slightly. For the largest smooth, the peak \textit{width} is noticeably increased. In the figure on the right, the original peak (in red) has a true height of 1.0 and a half-width of 33 points. (It is also less noisy than the example on the left.) The three superimposed green lines are the results of the \textit{same} three triangular smooths of width 7, 25, and 51 points. But because the peak width, in this case, is only 33 points, the \textit{smooth ratios} of these three smooths are \textit{larger} - 0.21, 0.76, and 1.55, respectively. You can see that the peak distortion effect (reduction of peak height and increase in peak width) is greater for the narrower peak because the smooth ratios are higher. Smooth ratios of greater than 1.0 are seldom used because of excessive peak distortion. Note that even in the worst case, the peak positions are not affected (assuming that the original peaks were symmetrical and not overlapped by other peaks). If retaining the shape of the peak is more important than optimizing the signal-to-noise ratio, the Savitzky-Golay has the advantage over sliding-average smooths. In all cases, the total \href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html}{area under the peak} remains unchanged. If the peak widths vary substantially, an \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#SegmentedSmooth}{adaptive smooth}, which allows the smooth width to vary across the signal, may be used.

\section{The problem with smoothing \label{ref-0059}}

Smoothing \textit{is often less beneficial than you might think}. It is important to point out that smoothing results such as illustrated in the figures above could be viewed as \textit{deceptively impressive} because they employ a \textit{single sample} of a noisy signal that is smoothed to different degrees. This causes the viewer to underestimate the contribution of \textit{low-frequency} noise, which is hard to estimate visually because there are \textit{so few low-frequency cycles} in the signal record. This problem can be visualized by recording a few independent samples of a noisy signal consisting of a single peak, as illustrated in the two figures below.


\begin{table}
\begin{tabularx}{\textwidth}{|
p{\dimexpr 0.492\linewidth-2\tabcolsep-2\arrayrulewidth}|
p{\dimexpr 0.508\linewidth-2\tabcolsep-\arrayrulewidth}|} \hline 
\centering\arraybackslash{}\InsImage{1.0}{10unsmoothed.png} & \centering\arraybackslash{}\InsImage{1.0}{10smoothed.png} \\\hline 
\texttt{x=1:1000;} \par \texttt{for n=1:10,} \par \texttt{y(n,:)=2.*gaussian(x,500,150){\ldots} +whitenoise(x);} \par \texttt{end} \par \texttt{plot(x,y)} & x\texttt{=1:1000;} \par \texttt{for n=1:10,} \par \texttt{y(n,:)=2.*gaussian(x,500,150)+whitenoise(x);} \par  \texttt{y(n,:)=fastsmooth(y(n,:),50,3);} \par \texttt{end} \par \texttt{plot(x,y)} \\\hline 
\end{tabularx}
\end{table}
These figures show ten superimposed plots with the same peak but with independent white noise, each plotted with a different line color, unsmoothed on the left and smoothed on the right. Clearly, the noise reduction is substantial, but close inspection of the smoothed signals on the right shows that there is still variation in peak position, height, and width between the 10 samples, which is caused by the low-frequency noise remaining in the smoothed signals. Without the noise, each peak would have a peak height of 2, peak center at 500, and a width of 150. Just because a signal looks smooth does not mean there is no noise. Low-frequency noise remaining in the signals after smoothing can still interfere with the precise measurement of peak position, height, and width.\label{ref-0060}

(The generating scripts below each figure require that the functions gaussian.m, whitenoise.m, and fastsmooth.m be downloaded from \url{http://tinyurl.com/cey8rwh}.)

It should be clear that smoothing can seldom eliminate noise \textit{completely}, because most noise is spread out over a range of frequencies and smoothing simply reduces the noise in \textit{part} of its frequency range. Only for some very specific types of noise (e.g., discrete frequency sine-wave noise or single-point spikes) is there hope of anything close to complete noise elimination. Smoothing \textit{does} make the signal smoother and \textit{it does} reduce the standard deviation of the noise, but whether that makes for a \textit{better measurement} or not depends on the situation. And do not assume that just because a little smoothing is good that more will necessarily be better. Smoothing is like alcohol; sometimes you really need it - but you should never overdo it.

The figure on the right below is another example of a signal that illustrates some of these principles. The signal consists of two Gaussian peaks, one located at x=50 and the second at x=150. Both peaks have a peak height of 1.0 and a peak half-width of 10, and the same normally distributed random white noise with a standard deviation of 0.1 has been added to the entire signal. The \textit{x-axis sampling interval}, \InsImageInline{0.5}{l}{udx10noise.png}however, is different for the two peaks: it is 0.1 for the first peak from x=0 to 100) and 1.0 for the second peak (from x=100 to 200). This means that the first peak is characterized by \textit{ten times more points} than the second peak. It may \textit{look} like the first peak is noisier than the second, but that is just an illusion; the signal-to-noise ratio for both peaks is 10. The second peak looks less noisy only because there are fewer noise samples there and we tend to underestimate the dispersion of small samples. The result of this is that when the signal is smoothed, the \textit{second peak} is much more likely to be distorted by the smooth (it becomes shorter and wider) than the first peak. The first peak can tolerate a much wider smooth width, resulting in a greater degree of noise reduction. Similarly, if both peaks are measured with the \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{least-squares curve fitting} method to be covered later, the \href{https://terpconnect.umd.edu/~toh/spectrum/udx10noiseAnimation.gif}{fit of the first peak is more stable} with the noise and the measured parameters of that peak will be about \textit{3 times more accurate} than the second peak, because there are 10 times more data points in that peak, and the measurement precision improves roughly with the square root of the number of data points if the noise is white. You can download this data file, "udx", in \href{https://terpconnect.umd.edu/~toh/spectrum/udx.txt}{TXT format} or in Matlab \href{https://terpconnect.umd.edu/~toh/spectrum/udx.mat}{MAT format}.

\section{Optimization of smoothing\label{ref-0061}}

\InsImageInline{0.6}{l}{SmoothWidthTest.gif.png}
As smooth width increases, the smoothing ratio increases, noise is reduced quickly at first, then more slowly, and the peak height is also reduced, slowly at first, then more quickly. The \textit{noise reduction} depends on the smooth width, the smooth type (e.g., rectangular, triangular, etc.), and the noise color, but the \textit{peak height reduction} also depends on the peak width. The result is that the signal-to-noise (defined as the ratio of the peak height of the standard deviation of the noise) increases quickly at first, then reaches a maximum. This is illustrated in the graphic on the left which shows the result of smoothing a \textit{Gaussian peak plus white noise} (produced by this \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothWidthTest.m}{Matlab/Octave script}). The maximum improvement in the signal-to-noise ratio depends on the number of points in the peak: the more points in the peak, the greater smooth widths can be employed and the greater the noise reduction. This figure also illustrates that most of the noise reduction is due to \textit{high-frequency} components of the noise, whereas much of the \textit{low-}frequency noise remains in the signal even as it is smoothed.

Which is the best smooth ratio? It depends on the purpose of the peak measurement. If the ultimate objective of the measurement is to measure the peak height or width, then smooth ratios below 0.2 should be used and the \href{http://en.wikipedia.org/wiki/Savitzky\%96Golay\_smoothing\_filter}{\textit{Savitzky-Golay}} (or wavelet denoise: see page \pageref{ref-0177}) smooth is preferred. But if the objective of the measurement is to measure the peak position (x-axis value of the peak), larger smooth ratios can be employed if desired, because smoothing has little effect on the peak position (unless peak is asymmetrical or the increase in peak width is so much that it causes adjacent peaks to overlap). If the peak is actually formed of two underlying peaks that overlap so much that they appear to be one peak, then \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#FittingPeaks}{curve fitting} is the only way to measure the parameters of the underlying peaks. Unfortunately, the optimum signal-to-noise ratio corresponds to a smooth ratio that significantly distorts the peak, which is why curve fitting the unsmoothed data is often the preferred method for measuring peaks position, height, and width. The peak \textit{area} is not changed by a properly constructed smoothing operation unless it changes your estimate of the beginning and the ending of the peak.

In \textit{quantitative chemical analysis} applications based on calibration by standard samples, the peak height reduction caused by smoothing is not so important. If the \textit{same} signal processing operations are applied to the samples and to the standards, the peak height reduction of the standard signals will be \textit{the same} as that of the sample signals and the effect will \textit{cancel out} exactly. In such cases, smooth widths from 0.5 to 1.0 can be used if necessary, to further improve the signal-to-noise ratio, as shown in the figure on the previous page (for a simple sliding-average rectangular smooth). In practical analytical chemistry, absolute peak height measurements are seldom required; calibration against standard solutions is the rule. (Remember: the objective of quantitative analysis is not to measure a signal but rather to measure the concentration of the unknown.) It is very important, however, to apply \textit{the same} signal processing steps to the standard signals as to the sample signals, otherwise a large systematic error will result.

For a more detailed comparison of all four smoothing types considered above, see \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothingComparison.html}{page} \pageref{ref-0076}.

\section{When should you smooth a signal? \label{ref-0062}}

There are four reasons to smooth a signal:\begin{enumerate}[{(a)}]


\item for cosmetic reasons, to prepare a nicer-looking or more dramatic graphic of a signal for visual inspection or publication, especially in order to emphasize \textit{long-term} behavior over \textit{short-term}, or

\item \textbf{(b)} If the signal contains mostly \textit{high-frequency} ("blue") noise, which can look bad but has less effect on the low-frequency signal components (e.g. the positions, heights, widths, and areas of peaks) than white noise, or

\item \textbf{(c)} if the signal will be subsequently analyzed by a method that would be degraded by the presence of too much noise in the signal, for example, if the heights of peaks are to be determined \textit{visually or graphically} or by using the MAX function, of the widths of peaks is measured by the \href{https://terpconnect.umd.edu/~toh/spectrum/halfwidth.m}{halfwidth }function, or if the location of maxima, minima, or inflection points in the signal is to be determined automatically by detecting zero-crossings in \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html}{derivatives} of the signal. Optimization of the amount and type of smoothing is important in these cases (see \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html\#Smoothing}{page} \pageref{ref-0053}). Generally, if a computer is available to make quantitative measurements, it is better to use \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html}{least-squares methods} on the \textit{unsmoothed} data, rather than graphical estimates on smoothed data. If a commercial instrument has the option to smooth the data for you, it is best to disable the smoothing and record and save the \textit{unsmoothed} data; you can always smooth it yourself later for visual presentation and it will be better to use the unsmoothed data for a least-squares fitting or other processing that you may want to do later. Smoothing can be used to \textit{locate peaks,} but it should not be used to \textit{measure peaks}. 

(d) The formal limit of detection and limit of quantification of an analytical method (\href{https://www.longdom.org/open-access/about-estimating-the-limit-of-detection-by-the-signal-to-noise-approach-2153-2435-1000355.pdf}{references 91, 92}) may be improved by smoothing or averaging, depending on the method of signal measurement, as was described on page \pageref{ref-0029} and demonstrated by the Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/SNRdemo.m}{SNRdemo.m}.

\end{enumerate}
You must use care in the design of algorithms that employ smoothing. For example, in a popular technique for \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{peak finding and measurement} discussed later (page \pageref{ref-0290}), peaks are located by detecting downward zero-crossings in the \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html}{\textit{smoothed} first derivative}, but the position, height, and width of each peak is determined by \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html}{least-squares curve-fitting} (page \pageref{ref-0229}) of a segment of original \textit{unsmoothed} data in the vicinity of the zero-crossing (page \pageref{ref-0304}), rather than simply taking the maximum of the smoothed data. That way, even if heavy smoothing is necessary to provide reliable discrimination against noise peaks, the peak parameters extracted by curve fitting are not distorted by the smoothing.

\section{When should you NOT smooth a signal? \label{ref-0063}\label{ref-0064}}

One common situation where \href{http://wmbriggs.com/blog/?p=195}{you should }\href{http://wmbriggs.com/blog/?p=195}{\textbf{\textit{not}}}\href{http://wmbriggs.com/blog/?p=195}{ smooth signals is prior to statistical procedures} such as \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Smoothing}{least-squares curve-fitting}. There are several reasons (reference 43).

\textbf{(a)} Smoothing will not significantly improve the accuracy of parameter measurement by least-squares measurements between separate independent signal samples.

\textbf{(b)} All smoothing algorithms are at least slightly "lossy", entailing at least some change in signal shape and amplitude. 

\textbf{(c)} It is harder to evaluate the fit by inspecting the residuals if the data are smoothed, because \textit{smoothed noise may be mistaken for an actual signal}.

\textbf{(d)} Smoothing the signal will seriously underestimate the parameter errors predicted by the algebraic \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Algebraic}{propagation-of-error calculations} and by the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#bootstrap}{\textit{bootstrap} method} (page \pageref{ref-0213}). Even a visual estimate of the quality of the signal is compromised by smoothing, which makes the signal look better than it really is.

\section{Dealing with spikes and outliers. \label{ref-0065}}

Sometimes signals are contaminated with very tall, narrow ``spikes'' or "outliers" occurring at random intervals and with random amplitudes, but with widths of only one or a few points. It not only looks ugly, but it also upsets the assumptions of least-squares computations because it is not \textit{normally distributed} random noise. This type of interference is difficult to eliminate using the above smoothing methods without distorting the signal. However, a ``median'' filter, which replaces each point in the signal with the \href{https://en.wikipedia.org/wiki/Median}{\textit{median}} (rather than the \textit{average}) of \textit{m} adjacent points, can completely eliminate narrow spikes, with little change in the signal, if the width of the spikes is only one or a few points and equal to or less than \textit{m}. See \url{http://en.wikipedia.org/wiki/Median_filter}. 

A different approach is used by my \href{https://terpconnect.umd.edu/~toh/spectrum/killspikes.m}{killspikes.m} function; it locates and eliminates the spikes by "patching over them" using linear interpolation from the signal points immediately before and after the spike. See page \pageref{ref-0074} for details. Unlike conventional smooths, these functions can be profitably applied \textit{prior} to least-squares fitting functions. (On the other hand, if the \textit{spikes themselves} are the signal of interest, and the other components of the signal are interfering with their measurement, see page \pageref{ref-0358}). 

\section{Ensemble Averaging\label{ref-0066}}

Another way to reduce noise in repeatable signals, such as the \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#limits}{set of ten unsmoothed signals} on page \pageref{ref-0060}, is simply to compute their average, called \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#EnsembleAveraging}{\textit{ensemble averaging}}, which can be performed in this case very simply by the Matlab/Octave code \textbf{plot(x, mean(y));} \href{https://terpconnect.umd.edu/~toh/spectrum/10EnsembleAverage.png}{the result} shows a reduction in white noise by about sqrt(10)=3.2. This improves the signal-to-noise ratio enough to see that there is a single peak with Gaussian shape, which can then be measured by curve fitting (covered in a \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{later section}, page \pageref{ref-0258}) using the Matlab/Octave code \textbf{peakfit([x; mean(y)],0,0,1)}, with \href{https://terpconnect.umd.edu/~toh/spectrum/10EnsembleAverageCurveFit.png}{the result} showing excellent agreement with the position (500), height (2), and width (150) of the Gaussian peak created in the third line of the generating script (on page \pageref{ref-0060}). A huge advantage of ensemble averaging is that the \textit{noise at all frequencies} \textit{is reduced}, not just the \textit{high}-frequency noise as in smoothing. This is a big advantage if either the signal or the baseline drift.

\section{Condensing oversampled signals\label{ref-0067}}

Sometimes signals are recorded more densely (that is, with higher sampling frequency or with smaller x-axis intervals) than necessary to capture all the important features of the signal. This results in larger-than-necessary data sizes, which slows down signal processing procedures and may tax storage capacity. To correct this, oversampled signals can be reduced in size either by eliminating data points (say, dropping every second point or every third point) or better by replacing groups of adjacent points by their \textit{averages}. This is often called \textit{bunching}. It has the advantage of \textit{using} rather than \textit{discarding} data points, and it acts like smoothing to provide some measure of noise reduction. If the noise in the original signal is white, and the signal is condensed by averaging every ``\textit{n''} points, the noise is reduced in the condensed signal by the square root of \textit{n}, but with \textit{no change} in the frequency distribution of the noise. The Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/testcondense.m}{testcondense.m} demonstrates the effect of boxcar averaging using the \href{https://terpconnect.umd.edu/~toh/spectrum/consense.m}{condense.m} function to reduce noise without changing the noise color. Shows that the boxcar \href{https://terpconnect.umd.edu/~toh/spectrum/testcondense.png}{reduces the measured noise}, \href{https://terpconnect.umd.edu/~toh/spectrum/testcondense2.png}{removing the high-frequency components} but has little effect on the peak parameters. Least-squares curve-fitting on the condensed data is \href{https://terpconnect.umd.edu/~toh/spectrum/testcondense.txt}{faster and results in a lower fitting error}, but \textit{no more accurate measurement} of peak parameters. If you find yourself resorting to very large smooth widths, consider using the condense function first.

\textbf{Video Demonstration.} This 18-second, three MByte video (\href{https://terpconnect.umd.edu/~toh/spectrum/Smooth3.wmv}{Smooth3.wmv}) demonstrates the effect of triangular smoothing on a single Gaussian peak with a peak height of 1.0 and a peak width of 200. The initial white noise amplitude is 0.3, giving an initial signal-to-noise ratio of about 3.3. An attempt to measure the peak amplitude and peak width of the noisy signal, shown at the bottom of the video, are initially seriously inaccurate because of the noise. As the smooth width increases, however, the signal-to-noise ratio and the accuracy of the measurements of peak amplitudes and peak widths are both improved. However, above a smooth width of about 40 (smooth ratio 0.2), the smoothing causes the peak to be shorter than 1.0 and wider than 200, \textit{even though the signal-to-noise ratio continues to improve} as the smooth width is increased.

\href{https://terpconnect.umd.edu/~toh/spectrum/SPECTRUM.html}{SPECTRUM,} the old freeware Macintosh signal-processing application, includes rectangular and triangular smoothing functions for any number of points. See page \pageref{ref-0481}.

\section{Smoothing in spreadsheets\label{ref-0068}\label{ref-0069}\label{ref-0070}}

\InsImageInline{0.7}{l}{image28.png}Smoothing can be done in spreadsheets using the "shift and multiply" technique \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#algorithms}{described above}. In the spreadsheets \href{https://terpconnect.umd.edu/~toh/spectrum/smoothing.ods}{smoothing.ods} and \href{https://terpconnect.umd.edu/~toh/spectrum/smoothing.xls}{smoothing.xls} (\href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.png}{screen image}) the set of multiplying coefficients is contained in the formulas that calculate the values of each cell of the smoothed data in columns C and E. Column C performs a 7-point \textit{rectangular} smooth (1 1 1 1 1 1 1). Column E performs a 7-point \textit{triangular} smooth (1 2 3 4 3 2 1), applied to the data in column A. You can type in (or Copy and Paste) any data you like into column A, and you can extend the spreadsheet to longer columns of data by dragging the last row of columns A, C, and E down as needed. But to change the smooth width, you would have to change the equations in columns C or E and copy the changes down the entire column. It is common practice to divide the results by the sum of the coefficients so that the net gain is unity and the area under the curve of the smoothed signal is preserved. The spreadsheets\href{https://terpconnect.umd.edu/~toh/spectrum/UnitGainSmooths.xls}{ UnitGainSmooths.xls} and \href{https://terpconnect.umd.edu/~toh/spectrum/UnitGainSmooths.ods}{UnitGainSmooths.ods} (\href{https://terpconnect.umd.edu/~toh/spectrum/UnitGainSmooths.png}{screen image}) contain a collection of unit-gain convolution coefficients for rectangular, triangular, and p-spline smooths of width 3 to 29 in both vertical (column) and horizontal (row) format. You can Copy and Paste these into your own spreadsheets. 

The spreadsheets \href{https://terpconnect.umd.edu/~toh/spectrum/MultipleSmoothing.xls}{MultipleSmoothing.xls} and \href{https://terpconnect.umd.edu/~toh/spectrum/MultipleSmoothing.ods}{MultipleSmoothing.ods} (\href{https://terpconnect.umd.edu/~toh/spectrum/MultipleSmoothing.png}{screen image} on the left) demonstrate another method in which the coefficients are contained in a group of 17 adjacent cells (in row 5, columns I through Y), making it easier to change the \textit{smooth shape} and width (up to a \textit{maximum} of 17) just by changing those 17 cells. (To make a smaller smooth, just insert zeros for the unused coefficients; in this example, a 7-point triangular smooth is defined in columns N - T and the rest of the coefficients are zeros). In this spreadsheet, the smooth is applied \textit{three times} in succession in columns C, E, and G, resulting in an effective maximum smooth width of n*w-n+1 = 49 points applied to column G. A disadvantage of the above technique for smoothing in spreadsheets is that is cumbersome to expand them to very large smooth widths.

\InsImageInline{0.5}{l}{image29.png} A more flexible and powerful technique, especially for very large and variable smooth widths, is to use the built-in spreadsheet function AVERAGE, which by itself is equivalent to a rectangular smooth, but if applied two or three times in succession, generates \href{https://terpconnect.umd.edu/~toh/spectrum/VariableSmoothExample.png}{triangle and P-spline-shaped smooths}. It is best used in conjunction with the INDIRECT function (page \pageref{ref-0420}) to \href{https://www.lifewire.com/excel-sum-indirect-dynamic-range-formula-3124100}{control a dynamic range of values}, as is demonstrated in the spreadsheet \href{https://terpconnect.umd.edu/~toh/spectrum/VariableSmooth.xlsx}{VariableSmooth.xlsx} (right) in which the data in column A are smoothed by three successive applications of AVERAGE, in columns B, C, and D, each with a smooth width specified in a single cell F3. If \textit{w} is the smooth width, which can be \textit{any odd positive number}, the resulting smooth in column D has a \textit{total} width of n*\textit{w}-n+1 = 3*\textit{w}-2 points. The cell formula of the smooth operations (\texttt{=AVERAGE(INDIRECT("A"\&ROW(A17)-(\$F\$3-1)/2\&":A"\&ROW(A17) + (\$F\$3-1)/2))}) uses the \href{https://support.office.com/en-us/article/indirect-function-474b3a3a-8a26-4f44-b491-92b6306fa261}{INDIRECT} function to apply the \href{https://support.office.com/en-us/article/AVERAGE-function-047BAC88-D466-426C-A32B-8F33EB960CF6}{AVERAGE} function to the data in the rows from \textit{w}/2 rows \textit{above} to \textit{w}/2 rows \textit{below} the current row, where the smooth width \textit{w} is in cell F3. If you Copy and Paste this formula to your own spreadsheets, you must manually \textit{change all references to column "A"} to the column that contains the data to be smoothed in your spreadsheet and change all references to "\$F\$3" to the location of the smooth width in your spreadsheet. Then when you drag-copy down to cover all your data points, the row cell references will take care of themselves. \label{ref-0071}

The example in the graphic above shows smoothing applied to a DC (direct current) signals with a step change occurring at x=111. Without smoothing (blue line) the step is almost invisible. As an application example, the smoothed signal might be used to trigger an alarm whenever it exceeds a value of .2, warning that something has occurred, whereas the raw unsmoothed signal would be completely unsuitable for that purpose.

Another set of spreadsheets that uses this same AVERAGE(INDIRECT()) technique is \href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSmoothTemplate.xlsx}{SegmentedSmoothTemplate.xlsx}, a \textit{segmented}\index{\textcolor{color-3}{segmented}} multiple-width data smoothing spreadsheet template that can apply individually specified different smooth widths to different regions of the signal. This is especially useful if the widths or the noise level of the peaks vary substantially across the signal. In this version, there are 20 segments. Similar templates could be constructed with any number of segments.

\href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSmoothExample.xlsx}{SegmentedSmoothExample.xlsx} is an example with data (\href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSmoothExample.png}{graphic}); note that the plot is conveniently lined up with the columns containing the smooth widths for each segment. A related sheet, \href{https://terpconnect.umd.edu/~toh/spectrum/GradientSmoothTemplate.xlsx}{GradientSmoothTemplate.xlsx} or \href{https://terpconnect.umd.edu/~toh/spectrum/GradientSmoothExample2.xls}{GradientSmoothExample2.xls}\uline{x} (\href{https://terpconnect.umd.edu/~toh/spectrum/GradientSmoothExample2.png}{graphic}), performs a linearly increasing (or decreasing) smooth width across the entire signal, given only the starting and ending values, automatically generating as many segments and different smooth widths as are necessary. (It also enforces the restriction, in column C, that each smooth width must be an odd number, to prevent an x-axis shift in the smoothed data). 

\section{Smoothing in \href{http://en.wikipedia.org/wiki/MATLAB}{Matlab} and \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{Octave}\label{ref-0072}\label{ref-0073}}

\InsImageInline{0.5}{l}{SmoothTypes.png}The custom function \href{https://terpconnect.umd.edu/~toh/spectrum/fastsmooth.m}{fastsmooth} implements shift and multiply type smooths \href{http://www.analog.com/media/en/technical-documentation/dsp-book/dsp\_book\_Ch15.pdf\#page=282}{using a recursive algorithm}. "Fastsmooth" is a Matlab function of the form \textbf{s=}\href{https://terpconnect.umd.edu/~toh/spectrum/fastsmooth.m}{fastsmooth}\uline{(a,w, type, edge}). The argument "a" is the input signal vector; "w" is the smooth width (a positive integer); "type" determines the smooth type: type=1 gives a rectangular (sliding-average or boxcar) smooth; type=2 gives a triangular smooth, equivalent to two passes of a sliding average; type=3 gives a p-spline smooth, equivalent to three passes of a sliding average; these shapes are compared in the figure on the left. (See page \pageref{ref-0077} for a comparison of these smoothing modes). The argument "edge" controls how the "edges" of the signal (the first w/2 points and the last w/2 points) are handled. If edge=0, the edges are zero. (In this mode the elapsed time is independent of the smooth width. This gives the fastest execution time). If edge=1, the edges are smoothed with progressively smaller smooths the closer to the end. (In this mode the execution time increases with increasing smooth widths). The smoothed signal is returned as the vector "s". (You can leave off the last two input arguments: fastsmooth(Y,w,type) smooths with edge=0 and fastsmooth(Y,w) smooths with type=1 and edge=0). Compared to convolution-based smooth algorithms, fastsmooth uses a simple recursive algorithm that typically gives faster execution times for large smooth widths; it can smooth a 1,000,000-point signal with a 1,000-point sliding average in less than 0.1 seconds on a standard Windows PC. Here's a simple example of fastsmooth demonstrating the effect on white noise (\href{https://terpconnect.umd.edu/~toh/spectrum/SmoothingWhiteNoise.png}{graphic}).

\texttt{x=1:100;}

\texttt{y=randn(size(x));} 

\texttt{plot(x,y,x, fastsmooth(y,5,3,1),'r')}

\texttt{xlabel('Blue: white noise. Red: smoothed white noise.')}



\href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSmooth.m}{SegmentedSmooth.m} is a \textit{segmented}\index{\textcolor{color-3}{segmented}} version of fastsmooth. \textit{The syntax is the same as} \uline{fastsmooth.m}, except that the second input argument "smoothwidths" can be a \textit{vector}: \texttt{SmoothY = SegmentedSmooth (Y, smoothwidths, type, ends)}. The function divides Y into several equal-length regions defined by the length of the vector 'smoothwidths', then smooths each region with a smooth of type 'type' and width defined by the elements of \textit{vector} 'smoothwidths'. In the graphic example on the next page, \texttt{smoothwidths=[31 52 91]}, which divides up the signal into three equal regions and smooths the first region with smoothwidth 31, the second with smoothwidth 51, and the last with smoothwidth 91. \textit{You may use any number of smooth widths and any sequences of smooth widths}, just by how you define the vector ``smoothwidths''; no other change is needed. Type "help SegmentedSmooth" for other examples. 

\textbf{Segmented smoothing} 

The demonstration script \href{https://terpconnect.umd.edu/~toh/spectrum/DemoSegmentedSmooth.m}{DemoSegmentedSmooth.m} shows the operation with different signals consisting of noisy variable-width peaks that get progressively wider, as the figure on the right. If the peak widths increase or decrease regularly across the signal, you can calculate the smoothwidths vector by giving only the number of segments ("NumSegments"), the first value, "startw", and the last value, "endw", like so:

\texttt{wstep=(endw-startw)/NumSegments;}

\texttt{smoothwidths=startw:wstep:endw;}

\InsImageInline{0.5}{l}{SegmentedSmoothDemo.png}\textbf{Other smoothing functions}. 

~\href{http://www.mathworks.com/matlabcentral/fileexchange/authors/62607}{Diederick} has published a \href{http://www.mathworks.com/matlabcentral/fileexchange/30299-savitzky-golay-smoothdifferentiation-filters-and-filter-application}{Savitzky-Golay} smooth function in Matlab, which you can download from the \href{http://www.mathworks.com/matlabcentral/fileexchange/30299-savitzky-golay-smoothdifferentiation-filters-and-filter-application}{Matlab File Exchange}. It is included in the \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#Interactive\_Smoothing}{iSignal function} (page \pageref{ref-0433}). \href{http://uk.mathworks.com/matlabcentral/profile/authors/1859625-greg-pittam}{Greg Pittam} has published a modification of my fastsmooth function that tolerates NaNs ("Not a Number") in the data file (\href{http://uk.mathworks.com/matlabcentral/fileexchange/52688-nan-tolerant-fast-smooth}{nanfastsmooth(Y,w,type,tol)}) and another version for smoothing ``angle'' data that repeats every 360\textsuperscript{o} or 2 ${\uppi}$ radians (\href{http://uk.mathworks.com/matlabcentral/fileexchange/52689-angular-fast-smooth-nan-tolerant}{nanfastsmoothAngle(Y,w,type,tol)}).

\InsImageInline{0.5}{l}{SmoothWidthTest.png}
~\href{https://terpconnect.umd.edu/~toh/spectrum/SmoothWidthTest.m}{\textit{SmoothWidthTest.m}} is a demonstration script that uses the fastsmooth function to demonstrate the effect of smoothing on peak height, noise, and signal-to-noise ratio of a peak. You can change the peak shape in line 7, the smooth type in line 8, and the noise in line 9. A typical result for a Gaussian peak with white noise smoothed with a p-spline smooth is shown on the left. Here, as it is for most peak shapes, the optimal signal-to-noise ratio occurs at a smooth ratio of about 0.8. However, that optimum corresponds to a \textit{significant reduction in peak height}, which could be a problem. A smooth width about \textit{half} the width of the original unsmoothed peak produces less distortion of the peak but still achieves a reasonable noise reduction. \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothVsCurvefit.m}{SmoothVsCurvefit.m} is a similar script but is also compares \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{curve fitting} as an alternative method to measure the peak height \textit{without smoothing}.

This effect is explored more completely by the code below, which shows an \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothExperiment.m}{experiment in Matlab or Octave} that creates a Gaussian peak, smooths it, compares the smoothed and unsmoothed version, then uses the max(), \href{https://terpconnect.umd.edu/~toh/spectrum/halfwidth.m}{halfwidth()}, and trapz() functions to print out the \textit{peak height, halfwidth, and area}. (max and trapz are both built-in functions in Matlab and Octave, but you must download \href{https://terpconnect.umd.edu/~toh/spectrum/halfwidth.m}{halfwidth.m}. To learn more about these functions, type "help" followed by the function name).

\texttt{x=[0:.1:10]';}

\texttt{y=exp(-(x-5).\textasciicircum{}2);}

\texttt{plot(x,y)}

\texttt{ysmoothed=fastsmooth(y,11,3,1);}

\texttt{plot(x,y,x, ysmoothed,} \texttt{\textcolor{color-5}{'r'}}\texttt{)}

\texttt{disp([max(y) halfwidth(x,y,5) trapz(x,y)])}

\texttt{disp([max(ysmoothed) halfwidth(x,ysmoothed,5) trapz(x, ysmoothed)])}

       \texttt{\textcolor{color-6}{max halfwidth Area}}\index{Area}

            \texttt{1 1.6662 1.7725}

      \texttt{0.78442 2.1327 1.7725}

These results show that smoothing \textit{reduces} the peak height (from 1 to 0.784) and \textit{increases} the peak width (from 1.66 to 2.13) but has \textit{no effect} on the peak area if you measure the \textit{total area} under the broadened peak. Smoothing is useful if the signal is contaminated by non-normal noise such as sharp spikes or if the peak height, position, or width are measured by simple methods, but there is no need to smooth the data if the noise is white and the peak parameters are measured by least-squares methods, because the least-squares results obtained on the unsmoothed data will be more accurate (see page \pageref{ref-0284}). 

\textbf{Other noise-reduction functions.} The Matlab/Octave user-defined function \href{https://terpconnect.umd.edu/~toh/spectrum/condense.m}{condense.m}\textit{, condense(y,n)}, returns a condensed version of \textit{y} in which each group of \textit{n} points is replaced by its average, reducing the length of \textit{y} by the factor \textit{n}. (For \textit{x,y} data sets, use this function on \textbf{both} independent variable \textit{x} \textbf{and} dependent variable \textit{y} so that the features of \textit{y} will appear at the same \textit{x} values).  Random white noise in the signal is reduced by sqrt(\textit{n}) but the noise color is unchanged.\label{ref-0074}

 The Matlab/Octave user-defined function \href{https://terpconnect.umd.edu/~toh/spectrum/medianfilter.m}{medianfilter.m}, \texttt{medianfilter(y,w)}, performs a median-based filter operation that replaces each value of \textit{y} with the median of \textit{w} adjacent points (which must be a positive integer). \href{https://terpconnect.umd.edu/~toh/spectrum/killspikes.m}{killspikes.m} is a threshold-based filter for eliminating narrow spike artifacts. The syntax is \texttt{fy= killspikes(x, y, threshold, width)}. Each time it finds a positive or negative jump in the data between y(n) and y(n+1) that exceeds "threshold", it replaces the next "width" points of data with a linearly interpolated segment spanning x(n) to x(n+width+1). The script \href{https://terpconnect.umd.edu/~toh/spectrum/TestSpikefilters.m}{TestSpikefilters} compares both spike filters on a Gaussian with spikes and shows how accurately they recover the original peak area.

\href{https://terpconnect.umd.edu/~toh/spectrum/ProcessSignal.m}{ProcessSignal} is a Matlab/Octave command-line function that performs smoothing and differentiation on the time-series data set x,y (column or row vectors). It can employ all the types of smoothing described above. Type "help ProcessSignal" at the command line. This function returns the processed signal as a vector that has the same shape as x, regardless of the shape of y. The syntax is Processed = ProcessSignal(x, y, DerivativeMode, w, type, ends, Sharpen, factor1, factor2, Symize, Symfactor, SlewRate, MedianWidth). 

\textit{Real-time} smoothing in Matlab is discussed on page \pageref{ref-0416}\textbf{.}

\InsImageInline{0.5}{l}{iSignalSmoothAnimation.gif.png}
~\href{http://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{\textbf{iSignal}} (page \pageref{ref-0433}) is an interactive function for Matlab that includes smoothing for time-series signals using \textit{all the algorithms discussed above}, including the Savitzky-Golay smooth, the segmented\index{\textcolor{color-3}{segmented}} smooth, a median filter, and a condense function, with keystrokes that allow you to adjust any of the smoothing parameters continuously while observing the effect on your signal instantly, making it easy to observe how different types and amounts of smoothing effect noise and signal, such as the height, width, and areas of peaks. Other functions of iSignal include differentiation, peak sharpening, interpolation, least-squares peak measurement, and a frequency spectrum mode that shows how smoothing and other functions can change the frequency spectrum of your signals. The simple script ``\href{https://terpconnect.umd.edu/~toh/spectrum/iSignalDeltaTest.m}{iSignalDeltaTest}'' demonstrates the frequency response of iSignal's smoothing functions by applying them to a \href{https://en.wikipedia.org/wiki/Dirac\_delta\_function}{single-point spike}, allowing you to change the smooth type and the smooth width to see how the frequency response changes. View the code \href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{here} or download the \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal8.zip}{ZIP file} with sample data for testing.

\textbf{You try it:} Here's an experiment you can try using \textit{iSignal}. This uses a previously recorded example of a very noisy signal with lots of high-frequency (blue) noise \textit{totally obscuring a perfectly good peak} in the center at x=150, height=1e-4; SNR=90. First, download \href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{iSignal.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/NoisySignal.mat}{NoisySignal}\uline{\textcolor{color-6}{.mat}} into the Matlab path, then execute these statements:

\texttt{{\textgreater}{\textgreater} load} \href{https://terpconnect.umd.edu/~toh/spectrum/NoisySignal.mat}{\texttt{NoisySignal}}

\texttt{{\textgreater}{\textgreater} isignal(x,y);}

Use the \textbf{A} and \textbf{Z} keys to increase and decrease the smooth width, and the \textbf{S} key to cycle through the available smooth types. Hint: use the p-spline smooth and keep increasing the smooth width until the peak becomes visible. (Unfortunately, iSignal does not currently work in \textit{Octave}, but it does work in a Web browser using \textit{Matlab Online}. See \url{https://www.mathworks.com/products/matlab-online.html}).

\textbf{Note:} If you are reading this online, you can right-click on any of the m-file links on this site and select \textbf{Save Link As...} to download them to your computer for use within Matlab. 

\section{Smoothing performance comparison\label{ref-0075}\label{ref-0076}\label{ref-0077}\label{ref-0078}\label{ref-0079}}

The Matlab/Octave function "\href{https://terpconnect.umd.edu/~toh/spectrum/smoothdemo.m}{MultiPeakOptimization.m}" is a self-contained function that compares the performance of four types of linear smooth operations: (1) sliding-average rectangular, (2) triangular, (3) p-spline (equivalent to three passes of a sliding-average), and (4) Savitzky-Golay. These are the four smooth types discussed above, corresponding to the four values of the ``SmoothMode'' input argument of the \href{https://terpconnect.umd.edu/~toh/spectrum/ProcessSignal.m}{ProcessSignal} and the interactive \href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{iSignal} functions. These four smooth operations are applied to a 18000-point signal consisting of 181 Gaussian peaks with a height of 1.0 and a FWHM (\href{https://en.wikipedia.org/wiki/Full\_width\_at\_half\_maximum}{full-width at half-maximum}) of 20 points (``wid'', line 10), which are all separated by an x-value of 160.01 (line 16), plus added noise consisting of normally-distributed random white noise \InsImage{1.0}{image31.png}with a mean of zero and a standard deviation of ``Noise'' (line 20). The x-axis peak position and y-axis height of each smoothed peak is determined by the height and position of the maximum single signal point for each peak. The relative standard deviation of the measured peak heights is recorded as a function of ``total smooth width'', \textit{tsw}, which is the halfwidth of the impulse response of each smooth type. The results are shown in the figure below for a peak halfwidth of 20 and a noise standard deviation of 0.2 (i.e., 20\% of the peak height). The four quadrants of the graph are: (upper left) peak position error expressed as a percentage of the peak separation; (upper right), the mean peak height of the smoothed peaks; (lower left), the standard deviation of the smoothed noise; and (lower right) the relative standard deviation of the measured peak heights. The different smooth types are indicated by color: blue - sliding-average; red - triangular; yellow - P-spline, and purple - Savitzky-Golay.

The four quadrants of the graph are: (upper left) peak position error expressed as a percentage of the peak separation; (upper right), the mean peak height of the smoothed peaks; (lower left), the standard deviation of the smoothed noise; and (lower right) the relative standard deviation of the measured peak heights. The different smooth types are indicated by color: blue - sliding-average; red - triangular; yellow - p-spline, and purple - Savitzky-Golay.\label{ref-0080}

These results show that the results of these different smooth types are quite similar but that, the Savitzky-Golay smooth gives the smallest reduction in peak height but the smallest reduction in noise amplitude, compared to the other methods. All these smoothing methods result in similar improvements in the standard deviation of the peak height (bottom right panel) and in the peak position error (upper left panel). Moreover, in all cases, the optimum performance is achieved when the total smooth width is approximately equal to the halfwidth of the peak. The conclusions are the same for a Lorentzian peak, as demonstrated by a similar function "\href{https://terpconnect.umd.edu/~toh/spectrum/MultiPeakOptimizationLorentzian.m}{MultiPeakOptimizationLorentzian.m}", \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothingComparisonMultiplePeaksLorentzianFigure2Noise01.png}{graphic}, the difference being that the peak height reduction is greater for the Lorentzian. For applications where the shape of the signal must be preserved as much as possible, the Savitzky-Golay is the method of choice. In peak detection applications (page \pageref{ref-0087}), on the other hand, where the purpose of smoothing is to reduce the noise in the derivative signal, the retention of the shape of that derivative is less important because peak parameters are determined by least-squares fitting. Therefore, the triangular or p-spline smooth is well suited to this purpose and can be faster for very large smooth widths. 

The differences between these methods is even less when the abscissas in the above graphs are changed from \textit{total smooth bandwidth} to \textit{white} \textit{noise reduction factor}, defined as the square root of the reciprocal of the sum of the square of the impulse response function, as shown in figure window 3.

\InsImage{1.0}{SmoothingComparisonMultiplePeaksFigure3Noise01.png}An important detail is that these results apply only of the noise in the signal is \textit{white} (page \pageref{ref-0032}). If you smooth a signal that has been differentiated, for example, the second derivative of a Gaussian peak with white noise (\href{https://terpconnect.umd.edu/~toh/spectrum/dsmooth2b.GIF}{graphic}), high-frequency content of both the signal and the noise are greatly enhanced, and these results will be different, showing much poorer relative performance for the simple moving average (\href{https://terpconnect.umd.edu/~toh/spectrum/SmoothingComparisonGaussian2ndDerivativeFigure2Noise01.png}{graphic}). The Savitzky-Golay smooth remains superior in this case also.

A more sophisticated method of noise reduction, called \textit{wavelet denoising}, will be introduced on page \pageref{ref-0171}.

\chapter{Differentiation\label{ref-0081}\label{ref-0082}}

The symbolic \href{http://en.wikipedia.org/wiki/Derivative}{differentiation} of functions is a topic that is introduced in all elementary Calculus courses. The numerical differentiation of digitized signals is an application of this concept that has many uses in analytical signal processing. The first derivative of a signal is the rate of change of y with x, that is, dy/dx, which we interpret as the \href{http://en.wikipedia.org/wiki/Slope}{\textit{slope}} of the \href{http://en.wikipedia.org/wiki/Tangent}{tangent} to the signal at each point, as illustrated by the animation generated by this \href{https://terpconnect.umd.edu/~toh/spectrum/SlopeAnimation.m}{script} (If you are reading this online, click for \href{https://terpconnect.umd.edu/~toh/spectrum/SlopeAnimation.gif}{GIF animation}). The simplest algorithm for computing the first derivative is called a ``finite difference'' method:


\begin{center}
${Y_{j}}' = \frac{Y_{j+1} - Y_{j}}{X_{j+1} - X_{j}} = \frac{Y_{j+1} - Y_{j}}{\Delta X}\:\:\:\:\:\:\:\:\:\:{X_{j}}' = \frac{X_{j+1} - X_{j}}{2}$

(for 1{\textless} j {\textless}n-1).
\end{center}


where X'\raisebox{-4pt}{j} and Y'\raisebox{-4pt}{j} are the X and Y values of the j\raisebox{4pt}{th} point of the derivative, n = number of points in the signal, and \InsImageInline{.001}{l}{delta.GIF.png}X is the difference between the X values of adjacent data points. A commonly used variation of this algorithm computes the average slope between three adjacent points:


\begin{center}
${Y_{j}}' = \frac{Y_{j+1} - Y_{j-1}}{2 \Delta X}\:\:\:\:\:\:\:{X_{j}}' = X_{j}$

(for 2 {\textless} j {\textless}n-1).
\end{center}


This is a \textit{central-difference} method; its advantage is that it does not involve a shift in the x-axis position of the derivative. It is also possible to compute \textit{gap-segment} derivatives in which the x-axis interval between the points in the above expressions is greater than one; for example, Y\raisebox{-4pt}{j-2} and Y\raisebox{-4pt}{j+2}, or Y\raisebox{-4pt}{j-3} and Y\raisebox{-4pt}{j+3}, etc. This is equivalent to applying a sliding-average (rectangular) smooth (page \pageref{ref-0045}) in addition to the derivative. 

The \textit{second derivative} is the derivative of the derivative: it is a measure of the \textit{curvature} of the signal, that is, the rate of change of the slope of the signal. It can be calculated by applying the first derivative calculation twice in succession. The simplest algorithm for direct computation of the second derivative in one step is:


\begin{center}
${Y_{j}}'' = \frac{Y_{j+1} - 2Y_{j} + Y_{j-1}}{\Delta X^{2}}\:\:\:\:\:\:\:{X_{j}}' = X_{j}$

(for 2 {\textless} j {\textless}n-1).
\end{center}


Similarly, higher derivative orders can be computed using the appropriate sequence of coefficients: for example, +1, -2, +2, -1 for the third derivative and +1, -4, +6, -4, +1 for the 4\raisebox{4pt}{th} derivative, although these derivatives can also be computed simply by taking successive lower order derivatives. The first derivative we interpret as the \textit{slope} of the original at each point, and the second derivative as the \textit{curvature}. Where the signal curvature is concave-\textit{down}, the second derivative is \textit{negative}, and where the signal is concave-\textit{up}, the second derivative is \textit{positive}. For higher derivatives, we have no single-word labels, at least in English; each derivative is just the rate of change of the one before it.

The \href{http://en.wikipedia.org/wiki/Savitzky–Golay\_smoothing\_filter}{\textit{Savitzky-Golay}} smooth (page \pageref{ref-0045}) can also be used as a differentiation algorithm with the appropriate choice of input arguments; it neatly combines differentiation and smoothing into one algorithm.

The \textit{accuracy} of numerical differentiation is demonstrated by the Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/GaussianDerivatives.m}{GaussianDerivatives.m} (\href{https://terpconnect.umd.edu/~toh/spectrum/GaussianDerivatives.png}{graphic} \uline{link}), which compares the exact \textit{analytical} expressions for the derivatives of a Gaussian (\href{http://www.wolframalpha.com/input/?i=second+derivative+of+gaussian}{readily obtained from Wolfram Alpha\index{Wolfram Alpha}}) to the \textit{numerical} values obtained by the expressions above, demonstrating that the shape and amplitude of the derivatives are an exact match as long as the sampling interval is not too coarse. It also demonstrates that you can obtain the numerical \textit{n}th derivative exactly by applying \textit{n} successive first differentiations. Ultimately, the numerical precision limitation of the computer can be a limitation only \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#Numerical}{in some extreme cases} (as demonstrated on page \pageref{ref-0405}). An alternative method based on the Fourier Transform (page \pageref{ref-0121}) has slightly lower numerical errors but is seldom used in practice (reference 88).

\section{ Basic Properties of Derivative Signals\label{ref-0083}\label{ref-0084}}

\InsImageInline{0.6}{l}{shapes.GIF.png}The figure on the left shows the results of the successive differentiation of a computer-generated Gaussian peak. The signal in each of the four windows is the first derivative of the one before it; that is, Window 2 is the first derivative of Window 1, Window 3 is the first derivative of Window 2, Window 3 is the \textit{second} derivative of Window 1, and so on. You can predict the shape of each signal by recalling that the derivative is simply the slope of the original signal: where a signal slopes up, its derivative is positive; where a signal slopes down, its derivative is negative; and where a signal has a slope of zero, its derivative is zero. (\href{https://terpconnect.umd.edu/~toh/spectrum/derivdemo1.m}{Matlab/Octave code} for this figure.)

The \href{https://en.wikipedia.org/wiki/Sigmoid\_function}{sigmoidal }signal shown in Window 1 has an \textit{inflection point} (the point where the slope is maximum) at the center of the x-axis range. This corresponds to the \textit{maximum} in its first derivative (Window 2) and to the \textit{zero-crossing} (point where the signal crosses the x-axis going either from positive to negative or \textit{vice versa}) in the second derivative in Window 3. This behavior can be useful for precisely locating the inflection point in a sigmoid signal, by computing the location of the zero-crossing in its second derivative. Similarly, the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{location of the maximum} in a peak-type signal can be computed precisely by computing the location of the zero-crossing in its first derivative. Different peak shapes have different derivatives shapes: the Matlab/Octave function \href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeShapeDemo.m}{DerivativeShapeDemo.m} demonstrates the first derivative forms of 16 different model peak shapes (graphic on page \pageref{ref-0469}). Any smooth peak shape with a single maximum has sequential derivatives that exhibit a series of \textit{alternating maxima and minima, the total number of which is one more than the derivative order.} The even-order derivatives have a maximum or a minimum at the peak center, and the odd-order derivatives have a zero-crossing at the peak center \uline{(}\href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeShapes.m}{Matlab/Octave code)}. You can also see here that the numerical magnitude of the derivatives (y-axis values) is much less than the original signal because derivatives are the \textit{differences} between adjacent y values, divided by the independent variable increment. (It is the same reason the \href{https://en.wikipedia.org/wiki/Odometer}{odometer }in your car usually displays a much larger number than the speedometer unless your car is very new, and you drive very fast. The speedometer is essentially the first derivative of the odometer).

\InsImageInline{0.5}{l}{width.GIF.png}An important property of the differentiation of peak-type signals is the effect of the peak \textit{width} on the amplitude of derivatives. The figure on the left shows the results of the successive differentiation of two computer-generated Gaussian bands. The two bands have the same amplitude (peak height) but one of them is exactly twice the width of the other. As you can see, the \textit{wider} peak has a \textit{smaller} derivative amplitude, and the effect becomes more noticeable at higher derivative orders. In general, the amplitude of the n\raisebox{4pt}{th} derivative of a peak is inversely proportional to the n\raisebox{4pt}{th} power of its width, for signals having the same shape and amplitude. Thus, differentiation in effect discriminates against wider peaks and the higher the order of differentiation the greater the discrimination. This behavior can be useful in quantitative analytical applications for detecting peaks that are superimposed on and obscured by stronger but broader background peaks. (\href{https://terpconnect.umd.edu/~toh/spectrum/derivdemo2.m}{Matlab/Octave code} for this figure). The amplitude of a derivative of a peak also depends on the \textit{shape} of the peak and is directly proportional to its peak \textit{height}. Gaussian and Lorentzian peak shapes have slightly different \href{https://terpconnect.umd.edu/~toh/spectrum/FirstDerivativeGaussVsLor.png}{first} and \href{https://terpconnect.umd.edu/~toh/spectrum/SecondDerivativeGaussVsLor.png}{second} derivative shapes and amplitudes. The amplitude of the n\raisebox{4pt}{th} derivative of a Gaussian peak of height H and width W can be estimated by the \href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeScaling.m}{empirical equation} H*(10\textasciicircum{}(0.027*n\textasciicircum{}2+n*0.45-0.31)).*W\textasciicircum{}(-n), where W is the full width at half maximum (FWHM) measured in the number of x,y data points. 

Although differentiation completely changes the shape of \textit{peak-type} signals, a \textit{periodic signal} like a \href{https://en.wikipedia.org/wiki/Sine\_wave}{sine wave} signal behaves very differently. The derivative of a sine wave of frequency \textit{f} is a \textit{phase-shifted} sine wave, or \href{http://whatis.techtarget.com/definition/cosine-wave}{cosine wave}, of the \textit{same frequency} and with an amplitude that is proportional to \textit{f}, as can be demonstrated in \href{http://www.wolframalpha.com/input/?i=deriv\%28sin\%28f*t\%29\%29}{Wolfram Alpha\index{Wolfram Alpha}}. The derivative of a periodic signal containing several sine components of different frequency will \textit{still contain those same frequencies}, but with altered amplitudes and phases. For this reason, when you take the derivative of a music or speech signal, the music or speech is still completely recognizable, but with the high frequencies increased in amplitude compared to the low frequencies, and as a result, it sounds "thin" or "tinny". See \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html\#sounds}{page} ~\ref{ref-0443}\pageref{ref-0443} for an example.

\section{Applications of Differentiation\label{ref-0085}}

A simple example of the application of the differentiation of experimental signals is shown in the figure below. This signal is typical of the type of signal recorded in \href{https://en.wikipedia.org/wiki/Amperometric\_titration}{amperometric titrations} and some kinds of \href{https://en.wikipedia.org/wiki/Thermal\_analysis}{thermal analysis} and kinetic experiments: a series of straight-line segments of different slope. The objective is to determine how many segments there are, where the breaks between then fall, and the slopes of each segment. This is difficult to do from the raw data because the slope differences are small, and the resolution of the computer screen display is limiting. The task is much simpler if the first derivative (slope) of the signal is calculated (below right). Each segment is now clearly seen as a separate step whose height (y-axis value) is the slope. The y-axis now takes on the units of dy/dx. Note that in this example the steps in the derivative signal are not completely flat, indicating that the line segments in the original signal were not perfectly straight. This is most likely due to random noise in the original signal. Although this noise was not particularly evident in the original signal, it is more noticeable in the derivative. 

\InsImage{1.0}{Figure5.GIF.png}
\begin{center}
\textit{The signal on the left seems to be a more-or-less straight line, but its numerically calculated} \textbf{\textit{derivative}} \textit{(dx/dy), plotted on the right, shows that the line actually has several approximately straight-line segments with distinctly different slopes and with well-defined breaks between each segment.}
\end{center}


It is commonly observed that differentiation degrades the signal-to-noise ratio unless the differentiation algorithm includes smoothing (page \pageref{ref-0045}) that is carefully optimized for each application. Numerical algorithms for differentiation are as numerous as for smoothing and must be carefully chosen to control signal-to-noise ratio degradation (page \pageref{ref-0092}).

A classic use of second differentiation in chemical analysis is in the \href{http://zimmer.csufresno.edu/~davidz/Chem102/Derivative/Derivative.html}{location of endpoints} in \href{https://en.wikipedia.org/wiki/Potentiometric\_titration}{potentiometric titration}. In most titrations, the titration curve has a sigmoidal shape and the inflection point, the point where the slope is maximum and the curvature is zero, indicates the endpoint. The first derivative of the titration curve will, therefore, exhibit a \textit{maximum} at the inflection point, and the second derivative will exhibit a \textit{zero-crossing} at that point. Maxima and zero crossings are usually much easier to locate precisely than inflection points.

\InsImage{1.0}{Figure6.GIF.png}
\begin{center}
\textit{The signal on the left is the pH titration curve of a very weak acid with a strong base, with volume in mL on the X-axis and pH on the Y-axis. The endpoint is the point of the greatest slope; this is also an inflection point, where the curvature of the signal is zero. With a weak acid such as this, it is difficult to locate this point precisely from the original titration curve. The endpoint is much more easily located in the} \textbf{\textit{second derivative}}\textit{, shown on the right, as the zero-crossing.}
\end{center}


The figure above shows a pH titration curve of a very weak acid with a strong base, with volume in mL on the X-axis and pH on the Y-axis. The volumetric equivalence point (the "theoretical" endpoint) is 20 mL. The endpoint is the point of the greatest slope; this is also an inflection point, where the curvature of the signal is zero. With a weak acid such as this, it is difficult to locate this point precisely from the original titration curve. The second derivative of the curve is shown in Window 2 on the right. The zero-crossing of the second derivative corresponds to the endpoint and is much more precisely measurable. Note that in the second derivative plot, both the x-axis and the y-axis scales have been expanded to show the zero-crossing point more clearly. The dotted lines show that the zero-crossing falls at about 19.4 mL, close to the theoretical value of 20 mL.\label{ref-0086}

\InsImageInline{0.5}{l}{image35.png}Derivatives can also be used to detect unexpected asymmetry in otherwise symmetrical peaks. For example, pure Gaussian peaks are symmetrical, but if they are subjected to exponential broadening (page \pageref{ref-0174}), they can become asymmetrical. If the degree of broadening is small, it can be difficult to detect visually, and that is where differentiation can help. The Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeEMGDemo.m}{DerivativeEMGDemo.m} (\href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeEMGDemo.png}{graphic}) shows the 1\textsuperscript{st} through 5\textsuperscript{th} derivatives of a slightly exponentially broadened Gaussian (EMG); of those derivatives, the second clearly shows unequal positive peaks that would be expected to be equal for a purely symmetrical peak (on the left). The higher derivatives offer no clear advantage and are more susceptible to white noise in the signal. For another example, if a Gaussian peak is heavily overlapped by a smaller peak, the result is usually asymmetrical. The script \href{https://terpconnect.umd.edu/~toh/spectrum/DerivativePeakOverlapDemo.m}{DerivativePeakOverlapDemo} (\href{https://terpconnect.umd.edu/~toh/spectrum/DerivativePeakOverlapDemo.png}{graphic}) shows the 1\textsuperscript{st} through 5\textsuperscript{th} derivatives of two overlapping Gaussians where the second peak is so small and so close that it is impossible to discern visually, but again the second derivative shows the asymmetry clearly by comparing the heights of the two positive peaks. \href{https://terpconnect.umd.edu/\%7Etoh/spectrum/DerivativePeakOverlapDemo.m}{DerivativePeakOverlap.m} detects the minimum extent of peak overlap by the first and second derivatives, looking for the point at which two peaks are visible; for each trial separation, it prints out the separation, resolution, and the number of peaks detected in the first and second derivatives.

\section{Peak detection\label{ref-0087}\label{ref-0088}}

\InsImageInline{0.5}{l}{peakdetectionexample.png} Another common use of differentiation is in the detection of peaks in a signal, especially when you do not know the number of peaks or their locations. It is clear from the basic properties described in the previous section that the first derivative of a peak has a downward-going zero-crossing at the peak maximum, which can be used to locate the x-value of the peak, as shown on the right (\href{https://terpconnect.umd.edu/~toh/spectrum/peakdetection.m}{script}). If there is \textit{no noise} in the signal, then any data point that has lower values on both sides of it will be a peak maximum. But there is always at least a little noise in real experimental signals, and that will cause many false zero-crossings simply due to the noise. To avoid this problem, one \href{https://www.google.com/search?q=peak+detection&oq=peak+detection&aqs=chrome..69i57j0l5.3200j0j4&sourceid=chrome&ie=UTF-8}{popular technique} smooths the first derivative of the signal first, before looking for downward-going zero-crossings, and then takes only those zero-crossings whose slope exceeds a certain predetermined minimum (called the "slope threshold") at a point where the original signal amplitude exceeds a certain minimum (called the "amplitude threshold"). By carefully adjusting the smooth width, slope threshold, and amplitude threshold, it is possible to detect only the desired peaks over a wide range of peak widths and ignore peaks that are too small, too wide, or too narrow. Moreover, because \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#Optimization}{smoothing can distort peak signals}, reducing peak heights, and increasing peak widths (page \pageref{ref-0045}), this technique can be extended to measure the position, height, and width of each peak by \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html}{least-squares curve-fitting} of a segment of original \textit{unsmoothed signal} \textit{near the top of the peak} (where the signal-to-noise ratio is usually the best). Thus, even if heavy smoothing is necessary to provide reliable discrimination against noise, the peak parameters extracted by curve fitting are not distorted and the effect of random noise in the signal is reduced by curve fitting over multiple data points in the peak. This technique has been implemented in \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{Matlab/Octave} (page \pageref{ref-0096}) and in \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#Spreadsheet}{spreadsheets} (page \pageref{ref-0093}). 

\section{Derivative Spectroscopy\label{ref-0089}}

In spectroscopy, the differentiation of spectra is a widely used technique, particularly in infra-red, u.v.-visible \href{http://www.youngin.com/application/AN-0608-0115EN.pdf}{absorption}, \href{http://books.google.com/books?id=Q5V4AiCpqHYC&pg=RA1-PA155&lpg=RA1-PA155&dq=derivative+fluorescence+o\%27haver&source=web&ots=6CpSnrbGP1&sig=GimVmSI-S6v6-pWh4XZWqCfEuLE&hl=en&sa=X&oi=book\_result&resnum=1&ct=result}{fluorescence}, and \href{http://ser.sese.asu.edu/SPECTRA/}{reflectance spectrophotometry}, referred to as \href{http://www.google.com/search?hl=en&client=firefox-a&rls=org.mozilla\%3Aen-US\%3Aofficial&hs=TM1&q=derivative+spectroscopy+application&btnG=Search}{derivative spectroscopy}\textit{.} Derivative methods have been used in analytical spectroscopy for three main purposes:

(\textbf{a}) spectral discrimination, as a qualitative fingerprinting technique to accentuate small structural differences between nearly identical spectra; 

(\textbf{b}) spectral resolution enhancement (peak sharpening), as a technique for increasing the apparent resolution of overlapping spectral bands in order to more easily determine the number of bands and their wavelengths; 

(\textbf{c}) quantitative analysis, as a technique for the correction for irrelevant background absorption and as a way to facilitate multicomponent analysis. (Because differentiation is a linear technique, the amplitude of a derivative is proportional to the amplitude of the original signal, which allows quantitative analysis applications employing any of the \href{https://terpconnect.umd.edu/~toh/models/Bracket.html}{standard calibration techniques} (page \pageref{ref-0490}). Most commercial spectrophotometers now have a built-in derivative capability. Some instruments are designed to measure the spectral derivatives optically, using \href{https://terpconnect.umd.edu/~toh/models/DualWave1.html}{dual-wavelength} or \href{https://terpconnect.umd.edu/~toh/models/modspec.html}{wavelength modulation} designs.

Because the amplitude of the n\raisebox{4pt}{th} derivative of a peak-shaped signal is inversely proportional to the n\raisebox{4pt}{th} power of the width of the peak, differentiation may be employed as a general way to discriminate against broad spectral features in favor of narrow components. This is the basis for the application of differentiation as a method of correction for background signals in quantitative spectrophotometric analysis. Very often in the practical applications of spectrophotometry to the analysis of complex samples, the spectral bands of the analyte (i.e., the compound to be measured) are superimposed on a broad, gradually curved background. Background signals of this type can be reduced by differentiation.

\InsImage{1.0}{PeakOnBackgroundSmall.GIF.png}


\begin{center}
\InsImageInline{0.5}{l}{2ndSmall.GIF.png}
\end{center}



\begin{center}
Reducing the effect of background by differentiation. 
\end{center}


This is illustrated by the figure above, which shows a simulated UV spectrum (absorbance vs wavelength in nm), with the green curve representing the spectrum of the pure analyte and the red line representing the spectrum of a mixture containing the analyte plus other compounds that give rise to the large sloping background absorption. The first derivatives of these two signals are shown in the center; you can see that the difference between the pure analyte spectrum (green) and the mixture spectrum (red) is reduced. This effect is considerably enhanced in the second derivative, shown at the bottom. In this case, the spectra of the pure analyte and of the mixture are almost identical. For this technique to work, it is necessary that the background absorption be broader (that is, have lower curvature) than the analyte spectral peak, but this turns out to be a rather common situation. Because of their greater discrimination against broad background, second (and sometimes even higher order) derivatives are often used for such purposes. See \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html\#DerivativeDemo}{DerivativeDemo.m} for a Matlab/Octave demonstration of this application.

It is sometimes (mistakenly) said that differentiation "increases the sensitivity" of analysis. You can see how it would be tempting to say something like that by inspecting the three figures above; it does seem that the signal amplitude of the derivatives is greater than that of the original analyte signal (at least graphically). However, it is not valid to compare the amplitudes of signals and their derivatives because they \textit{have different units}. The units of the original spectrum are absorbance; the units of the first derivative are absorbance per nm, and the units of the second derivative are absorbance per nm\raisebox{4pt}{2}. You cannot compare absorbance to absorbance per nm any more than you can compare miles to miles per hour. (It is meaningless, for instance, to say that a speed of 30 miles per hour is greater than a distance of 20 miles.) You \textit{can}, however, compare the \textit{signal-to-background ratio} and the \textit{signal-to-noise ratio}. For instance, in the above example, it would be valid to say that the signal-to-background ratio is better (higher) in the derivatives.

Loosely speaking, \textcolor{color-7}{the} \href{https://www.google.com/search?q=opposite+of+differentiation+is+integration&ie=utf-8&oe=utf-8&aq=t&rls=org.mozilla:en-US:unofficial&client=seamonkey-a}{opposite of differentiation is integration}, so if you take the first derivative of a signal, you might expect to be able to regenerate the original (zeroth derivative) by integration. However, there is a catch; the constant term in the original signal (like a flat baseline) is completely lost in differentiation; integration cannot restore it. So strictly speaking, differentiation represents a net \textit{loss} of information, and therefore differentiation should only be used only in situations where the constant term in the original signal is not of interest.

There are several ways to measure the amplitude of a derivative spectrum for quantitative analysis: the absolute value\index{absolute value} of the derivative at a specific wavelength, the value of a specific feature (such as a maximum), or the difference between a maximum and a minimum. Another widely used technique is the zero-crossing measurement - taking readings derivative amplitude at the wavelength where an interfering peak crosses the zero on the y (amplitude) axis. In all these cases, it is important to measure the standards and the unknown samples in the same way. Also, because the amplitude of a derivative of a peak depends strongly on its width, it is important to control environmental factors that might change spectral peak width subtlety, such as temperature.

\section{Trace Analysis\label{ref-0090}}

One of the widest uses of the derivative signal processing technique in practical analytical work is in the measurement of small amounts of substances in the presence of large amounts of potentially interfering materials. In such applications, it is common that the analytical signals are weak, noisy, and superimposed on large background signals. Measurement precision is often degraded by sample-to-sample baseline shifts due to non-specific broadband interfering absorption, non-reproducible cuvette (sample cell) positioning, dirt or fingerprints on the cuvette walls, imperfect cuvette transmission matching, and solution turbidity. Baseline shifts from these sources are usually either wavelength-independent (light blockage caused by bubbles or large suspended particles) or exhibit a weak wavelength dependence (small-particle turbidity). Therefore, you can expect that differentiation will in general help to discriminate relevant absorption from these sources of baseline shift. An obvious benefit of the suppression of broad background by differentiation is that \textit{variations} in the background amplitude from sample to sample are also reduced. This can result in improved precision or measurement in many instances, especially when the analyte signal is small relative to the background and if there is a lot of uncontrolled variability in the background. An example of the improved ability to detect trace component in the presence of strong background interference is shown in this figure:\label{ref-0091}

\InsImage{1.0}{Figure7.GIF.png}
\begin{center}
\textit{The absorption spectrum on the left shows a weak shoulder near the center due to a small concentration of the substance that is to be measured (e.g., the active ingredient in a pharmaceutical preparation). It is difficult to measure the intensity of this peak because it is obscured by the strong background caused by other substances in the sample. The} \textbf{\textit{fourth derivative}} \textit{of this spectrum is shown on the right. The background has been almost completely suppressed and the analyte peak now stands out clearly, facilitating measurement.}
\end{center}


The spectrum on the left shows a weak shoulder near the center due to the analyte. The signal-to-noise ratio is very good in this spectrum, but despite that, the broad, sloping background obscures the peak and makes quantitative measurement very difficult. The fourth derivative of this spectrum is shown on the right. The background has been almost completely suppressed and the analyte peak now stands out clearly, facilitating measurement. An even more dramatic case is shown below. This is essentially the same spectrum as in the figure above, except that the concentration of the analyte is ten times lower. The question is: is there a detectable amount of analyte in this spectrum? This is quite impossible to say from the normal spectrum, but inspection of the fourth derivative (right) shows that the answer is \textit{yes}. Some noise is visually evident here, but nevertheless the signal-to-noise ratio is sufficiently good for a reasonable quantitative measurement.

\InsImage{1.0}{Figure8.GIF.png}
\begin{center}
\textit{Like the previous figure, but in the case the peak is ten times lower - so weak that it cannot even be seen in the spectrum on the left. The fourth derivative (right) shows that a peak is still there, but much reduced in amplitude (note the smaller y-axis scale) and in signal-to-noise ratio.}
\end{center}


This use of signal differentiation has become widely used in \href{http://www.google.com/search?hl=en&client=firefox-a&rls=org.mozilla\%3Aen-US\%3Aofficial&hs=TM1&q=derivative+spectroscopy+application&btnG=Search}{quantitative spectroscopy}, particularly for quality control in the \href{http://www.google.com/search?hl=en&client=firefox-a&rls=org.mozilla\%3Aen-US\%3Aofficial&hs=x1L&q=derivative+spectroscopy+pharmaceutical&btnG=Search}{pharmaceutical industry}. In that application, the analyte would typically be the active ingredient in a pharmaceutical preparation and the background interferences might arise from the presence of fillers, emulsifiers, flavoring or coloring agents, buffers, stabilizers, or other excipients. Of course, in trace analysis applications, care must be taken to \href{https://terpconnect.umd.edu/~toh/models/AbsSlitWidth.html}{optimize the signal-to-noise ratio} of the instrument as much as possible.

Although it will eventually be shown that more advanced techniques such as \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#comparison}{curve fitting} can also perform many of these quantitative measurement tasks quite well (page \pageref{ref-0351}), the derivative techniques have the advantage of conceptual and mathematical simplicity and an easily understood graphical way of presenting data.\label{ref-0092}

\textbf{Derivatives and Noise: The Importance of Smoothing}

\InsImageInline{0.5}{l}{dsmooth.GIF.png}It is often said that "differentiation increases the noise". That is true, but it is not the main problem. In fact, computing the unsmoothed first derivative of a set of random numbers \textit{increases its standard deviation by only the square root of 2}, simply due to the usual propagation of errors of the sum or difference between two numbers. As an example, the standard deviation (std) of the numbers generated by the Matlab/Octave randn function is 1.0 and the standard deviation of its first derivative, \texttt{\textbf{std(}}\href{https://terpconnect.umd.edu/~toh/spectrum/deriv1.m}{\texttt{deriv1}}\texttt{\textbf{(randn(size(1:10000))))}}\textbf{,} equals about 1.4. But even a little bit of smoothing (page \pageref{ref-0045}) applied to the derivative will reduce this standard deviation greatly, e.g. a 2-point smooth applied by the fastsmooth function, \texttt{\textbf{std(}}\href{https://terpconnect.umd.edu/~toh/spectrum/fastsmooth.m}{\texttt{\textbf{fastsmooth}}}\texttt{\textbf{(deriv1(randn(size(1:10000))),2,3))}}\texttt{\textbf{,}} equals about 0.4. More important is the fact that the \textit{signal-to-noise ratio} of an \textit{unsmoothed} derivative is almost always much lower (poorer) than that of the original signal, mainly because \textit{the numerical amplitude of the derivative is usually much smaller} (as you can see for yourself in all the examples on this page). But smoothing is \textit{always} used in any practical application to control this problem; with \textit{optimal} smoothing, the signal-to-noise of a derivative can be \textit{greater} than the unsmoothed original. For the successful application of differentiation in quantitative analytical applications, it is essential to use differentiation in combination with sufficient smoothing, to optimize the signal-to-noise ratio. This is illustrated in the figure on the left. (\href{https://terpconnect.umd.edu/~toh/spectrum/derivdemo3.m}{Matlab/Octave code} for this figure.) Window 1 shows a Gaussian band with a small amount of added white noise. Windows 2, 3, and 4, show the first derivative of that signal with increasing smooth widths. As you can see, \textit{without enough smoothing, the signal-to-noise ratio of the derivative can be substantially poorer than the original signal}. However, with adequate amounts of smoothing, the signal-to-noise ratio of the smoothed derivative is much better and can even be visibly better than that of the unsmoothed original. 

\InsImageInline{0.5}{l}{dsmooth2b.GIF.png}This effect of smoothing derivatives is even more striking in the \textit{second} derivative, as shown on the right (\href{https://terpconnect.umd.edu/~toh/spectrum/derivdemo4.m}{Matlab/Octave code} for this figure). In this case, the signal-to-noise ratio of the unsmoothed second derivative (Window 2) is so poor you cannot even see the signal visually, but the smoothed second derivative looks fine. \textit{Differentiation does not actually add noise to the signal}; if there were no noise at all in the original signal, then the derivatives would also have no noise (exception: see page \pageref{ref-0405}).

What is particularly interesting about the noise in these derivative signals, however, is the noise "\href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#Frequency}{color}". This noise is not \textit{white;} rather, it is \textit{blue} - that is, it has much more power at \textit{high} frequencies than white noise. The consequence of this is that the noise in the differentiated signal is easily reduced greatly by \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{\textit{smoothing}}, as demonstrated above.

Because sliding-average smoothing and differentiation are both linear operations, it makes no difference whether the smooth operation is applied before or after the differentiation. What is important, however, is the nature of the smooth, its smooth ratio (ratio of the smooth width to the width of the original peak), and the number of times the signal is smoothed. The optimum values of the smooth ratio for derivative signals are approximately 0.5 to 1.0. For a first derivative, \textit{two} applications of a simple rectangular sliding-average smooth (or one application of a triangular smooth) is adequate. For a second derivative, \textit{three} applications of a simple rectangular smooth or two applications of a triangular smooth are adequate. The general rule is this: for the n\raisebox{4pt}{th} derivative, use a smooth that is the equivalent of at least n+1 applications of a rectangular smooth. The \href{https://terpconnect.umd.edu/~toh/spectrum/SavitzkyGolayHelpFile.txt}{Savitzky-Golay method} is ideal for computing smoothed derivatives because it combines differentiation with the right kind of smoothing. The Matlab signal processing program \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{\textit{iSignal}}, discussed on page \pageref{ref-0433}, uses this approach.

If the peak widths vary substantially across the signal recording - for example, if the peaks get regularly wider as the x-value increases - then it may be helpful to use an \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#SegmentedSmooth}{\textit{adaptive} segmented\index{segmented} smooth} (page \pageref{ref-0396}), which makes the smooth width vary across the signal.

Smoothing derivative signals usually results in a substantial attenuation of the derivative amplitude; in the figure on the right above, the amplitude of the most heavily smoothed derivative (in Window 4) is much less than its less-smoothed version (Window 3). However, this will not be a problem in \textit{quantitative analysis} applications, if the standard (analytical) curve is prepared using the exact same derivative, smoothing, and measurement procedure as is applied to the unknown samples. Because differentiation and smoothing are both \href{http://en.wikipedia.org/wiki/Linearity}{linear techniques}, the amplitude of a smoothed derivative is exactly proportional to the amplitude of the original signal, which allows quantitative analysis applications employing any of the \href{https://terpconnect.umd.edu/~toh/models/Bracket.html}{standard calibration techniques} (page \pageref{ref-0490}). If you apply the \textit{same} signal-processing techniques to the standards as well as to the samples, everything works.

Because of the different kinds and degrees of smoothing that might be incorporated into the computation of digital differentiation of experimental signals, it is difficult to compare the results of different instruments and experiments unless the details of these computations are known. In commercial instruments and software packages, these details may well be hidden. However, if you can obtain both the original (zeroth derivative) signal, as well as the derivative and/or smoothed version from the same instrument or software package, then the technique of Fourier deconvolution, \href{https://terpconnect.umd.edu/~toh/spectrum/Deconvolution.html}{which will be discussed later}, can be used to discover and duplicate the underlying hidden computations.

Interestingly, neglecting to smooth a derivative was ultimately responsible for the failure of the first spacecraft of \href{http://en.wikipedia.org/wiki/Mariner\_program\#Mariners\_1\_and\_2}{NASA's Mariner program} on July 22, 1962, which was reported in InfoWorld's "\href{http://www.infoworld.com/d/security-central/epic-failures-11-infamous-software-bugs-891?page=0,1}{11 infamous software bugs}". In his 1968 book "\href{http://www.amazon.com/The-Promise-Space-Arthur-Clarke/dp/0425075656}{The Promise of Space}", Arthur C. Clarke described the mission as "wrecked by the most expensive hyphen in history." The "hyphen" was in fact a superscript bar over the symbol for velocity (the first derivative of position), handwritten in a notebook. An overbar conventionally signifies an \textit{averaging} or \textit{smoothing} function, so the formula \textit{should} have calculated the \textit{smoothed} value of the time derivative of position. \textit{Without} the smoothing function, even minor variations would cause its derivative to be very noisy and to trigger the corrective boosters to kick in prematurely, causing the rocket's flight to become unstable. 



\textbf{Video Demonstrations}

The first 13-second, 1.5 MByte video (\href{https://terpconnect.umd.edu/~toh/spectrum/SmoothDerivative2.wmv}{SmoothDerivative2.wmv) }demonstrates the huge signal-to-noise ratio improvements that are possible when smoothing derivative signals, in this case, a 4th derivative.

The second video, 17-second, 1.1 MByte, (\href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeBackground2.wmv}{DerivativeBackground2.wmv }) demonstrates the measurement of a weak peak buried in a strong sloping background. At the beginning of this brief video, the amplitude (Amp) of the peak is varied between 0 and 0.14, but the background is so strong that the changes in peak amplitude, located at x = 500, are hardly visible. Then the fourth derivative (Order=4) is computed, and the scale expansion (Scale) is increased, with a smooth width (Smooth) of 88. Finally, the amplitude (Amp) of the peak is varied again over the same range, but now the changes in the signal are now quite noticeable and easily measured. 

\textbf{The differentiation of} \textbf{\textit{analog}} \textbf{signals} can be performed with a simple \href{https://terpconnect.umd.edu/~toh/ElectroSim/Differentiator.html}{operational amplifier circuit}; two or more such circuits can be cascaded to obtain second and higher-order derivatives. The same noise problems described above apply to analog differentiation also, requiring the use of low-pass filter circuits that are analogous to smoothing. 

\uline{\textbf{SPECTRUM,}} (page \pageref{ref-0481}) a freeware signal-processing application for Macintosh OS8, includes first and second derivative functions, which can be applied successively to compute any derivatives order. 

\section{Differentiation in Spreadsheets\label{ref-0093}\label{ref-0094}\label{ref-0095}}

\InsImageInline{0.5}{l}{image38.png}\InsImageInline{0.5}{l}{image39.png}Differentiation operations such as described above can readily be performed in spreadsheets such as Excel or OpenOffice Calc. Both the derivative and the required smoothing operations can be performed by the shift-and-multiply method described in the chapter on \href{https://terpconnect.umd.edu/~toh/spectrum/smoothing.html}{smoothing} (page \pageref{ref-0045}). In principle, it is possible to combine any degree of differentiation and smoothing into one set of shift-and-multiply coefficients (\href{https://terpconnect.umd.edu/~toh/spectrum/CombinedDerivativesAndSmooths.txt}{as illustrated here}), but it is more flexible and easier to adjust if you compute the derivatives and each stage of smoothing separately in successive columns. This is illustrated by\href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeSmoothing.ods}{ DerivativeSmoothing.ods} for OpenOffice Calc and \href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeSmoothing.xls}{DerivativeSmoothing.xls} for Excel, which smooths the data and computes the first derivative of Y (column \textbf{B}) with respect to X (column \textbf{A}), then applies that smoothing and differentiation process successively to compute the smoothed second and third derivatives. The same smoothing coefficients (in row \textbf{5}, columns \textbf{K} through \textbf{AA}) are applied successively for each stage of differentiation; you can enter any set of numbers here (preferably symmetrical about the center number in column \textbf{S}). You can type or paste your own data into columns \textbf{A} and \textbf{B} (X and Y), rows \textbf{8} to \textbf{26}3.

\href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeSmoothingWithNoise.xlsx}{DerivativeSmoothingWithNoise.xlsx} (above \href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeSmoothingWithNoise.png}{left}) demonstrates the effect of smoothing on the signal-to-noise ratio of derivatives of a weak peak located at x = 1200 on a sloping baseline. It uses the same data as \href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeSmoothing.xls}{DerivativeSmoothing.xls} but adds simulated white noise to the Y data. You can control the amount of added noise (cell \textbf{D5}).

Another example of a derivative application is the spreadsheet \href{https://terpconnect.umd.edu/~toh/spectrum/SecondDerivativeXY2.xlsx}{SecondDerivativeXY2.xlsx} (\href{https://terpconnect.umd.edu/~toh/spectrum/SecondDerivativeXY2.png}{right}), which demonstrates locating and measuring changes in the second derivative (a measure of curvature or acceleration) of a time-changing signal. This spreadsheet shows the apparent increase in noise caused by differentiation and the extent to which the noise can be reduced by smoothing (in this case by two passes of a 5-point triangular smooth). The smoothed second derivative shows a large peak the point at which the acceleration changes (at x=30), and the baseline on either side of the peak is distinctly unequal, showing the change in the acceleration before and after the peak (y=2 and 4, respectively).\label{ref-0096}

\section{Differentiation in Matlab and \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{Octav}\href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{e}\label{ref-0097}\label{ref-0098}}

Finite difference differentiation functions such as described above can easily be created in Matlab or \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{Octave}. Some simple derivative functions for equally-spaced time series data: \href{https://terpconnect.umd.edu/~toh/spectrum/deriv.m}{deriv}, a first derivative using the 2-point central-difference method, \href{https://terpconnect.umd.edu/~toh/spectrum/deriv1.m}{deriv1}, an unsmoothed first derivative using adjacent differences, \href{https://terpconnect.umd.edu/~toh/spectrum/deriv2.m}{deriv2}, a second derivative using the 3-point central-difference method, a third derivative \href{https://terpconnect.umd.edu/~toh/spectrum/deriv3.m}{deriv3} using a 4-point formula, and \href{https://terpconnect.umd.edu/~toh/spectrum/deriv4.m}{deriv4}, a 4th derivative using a 5-point formula. Each of these is a simple Matlab function of the form \textbf{d=deriv(y)}; the input argument is a signal vector "\textbf{y}", and the differentiated signal is returned as the vector "\textbf{d}". For data that are \textit{not} equally-spaced on the independent variable (x) axis, there are versions of the first and second derivative functions, \href{https://terpconnect.umd.edu/~toh/spectrum/derivxy.m}{derivxy }and \href{https://terpconnect.umd.edu/~toh/spectrum/secderivxy.m}{secderivxy}, that take two input arguments (x,y), where \textbf{x} and \textbf{y} are vectors containing the independent and dependent variables

\href{https://terpconnect.umd.edu/~toh/spectrum/SmoothDerivative.m}{SmoothDerivative.m} combines differentiation and smoothing. The syntax is SmoothedDeriv = SmoothedDerivative(x,y,DerivativeOrder,w,type,ends) where 'DerivativeOrder' determines the derivative order (0 through 5), 'w' is the smooth width, 'type' determines the smooth mode:

If type=0, the signal is not smoothed

If type=1, rectangular (sliding-average or boxcar)

If type=2, triangular (2 passes of sliding-average)

If type=3, p-spline (3 passes of sliding-average)

If type=4, Savitzky-Golay smooth

'ends' controls how the "ends" of the signal (the first w/2 points and the last w/2 points) are handled: If ends=0, the ends are zeroed: If ends=1, the ends are smoothed with progressively smaller smooths the closer to the end. Type ``help SmoothDerivative'' for some examples (\href{https://terpconnect.umd.edu/~toh/spectrum/SmoothDerivative.png}{graphic}).

\textbf{Peak detection}. The simplest code to find peaks in \textit{x,y} data sets simply looks for every \textit{y} value that has lower \textit{y} values on both sides (\href{https://terpconnect.umd.edu/~toh/spectrum/allpeaks.m}{allpeaks.m}). An alternative approach is to use the first derivative to find all the maxima by locating the points of zero-crossing, that is, the points at which the first derivative "d" (computed by \href{https://terpconnect.umd.edu/~toh/spectrum/derivxy.m}{derivxy.m}) passes from positive to negative. In this example, the ``sign'' function is a built-in function that returns 1 if the element is greater than zero, 0 if it equals zero, and -1 if it is less than zero. The routine prints out the value of x and y at each zero-crossing: 

\texttt{d=derivxy(x,y);}

\texttt{for j=1:length(x)-1} 

  \texttt{if sign(d(j)){\textgreater}sign(d(j+1))}

  \texttt{disp([x(j) y(j)])}

  \texttt{end}

\texttt{end}

If the data are noisy, many false zero crossings will be reported, but smoothing the data will reduce that. If the data are sparsely sampled, a more accurate value for the peak position (x-axis value at the zero-crossing) can be obtained by interpolating between the point before and the point after the zero-crossing, using the Matlab/Octave ``interp1'' or ``spline'' function: 

\texttt{interp1([d(j) d(j+1)],[x(j) x(j+1)],0)} 

\InsImage{1.0}{DerivativeDemoMedium.png}
~\href{https://terpconnect.umd.edu/~toh/spectrum/ProcessSignal.m}{\textbf{ProcessSignal}}\uline{\textbf{.m}} is a Matlab/Octave command-line function that performs smoothing and differentiation on the time-series data set x,y (column or row vectors). Type "help ProcessSignal". It returns the processed signal as a vector that has the same shape as x, regardless of the shape of y. The syntax is \texttt{Processed = ProcessSignal(x, y, DerivativeMode, w, type, ends, Sharpen, factor1, factor2, Symize, Symfactor, SlewRate, MedianWidth)}

\textbf{DerivativeDemo.m} is a self-contained Matlab/Octave demo function that uses \href{https://terpconnect.umd.edu/~toh/spectrum/ProcessSignal.m}{ProcessSignal.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/plotit.m}{plotit.m} to demonstrate an application of differentiation to the quantitative analysis of a peak buried in an unstable background (e.g. as in various forms of spectroscopy). The object is to derive a measure of peak amplitude that varies linearly with the actual peak amplitude and is minimally affected by the background and the noise. To run it, just type DerivativeDemo at the command prompt. You can change several of the internal variables (e.g., Noise, BackgroundAmplitude) to make the measurement harder or easier. Note that, even though the magnitude of the derivative seems to be numerically smaller than the original signal (because it has different units), the signal-to-noise ratio of the derivative is better than that of the original signals and is much less affected by the background instability. 

\InsImageInline{0.5}{l}{AnimatedDerivative.gif.png}
~\href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{\textbf{iSignal}}\textbf{.m} (page \pageref{ref-0433}), shown on the left) is an interactive function for Matlab that performs many signal-processing operations that are covered in this book, including differentiation and smoothing for time-series signals, up to the 5\raisebox{4.5pt}{th} derivative, automatically including the required type of smoothing. Simple keystrokes allow you to adjust the smoothing parameters (smooth type, width, and ends treatment) while observing the effect on your signal dynamically. In the \href{https://terpconnect.umd.edu/~toh/spectrum/AnimatedDerivative.gif}{animated GIF example} shown here, a series of three peaks at x=100, 250, and 400, with heights in the ratio 1:2:3, are buried in a strong curved background; the smoothed second and fourth derivatives are computed to suppress that background. View the code \href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{here} or download the \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal7.zip}{ZIP file} with sample data for testing. The interactive keypress operation works even if you run \href{https://www.mathworks.com/products/matlab-online.html}{Matlab in a web browser}, but not on \href{https://itunes.apple.com/us/app/matlab-mobile/id370976661?mt=8}{Matlab Mobile} or in Octave. (Note: figures like the one above that display ``Screencast-O-Matic'' in the lower-left are \textit{animated} graphics that can be viewed in a web browser but will not animate in most PDF viewers.)

As an example of smoothing in iSignal, the following statements generate the 4\raisebox{4.5pt}{th} derivative of a noisy Gaussian peak and display it in \textbf{iSignal}. You will need to download \href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{isignal.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m,}{gaussian.m,} and \href{https://terpconnect.umd.edu/~toh/spectrum/deriv4.m}{deriv4.m} before executing the following statements. 

\texttt{{\textgreater}{\textgreater} x=[1:.1:300]';}

\texttt{{\textgreater}{\textgreater} y=deriv4(100000.*gaussian(x,150,50)+.1*randn(size(x)));}

\texttt{{\textgreater}{\textgreater} isignal(x,y);}

The signal is mostly blue noise (because of the differentiated white noise) unless you smooth it considerately. Use the \textbf{A} and \textbf{Z} keys to increase and decrease the smooth width and the \textbf{S} key to cycle through the available smooth types. Hint: use the P-spline smooth and keep increasing the smooth width. 

The script ``\href{https://terpconnect.umd.edu/~toh/spectrum/iSignalDeltaTest.m}{iSignalDeltaTest}'' demonstrates the frequency response of the smoothing and differentiation functions of iSignal by applying them to a \href{https://en.wikipedia.org/wiki/Dirac\_delta\_function}{delta function}. Change the smooth type, smooth width, and derivative order and see how the power spectrum changes. 

\textit{Real-time} differentiation in Matlab is discussed \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#realtime}{on} page \pageref{ref-0417}\textbf{.} \label{ref-0099}

\chapter{Peak Sharpening\label{ref-0100}\label{ref-0101}}

The figure below shows a spectrum on the left that consists of several poorly-resolved (that is, partly overlapping) bands. The extensive overlap of the bands makes the accurate measurement of their intensities and positions impossible, even though the signal-to-noise ratio is very good. Things would be easier if the bands were more completely resolved, that is, if the bands were narrower. 



\InsImage{1.0}{image41.gif.png}
\begin{center}
\textit{A peak sharpening algorithm applied to the signal on the left artificially improves the apparent resolution of the peaks. In the resulting signal, right, you can measure the intensities and positions of the peaks more accurately, but at the cost of a decrease in signal-to-noise ratio.}
\end{center}


\section{\textbf{Even derivative sharpening}\label{ref-0102}}

The technique used here, called \textit{peak sharpening or resolution enhancement}, uses algorithms to artificially improve the apparent resolution of the peaks. One of the simplest such algorithms computes the weighted sum of the original signal and the negative of its second derivative:


\begin{center}
 R\textsubscript{j} = Y\textsubscript{j} - k\textsubscript{2}Y\textsuperscript{''}
\end{center}


where R\textsubscript{j} is the resolution-enhanced signal, Y is the original signal, Y'' is the second derivative of Y, and k\textsubscript{2} is a user-selected 2\textsuperscript{nd} derivative weighting factor. It is left to the user to select the weighting factor k\textsubscript{2} which gives the best trade-off between the extent of sharpening, signal-to-noise degradation, and baseline flatness. The optimum choice depends upon the width, shape, and digitization interval of the signal. As an inevitable trade-off, the signal-to-noise ratio is degraded, but this can be moderated by \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothing} (page \pageref{ref-0045}), but at the expense of reducing the sharpening. Nevertheless, this technique will be \textit{useful only if the overlap of peaks rather than the signal-to-noise ratio is the limiting factor}. 

Here is how it works. The figure below shows, in Window 1, a computer-generated peak (with a Lorentzian shape) in red, superimposed on the \textit{negative} of its second derivative in green). 


\begin{center}
\InsImage{1.0}{re1.GIF.png} 
\end{center}


The second derivative is amplified (by multiplying it by an adjustable constant) so that the negative sides of the inverted second derivative (from approximately X = 0 to 100 and from X = 150 to 250) are a mirror image of the sides of the original peak over those regions. In this way, when the original peak is added to the inverted second derivative, the two signals will \textit{approximately} cancel out in the two side regions but will reinforce each other in the central region (from X = 100 to 150). The result, shown in Window 2, is a substantial (about 50\%) reduction in the width, and a corresponding increase in height, of the peak. This effect is most dramatic with Lorentzian-shaped peaks; with Gaussian-shaped peaks, \InsImageInline{0.5}{l}{LorentzianSharpeningExample.png}the resolution enhancement is less dramatic (\href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedGaussian.png}{only about 20 - 30\%}). 

The reduced widths of the sharpened peaks make it easier to distinguish overlapping peaks. In the example on the right, the computer-synthesized raw signal (blue line) is the sum of three overlapping Lorentzian peaks at x=200, 300, and 400. The peaks are very wide; their halfwidths are 200, which is greater than their separation. The result is that the peaks overlap so much in the raw data, that they form what \textit{looks like a single wide asymmetrical peak} (blue line) with a maximum at x=220. However, the result of the even derivative sharpening algorithm (red line) shows the underlying component peaks \textit{at their correct positions}. The baseline, however, which was originally zero far from the peak center, has been shifted, as you can see from x=25 to 100.

Note that the baseline of either side of the resolution-enhanced peak is not quite flat, especially for a Lorentzian peak, because the cancellation of the original peak and the inverted second derivative is only approximate; the adjustable weighting factor k is selected to minimize this effect. Peak sharpening will have little or no effect on the baseline, because if the baseline is linear, its derivative will be zero, and if it is gradually curved, its second derivative will be very small compared to that of the peak.

This technique has been used in various forms of spectroscopy and chromatography for many years (references 74-76), even in some cases using analog electronics. Mathematically, this technique is a simplified version of a converging \href{https://en.wikipedia.org/wiki/Taylor\_series}{Taylor series} expansion, in which only the even order derivative terms in the expansion are taken and for which their coefficients alternate in sign. The above example is the simplest possible version that includes only the first two terms - the original peak and its negative second derivative. Slightly better results can be obtained by adding a fourth derivative term, with two adjustable factors k2 and k4: 

  Rj = Yj - k2Y'' + k4Y''''

where Y'' and Y'''' are the 2\textsuperscript{nd} and 4\textsuperscript{th} derivatives of Y. The result is a 21\% reduction in width for a Gaussian peak, as shown in the figure on the left (\href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedGaussianDemo.m}{Matlab/Octave script}), and a 60\% reduction for a \href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedLorentzianDemo.png}{Lorentzian} peak (\href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedLorentzianDemo.m}{script}). This algorithm was used in the overlapping peak example above. (It is possible to add a \href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedGaussianDemo4terms.m}{sixth derivative term}, but the series converges quickly and the \href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedGaussianDemo4terms.png}{results }are only slightly improved, at the cost of the increased complexity of three adjustable factors). 

\InsImage{0.5}{image44.png}\InsImage{0.5}{image45.png}There is no universal optimum value for the derivative weighting factors; it depends on what you consider the best trade-off between peak sharpening and baseline flatness. However, a good place to start for a Gaussian peak is k2 = W\textsuperscript{2}/32 and k4 = W\textsuperscript{4}/900, where W is the halfwidth of the peak in x units (for example, time in chromatography). With those weighting factors, a Gaussian peak will be reduced in width by 21\%, the baseline will still be visually flat, and the resulting peak will fit a Gaussian model with a percent fitting error of less than 0.3\% and an R\textsuperscript{2} of 0.9999. Larger values of k will result in a narrower peak, but the baseline will not be so flat.  For a Lorentzian original shape (right), with k2=W\textsuperscript{3}/3 and k4 = W\textsuperscript{4}/600, the peak width is reduced by a factor of 3, but the resulting peak fits a Gaussian model with a larger percent fitting error of 1.15\% and an R\textsuperscript{2} of 0.9966. Note that the k factors for the second and fourth derivatives vary with the width W raised to the 2\textsuperscript{nd }and 4\textsuperscript{th} power respectively, so they can vary over a very wide numerical range for peaks of different width. For this reason, if the peak widths vary substantially across the signal, it is useful to use \textit{segmented}\index{\textit{segmented}} and \textit{gradient} versions of this method so that the sharpening can be optimized for each region of the signal (\href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html\#segmented}{see below}). ``Segmented'' means each segment is defined independently; ``gradient" means a gradual increase or decrease between specified start and end values.

\section{\textbf{Constant-area first-derivative symmetrization (``de-tailing'')}\label{ref-0103}\label{ref-0104}\label{ref-0105}\label{ref-0106}\label{ref-0107}\label{ref-0108}\label{ref-0109}\label{ref-0110}}

If the peak is \textit{asymmetrical} - that is, slopes down faster on one side than the other - then the weighted addition (or subtraction) of a \textit{first derivative} term, Y', may be helpful, because the first derivative of a peak is \textit{antisymmetric} (positive on one side and negative on the other). In the graphic example below, on the left, the asymmetrical peak (in blue) tails to the right, and its first derivative, Y', (dotted yellow) has a positive lobe on the left and a broader but smaller negative lobe on the right. When the peak is added to the weighted first derivative, the \textit{positive lobe of the derivative reinforces the leading edge} and the \textit{negative lobe suppresses the trailing edge,} resulting in improved symmetry. (Had the EMG sloped to the \textit{left}, the \textit{negative} of its derivative would be added). This is also an old technique, having been used in chromatography since at least 1965 (reference 75, 76), where it has been called ``de-tailing''. 


\begin{center}
S\textsubscript{j} = Y\textsubscript{j} + k\textsubscript{1}Y’
\end{center}


\InsImageInline{0.5}{l}{image46.png}

 In fact, this simple technique can be shown to work perfectly for \textit{exponentially broadened} peaks \textit{of any shape}, such as the "\href{https://en.wikipedia.org/wiki/Exponentially\_modified\_Gaussian\_distribution}{exponentially modified Gaussian}" (\href{https://terpconnect.umd.edu/~toh/spectrum/expgaussian2.m}{EMG}) shape shown here (reference 73).

With the correct first derivative weighting factor, k\textsubscript{1}, the result is a symmetrical Gaussian with a \href{https://en.wikipedia.org/wiki/Full\_width\_at\_half\_maximum}{half-width} substantially less than that of the original (orange line); in fact, it is exactly the underlying Gaussian to which the exponential convolution has been applied ({\hyperref[ref-0533]{References}} 70, 71). The first derivative weighting factor k\textsubscript{1} is independent of the peak height and width and is simply equal to the exponential time constant \textit{tau} (1/lambda, in some formulations of the EMG). It works perfectly if the \textit{tau} of the peak is the same. In practice, k\textsubscript{1} must be determined experimentally, which is most easily done for the last peak in a group of peaks (\href{https://terpconnect.umd.edu/~toh/spectrum/SymmetricalizationAnimation3peaks.png}{graphic}, \href{https://terpconnect.umd.edu/~toh/spectrum/SymmetricalizationAnimation3peaks.gif}{animation}). Put simply, if you get k\textsubscript{1} too high, the result will dip below the baseline after the peak. It is easy to determine the optimal value experimentally; just increase it until the processed signal dips below the baseline after the peak, then reduce it until the baseline is flat, as shown in the \href{https://terpconnect.umd.edu/~toh/spectrum/SymmetricalizationAnimation3peaks.gif}{GIF animation at this link}. And if one stage of derivative addition does not do the trick, try one of the double exponential routines described below.

Furthermore, this appears to be a general behavior and it works similarly for any other peak shape that is broadened by exponential convolution, such a Lorentzian, and it even works for peaks that are already broadened by a previous exponential convolution (i.e., a double exponential), which can be handled by two successive stages of derivative addition with different \textit{taus}.

The symmetrized peak Sj resulting from the first-derivative addition procedure can still be further sharpened by the even-derivative techniques described above, assuming that the signal-to-noise ratio of the original is good enough.

A useful property of all these derivative addition algorithms is that they do not change the \textit{total} area under the peaks because the \textit{total} area under the curve of \textit{any} derivative of any peak-shaped signal that returns to the baseline is essentially \textit{zero} (the area under the negative lobes cancels the area under the positive lobes). Therefore, these techniques can be helpful in measuring the \href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html}{areas under overlapped peaks} (page \pageref{ref-0172}). However, a remaining problem is that the baseline on either side of the sharpened peak may not be \textit{perfectly flat}, leaving some interference from nearby peaks, even if baseline resolution of adjacent peaks is achieved. For the even-derivative technique applied to a Gaussian peak, about 99.7\% (\href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedGaussian.png}{graphic link)} of the area of the peak is contained in the central maximum, and for a Lorentzian peak, about 80\% of the area of the peak (\href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedLorentzian.png}{graphic link)} is contained in the central maximum.

Because differentiation and smoothing are both \href{https://en.wikipedia.org/wiki/Linear\_system}{linear techniques}, the \href{https://en.wikipedia.org/wiki/Superposition\_principle}{superposition principl}e applies and the amplitude of a symmetrized or sharpened signal is directly proportional to the amplitude of the original signal, which allows quantitative analysis applications employing any of the standard calibration techniques (page \pageref{ref-0488}. Provided you apply the same signal-processing techniques to the standards as well as to the samples, everything works. 

Peak sharpening can be useful in automated peak detection and measurement (page \pageref{ref-0289}) to increase the ability to detect weak overlapping peaks that appear only as shoulders in the original signal. If you are reading this online, click for an \href{https://terpconnect.umd.edu/~toh/spectrum/demo5.gif}{animated example}. Peak sharpening can also be useful before \href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html}{measuring the areas} (page \pageref{ref-0188}) under overlapping peaks, because it is easier and more accurate to measure the areas of peaks that are more completely separated. 

\section{\textbf{The Power Law Method}\label{ref-0111}\label{ref-0112}}

\InsImageInline{0.5}{l}{PowerLawDemo.png} A simple method for peak sharpening involves raising each data point to a power \textbf{\textit{n}} greater than (\href{https://terpconnect.umd.edu/~toh/spectrum/Introduction.html}{reference 61}, \href{https://terpconnect.umd.edu/~toh/spectrum/Introduction.html}{63}). The effect of this is to change the peak shapes, essentially stretching out the highest center region of the peak to greater amplitudes and placing more weight on the points near the peak, resulting in a \textit{smaller peak width.} For Gaussian peaks specifically, the result is another Gaussian with a width reduced by the square root of the power \textbf{\textit{n}}. The technique is demonstrated by the Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/PowerLawDemo.m}{PowerLawDemo.m}, shown in the figure on the right, which plots noisy Gaussians raised to the power p=1 to 7, peak heights normalized to 1.0, showing that as the power increases, peak width decreases and noise is reduced on the baseline but increased on the peak maximum. Since this process does not move the positions of the peaks, the peak resolution (defined as the ratio of peak separation to peak base width) is increased. For Gaussian peaks, the area under the original peak can be calculated from the area under the normalized power-sharpened curve (\href{https://link.springer.com/article/10.1007\%2Fs10337-018-3607-0}{reference 63}). 

\InsImageInline{0.5}{l}{PowerMethod.png}In the figure on the left, the blue line shows two slightly overlapping EMG (exponentially modified Gaussian) peaks. The other lines are the result of raising the data to the power of \textbf{\textit{n}} = 2, 3, and 4 and normalizing each to a height of 1.00. The results are more nearly Gaussian peak shapes (only because most peak shapes are locally Gaussian near the peak maximum), and the peak widths, measured with the\href{https://terpconnect.umd.edu/~toh/spectrum/halfwidth.m}{ halfwidth.m} function, are reduced: 19.2, 12.4, 9.9, and 8.4 units for powers 1 through 4, respectively. This method is independent of, and can be used in conjunction with, the derivative method discussed above. However, for a signal of two overlapping Gaussians, the result of raising the signal to a power is not the same as adding two power-narrowed Gaussians: simply, a\textsuperscript{\textit{n}}+b\textsuperscript{\textit{n}} is not the same as (a+b)\textsuperscript{\textit{n}} for \textit{n}{\textgreater}1. This can be demonstrated graphically by the script \href{https://terpconnect.umd.edu/~toh/spectrum/PowerPeaks.m}{PowerPeaks.m} (\href{https://terpconnect.umd.edu/~toh/spectrum/PowerPeaks.png}{graphic}), which curve-fits a two-Gaussian model to the power-raised sum of two overlapping Gaussians; as the power \textit{n} increases, the peaks are narrowed and the valley between them is deepened, but the resulting signal is no longer the sum of two Gaussians unless the resolution is sufficiently high that the two peaks do not overlap significantly.

Some limitations to the power-law method are:

 (a) It only works if the peaks of interest make a distinct maximum (it is not effective for side peaks that are so small that they only form \textit{shoulders}; there \textit{must} be a valley between the peaks). 

 (b) The baseline must be zero for best results. 

 (c) For noisy signals there is a decrease in signal-to-noise ratio because the smaller width means fewer data points are contributing to the measurement (smoothing, page \pageref{ref-0045}, can help). 

\textbf{Compensating for the non-linearity}. Naturally, the power method introduces severe non-linearity into the signal, changing the ratios between peak heights (as is evident in the figure above right) and complicating further processing, especially quantitative measurement calibration. But there is an easy way to compensate for this: after the raw data have been raised to the power \textit{n} and peaks heights and/or areas have been measured, the resulting peak measures can be simply raised to the power 1/\textit{n}, restoring the original linearity (but, notably, not the slope) of the calibration curves used in quantitative analytical measurements. (This works because the peak area is proportional to the height times width, and peak height of the power transformed peaks is proportional to the nth power of the original height, but the width of the peak is not a function of peak height at constant \textit{n}, thus the area of the transformed peaks remains proportional to nth power of the original height). The technique is demonstrated quantitatively for two variable overlapping peaks by the Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/PowerLawCalibrationDemo.m}{PowerLawCalibration-Demo.m} (\href{https://terpconnect.umd.edu/~toh/spectrum/PowerLawCalibrationDemo.png}{graphic}), which takes the \textit{n}\textsuperscript{th} power of the overlapping-peak signal, measures the areas of the power-narrowed peaks, and then takes the 1/\textit{n} power of the measured areas, constructing and using a calibration curve to convert areas to concentration. Peak areas are measured by perpendicular drop, using the half-way point to mark the boundary between the peaks. The script simulates a mixture signal with concentrations that you can specify in lines 15 and 16. You can change the power and any of the parameters in lines 14-22. The results show that the power method improves the accuracy of the measurements as long as the 4-sigma resolution (the ratio of peak separation to 4 times the sigma of the Gaussians) is above about 0.4. It is most accurate when the peaks are roughly equal in width and when the ratio of the two concentrations are not very different from the ratio in the standards from which the calibration curve is constructed. Note that, even when the random noise (in line 22) is zero, the results are not perfect because of peak overlap on area measurement, which varies depending upon the ratio of two components in the mixture. 

The self-contained function \href{https://terpconnect.umd.edu/~toh/spectrum/PowerMethodDemo.m}{PowerMethodDemo.m} demonstrates the power method for measuring the area of small shouldering peak that is partly overlapped by a much stronger interfering peak (\href{https://terpconnect.umd.edu/~toh/spectrum/PowerMethod2.png}{graphic}). It shows the effect of random noise, smoothing, and any uncorrected background under the peaks. 

\textbf{Combining sharpening methods}. The power method is independent of, and can be used in conjunction with, the derivative methods discussed above. However, because the power method is non-linear, the \textit{order in which the operations are} performed is important. The \textit{first} step should be the first-derivative symmetrization if the signal is exponentially broadened, the \textit{second} step should be even-derivative sharpening, and the power method should be used \textit{last}. The reason for this order is that the power method depends on, but cannot create, a valley between highly overlapped peaks, whereas the derivative methods may be able to create a valley between peaks if the overlap is not too severe. Moreover, when used last, the power method reduces the severity of baseline oscillations that are a residue of the even-derivative sharpening (particularly noticeable on a Lorentzian peak). The Matlab/Octave scripts \href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedGaussianDemo2.m}{SharpenedGaussianDemo2.m} (\href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedGaussianDemo2.png}{Graphic}) and \href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedLorentzianDemo2.m}{SharpenedLorentzianDemo2.m} (\href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedLorentzianDemo2.png}{Graphic}) make this point for Gaussian and Lorentzian peaks respectively, comparing the result of even-derivative sharpening alone with even-derivative sharpening followed by the power method (and preforming the power method two ways, taking the square of the sharpened peak or multiplying it by the original peak). For both the Gaussian and Lorentzian original peak shapes, the final sharpened results are fit to Gaussian models to show the changes in peak parameters. The result is that the combination of methods yields (a) the narrowest final peak and (b) the closest to Gaussian final shape. Of course, the linearity issues of the power method remain, but they can be compensated as before.

\textbf{Deconvolution}. Another signal processing technique that can increase the resolution of overlapping peaks is \textit{deconvolution} (page \pageref{ref-0147}). It is applicable in the situation where the original shape of the peaks has been broadened and/or made asymmetrical by some broadening process or function. If the broadening process can be described mathematically or measured separately, then deconvolution from the observed broadened peaks is in principle capable of extracting the shape of the underlying peak. 

\href{https://terpconnect.umd.edu/~toh/spectrum/SPECTRUM.html}{SPECTRUM,} page \pageref{ref-0481}, the freeware signal-processing application for Mac OS8 and earlier, includes this resolution-enhancement algorithm, with adjustable weighting factor and derivative smoothing width. 

\section{\textbf{Peak Sharpening for Excel and Calc Spreadsheets}\label{ref-0113}}

The even-derivative sharpening method with two derivative terms (2\textsuperscript{nd} and 4\textsuperscript{th}) is available for Excel and Calc in the form of an empty template (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningDeriv.xlsx}{PeakSharpeningDeriv.xlsx} and \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningDeriv.ods}{.ods}) or with example data entered (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningDerivWithData.xlsx}{PeakSharpeningDerivWithData.xlsx} and \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningDerivWithData.ods}{.ods}). You can either type in the values of the derivative weighting factors K1 and K2 directly into cells \textbf{J3} and \textbf{J4}, or you can enter the estimated peak width (FWHM in number of data points) in cell \textbf{H4} and the spr\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningDemo.xlsm}{}eadsheet will calculate K1 and K2. There is also a demonstration version with adjustable simulated peaks which you can experiment with (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningDemo.xlsx}{PeakSharpeningDemo.xlsx} and \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningDemo.ods}{PeakSharpeningDemo.ods}). 

 There are also \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningDemo.xlsm}{versions that have clickable buttons} (detail on left) for convenient interactive adjustment of the K1 and K2 factors by 1\% or by 10\% for each click. You can type in first estimates for K1 and K2 directly into cells J4 and J5 and then use the buttons to fine-tune the values. If the signal is noisy, adjust the smoothing using the 17 coefficients in row 5 columns \textbf{K} through \textbf{AA}, just as with the \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html\#Spreadsheets}{smoothing spreadsheets} (page \pageref{ref-0068}). (Note: Unfortunately, these ActiveX buttons do not work in the iPad version of Excel).

\InsImageInline{0.5}{l}{ClickButtons.png}
There is also a ``segmented'' template version where the sharpening constants can be specified for each of 20 signal segments (\href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedPeakSharpeningDeriv.xlsx}{SegmentedPeakSharpeningDeriv.xlsx}). For those applications in which the peak widths gradually increase (or decrease) with time, there is also a \textit{gradient} peak sharpening template where you need only set the starting and ending peak widths and the spreadsheet will apply the required sharpening factors K1 and K2. (\href{https://terpconnect.umd.edu/~toh/spectrum/GradientPeakSharpeningDeriv.xlsx}{GradientPeakSharpeningDeriv.xlsx}) and an example with data already entered (\href{https://terpconnect.umd.edu/~toh/spectrum/GradientPeakSharpeningDerivExample.xlsx}{GradientPeakSharpeningDerivExample.xlsx});

The template \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSymmetricalizationTemplate.xlsm}{PeakSymmetricalizationTemplate.xlsm} (screen image on the next page) performs symmetrization of exponentially modified Gaussians (EMG) by the weighted addition of the first derivative. \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSymmetricalizationTemplate.xlsm}{PeakSymmetricalizationExample.xlsm} is an example application with sample data already typed in. It is shown on the next page.

There is also a demo version that allows you to determine the accuracy of the technique by synthesizing overlapping peaks with specified resolution, asymmetry, relative peak height, noise, and baseline: \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningAreaMeasurementEMGDemo2.xlsm}{PeakSharpeningAreaMeasurementEMGDemo2.xlsm} (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningAreaMeasurementDemoEMG3.png}{graphic}). These spreadsheets also allow further second derivative sharpening of the resulting symmetrical peak. 

\href{https://terpconnect.umd.edu/~toh/spectrum/PeakDoubleSymmetrizationExample.xlsm}{PeakDoubleSymmetrizationExample.xlsm} performs the symmetrization of a doubly exponential broadened peak. It has buttons to interactively adjust the two first-derivative weightings. Two variations (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakDoubleSymmetrizationExample1.xlsm}{1}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDoubleSymmetrizationExample2.xlsm}{2}) include data for two overlapping peaks, for which the areas are measured by perpendicular drop.

\href{https://terpconnect.umd.edu/~toh/spectrum/EffectOfNoiseAndBaselineNormalVsPower.xlsx}{EffectOfNoiseAndBaselineNormalVsPower.xlsx} demonstrates the effect of the power method on area measurements of Gaussian and exponentially broadened Gaussian peaks, including the different effects that random noise and non-zero baseline has on the power sharpening method.

\InsImageInline{0.5}{l}{image50.png}\textit{The spreadsheet} \textit{template} \textbf{\textit{\textnormal{``}}}\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSymmetricalizationTemplate.xlsm}{\textit{PeakSymmetricalizationTemplate.xlsm}}\textit{'' is shown measuring the areas of the first of two pairs of overlapping asymmetrical peaks after applying first derivative symmetrization.}\label{ref-0114}

\section{\textbf{Peak Sharpening for Matlab and} \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{\textbf{Octave}}\label{ref-0115}}

The custom Matlab/Octave function \href{https://terpconnect.umd.edu/~toh/spectrum/sharpen.m}{sharpen} has the form \texttt{SharpenedSignal = sharpen (signal,k1,k2, SmoothWidth)}, where "signal" is the original signal vector, the arguments k2 and k4 are 2\textsuperscript{nd} and 4\textsuperscript{th}derivative weighting factors, and SmoothWidth is the width of the built-in smooth. The resolution-enhanced signal is returned in the vector "SharpenedSignal ". If you are reading this online, you can click on the link above to inspect the code, or right-click to download for use within Matlab or Octave. The \textbf{k} values determine the trade-off between peak sharpness and baseline flatness; the values vary with the peak shape and width and should be adjusted for your own needs. For peaks of Gaussian shape, a reasonable value for k\textsubscript{2} is PeakWidth\textsuperscript{2}/32 and for k\textsubscript{4} is PeakWidth\textsuperscript{4}/900 (or PeakWidth\textsuperscript{2}/8 and PeakWidth\textsuperscript{4}/700 for Lorentzian peaks), where PeakWidth is the full width at half maximum of the peaks in x units. Because sharpening methods are typically sensitive to random noise in the signal, it is usually necessary to apply a smoothing operation: the Matlab/Octave \href{https://terpconnect.umd.edu/~toh/spectrum/ProcessSignal.m}{ProcessSignal.m} function allows both sharpening and smoothing to be applied in one function. 

Here is a simple Matlab/Octave example that creates a signal consisting of four partly overlapping Gaussian peaks of equal height and width, applies both the derivative sharpening method and the power method, and compares a plot (shown below) comparing the original signal (in blue) to the resolution-enhanced version (in red).

 \texttt{x=0:.01:18;}

  \texttt{y=exp(-(x-4).\textasciicircum{}2)+exp(-(x-9).\textasciicircum{}2)+exp(-(x-12).\textasciicircum{}2)+exp(-(x-13.7).\textasciicircum{}2);}

  \texttt{y=y+.001.*randn(size(x));}

  \texttt{k1=1212;k2=1147420;} 

  \texttt{SharpenedSignal=ProcessSignal(x,y,0,35,3,0,1,k1,k2,0,0,0,0);}

  \texttt{figure(1)}

  \texttt{plot(x,y,x,SharpenedSignal,'r')}

  \texttt{title('Peak sharpening (red) by the derivative method')}

  \texttt{figure(2)}

  \texttt{plot(x,y,x,y.\textasciicircum{}6,'r')}

  \texttt{title('Peak sharpening (red) by the power method')}


\begin{center}
\InsImageInline{0.5}{l}{DerivSharp4peaks.png}
\end{center}



\begin{center}
\textit{Four overlapping Gaussian peaks of equal height and width.} 
\end{center}



\begin{center}
\textit{Blue: Original. Red: After sharpening by the even-derivative method.} 
\end{center}


\InsImageInline{0.5}{l}{SharpenedOverlapDemo2.png}\href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedOverlapDemo.m}{SharpenedOverlapDemo.m} is a script that \textit{automatically determines the optimum degree of even-derivative sharpening} that minimizes the errors of measuring peak areas of two overlapping Gaussians by the perpendicular drop method using the autopeaks.m function. It does this by applying different degrees of sharpening and plotting the area errors (percent difference between the true and measured errors) vs the sharpening factor, as shown on the right. It also shows the height of the valley between the peaks (yellow line). This shows that:

(1) the optimum sharpening factor depends upon the width and separation of the two peaks and on their height ratio; 

(2) the degree of sharpening is not overly critical, often exhibiting a broad optimum region; 

(3) the optimum for the two peaks is not necessarily the same; and 

(4) the optimum for area measurement does not necessarily occur at the point where the valley is zero. 

(To run this script, you must have gaussian.m, derivxy.m, autopeaks.m, val2ind.m, and halfwidth.m in the path. Download these from \url{https://terpconnect.umd.edu/~toh/spectrum/}).

\InsImageInline{0.5}{l}{PowerLaw4peaks.png}\textbf{The power method} is effective as long as there is a valley between the overlapping peaks, but it introduces non-linearity, which must be corrected later, whereas the derivative method preserves the original peak areas and the ratio between the peak heights. \href{https://terpconnect.umd.edu/~toh/spectrum/PowerLawCalibrationDemo.m}{PowerLawCalibrationDemo} demonstrates the linearization of the power transform calibration curves for two overlapping peaks by taking the nth power of data, locating the valley between them, measuring the areas by the perpendicular drop method (page \pageref{ref-0184}), and then taking the 1/n power of the measured areas (\href{https://terpconnect.umd.edu/~toh/spectrum/PowerTransformCalibrationCurve.png}{graphic}). 

\InsImageInline{0.5}{l}{SymmetizedOverlapDemo.png}\textbf{The constant-area symmetrization (de-tailing) of asymmetric peaks} by the weighted addition of the first derivative is performed by the function ySym =\href{https://terpconnect.umd.edu/~toh/spectrum/symmetrize.m}{ symmetrize}(t,y,factor,smoothwidth,type,ends); "t" and "y" are the independent and dependent variable vectors, "factor" is the first derivative weighting factor,  and "smoothwidth", "type", and "ends" are the \href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSmooth.m}{SegmentedSmooth} parameters for the internal smoothing of the derivative. To perform a segmented symmetrization, "factor" and "smoothwidth" can be vectors. In version 2, symmetrize.m smooths only the derivative, not the entire signal. \href{https://terpconnect.umd.edu/~toh/spectrum/SymmetrizeDemo.m}{SymmetrizeDemo.m} runs all five examples in the symmetrize.m help file, each in a different figure window.  If the tau is \textit{not} known, it can be determined for a single isolated peak by using AutoSymmetrize(t, y, SmoothWidth, plots), which finds the value of tau that produces the most symmetrical peak, judged by comparing the slope of the tangents to the leading and trailing edges. In the example shown on the left, the original peak (blue line) is a mathematically calculated exponentially modified Gaussian with a tau value of 100 and the red line is the output generated by AutoSymmetrize, which estimates the tau to an accuracy of 1\%. Type ``help AutoSymmetrize''. The areas of the two are equal within 0.01\%. \href{https://terpconnect.umd.edu/~toh/spectrum/SymmetizedOverlapDemo.m}{SymmetizedOverlapDemo.m} demonstrates the optimization of the first derivative symmetrization for the measurement of the areas of two overlapping exponentially broadened Gaussians. 

\InsImageInline{0.5}{l}{SegmentedSharpenDemo.png}\textbf{Segmented even-derivative peak sharpening}. If the peak widths or the noise variance changes substantially across the signal, you can use the \textit{segmented}\index{\textcolor{color-3}{segmented}} version \href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSharpen.m}{SegmentedSharpen.m}, for which the input arguments factor1, factor2, and SmoothWidth are \textit{vectors}. The script \href{https://terpconnect.umd.edu/~toh/spectrum/DemoSegmentedSharpen.m}{DemoSegmentedSharpen.m}, shown on the right, uses this function to sharpen four Gaussian peaks with gradually increasing peak widths from left to right with increasing degrees of sharpening, showing that the peak width is \href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSharpenDemo.txt}{reduced by 20\% to 22\%} compared to the original. 

\href{https://terpconnect.umd.edu/~toh/spectrum/DemoSegmentedSharpen2.m}{DemoSegmentedSharpen2.m} shows four peaks of the \textit{same} width sharpened to increasing degrees.

\textbf{\InsImageInline{0.5}{l}{image56.png}Double exponential symmetrization} in Matlab/Octave is performed by the function \href{https://terpconnect.umd.edu/~toh/spectrum/DEMSymm.m}{DEMSymm.m} which applies two successive applications of weighted addition of the first derivative, with weighting factors ideally equal to the two taus. The objective is to make the peaks more symmetrical and narrower while preserving the peak area. \uppercase{A} three-level plus-and-minus bracketing technique helps you to determine the best values for the two weighting factors. The technique is demonstrated by the script \href{https://terpconnect.umd.edu/~toh/spectrum/DemoDEMSymm.m}{DemoDEMSymm.m} and its two variations (\href{https://terpconnect.umd.edu/~toh/spectrum/DemoDEMSymm2.m}{1}, \href{https://terpconnect.umd.edu/~toh/spectrum/DemoSymm3.m}{2}), which creates two overlapping double exponential peaks from Gaussian originals, then calls the function DEMSymm.m to perform the symmetrization. In the example on the left, the middle of the three bracketing lines is the optimum value.\label{ref-0116}

In summary, if you attempt to symmetrize an asymmetrical peak by weighted first-derivative addition and the result is still asymmetrical, it may be that the remaining asymmetry could be due to another stage of exponential broadening with a different \textit{tau}, so in that case, the application of DEMSymm.m will likely produce a more symmetrical final result.

\href{https://terpconnect.umd.edu/~toh/spectrum/ProcessSignal.m}{ProcessSignal}, a Matlab/Octave command-line function that performs smoothing, differentiation, and peak sharpening on the time-series data set x,y (column or row vectors). Type "help ProcessSignal". It returns the processed signal as a vector that has the same shape as x, regardless of the shape of y. 

\texttt{Processed=Processed=ProcessSignal(x,y,DerivativeMode, w, type, ends, Sharpen, factor1, factor2, Symize, Symfactor, SlewRate, MedianWidth)}

\href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{\textbf{iSignal}} (Version 8.3, page \pageref{ref-0433}) is a multi-function interactive Matlab function that includes peak sharpening for time-series signals, using \textit{both} the even-derivative method (\textit{sharpen} function)  and the first-derivative symmetrization method, with keystrokes that allow you to adjust the derivative weighting factors and the smoothing continuously while observing the effect on your signal dynamically. The \textbf{E} key turns the peak sharpening function on and off. View the code \href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{here} or download the \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal7.zip}{ZIP file} with sample data for testing. \textit{iSignal} estimates the sharpening and smoothing settings for Gaussian and for Lorentzian peak shapes using the \textbf{Y} and \textbf{U} keys, respectively, using the expression given above. Just isolate a single typical peak in the upper window using the pan and zoom keys, press \textbf{P} to your on the peak \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html\#Top}{}measurement mode, then press \textbf{Y} for Gaussian or \textbf{U} for Lorentzian peaks. You can fine-tune the sharpening with the \textbf{F/V} and \textbf{G/B} keys and the smoothing with the \textbf{A/Z} keys. (If your signal has peaks of widely different widths, one setting will not be optimum for all the peaks. In such cases, you can use the segmented\index{\textcolor{color-3}{segmented}} sharpen function, \href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSharpen.m}{SegmentedSharpen.m}). 


\begin{center}
\InsImageInline{0.5}{l}{ps1.png}\InsImageInline{0.5}{l}{ps2.png} \textit{Before Peak Sharpening in iSignal After peak sharpening in iSignal}
\end{center}


\InsImageInline{0.5}{l}{image59.png} In \textit{iSignal} and in \textit{iPeak}, the \textbf{Shift-Y} key engages the first-derivative symmetrization technique (left) and uses the \textbf{1}, \textbf{Shift-1}, \textbf{2}, and \textbf{Shift-2} keys to adjust the weighting factor by 10\% or 1\% per keypress. Increase the factor until the baseline after the peak goes negative, then increase it slightly so that it is \textit{as low as possible but not negative}.

\textit{iSignal} can also use the power transform method (press the \textbf{\textasciicircum{}} key, enter the power, \textit{n} (any positive number greater than 1.00) and press \textbf{Enter}. To reverse this, simply raise to the 1/\textit{n} power. \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#ipeak}{\textit{iPeak}}, (page \pageref{ref-0320}), a Matlab interactive peak detection and measurement program, has a built-in peak sharpening mode that is based on the even derivative technique, as well as the first-derivative symmetrization using the same keystrokes as \textit{iSignal}. See \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#demos}{ipeakdemo5} on page \pageref{ref-0324}. The GIF animation \url{https://terpconnect.umd.edu/~toh/spectrum/iPeakShiftYDeTail.gif} demonstrates this in action.

\textit{Real-time} peak sharpening in Matlab is discussed on page \pageref{ref-0418}. \label{ref-0117}\label{ref-0118}\label{ref-0119}\label{ref-0120}

\chapter{Harmonic analysis and the Fourier Transform.\label{ref-0121}\label{ref-0122}}

Some signals exhibit periodic components that repeat at fixed intervals throughout the signal, like a sine wave. It is often useful to describe the amplitude and frequency of such periodic components exactly. In fact, it is possible to analyze \textit{any} arbitrary set of data into periodic components, whether or not the data appear periodic. \href{http://resonanceswavesandfields.blogspot.com/2009/01/spectrum-of-waveform-fourier-analysis.html}{Harmonic analysis} is conventionally based on the \href{http://en.wikipedia.org/wiki/Discrete\_Fourier\_transform}{\textit{Fourier transform}}, which is a way of expressing a signal as a weighted sum of \href{http://en.wikipedia.org/wiki/Sine\_wave}{sine and cosine waves}. It can be shown that any arbitrary discretely sampled signal can be described completely by the sum of a finite number of sine and cosine components whose frequencies are 0, 1, 2, 3 ... n/2 times the frequency f=1/n${\Updelta}$x, where ${\Updelta}$x is the interval between adjacent x-axis values and n is the total number of points. The Fourier transform is simply the set of amplitudes of those sine and cosine components (or, which is equivalent mathematically, \href{ftp://www.myphysicslab.com/trig\_identity1.html}{the frequency and phase of sine components}). You could calculate those coefficients yourself simply but laboriously by multiplying the signal point-by-point with each of those sine and cosine components and adding up the products. The famous ``\href{https://pdfs.semanticscholar.org/1790/fe007bc1ab161a1ea814748b42e3acbdc958.pdf}{Fast Fourier Transform}'' (FFT) dates from 1965 and is a faster and more efficient algorithm that makes use of the symmetry of the sine and cosine functions and other math shortcuts to get the same result \href{https://terpconnect.umd.edu/~toh/spectrum/HarmonicAnalysis.html\#sft}{\textit{much} more quickly}. The \textit{inverse} Fourier transform (IFT) is a similar algorithm that converts a Fourier transform back into the original signal. As a mathematical convenience, Fourier transforms are traditionally expressed in terms of ``\href{https://en.wikipedia.org/wiki/Complex\_number}{complex numbers}'', because the ``real'' and ``imaginary'' parts allows one to combine the sine and cosine (or amplitude and phase) information at each frequency onto a single complex number, using the identity $\exp \left(i2\pi ft\right)=\cos \left(2\pi ft\right)+isin\left(2\pi ft\right).$ Even for data that are not complex, using the ``exp'' notation rather than ``sin+cos'' is more compact and elegant, and many computer languages can handle complex arithmetic automatically when the quantities are complex. But this terminology can be misleading: the sine and cosine parts are \textit{equally important}; just because the two parts are called "real" and "imaginary" in mathematics does not imply that the first is less significant that the second.

The concept of the Fourier transform is involved in two very important modern instrumental methods in chemistry. In \href{http://en.wikipedia.org/wiki/Fourier\_transform\_spectroscopy}{Fourier transform infrared spectroscopy (FTIR)}, the Fourier transform of the spectrum is measured directly by the instrument, as the interferogram formed by plotting the detector signal vs mirror displacement in a scanning Michaelson interferometer. In \href{http://en.wikipedia.org/wiki/NMR\#Fourier\_spectroscopy}{Fourier Transform Nuclear Magnetic Resonance spectroscopy (FTNMR)}, excitation of the sample by an intense, short pulse of radio-frequency energy produces a free induction decay signal that is the Fourier transform of the resonance spectrum. In both cases, a computer is used to recover the spectrum by inverse Fourier transformation of the measured (interferogram or free induction decay) signal. 

The \href{http://en.wikipedia.org/wiki/Frequency\_spectrum}{\textit{power spectrum}} or \textit{frequency spectrum} is a simple way of showing the total amplitude at each of these frequencies. It is calculated as the square root of the sum of the squares of the coefficients of the sine and cosine components. The power spectrum retains the \textit{frequency} information but discards the \textit{phase} information, so that the power spectrum of a sine wave would be the same as that of a cosine wave of the same frequency, even though the complete Fourier transforms of sine and cosine waves are different in phase. In situations where the \textit{phase components} of a signal are the major source of noise (e.g. random shifts in the horizontal x-axis position of the signal), it can be advantageous to base measurement on the power spectrum, which discards the phase information, by \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#EnsembleAveraging}{ensemble averaging} (page \pageref{ref-0027}) the power spectra of repeated signals: this is demonstrated by the Matlab/Octave scripts \href{https://terpconnect.umd.edu/~toh/spectrum/EnsembleAverageFFT.m}{EnsembleAverageFFT.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/EnsembleAverageFFTGaussian.m}{EnsembleAverageFFTGaussian.m}. 

\InsImageInline{0.5}{l}{image60.png}A time-series signal with \textit{n} points gives a power spectrum with only (n/2)+1 points. The first point is the zero-frequency (constant) component, corresponding to the DC (direct current) component of the signal; it looks like a straight flat line. The second point corresponds to a frequency of 1/n${\Updelta}$x (whose period is exactly equal to the time duration of the data), the next point to 2/n${\Updelta}$x, the next point to 3/n${\Updelta}$x, etc., where ${\Updelta}$x is the interval between adjacent x-axis values and n is the total number of points. The last (highest frequency) point in the power spectrum (n/2)/n${\Updelta}$x=1/2${\Updelta}$x, which is one-half the sampling rate. This is illustrated in the figure on the left, which shows a one-second, 1000-point signal with a sampling rate of 1000 Hz. The signal here contains only three sine waves (shown in different colors in the top panel), all of which are clearly distinguishable when added up in the signal itself (middle panel). \textit{You can even count the cycles of the sine waves to confirm their frequencies}. The frequencies all show up at the expected places and with the expected relative amplitudes in the Fourier spectrum, which I have drawn here as a bar graph (bottom panel), showing frequencies only up to 50 Hz, out of a maximum of 500. This also works similarly with \textit{cosine} waves, which differ from sine waves in their \textit{phase} (x-axis shift).

The \textit{highest} frequency that can be represented in a discretely sampled waveform is one-half the sampling frequency, which is called the \href{http://en.wikipedia.org/wiki/Nyquist\_frequency}{\textit{Nyquist frequency}}. Attempts to digitize signals with higher frequencies are "folded back" to lower frequencies, severely distorting the signal. The frequency resolution, that is, the difference between the frequencies of adjacent points in the calculated frequency spectrum, is simply the reciprocal of the time duration of the signal. In the figure above, the highest frequency in the spectrum is 500 Hz, and the frequency resolution is 1/1 sec = 1 Hz. 

\InsImageInline{0.5}{l}{SingleFreq.png}\InsImageInline{0.5}{l}{DeltaSpectrum.png}\InsImageInline{0.5}{l}{WhiteNoiseSpectrum.png}


\fbox{\begin{minipage}[t]{0.8\textwidth}\end{minipage}}

\InsImageInline{0.5}{l}{ECGsmall.png}A pure sine or cosine wave that has an exact integral number of cycles within the recorded signal has a \href{https://terpconnect.umd.edu/~toh/spectrum/SingleFreq.png}{\textit{single non-zero Fourier component} }corresponding to its frequency (previous page, left). Conversely, a signal consisting of zeros everywhere except at a single point, called a \textit{delta function}, has \href{https://terpconnect.umd.edu/~toh/spectrum/DeltaSpectrum.png}{\textit{equal} Fourier components at all frequencies} (previous page, center). \textit{Random noise} also has a power spectrum that is spread out over a wide frequency range. The noise amplitude depends on the \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#Frequency}{\textit{noise color}} (page \pageref{ref-0032}) with pink noise having more power at low frequencies, blue noise having more power at high frequencies, and white noise having roughly \href{https://terpconnect.umd.edu/~toh/spectrum/WhiteNoiseSpectrum.png}{the same power at all frequencies} (previous page, right). 

The figure on the left shows a real-data 60 second recording of a heartbeat, called an \href{http://en.wikipedia.org/wiki/Electrocardiography}{electrocardiograph} (ECG), which is \href{https://terpconnect.umd.edu/~toh/spectrum/ECGlarge.png}{}an example of a periodic waveform that repeats over time. The figure shows the waveform in blue in the top panel and its frequency spectrum in red in the bottom panel. The smallest repeating unit of the signal is called the \textit{period}, and the reciprocal of that period is called the \href{http://en.wikipedia.org/wiki/Fundamental\_frequency}{fundamental frequency}. Non-sinusoidal periodic waveforms like this exhibit a series of frequency components that are multiples of the fundamental frequency, which are called "harmonics". This spectrum shows a fundamental frequency of 0.6685 Hz (which is 40.1 bpm, somewhat slow for a human heart rate), with \textit{multiple harmonics} at frequencies that are \textbf{${\times}$}2, \textbf{${\times}$}3, \textbf{${\times}$}4..., etc., times the fundamental frequency. The lowest frequency in the spectrum is 0.067 Hz (the reciprocal of the recording time) and the highest is 400 Hz (one-half the sampling rate). The fundamental and the harmonics are \textit{sharp peaks}, and they are labeled with their frequencies on tis graph. The spectrum is qualitatively similar to that for \href{https://terpconnect.umd.edu/~toh/spectrum/EvenlySpacedSpikes.png}{perfectly regular identical peaks} \uline{\textcolor{color-8}{(graphic)}}. Recorded vocal sounds, especially vowels, also have a \href{https://terpconnect.umd.edu/~toh/spectrum/VocalHarmonics.png}{periodic waveform with harmonics} (\uline{\textcolor{color-8}{graphic)}}. The sharpness of the peaks in these spectra shows that the amplitude and the frequency are very constant over the 60 second recording interval in this example (which is normal behavior for a heart). Changes in amplitude or frequency over the recording interval will produce \textit{clusters} or \textit{bands} of Fourier components rather than sharp peaks, as in \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#F}{the example} on page \pageref{ref-0356}. 

Another familiar example of periodic change is the seasonal variation in temperature, for example, the \href{https://terpconnect.umd.edu/~toh/spectrum/NYCTemp.png}{average daily temperature measured in New York City between 1995 and 2015}, shown in the figure above. (The negative spikes are missing data points \textendash{} perhaps power outages?). Note the logarithmic scale on the y-axis of the spectrum in the bottom panel; this spectrum covers a \textit{very} wide range of amplitudes.

\InsImageInline{0.5}{l}{NYCTemp.png}In this example, the spectrum in the lower panel is plotted with \textit{time} (the reciprocal of frequency) on the x-axis (called a \href{http://coolwiki.ipac.caltech.edu/index.php/What\_is\_a\_periodogram\%3F}{\textit{periodogram}}). Despite the considerable random noise due to local weather variations and missing data, this shows the expected peak at exactly 1 year; that peak is \textit{over 100 times stronger} than the background noise and is very \textit{sharp} because the periodicity is extremely precise (in fact, it is literally \textit{astronomically} precise). In contrast, the random noise is \textit{not} periodic but rather is spread out roughly equally over the entire periodogram. 

\InsImageInline{0.5}{l}{PlotFrequencySpectrum.png} The figure on the right is a simulation that shows how hard it is to see a periodic component in the presence of random noise, and yet how easy it is to pick it out in the frequency spectrum. In this example, the signal (top panel) contains an \textit{equal mixture} of random white noise and a single sine wave; the sine wave is almost completely obscured by the random noise. The frequency spectrum (created using my Matlab/Octave function "\href{https://terpconnect.umd.edu/~toh/spectrum/PlotFrequencySpectrum.m}{PlotFrequencySpectrum}") is shown in the bottom panel. The frequency spectrum of the white noise is spread out evenly over the entire spectrum, whereas the sine wave is concentrated into a \textit{single} spectral element, where it stands out clearly. Here is the Matlab/ Octave code that generated that figure; you can Copy and Paste it into Matlab/Octave: \label{ref-0123}

\texttt{x=[0:.01:2*pi]';}

\texttt{y=sin(200*x)+randn(size(x));}

\texttt{subplot(2,1,1);}

\texttt{plot(x,y);}

\texttt{subplot(2,1,2);}

\texttt{PowerSpectrum=}\href{https://terpconnect.umd.edu/~toh/spectrum/PlotFrequencySpectrum.m}{\texttt{PlotFrequencySpectrum}}\texttt{(x,y,1,0,1);}

A common practical application is the use of the power spectrum as a diagnostic tool to distinguish between signal and noise components. An example is the AC power-line pickup depicted in the figure below, which has a fundamental frequency of 60 Hz in the USA (\href{http://www.allaboutcircuits.com/news/why-is-the-us-standard-60-hz/}{why that frequency?}) or 50 Hz in many other countries. Again, the sharpness of the peaks in the spectrum shows that the amplitude and the frequency are very constant; power companies take pains to keep the frequency of the AC very constant to avoid problems between different sections of the power grid. Other examples of signals and their frequency spectra are \uline{\textcolor{color-8}{shown below}}. 


\begin{center}
\InsImageInline{0.5}{l}{SilenceBeforeSignal.png}\InsImageInline{0.5}{l}{SilenceAfterSignal.png}
\end{center}


\textbf{\textit{iSignal}}\textit{, showing data from an audio recording, zoomed in to the ``quiet'' period immediately before (left) and immediately after (right) the actual sound. This shows there is some background noise with a regular sinusoidal oscillation (x = time in seconds). In the lower panel, the power spectrum of each signal (x = frequency in Hz) shows a strong sharp peak very near 60 Hz, suggesting that this oscillation is caused by stray pick-up from the} \href{http://www.allaboutcircuits.com/news/why-is-the-us-standard-60-hz/}{\textit{60 Hz power line (since it was recorded in the USA}}\textit{; it would be 50 Hz had the recording been made in Europe). Improved shielding and grounding of the equipment might reduce this interference. The "before" spectrum, on the left, has a frequency resolution of only 10 Hz (the reciprocal of the recording time of about 0.1 seconds) and it includes only about 6 cycles of the 60 Hz frequency (which is why that peak in the spectrum is the 6th point); to achieve a better resolution you would have had to have begun the recording earlier, to achieve a longer recording. The "after" spectrum, on the right, has an even shorter recording time and thus a poorer frequency resolution.}

Peak-type signals have power spectra that are concentrated in a range of low frequencies, whereas random noise often spreads out over a much wider frequency range. This is the reason smoothing (low-pass filtering) can make a noisy signal \textit{look} nicer, but also why smoothing does not usually help with quantitative measurement, because most of the peak information is found at \textit{low} frequencies, where low-frequency noise remains unchanged by smoothing (See page \pageref{ref-0054}).

\InsImageInline{0.5}{l}{SunspotSpectrumMode2.png}\textcolor{color-8}{ } \InsImageInline{0.5}{l}{SunspotSpectrumMode.png}

\InsImageInline{0.5}{l}{PageLoads.png}The figures above show a classic example of harmonic analysis; it shows the annual variation in the number of observed sunspots, which have been recorded since the year 1700! In this case, the time axis is in \textit{years} (top window). A plot of the power spectrum (bottom window, left) shows a strong peak at 0.09 cycles/year and the periodogram (right) shows a peak at the well-known 11-year cycle, plus some evidence of a weaker cycle at around a 100-year period. (You can download \href{https://terpconnect.umd.edu/~toh/spectrum/sunspots.txt}{this data set} or the latest \href{https://www.ngdc.noaa.gov/stp/solar/ssndata.html}{yearly sunspot data from NOAA}. These frequency spectra are plotted using my Matlab function \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{iSignal} (page \pageref{ref-0432}). In this case, the peaks in the spectrum are \textit{not} sharp single peaks, but rather form a \textit{cluster} of Fourier components, because \textit{the amplitude and the frequency are not constant} over the nearly 300-year interval of the data, as is obvious by inspecting the data in the time domain. 

\InsImageInline{0.5}{l}{iSignal27spectrum.png}\href{https://terpconnect.umd.edu/~toh/spectrum/HarmonicAnalysis.html\#Top}{}An example of a time series with complex multiple periodicities is the world-wide \href{https://terpconnect.umd.edu/~toh/spectrum/Summary4.txt}{daily page views} (x=days, y=page views) for \href{http://terpconnect.umd.edu/~toh/spectrum/}{this web site} over a 2070-day period (about 5.5 years). \href{https://terpconnect.umd.edu/~toh/spectrum/PageLoads.png}{In the periodogram plot} (shown on the previous page) you can clearly see at sharp peaks at 7 and 3.5 days, corresponding to the first and second harmonics of the expected workday/weekend cycle. It also shows smaller peaks at 365 days (corresponding to a sharp dip each year during the winter holidays) and at 182 days (roughly a half-year), probably caused by increased use in the two-per-year semester cycle at universities. The large values at the longest times are caused by the gradual increase in use over the entire data record, which can be thought of as a very low-frequency component whose period is much longer than the entire data record. 

In the example shown on the left, the signal (in the top window) contains no visually evident periodic components; it \textit{seems} to be just random noise. However, the frequency spectrum (in the bottom window) shows that there is much more to this signal than meets the eye. There are two major frequency components: one at low frequencies around 0.02 and the other at high frequencies between 0.5 and 5. (If the x-axis units of the signal plot had been \textit{seconds}, the units of the frequency spectrum plot would be \textit{Hz}; note that the x-axis is logarithmic). In this case, the \textit{lower} frequency component is, in fact, the \textit{signal}, and the frequency component is residual \uline{\textit{\textcolor{color-8}{blue noise}}} remaining from previous signal processing operations. The two components are fortunately well separated on the frequency axis, suggesting that low-pass filtering (i.e., smoothing) will be able to remove the noise without distorting the signal. 

In all the examples shown above, the signals are time-series signals with \textit{time} as the independent variable. More generally, it is also possible to compute the Fourier transform and power spectrum of \textit{any} signal, such as an optical spectrum, where the independent variable might be wavelength or wavenumber, or an electrochemical signal, where the independent variable might be volts, or a spatial signal, where the independent variable might be in length units. In such cases, the units of the x-axis of the power spectrum are simply the reciprocal of the units of the x-axis of the original signal (e.g., nm\textsuperscript{-1} for a signal whose x-axis is in nm). 

\InsImageInline{0.5}{l}{EffectOfWidth.gif.png}Analysis of the frequency spectra of signals provides another way to understand signal-to-noise ratio, filtering, \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothing}, and \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html}{differentiation}. Smoothing is a form of \textit{low-pass} filtering, reducing the high-frequency components of a signal. If a signal consists of smooth features, such as Gaussian peaks, then its spectrum will be concentrated mainly at \textit{low} frequencies. The wider the width of the peak, the more concentrated the frequency spectrum will be at low frequencies. (If you are reading this online, click this \href{https://terpconnect.umd.edu/~toh/spectrum/EffectOfWidth.gif}{link} to see this figure animated). If that signal has white noise (spread out evenly over all frequencies), then smoothing will make the signal look better, because it reduces the high-frequency components of the noise. However, the low-frequency noise will remain in the signal after smoothing, where it will continue to interfere with the measurement of signal parameters such as peak heights, positions, widths, \InsImageInline{0.5}{l}{DerivGaussSpectrum.gif.png}and areas. This can be \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Smoothing}{demonstrated by a least-squares measurement}. \label{ref-0124}

Conversely, differentiation is a form of \textit{high-pass} filtering, reducing the \textit{low-}frequency components of a signal and emphasizing any \textit{high}-frequency components present in the signal. A simple computer-generated Gaussian peak (shown on the right; click for \href{https://terpconnect.umd.edu/~toh/spectrum/DerivGaussSpectrum.gif}{GIF animation}) has most of its power is concentrated in just a few low frequencies, but as successive orders of differentiation are applied, the waveform of the derivative swings from positive to negative like a sine wave, and its frequency spectrum shifts progressively to higher frequencies. This behavior is typical of \href{https://terpconnect.umd.edu/~toh/spectrum/DerivGaussSpectrum2.gif}{any signal with smooth peaks}. So, the optimum range for signal information of a \textit{differentiated signal} is restricted to a relatively narrow range, with little useful information above and below that range. 

The fact that white noise (page \pageref{ref-0032}) is spread out in the frequency domain roughly equally over all frequencies has a subtle advantage over other noise colors when the signal and the noise cannot be cleanly separated in the time domain; you can more easily estimate the intensity of the noise by observing it in frequency regions where the signal does not interfere, since most signals do not occupy the entire spectrum frequency range. This idea will be used later as a method for estimating the errors of measurements that are based on least-squares curve fitting of noisy data (page \pageref{ref-0221}).

\href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html}{Peak sharpening} (page \pageref{ref-0099}) also emphasizes the high-frequency components by adding a portion of the second and fourth derivatives to the original signal. You can see this clearly in the Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningFrequencySpectrum.m}{PeakSharpeningFrequencySpectrum.m}, which shows the frequency spectrum of the original and sharpened version of a signal consisting of several peaks (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningFrequencySpectrum.png}{graphic}). 

\href{https://terpconnect.umd.edu/~toh/spectrum/SineToDelta.m}{SineToDelta.m}. A demonstration animation (\href{https://terpconnect.umd.edu/~toh/spectrum/SineToDelta.gif}{animated graphic}) showing the waveform and the power spectrum of a rectangular pulsed sine wave of variable duration (whose power spectrum is a "sinc" function) changing continuously from a pure sine wave at one extreme (where its power spectrum is a delta function) to a single-point pulse at the other extreme (where its power spectrum is a flat line).\href{https://terpconnect.umd.edu/~toh/spectrum/GaussianSineToDelta.m}{GaussianSineToDelta.m} is similar, except that it shows a \textit{Gaussian} pulsed sine wave, whose power spectrum is a Gaussian function, but which is the same at the two extremes of pulse duration (\href{https://terpconnect.umd.edu/~toh/spectrum/GaussianSineToDelta.gif}{animated graphic}). 

Real experimental signals are often contaminated with drift and baseline shift, which are essentially \textit{low-frequency} effects, and random noise, which is usually spread out over \textit{all frequencies}. For these \InsImageInline{0.5}{l}{OriginalSignals.png}reasons, differentiation is always used in conjunction with smoothing. Working together, smoothing and differentiation act as a kind of frequency-selective \textit{bandpass} filter that optimally passes the band of frequencies containing the differentiated signal information but reduces both the \textit{lower-frequency} effects, such as slowly changing drift and background, as well as the \textit{high-frequency} noise. An example of this can be seen in the \href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeDemo.m}{DerivativeDemo.m} described in a previous section (page \pageref{ref-0098}). In the set of six original signals, shown on the right, the random noise occurs mostly in a high-frequency range, with \textit{many cycles} over the x-axis range, and the baseline shift occurs mostly in a much lower-frequency phenomenon, with only a \textit{small fraction of one cycle} occurring over that range. In contrast, the peak of interest, in the center of the x-range, occupies an intermediate frequency range, with \textit{a few cycles} over that range. Therefore, we could predict that a quantitative measure based on differentiation and smoothing might work well. 

Smoothing and differentiation change the \textit{amplitudes} of the various frequency components of signals, but they do not change of shift the frequencies themselves. An \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html\#sounds}{experiment described later} (page \pageref{ref-0443}) illustrates this idea by smoothing and differentiating a brief recording of human speech. Interestingly, different degrees of smoothing and differentiation will change the \href{https://en.wikipedia.org/wiki/Timbre}{timbre }of the voice but has \textit{little effect on the intelligibility}, because the sequence of pitches is not shifted in pitch or time but merely changed in amplitude by smoothing and differentiation. Because of this, recorded speech can survive digitization, transmission over long distances, algorithmic compression, and playback via tiny speakers and headphones without significant loss of intelligibility\textit{.} Music, on the other hand, suffers greater loss under such circumstances, as you can tell by listening to \href{https://www.reddit.com/r/AskEngineers/comments/2dwj84/why\_does\_hold\_music\_always\_sound\_so\_terrible/}{telephone "hold" music}, which often sounds terrible \textit{even though speech over the same connection is completely intelligible}.

\section{Software details\label{ref-0125}}

In a spreadsheet or computer language, a sine wave can be described by the 'sin' function y=sin(2${\uppi}$\textit{fx}+\textit{p}) or y=sin(2${\uppi}$(1/\textit{t})\textit{x}+\textit{p}), where ${\uppi}$ is 3.14159...., \textit{f} is \textit{frequency} of the waveform, \textit{t} is the \textit{period} of the waveform, \textit{p} is the \textit{phase}, and \textit{x} is the independent variable (usually time). 

There are \href{https://www.google.com/search?q=fourier+transform&ie=utf-8&oe=utf-8&aq=t&rls=org.mozilla:en-US:unofficial&client=seamonkey-a\#q=calculate+discrete+fourier+transform+online}{several Web sites} that can compute Fourier transforms interactively (e.g. \href{http://www.wolframalpha.com/input/?i=Fourier+transform+calculator}{WolframAlpha}). Microsoft Excel has an add-in function that makes it possible to perform Fourier transforms relatively easily: (Click \textbf{Tools {\textgreater} Add-Ins... {\textgreater} Analysis Toolpak} {\textgreater} \textbf{Fourier Analysis}). See "\href{http://www.brainmapping.org/NITP/PNA/tests/ProblemSet3\_files/FourierExcel.htm}{Excel and Fourier}" for details. See ``\textit{Excellaneous}'' (\url{http://www.bowdoin.edu/~rdelevie/excellaneous/}) for an extensive and excellent collection of add-in functions and macros for Excel, by Dr. Robert deLevie of Bowdoin College. There are several dedicated FFT spectral analysis programs, including \textbf{ScopeDSP} (\url{https://iowegian.com/scopedsp/}) and \textbf{Audacity} (\url{http://sourceforge.net/projects/audacity/}). If you are reading this online, you can \textbf{Ctrl-Click} these links to open these sites automatically. \href{https://terpconnect.umd.edu/~toh/spectrum/SPECTRUM.html}{SPECTRUM}, page \pageref{ref-0481}, the old freeware signal-processing application for Macintosh OS8, includes a power spectrum function, as well as forward and reverse Fourier transformation. 

\section{\href{http://en.wikipedia.org/wiki/MATLAB}{\label{ref-0126}Matlab} and \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{Octave}\label{ref-0127}}

Matlab and Octave have built-in functions for computing the Fourier transform (\href{https://terpconnect.umd.edu/~toh/spectrum/FFT.txt}{fft} and \href{https://terpconnect.umd.edu/~toh/spectrum/IFFT.txt}{ifft}). These functions express their results as complex numbers. For example, if we compute the Fourier transform of a simple 3-element vector, we get a 3-element result of complex numbers:

\texttt{y=[0 1 0];}

\texttt{fft(y)}

\texttt{ans = 1.0000 -0.5000-0.8660i -0.5000+0.8660i}

where the "i" indicates the "imaginary" part. The first element of the fft is just the sum of elements in y. The \textit{inverse} fft, \texttt{ifft([1.0000 -0.5000-0.8660i -0.5000+0.8660i]),} returns the original vector \texttt{[0 1 0].} 

For another example, the fft of [0 1 0 1] is [2 0 -2 0]. In general, the fft of an n-element vector of real numbers returns an n-element vector of real or complex numbers, but only the first n/2+1 elements are unique; the remainder is a mirror image of the first. Operations on individual elements of the fft, such as in \href{https://terpconnect.umd.edu/~toh/spectrum/FourierFilter.html}{Fourier filtering}, must take this structure into account.

The frequency spectrum of a signal vector "s" can be computed as \texttt{\textbf{real(sqrt(fft(s) .* conj(fft(s))))}}. Here is a simple example where we know the answer in advance, at least qualitatively: an 8-element vector of integers that trace out a \textit{single cycle of a sine wave}: 

\texttt{s=[0 7 10 7  $0 -7 -10 -7$ ];}

\texttt{plot(s);}

\texttt{real(sqrt(fft(s) .* conj(fft(s))))} 

The frequency spectrum in this case is \texttt{[0 39.9 0 0.201 0 0.201 0 39.9]}. Again, the first element is the average (which is zero) and elements 2 through 4 are the mirror image of the last 4. The unique elements are the first four, which are the amplitudes of the sine wave components whose frequencies are 0, 1, 2, 3 times the frequency of a sine wave that would just fit a single cycle in the period of the signal. In this case, it the \textit{second} element (39.8) that is the largest by far, which is just what we would expect for a signal that approximates a single cycle of a \textit{sine} (rather than a \textit{cosine}) wave. Had the signal been \textit{two} cycles of a sine wave, s = [0 10 0 -10 0 10 0 -10], the \textit{third} element would have been the strongest (try it). The highest frequency that can be represented by an 8-element vector is one that has a period equal to 2 elements. It takes a minimum of 4 points to represent one cycle, e.g. [0 +1 0 -1]. 

If you are reading this online, click \href{https://terpconnect.umd.edu/~toh/FrequencySpectrumDemo.m}{here} for a Matlab script that creates and plots a sine wave and then uses the fft function to calculate and plot the power spectrum. Try different frequencies (third line). Watch what happens when the frequency approaches 50. Hint: the Nyquist frequency is 1/(2*Deltat) = 1/0.02=50. Also, see what happens when you change Deltat (first line), which determines how fine the sine wave is sampled. 

My function \href{https://terpconnect.umd.edu/~toh/spectrum/FrequencySpectrum.m}{FrequencySpectrum.m} (syntax \texttt{fs=FrequencySpectrum(x,y)}) returns real part of the Fourier power spectrum of x,y as a matrix. \href{https://terpconnect.umd.edu/~toh/spectrum/PlotFrequencySpectrum.m}{PlotFrequencySpectrum.m} can plot frequency spectra and periodograms on linear or log coordinates. Type "help PlotFrequencySpectrum" or try this example:

 \texttt{\textbf{x=[0:.01:2*pi]';}}

  \texttt{\textbf{f=25;}} \texttt{\textbf{\textcolor{color-9}{\% Frequency}}}

  \texttt{\textbf{y=sin(2*pi*f*x)+randn(size(x));}}

  \texttt{\textbf{subplot(2,1,1);}}

  \texttt{\textbf{plot(x,y);}}

  \texttt{\textbf{subplot(2,1,2);}}

  \texttt{\textbf{FS=PlotFrequencySpectrum(x,y,1,0,1);}}

 

The plot of the frequency spectrum FS (\texttt{plotit(FS)}; \href{https://terpconnect.umd.edu/~toh/spectrum/FrequencySpectrumExample.png}{graphic}) shows a single strong peak at 25. The frequency of the strongest peak in FS is given by \texttt{FS(val2ind(FS(:,2),max(FS(:,2))),1).}

For some other examples of using FFT, see \href{https://www.mathworks.com/examples/search?q=fourier}{these examples}. A ``\href{https://terpconnect.umd.edu/~toh/spectrum/sft.m}{Slow Fourier Transform}'' function has also been \href{http://www.mathworks.com/matlabcentral/fileexchange/2271-numerical-methods-for-physics/content/edition1/matlab4/sft.m}{published}; it is \textit{3000 to} \textit{7000 times slower} with a 10,000-point data vector, as can be shown by this bit of code: \texttt{y=cos(.1:.01:100);} \texttt{tic; fft(y); ffttime=toc; tic; sft(y); sfttime=toc; TimeRatio=sfttime/ffttime}

\InsImageInline{0.5}{l}{image76.png}\textbf{Time-segmented Fourier power spectrum.} The function \href{https://terpconnect.umd.edu/~toh/spectrum/PlotSegFreqSpect.m}{PlotSegFreqSpect.m}, syntax \texttt{PSM=PlotSegFreqSpect(x,y,NumSegments,MaxHarmonic,logmode)}, creates and displays a \textit{time-segmented} Fourier power spectrum. It breaks the signal into 'NumSegments' equal-length segments, multiplies each by an apodizing Hanning window, computes the power spectrum of each segment, and plots the magnitude of the first 'MaxHarmonic' Fourier components versus segment number as a \href{https://en.wikipedia.org/wiki/Contour\_line}{\textit{contour} plot}. The function returns the power spectrum matrix (time-frequency-amplitude) as a matrix of size NumSegments x MaxHarmonic. If logmode=1, it computes and plots the base10 log of the amplitudes as a contour plot with different colors representing amplitudes (blue=low; yellow=high). Other practical examples in the help file include the spectrum of a \href{https://terpconnect.umd.edu/~toh/spectrum/PlotSegFreqSpectExample1.png}{passing automobile horn}, showing the Doppler effect, and of a \href{https://terpconnect.umd.edu/~toh/spectrum/PlotSegFreqSpectExample2.png}{sample of human speech} (above, left). \label{ref-0128}\label{ref-0129}

The next example (\href{https://terpconnect.umd.edu/~toh/spectrum/PlotSegFreqSpectExample6.m}{script}) shows a complex signal consisting of three components added together (below left): two weak Gaussian peaks at x=5000 and 10000 (blue) with height=0.15, a strong swept-frequency sine-wave interference (red), and white noise (green). When you add up all three of those components, the Gaussian peaks are totally buried and are invisible in the raw signal, below right. (I will call this the ``buried peaks'' signal, and I will use it again later, page \pageref{ref-0169}). \label{ref-0130}

\InsImageInline{0.5}{l}{image77.png}\InsImageInline{0.5}{l}{PlotSegFreqSpectExample6b.png}\InsImageInline{0.5}{l}{image79.png} 

Inspection of the PlotSegFreqSpect function (left) reveals the strong diagonal stripe of yellow from the swept sine wave and the blue and white background from the random noise, but you also see two yellow blobs in time segments 2 and 4 at the bottom of the segmented spectrum (left). What that tells us is that there is something around the 2\textsuperscript{nd} and 4\textsuperscript{th} time segments that has higher frequencies than the surrounding area. Once you see that, you can constrain the range of further observation there and can verify the peaks by \href{https://terpconnect.umd.edu/~toh/spectrum/PlotSegFreqSpectExample6c.png}{smoothing to reduce the high} frequencies or by \href{https://terpconnect.umd.edu/~toh/spectrum/Example6CurveFitResults.png}{curve-fitting to the raw data} (introduced on page \pageref{ref-0229}). In fact, the curve fitting results shown below give good values (${\pm}$10\% or better) for the peak positions (true values =5000 and 10000), heights (0.15), and widths (2500), despite the invisibility of the peaks in the raw data. 

 \texttt{\textbf{Peak\# Position Height Width Area}}

     \texttt{\textbf{1 5031.4 0.15749 2280.6 382.33}}

      \texttt{\textbf{2 10036 0.16136 2407.2 413.46}}\label{ref-0131}

But you must \textit{see} the peaks to know to try that. Based on the raw data alone, you might never try. 

\section{Observing Frequency Spectra with \uline{\textcolor{color-8}{iSignal}}\label{ref-0132}}

\textbf{iSignal} (page \pageref{ref-0433}) is a multi-purpose interactive signal processing tool that has a \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html\#Spectrum}{\textbf{Frequency Spectrum mode}}, toggled on and off by the \textbf{Shift-S} key; it computes the frequency spectrum of the segment of the signal displayed in the upper window and displays it in the lower window (in red). You can use the pan and zoom keys to adjust the region of the signal to be viewed or press \textbf{Ctrl-A} to select the entire signal. Press \textbf{Shift-S} again to return to the normal mode. \href{https://terpconnect.umd.edu/~toh/spectrum/iSignalSpectrumMode.gif}{}In the frequency spectrum mode, you can press \textbf{Shift-A} to cycle through four plot modes (linear, semilog X, semilog Y, or log-log). Because of the wide range of amplitudes and frequencies exhibited by some signals, the log plot modes often result in a clearer graph that the linear modes. You can also press \textbf{Shift-X} to toggle the x-axis between \textit{frequency} and \textit{time}. Details and instructions are on page \pageref{ref-0433}. You can download a \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal8.zip}{ZIP file} that contains iSignal.m version 8 and some demos and sample data for testing. 

\subsection{Frequency visualization. \label{ref-0133}}

What happens if the frequency content changes with time? Consider, for example, the signal shown in the following figure. The signal (download from \href{https://terpconnect.umd.edu/~toh/spectrum/SineBursts.mat}{SineBursts.mat}) consists of three intermittent bursts of sinewaves of three different frequencies, with zeros between the bursts.


\begin{center}
\InsImageInline{0.5}{l}{IntermittentSinusoidiSignal.png}
\end{center}



\begin{center}
\textit{The Matlab function iSignal.m displaying a signal (top panel) and its frequency spectrum (bottom panel).}
\end{center}


Here the signal is shown in the upper panel in the \href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{iSignal.m} function (page \pageref{ref-0436}) by typing: 

\texttt{\textbf{load SineBursts}}

\texttt{\textbf{isignal(x,y);}}

at the Matlab command prompt. By pressing \textbf{Shift-S}, its frequency spectrum is displayed in the lower panel, which shows three discreet frequency components. But which one is which? The normal Fourier transform by itself offers no clue, but iSignal allows you to pan and zoom across the signal, using the cursor arrow keys, so you would be able to isolate each.

Alternatively, my function \href{https://terpconnect.umd.edu/~toh/spectrum/PlotSegFreqSpec.m}{PlotSegFreqSpec.m}, just described in the previous section (figures below) is another way to display this signal in a \textit{single static graphic that clearly displays the time and frequency variation of the signal}. 

\InsImageInline{0.5}{l}{IntermittentSinusoidPlotSegFreqSpect.png}\InsImageInline{0.5}{l}{IntermittentSinusoidPlotSegFreqSpectLog.png}

\InsImageInline{0.5}{l}{IntermittentSinusoidSTFT.png}You can do a similar visualization with Matlab’s inbuilt ``Short Time Fourier Transform'' function stft.m (below), which displays both positive and negative frequencies. 

\subsection{Signal enhancement  \label{ref-0134}}

A very important feature of iSignal is that \textit{all signal processing functions remain active in the frequency spectrum mode} (smooth, derivative, etc.), so you can observe the effect of these functions on the frequency spectrum of the signal immediately. Some signal processing operations may have the side-effect of increasing the effect of random noise or of distorting the signal. The advantage of iSignal is that you can directly observe the trade-off between the desired effect and the side effects while adjusting the signal-processing variables interactively. The figure on the next page shows an example. It shows the effect of increasing the smooth width on the \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html}{2\textsuperscript{nd} derivative} of a signal containing three weak noisy peaks. Without smoothing, the signal seems to be all random noise; with enough smoothing, the three weak peaks are clearly visible (in derivative form) and measurable.


\begin{center}
\textbf{\InsImageInline{0.5}{l}{iSignalSpectrumMode.gif.png}}\textit{The} \textit{figure above} \textit{shows the} \textit{frequency spectrum mode, as the smooth width} \textit{is varied with the} \textbf{\textit{A}} \textit{and} \textbf{\textit{Z}} \textit{keys. This shows dramatically how the signal (top panel) and the frequency spectrum (below) are both affected by smooth width.(} \textit{If you are reading this online, you can observe the animation of this figure: c}\textit{lick for} \href{https://terpconnect.umd.edu/~toh/spectrum/iSignalSpectrumMode.gif}{\textit{GIF animation}}\textit{.)}
\end{center}


The script ``\href{https://terpconnect.umd.edu/~toh/spectrum/iSignalDeltaTest.m}{iSignalDeltaTest}'' demonstrates the frequency response of the smoothing and differentiation functions of iSignal by applying them to a \href{https://en.wikipedia.org/wiki/Dirac\_delta\_function}{delta function}. Change the smooth type, smooth width, and derivative order and see how the power spectrum changes.

\subsection{Showing that the Fourier frequency spectrum of a Gaussian is also a Gaussian\label{ref-0135}}

One special thing about the \textit{Gaussian} signal shape compared to all other shapes is that the Fourier frequency spectrum of a Gaussian is \textit{also} a Gaussian. You can demonstrate this to yourself numerically by downloading the \href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{isignal.m} functions and executing the following statements:

\texttt{x=-100:.2:100;}

\texttt{width=2; y=gaussian(x,0,width);}

\texttt{isignal([x;y],0,400,0,3,0,0,0,10,1000,0,0,1);}

Click on the figure window, press \textbf{Shift-T} to transfer the frequency spectrum to the top panel, then press \textbf{Shift-F}, press \textbf{Enter} three times, and click on the peak in the upper window. The program computes a least-squares fit of a Gaussian model to the frequency spectrum now in the top panel. The fit is essentially perfect. If you repeat this with Gaussians of \textit{different widths} (e.g., width=1 or 4), you will find that the width of the frequency spectrum peak is \textit{inversely proportional} to the width of the signal peak. In the limit of an infinitely \textit{narrow} peak width, the Gaussian becomes a \textit{delta function,} and its frequency spectrum is flat. In the limit of an infinitely \textit{wide} peak width, the Gaussian becomes a flat line, and its frequency spectrum is non-zero only at the zero frequency.

\chapter{Fourier Convolution\label{ref-0136}\label{ref-0137}\label{ref-0138}\label{ref-0139}\label{ref-0140}\label{ref-0141}\label{ref-0142}}

Convolution is an operation performed on two signals which involves multiplying one signal by a delayed or shifted version of another signal, integrating or averaging the product, and repeating the process for different delays. Convolution is a useful process because it accurately describes some effects that occur widely in scientific measurements, such as the influence of a\href{https://www.electronics-tutorials.ws/filter/filter\_2.html}{ frequency filter on an electrical signal} or of the \href{https://www.horiba.com/en\_en/bandpass-resolution/}{spectral bandpass of a spectrometer} on the shape of a recorded optical spectrum, which cause the signal to be spread out in time and reduced in peak amplitude. 


\begin{center}
\InsImageInline{0.5}{l}{Figure11a.GIF.png} \InsImageInline{0.5}{l}{Figure11b.GIF.png}
\end{center}



\begin{center}
\textbf{\textit{Fourier convolution}} \textit{is used here to determine how the optical spectrum in Window 1 (top left) will appear when scanned with a spectrometer whose slit function (spectral resolution) is described by the Gaussian function in Window 2 (top right). The Gaussian function has already been rotated so that its maximum falls at x=0. The resulting convoluted optical spectrum (bottom center) shows that the two lines near x=110 and 120 will not be resolved but the line at x=40 will be partly resolved. Fourier convolution is used in this way to correct the analytical curve non-linearity caused by spectrometer resolution, in hyperlinear} \textit{absorption spectroscopy} \textit{(Page} \pageref{ref-0329}\textit{).}
\end{center}


In practice, it is common to perform the calculation by point-by-point multiplication of the two signals in the Fourier domain. First, the Fourier transform of each signal is obtained. Then the two Fourier transforms are multiplied point-by-point by the rules for complex multiplication and the result is then inverse Fourier transformed. Fourier transforms are usually expressed in terms of "\href{https://en.wikipedia.org/wiki/Complex\_number}{complex numbers}", with real and imaginary parts; if the Fourier transform of the first signal is \textit{a} + \textit{ib}, and the Fourier transform of the second signal is \textit{c} + \textit{id}, then the product of the two Fourier transforms is (\textit{a} + \textit{ib})(\textit{c} + \textit{id})  =  (\textit{ac} - \textit{bd}) + \textit{i}(\textit{bc} + \textit{ad}). Although this seems to be a round-about method, it turns out to be faster than the shift-and-multiply algorithm when the number of points in the signal is large. Convolution can be used as a powerful and general algorithm for smoothing and differentiation. Many computer languages will perform this operation automatically when the two quantities divided are complex. In typeset mathematical texts, convolution is often designated by the symbol \texttt{⁕} (\href{https://en.wikipedia.org/wiki/Convolution}{Reference}).

Fourier convolution is used as a very general algorithm for the smoothing and differentiation of digital signals, by convoluting the signal with a (usually) small set of numbers representing the convolution vector. Smoothing is performed by convolution with sets of positive numbers, e.g. [1 1 1] for a 3-point boxcar. Convolution with [\textendash{}1 1] computes a first derivative; [1 -2 1] computes a second derivative. Successive convolutions by Conv1 and then Conv2 is equivalent to one convolution with the convolution of Conv1 and Conv2. First differentiation with smoothing is done by using a convolution vector in which the first half of the coefficients is negative, and the second half is positive (e.g. [-1 -2 0 2 1]). 

\section{Simple whole-number convolution vectors\label{ref-0143}}

\textbf{Smoothing vectors:}

\texttt{[1 1 1] = 3 point boxcar (sliding average) smooth}

\texttt{[1 1 1 1] = 4 point boxcar (sliding average) smooth}

\texttt{[1 2 1] = 3 point triangular smooth}

\texttt{[1 2 3 2 1] = 5 point triangular smooth}

\texttt{[1 4 6 4 1] = 5 point P-spline smooth} 

\texttt{[1 4 8 10 8 4 1] = 7 point P-spline smooth} 

\texttt{[1 4 9 14 17 14 9 4 1] = 9 point P-spline smooth} 

\textbf{Differentiation vectors:}

\texttt{[-1 1] First derivative}

\texttt{[1 -2 1] Second derivative}

\texttt{[1 -3 3 -1] Third derivative}

\texttt{[1 -4 6 -4 1] Fourth derivative}

\textbf{Results of successive convolution by two vectors Conv1 and Conv2: (}\texttt{⁕} \texttt{stands for convolution)}

\textbf{Conv1 Conv2}      \textbf{Result Description}

\texttt{[1 1 1]} \texttt{⁕}  \texttt{[1 1 1] = [1 2 3 2 1] Triangular smooth}

\texttt{[1 2 1]} \texttt{⁕}  \texttt{[1 2 1] = [1 4 6 4 1] P-spline smooth}

\texttt{[-1 1]} \texttt{⁕}  \texttt{[-1 1] = [1 -2 1] 2nd derivative}

\texttt{[-1 1]} \texttt{⁕}  \texttt{[1 -2 1] = [1 -3 3 -1] 3rd derivative}

\texttt{[-1 1]} \texttt{⁕}  \texttt{[1 1 1] = [1 0 0 -1) 1st derivative gap-segment}

\texttt{[-1 1]} \texttt{⁕}  \texttt{[1 2 1] = [1 1 -1 -1) Smoothed 1st derivative}

\texttt{[1 1 -1 -1]} \texttt{⁕}  \texttt{[1 2 1] = [1 3  $2 -2 -3 -1$ ] Same with more smoothing}

\texttt{[1 -2 1]} \texttt{⁕}  \texttt{[1 2 1] = [1 0 -2 0 1] 2nd derivative gap-segment}

\texttt{Rectangle ⁕ rectangle = triangle or trapezoid (depending on relative widths)}

\texttt{Gaussian ⁕ Gaussian = Gaussian of greater width}

\texttt{Gaussian ⁕ Lorentzian = Voigt profile (i.e. something in between Gaussian and Lorentzian)}

\section{Software details for convolution\label{ref-0144}}

\href{https://terpconnect.umd.edu/~toh/spectrum/SPECTRUM.html}{SPECTRUM,} page \pageref{ref-0481}, the freeware signal-processing application for Mac OS8 and earlier, includes convolution and auto-correlation (self-convolution) functions. 

\textbf{Spreadsheets} can be used to perform "shift-and-multiply" convolution (for example, \href{https://terpconnect.umd.edu/~toh/spectrum/MultipleSmoothing.xls}{MultipleConvolution.xls} or \href{https://terpconnect.umd.edu/~toh/spectrum/MultipleConvolution.xlsx}{MultipleConvolution.xls}\href{https://terpconnect.umd.edu/~toh/spectrum/MultipleConvolution.xlsx}{x} for Excel and \href{https://terpconnect.umd.edu/~toh/spectrum/MultipleConvolutionOO.ods}{MultipleConvolutionOO.ods} for Calc), but for larger data sets the performance is much slower than Fourier convolution (which is much easier done in Matlab or Octave than in spreadsheets). 

\href{http://en.wikipedia.org/wiki/MATLAB}{\textbf{Matlab}} \textbf{and} \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{\textbf{Octave}} have a built-in function for convolution of two vectors: \href{https://terpconnect.umd.edu/~toh/spectrum/conv.txt}{\textbf{conv}}. This function can be used to create filters and smoothing functions, such as \href{https://terpconnect.umd.edu/~toh/spectrum/bsmooth.m}{sliding-average} and \href{https://terpconnect.umd.edu/~toh/spectrum/tsmooth.m}{triangular} smooths. For example, 

  \texttt{ysmoothed=conv(y,[1 1 1 1 1],'same')./5;} 

smooths the vector y with a 5-point unweighted sliding average (boxcar) smooth, and 

   \texttt{ysmoothed=conv(y,[1 2 3 2 1],'same')./9;} 

smooths the vector y with a 5-point triangular smooth. The optional argument 'same' returns the central part of the convolution that is the same size as y. If that optional argument is "full", then the length of the result is ones less than the sum of the lengths of the two vectors.

Differentiation is carried out with smoothing by using a convolution vector in which the first half of the coefficients is negative, and the second half is positive (e.g. \texttt{[-1 0 1], [-2 -1 0 1 2]}, or \texttt{[-3 -2 -1 0 1 2 3]}) to compute a first derivative with increasing amounts of smoothing. 

The \textbf{conv} function in Matlab/Octave can easily be used to combine successive convolution operations, for example, a second differentiation followed by a 3-point triangular smooth:

\texttt{{\textgreater}{\textgreater} conv([1 -2 1],[1 2 1])}

\texttt{ans =}

  \texttt{1 0 -2 0 1}

The next example creates an exponential trailing transfer function (c), which has an effect like a simple RC low-pass filter and applies it to y. 

  \texttt{c=exp(-(1:length(y))./30);} 

  \texttt{yc=conv(y,c,'full')./sum(c);} 

In each of the above three examples, the result of the convolution is divided by the sum of the convolution transfer function, to ensure that the convolution has a net gain of 1.000 and thus does not affect the area under the curve of the signal. This makes the mathematical operation closer to the physical convolutions that spread out the signal in time and reduce the peak amplitude but conserve the total energy in the signal, which for a peak-type signal is proportional to the area under the curve.

Alternatively, you could perform the convolution yourself without using the built-in Matlab/Octave "conv" function by multiplying the Fourier transforms of y and c using the "fft.m" function, and then inverse transform the result with the "ifft.m" function. The results are essentially the same and the elapsed time is actually slightly faster than using the conv function. However,  c must be zero-filled to  match the size of yc because the point-by-point multiplication or division of two vectors requires that they have the same length.  The "conv" function performs any required zero filling automatically.

    \texttt{yc=ifft(fft(y).*fft(c));}

When using convolution for the purposes of smoothing, it's desirable that the area under the curve y remain the same after smoothing. This is easily ensured by dividing by the sum of the members of c:

\texttt{yc=ifft(fft(y).*fft(c))./sum(c);}

\href{https://terpconnect.umd.edu/~toh/spectrum/GaussConvDemo.m}{\textbf{GaussConvDemo.m}} shows that a Gaussian of unit height convoluted with a Gaussian of the same width is a Gaussian with a height of 1/sqrt(2) and a width of sqrt(2) and of equal area to the original Gaussian. \textbf{(}Figure window 2 shows an attempt to recover the original ``y'' from the convoluted ``yc'' by using the deconvgauss function). You can optionally add noise in line 9 to show how convolution smooths the noise and how deconvolution restores it. Requires gaussian.m, peakfit.m and deconvgauss.m in the path. 

\href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{\textbf{iSignal} }(page \pageref{ref-0433}) has a \textbf{Shift-V} keypress that displays the menu of Fourier convolution and deconvolution operations that allow you to convolute a Gaussian or exponential function with the signal and asks you for the Gaussian width or the time constant (in X units). 

\texttt{Fourier convolution/deconvolution menu} 

  \texttt{1. Convolution}

  \texttt{2. Deconvolution}

\texttt{Select mode 1 or 2:} \texttt{\textbf{1}}

\texttt{Shape of convolution/deconvolution function:}

  \texttt{1. Gaussian}

  \texttt{2. Exponential}

\texttt{Select shape 1 or 2:} \texttt{\textbf{2}}

\texttt{Enter the exponential time constant}:

Then you enter the time constant (in x units) and press \textbf{Enter}. 

\section{Multiple sequential convolution\label{ref-0145}\label{ref-0146}\label{ref-0147}\label{ref-0148}\label{ref-0149}\label{ref-0150}}

In the real world, signal broadening mechanisms are not always reducible to a single convolution. Sometimes two or more convolution mechanisms may be in play at the same time.  A good example of this occurs in the technique of twin-column recycling separation process (TCRSP), a novel chromatography technique in which the injected sample is recycled back to the two columns for obtaining better and better resolution, allowing chromatographers to solve challenging separation problems caused by the partition coefficients for the components being too similar and/or too low column efficiencies [reference 90]. In TCRSP, after the sample is separated by the first column, it flows into the second identical column, and after that separation, switching valves connect it back into the first column. That cycle repeats as many times as required. Each pass through a column increases the separation between the components slightly, so that with a sufficiently large number of cycles, very similar substances can be separated. As described in a later section (page \pageref{ref-0181}), chromatographic separations often involve broadening of the peaks by asymmetrical mechanisms, usually modeled as an exponentially modified Gaussian (EMG). Any broadening that occurs in the first pass will occur repeatedly in the subsequent passes. The net result will be a final peak shape that cannot be described by a single convolution. The success of the TCRSP technique depends on the fact that the separation \InsImageInline{0.5}{l}{image87.png}between the components increases faster than the width increase caused by the successive convolutions of broadening mechanisms.   But multiple sequential convolutions produce results that differ from a single large convolution. This is demonstrated by the simple example of two sequential exponential convolutions applied to a Gaussian, as shown in the figure on the left, generated by a \href{https://terpconnect.umd.edu/~toh/spectrum/TwiceBroadenedPeak2.m}{Matlab script}. The blue curve is the original Gaussian. The red curve is the result of a single convolution by an exponential function whose time constant \textit{tau} is 2. The cyan curve is the result of two successive convolutions with that same \textit{tau}. The orange curve is an attempt to duplicate that with a single wider convolution of \textit{tau} equal to 3. That attempt fails; the result is a poor match to the cyan curve. In fact, experiments show that \textit{no single wider exponential convolution can match the result of two (or more) successive convolutions}; the shape is fundamentally different. Multiple exponential convolutions result in a less asymmetrical peak, more shifted to larger x values. On the other hand, a single convolution by a function that is the \textit{product} of the Fourier transforms of the two separate functions does work (black dots). With greater numbers of successive convolutions, the peaks become more symmetrical and more Gaussian, as demonstrated by this \href{https://terpconnect.umd.edu/~toh/spectrum/MultipleBroadenedPeak.png}{graphic}, generated by this \href{https://terpconnect.umd.edu/~toh/spectrum/MultipleBroadenedPeak.m}{Matlab script}. (You can choose the number of convolutions in line 20).

\chapter{Fourier Deconvolution\label{ref-0151}\label{ref-0152}}

Fourier \href{http://www.dspguide.com/ch17/2.htm}{deconvolution} is the converse of Fourier \href{http://www.dspguide.com/ch6.htm}{convolution} in the sense that division is the converse of multiplication. If you know that \textbf{\textit{m}} times \textbf{\textit{x}} equals \textbf{\textit{n}}, where \textbf{\textit{m}} and \textbf{n} are known but \textbf{\textit{x}} is unknown, then \textbf{\textit{x}} equals \textbf{\textit{n}} divided by \textbf{\textit{m}}\textit{.} Similarly, if you know that the vector \textbf{\textit{M}} convoluted with the vector \textbf{\textit{X}} equals the vector \textbf{\textit{N}}, where \textbf{\textit{M}} and \textbf{\textit{N}} are known but \textbf{\textit{X}} is unknown, then \textbf{\textit{X}} equals \textbf{\textit{M}} deconvoluted from \textbf{\textit{N}}\textit{.} 

In practice, the deconvolution of one signal from another is usually performed by point-by-point \textit{division} of the two signals in the Fourier domain, that is, dividing the Fourier transforms of the two signals point-by-point and then inverse-transforming the result. Fourier transforms are usually expressed in terms of complex numbers, with ``real'' and ``imaginary'' parts representing the sine and cosine parts. If the Fourier transform of the first signal is \textit{a + ib}, and the Fourier transform of the second signal is \textit{c + id}, then the \textit{ratio} of the two Fourier transforms, by the rules for the \href{http://www.mesacc.edu/~scotz47781/mat120/notes/complex/dividing/dividing\_complex.html}{division of complex numbers}, is

\InsImage{1.0}{FourierDivide.gif.png}Many computer languages (such as Fortran and Matlab) will perform this operation automatically when two complex numbers are divided. 

\textbf{Note:} The word "\href{http://www.oxforddictionaries.com/us/definition/american\_english/deconvolution?q=deconvolution}{deconvolution}" can have \textit{two different meanings} in the scientific literature, which can lead to confusion. The Oxford dictionary defines it as "A process of resolving something into its constituent elements or removing complication in order to clarify it", which in one sense applies to Fourier deconvolution. However, the same word is also sometimes used for the process of resolving or decomposing a set of overlapping peaks into their separate additive components by the technique of \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{iterative least-squares curve fitting} (page \pageref{ref-0258}) of a proposed peak model to the data set. However, that process is conceptually distinct from \textit{Fourier} deconvolution, because in Fourier deconvolution, the underlying peak shape is unknown, but the broadening function is assumed to be known; whereas in iterative least-squares curve fitting, it is just the reverse: the peak shape must be known but the width of the broadening process, which determines the width and shape of the peaks in the recorded data, is usually unknown. Thus, the term "spectral deconvolution" is \textit{ambiguous}: it might mean the Fourier deconvolution of a response function from a spectrum, or it might mean the decomposing of a spectrum into its separate additive peak components. These are different processes; do not get them confused. 

\textbf{The practical significance of Fourier deconvolution} in signal processing is that it is used as a computational way to reverse the result of a convolution occurring in the physical domain, for example, to reverse the signal distortion effect of an electrical filter or of the finite resolution of a spectrometer. In some cases, the physical convolution can be measured experimentally by applying a single spike impulse ("delta") function to the input of the system, then that data can be used as a deconvolution vector. In that application, deconvolution works perfectly only when the signals contain no noise and when the original convolution function is known exactly, as in the case shown in the figure below where a square pulse is convoluted with a Gaussian convolution function, c. 

\InsImageInline{0.5}{l}{PerfectDeconvolution.png}Even if there is no known physical convolution that has broadened the signal, it is possible to use deconvolution as a method of peak sharpening by deconvolution a narrower version of that peak shape from the signal; that is referred to as "\href{https://www.google.com/search?q=self+convolution&oq=self+convolution&aqs=chrome..69i57j0l5.1447j0j7&sourceid=chrome&ie=UTF-8}{\textit{self-deconvolution}}", so-called because the shape of the deconvolution function is the same as the shape of the peaks in the signal.

Deconvolution can also be used to determine the form of a convolution operation that has been previously applied to a signal, by deconvoluting the original and the convoluted signals. Practical applications of Fourier deconvolution are shown in the following two pages.


\begin{center}
\InsImageInline{0.5}{l}{Figure12a.GIF.png} \InsImageInline{0.5}{l}{Figure12b.GIF.png}
\end{center}



\begin{center}
\textbf{\textit{Fourier deconvolution}} \textit{is used here to remove the distorting influence of an exponential tailing response function from a recorded signal (Window 1, top left) that is the result of a low-pass filter built into the electronics to reduce noise. The response function (Window 2, top right) must be known and is usually either calculated based on some theoretical model or is measured experimentally as the output signal produced by applying an impulse (delta) function to the input of the system. The response function, with its maximum at x=0, is deconvoluted from the original signal. The result (bottom, center) shows a closer approximation to the real shape of the peaks; however, the signal-to-noise ratio is unavoidably degraded compared to the recorded signal, because the Fourier deconvolution operation is simply recovering the original signal before the low-pass filtering, noise and all.}
\end{center}



\begin{center}
\textit{(If you are reading this online, click for} \href{https://terpconnect.umd.edu/~toh/spectrum/DeconvolutionDemo4.m}{\textit{Matlab/Octave script}}\textit{.}\textit{)}
\end{center}


Note that this process has an effect that is \textit{visually} similar to derivative\href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html}{ peak sharpening (page }\pageref{ref-0099}) although the latter requires no specific knowledge of the broadening function that caused the peaks to overlap.


\begin{center}
\InsImageInline{0.5}{l}{Figure13a.GIF.png} \InsImageInline{0.5}{l}{Figure13b.GIF.png}
\end{center}



\begin{center}
\textbf{\textit{A different application of Fourier deconvolution}} \textit{is to reveal the nature of an unknown data transformation function that has been applied to a data set by the measurement instrument itself. In this example, the figure in the top left is an ultraviolet-visible absorption spectrum}\index{absorption spectrum} \textit{recorded on a commercial photodiode array spectrometer (X-axis: nanometers; Y-axis: milliabsorbance). The figure in the top right is the} \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html}{\textit{first derivative}} \textit{of that spectrum produced by an (unknown) algorithm in the software supplied with the spectrometer. The objective here is to understand the nature of the} \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html\#Smoothing}{\textit{differentiation/ smoothing algorithm}} \textit{that the instrument's internal software uses. The signal in the bottom left is the surprisingly simple result of deconvoluting the derivative spectrum (top right) from the original spectrum (top left). This, therefore, must be the convolution function used by the differentiation algorithm in the spectrometer's software or its equivalent. Rotating and expanding it on the x-axis makes the function easier to see (bottom right). Expressed in terms of the smallest whole numbers, the convolution series is simply +2, +1, 0, -1, -2. This elementary example of ``}\href{https://en.wikipedia.org/wiki/Reverse\_engineering}{\textit{reverse engineering}}\textit{'' makes it easier to compare results from other instruments or to duplicate these results on other equipment.}
\end{center}


When applying Fourier deconvolution to experimental data, for example, to remove the effect of a known broadening or low-pass filter operator caused by the experimental system, there are \textit{four serious problems} that limit the utility of the method: 

(1) A mathematical convolution might not be an accurate model for the convolution occurring in the physical domain. 

(2) The width of the convolution - for example, the time constant of a low-pass filter operator or the shape and width of a spectrometer slit function - must be known, or at least adjusted by the user to get the best results. 

 (3) A serious signal-to-noise degradation commonly occurs; any noise added to the signal by the system \textit{after} the convolution by the broadening or low-pass filter operator will be greatly amplified when the Fourier transform of the signal is divided by the Fourier transform of the broadening operator, because the high frequency components of the broadening operator (the \textit{denominator} in the division of the Fourier transforms) are typically very small, with some individual components often of the order of 10\textsuperscript{-12} or 10\textsuperscript{-15}, resulting a huge amplification of those particular frequencies in the resulting deconvoluted signal, which is called "ringing". (See the Matlab/Octave code example at the bottom of this page). The problem can be reduced simply by \href{https://terpconnect.umd.edu/~toh/spectrum/Deconvolution.html\#convdeconv}{adding a small positive non-zero constant to the denominator}, which increases the excessively small high-frequency members in the denominator without significantly increasing the much greater low-frequency members (reference 86). Smoothing or filtering reduces the amplitude of the highest-frequency components, and denominator addition reduces the amplitude of the frequencies that are the most highly amplified by deconvolution. Both methods can have a similar effect, but they work in different ways and can sometimes be more effective when used together rather than separately.

You can see the amplification of high-frequency noise happening in the first graphic example above on the previous pages. On the other hand, this effect is \textit{not} observed in the second example, because in that case, the noise was present in the original signal, \textit{before} the convolution performed by the spectrometer's derivative algorithm. The high-frequency components of the denominator in the division of the Fourier transforms are typically much \textit{larger} than in the previous example, avoiding the noise amplification and divide-by-zero errors, and the only post-convolution noise comes from numerical round-off errors in the math computations performed by the derivative and smoothing operation, which is always much smaller than the noise in the original experimental signal. 

In many cases, the width of the physical convolution is not known exactly, so the deconvolution must be adjusted empirically to yield the best results. Similarly, the width of the final smooth operation must also be adjusted for the best results. The result will seldom be perfect, especially if the original signal is noisy, but it is often a better approximation to the real underlying signal than the recorded data without deconvolution. 

As a method for \textit{peak sharpening}, deconvolution can be compared to the \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html}{derivative peak sharpening method described earlier} or to the \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html\#power}{power method}, in which the raw signal is simply raised to some positive power \textit{n.} 

\section{Computer software for deconvolution\label{ref-0153}}

\href{https://terpconnect.umd.edu/~toh/spectrum/SPECTRUM.html}{SPECTRUM,} page \pageref{ref-0481}, the freeware signal-processing application for Mac OS8 and earlier, includes a Fourier deconvolution function.

\textbf{Matlab and Octave}\label{ref-0154}\label{ref-0155}

Matlab and Octave have a built-in function for Fourier deconvolution: \href{https://terpconnect.umd.edu/~toh/spectrum/deconv.txt}{\textit{deconv}}. An example of its application is shown in the figure on the next page: the vector \textit{yc} (line 6) represents a noisy rectangular pulse (\textit{y}) convoluted with a transfer function \textit{c} before being measured. In line 7, \textit{c} is deconvoluted from \textit{yc}, to recover the original \textit{y}. This requires that the transfer function \textit{c} be known. The \InsImageInline{0.5}{l}{deconvolution.png}rectangular signal pulse is recovered in the lower right (\textit{ydc}), complete with the noise that was present in the original signal. The Fourier deconvolution reverses not only the signal-distorting effect of the convolution by the exponential function, but also its low-pass noise-filtering effect. As explained above, there is a significant amplification of any noise that is added \textit{after} the convolution by the transfer function (line 5). This script demonstrates that there is a big difference between noise added \textit{before} the convolution (line 3), which is recovered unmodified by the Fourier deconvolution along with the signal, and noise added \textit{after} the convolution (line 6), which is amplified compared to that in the original signal. \href{https://terpconnect.umd.edu/~toh/spectrum/DeconvTest.m}{Download script}. Note that the ``sum(c2)'' term in line 7 is included simply to scale the amplitude of the result (specifically the area under the curve) to match the original y.

\texttt{x=0:.01:20;y=zeros(size(x));} \texttt{\textcolor{color-9}{\% 2000 point signal with 200-point}}

\texttt{y(900:1100)=1;} \texttt{\textcolor{color-9}{\% rectangle in center, y}} 

\texttt{y=y+.01.*randn(size(y));} \texttt{\textcolor{color-9}{\% Noise added}} \texttt{\textbf{\textcolor{color-9}{before}}} \texttt{\textcolor{color-9}{the convolution}}

\texttt{c=exp(-(1:length(y))./30);} \texttt{\textcolor{color-9}{\% exponential convolution function, c}}

\texttt{yc=conv(y,c,'full')./sum(c);} \texttt{\textcolor{color-9}{\% Create exponential trailing function, yc}}

\texttt{\textcolor{color-9}{\% yc=yc+.01.*randn(size(yc))}}\texttt{;} \texttt{\textcolor{color-9}{\% Noise added}} \texttt{\textbf{\textcolor{color-9}{after}}} \texttt{\textcolor{color-9}{the convolution}}

\texttt{ydc=deconv(yc,c).*sum(c);} \texttt{\textcolor{color-9}{\%}}  \texttt{\textcolor{color-9}{Recover y by deconvoluting c from yc}}

\texttt{\textcolor{color-9}{\% Plot all the steps}}

\texttt{subplot(2,2,1); plot(x,y); title('original y'); subplot(2,2,2); plot(x,c);title('c'); subplot(2,2,3); plot(x,yc(1:2001)); title('yc'); subplot(2,2,4); plot(x,ydc);title('recovered y')}

Alternatively, you could perform the Fourier deconvolution yourself \textit{without} using the built-in Matlab/ Octave "deconv" function by dividing the Fourier transforms of \textit{yc} and \textit{c} using the built-in Matlab/ Octave "fft.m" function and inverse transform the result with the built-in Matlab/Octave "ifft.m" function. Note that \textit{c} must be \href{https://www.techopedia.com/definition/10143/zero-filling}{zero-filled} to match the size of \textit{yc}. The results are essentially the same (except for the numerical floating-point precision of the computer, which is usually negligible), and it is ten \textit{times faster} than using the deconv function: 

\texttt{ydc=ifft(fft(yc)./fft([c zeros(1,2000)])).*sum(c);} 

If you are reading this online, \href{https://terpconnect.umd.edu/~toh/spectrum/ConvolutionDeconvolutionExample.txt}{click here} for a simple explicit example of Fourier convolution and deconvolution for a small 9-element vector, with the vectors printed out at each stage. 

\InsImageInline{0.5}{l}{GaussianDeconvolution5.png}The script \href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo3.m}{DeconvDemo3.m} is like the previous example, except that it demonstrates \textit{Gaussian} Fourier \InsImageInline{0.5}{l}{DeconvDemo3.gif.png}convolution and deconvolution of the same rectangular pulse, utilizing the fft/ifft formulation just described. The animated screen graphic on the right (If you are reading this online, click \href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo3.gif}{link for animation}) demonstrates the effect of changing the deconvolution width. The raw deconvoluted signal in this example (bottom left quadrant) is extremely noisy, but that noise is mostly \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#Frequency}{"blue" (high frequency) noise} that you can easily reduce by a little \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothing} (page \pageref{ref-0046}). As you can see in both animated examples here, deconvolution works \textit{best} when the deconvolution width exactly matches the width of the convolution to which the observed signal has been subjected; the further off you are, the worse will be the wiggles and other signal artifacts. In practice, you must try several different deconvolution widths to find the one that results in the \textit{smallest wiggles}, which of course becomes harder to see if the signal is very noisy. Note that in this example the deconvolution width must be within 1\% of the convolution width. In general, the wider the physical convolution width relative to the signal, the more accurately the deconvolution width must be matched the physical convolution width.

 \href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo5.m}{DeconvDemo5.m} \href{https://terpconnect.umd.edu/~toh/spectrum/GaussianDeconvolution5.png}{}(left) shows an example with \textit{two} closely-spaced underlying peaks of equal width that are \textit{completely unresolved} in the observed signal, but are recovered with their 2:1 height ratio intact in the deconvoluted and smoothed result. This is an example of Gaussian ``self-deconvolution''. \href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo6.m}{DeconvDemo6.m} is the same except that the underlying peaks are \href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo6.png}{Lorentzian}. Note that all these scripts require functions than can be \href{https://terpconnect.umd.edu/~toh/spectrum/functions.html}{downloaded} from \url{http://tinyurl.com/cey8rwh}. 

In all the above simulations, the deconvolution method works as well as it does because the signal-to-noise ratio of the "observed signal" (upper right quadrant) is quite good; the noise is not even visible on the scale presented here. In the absence of any knowledge of the width of the deconvolution function, finding the right deconvolution width depends upon experimentally minimizing the wiggles that appear when the deconvolution width is incorrect, and a poor signal-to-noise ratio will make this much more difficult. Of course, smoothing can reduce noise, especially high-frequency (blue) noise, but smoothing also slightly increases the width of peaks, which works \textit{counter} to the point of deconvolution, so it must not be overused. The image on the left shows the widths of the peaks (as full width at half maximum); the widths of the deconvoluted peaks (lower right quadrant) are only slightly larger than in the (unobserved) underlying peaks (upper left quadrant) either because of imperfect deconvolution or the broadening effects of the smoothing needed to reduce the high-frequency noise. As a rough but practical rule of thumb, if there is any \textit{visible} noise in the observed signal, it is likely that the results of self-deconvolution, of the type shown in \href{https://terpconnect.umd.edu/~toh/spectrum/GaussianDeconvolution5.png}{DeconvDemo5.m,} will be too noisy to be useful. 

\InsImageInline{0.5}{l}{DeconvDemo2.png}In the example shown on the right. (\href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo2.m}{Download this script}), the underlying signal (\textit{uyy}) is a \textit{Gaussian}, but in the observed signal (\textit{yy}) the peak is \textit{broadened exponentially} resulting in a \textit{shifted, shorter, and wider peak.} Assuming that the exponential broadening time constant ('\textit{tc}') is known, or can be guessed or measured (page \pageref{ref-0109}), the Fourier deconvolution of \textit{cc} from \textit{yy} successfully re\href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo2.png}{}moves the broadening (\textit{yydc}), and restores the original height, position, and width of the underlying Gaussian, but at the expense of considerable noise increase. The noise is caused by the fact that a little constant white noise has been added \textit{after} the broadening convolution (\textit{cc}), to make the simulation more realistic. However, the noise remaining in the deconvoluted signal is "\href{http://en.wikipedia.org/wiki/Colors\_of\_noise}{blue}" (\href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#Frequency}{high-frequency weighted}, see page \pageref{ref-0032}) and so is easily reduced by \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothing} (page \pageref{ref-0055}) and has less effect on least-square fits than does white noise. (For a greater challenge, try more noise in line 6 or a bad guess of the time constant ('\textit{tc}') in line 7). To plot the recovered signal overlaid with the underlying signal\texttt{:} \texttt{plot(xx,uyy,xx,yydc)}. To plot the observed signal overlaid with the underlying signal\texttt{:} \texttt{plot(xx,uyy,xx,yy)}. To curve fit the recovered signal to a Gaussian to determine peak parameters:

\texttt{[FitResults,FitError]=peakfit([xx;yydc],26,42,1,1,0,10)}, which yields excellent values for the original peak positions, heights, and widths. You can demonstrate to yourself that with \textit{ten times} the previous noise level (Noise=.01 in line 6), the values of peak parameters determined by curve fitting are still quite good, and even with \textit{100x more noise} (Noise=.1 in line 6) the peak parameters are \textit{more accurate than you might expect} for that amount of noise (because that noise is \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#Frequency}{blue}). Remember, there is no need to smooth the results of the Fourier deconvolution before curve fitting, \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#NOT\_smooth}{as seen previously} on page \pageref{ref-0063}. 

\texttt{\textcolor{color-9}{\% Deconvolution demo 2}}

\texttt{xx=5:.1:65;}

\texttt{\textcolor{color-9}{\% Underlying signal with a single peak (Gaussian) of unknown}} 

\texttt{\textcolor{color-9}{\% height, position, and width.}}

\texttt{uyy=gaussian(xx,25,10);}



\texttt{\textcolor{color-9}{\% Compute observed signal yy, using the expgaussian function with time}} 

\texttt{\textcolor{color-9}{\% constant tc, adding noise added AFTER the broadening convolution (ExpG)}}

\texttt{Noise=.001;} \texttt{\textcolor{color-9}{\% {\textless}{\textless}{\textless}{\textless} Change the noise here}}

\texttt{tc=70;} \texttt{\textcolor{color-9}{\% {\textless}{\textless}{\textless}{\textless} Change the exponential time constant here}}

\texttt{yy=expgaussian(xx,25,10,-tc)'+Noise.*randn(size(xx));}

\texttt{\textcolor{color-9}{\% Guess, or use prior knowledge, or curve fit one peak, to}} 

\texttt{\textcolor{color-9}{\% determine time constant (tc), then compute transfer function cc}}

\texttt{cc=exp(-(1:length(yy))./tc);} 

\texttt{\textcolor{color-9}{\% Use "deconv" to recover original signal uyy by deconvoluting cc}} 

\texttt{\textcolor{color-9}{\% from yy. It is necessary to zero-pad the observed signal as shown here.}}

\texttt{yydc=deconv([yy zeros(1,length(yy)-1)],cc).*sum(cc);} 

\texttt{\textcolor{color-9}{\% Plot the signals and results in 4 quadrants}}

\texttt{subplot(2,2,1);}

\texttt{plot(xx,uyy);title('Underlying 4 Gaussian signal, uyy');}

\texttt{subplot(2,2,2);}

\texttt{plot(xx,cc);title('Exponential transfer function, cc')}

\texttt{subplot(2,2,3);}

\texttt{plot(xx,yy);title('observed broadened and noisy signal, yy');}

\texttt{subplot(2,2,4);}

\texttt{plot(xx,yydc);title('After deconvoluting transfer function, yydc')}

An alternative to the above deconvolution approach is to use \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{iterative curve fitting} (page \pageref{ref-0258}) to fit the observed signal directly with an \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Exponential\_broadening}{exponentially broadened Gaussian} (shape number 5)\texttt{:} 

\texttt{{\textgreater}{\textgreater} [FitResults,FitError] = peakfit([xx;yy], 26, 50, 1, 5, 70, 10)}

\InsImageInline{0.5}{l}{CurveFitExponentialGaussian.png}Both methods give good values of the peak parameters, but the Fourier deconvolution method is faster because fitting the deconvoluted signal with a simple Gaussian model is faster than iteratively curve fitting the observed signal with the more complicated exponentially broadened Gaussian model.

If the exponential factor "tc" is not known, it can be determined by iterative curve fitting using ipf.m (page \pageref{ref-0461}), manually adjusting the exponential factor ('extra') interactively with the A and Z keys to get the best fit:

\texttt{{\textgreater}{\textgreater}ipf([xx;yy]);} 

which in this case gives a best fit when the exponential factor "tc" is adjusted to about 69.9 (close the correct value of 70 in this simulation). 

Alternatively, you can use \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m} with the \textit{unconstrained variable} exponentially broadened Gaussian (shape 31), which will automatically find the best value of "tc", but in that case the best results will be obtained if you give it a rough first guess ("start") as the eighth input argument, with values within a factor of two or so of the correct values: 

\texttt{{\textgreater}{\textgreater}[FitResults,FitError]=peakfit([xx;yy],0,0,1,31,70,10,} \texttt{\textbf{[20 10 50]}}\texttt{)}

\texttt{FitResults =} \texttt{\textcolor{color-10}{Peak\# Position Height Width Area}}\index{Area}  \texttt{\textcolor{color-10}{tc}}

  \texttt{1 25.006 0.99828 10.013 10.599 69.83}

\texttt{GoodnessOfFit =}

  \texttt{0.15575 0.99998}

The value of the exponential factor determined by this method is 69.8, again close to 70. However, if the signal is very noisy, there will be quite a bit of uncertainty in the value of the exponential factor so determined - for example, the value will vary a bit if slightly different regions of the signal are selected for measurement (e.g., by panning or zooming in ipf.m or by changing the center and window arguments in peakfit.m). See page \pageref{ref-0364} for another example with four overlapping Gaussians.

\subsection{Self-deconvolution\label{ref-0156}\label{ref-0157}}

\InsImageInline{0.5}{l}{image99.png}The figure on the right shows the application of self-deconvolution of two overlapping Lorentzian peaks with a narrower Lorentzian function, using a Fourier filter (next section) to reduce noise and ringing. The figure on the right shows the observed signal (green), the raw self-deconvoluted peak (blue) and the smoothed self-deconvoluted peak (red). The Matlab script also displays, in figure 2, the Fourier spectra of those three signals in the corresponding colors. The deconvolution function is a narrower zero-centered Lorentzian, shown in black. Because this is self-deconvolution, the shape of the peaks remains Lorentzian, but the widths are substantially narrowed, as the expense of degraded signal-to-noise ratio. 

The Matlab script also displays the Fourier spectra of those four signals in their corresponding colors, in figure window 2, shown on the left. Observing these spectra can be a useful guide to adjusting the deconvolution width and the noise/ringing reduction settings. Looking at the spectrum of the original raw data (black) you can see two distinct frequency regions: 

(a) below a frequency of 10\textsuperscript{2} there are a series of smooth descending humps, which are components of \InsImageInline{0.5}{l}{LorentzianSelfDeconvDemo2spectrum.png}the Fourier transform of the signal peaks, and 

(b) above 10\textsuperscript{2} there is a flat jagged region - that's the high frequency end of the spectrum of the white noise in the signal, for which the average value is roughly the same at all frequencies.

In order to sharpen the peaks, we need to raise the amplitude of the high-frequency components of the signal up to 10\textsuperscript{2} and decrease the amplitude of the noisy region above 10\textsuperscript{2}.  The deconvolution process by itself increases all the high-frequency components of the signal, as you can see in the figure above left, and smoothing reduces the highest-frequency components to control the noise in the result.

\subsection{Multiple sequential deconvolution\label{ref-0158}}

\InsImageInline{0.5}{l}{image101.png}In cases where the signal may have been subject to two or more sequential convolutions, as described on page \pageref{ref-0145}, the reversal those effect requires multiple sequential deconvolutions and cannot be undone accurately by a single larger deconvolution. As a simple example of that situation, the Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/DeconvoluteTwiceBroadenedPeak.m}{DeconvoluteTwiceBroadenedPeak.m}, demonstrates the attempted deconvolution of two exponential broadenings, represented by the vectors of b1 and b2, that have been applied to an originally Gaussian peak. In the resulting graphic on the left, the blue curve is the original underlying Gaussian, the yellow curve is the observed signal after the original has been twice exponentially broadened, and the red curve is an attempt to deconvolute a single wider exponential function with a larger time constant. That attempt is obviously unsuccessful; in fact, no single simple deconvolution can remove the effects of two or more convolutions. The black dotted line is the result of performing a deconvolution with the product \texttt{fft(b1)*fft(b2)}, which is the Fourier transform of the \textit{convolution} of b1 and b2. That attempt is successful and overlays the original Gaussian, in blue, exactly (page \pageref{ref-0142}). 

\subsection{Segmented deconvolution\label{ref-0159}}

If the peak widths or tailing vary substantially across the signal, you can use a \textit{segmented}\index{\textcolor{color-3}{segmented}} deconvolution, which allows the deconvolution vector to adapt to the local conditions in different signal regions. \href{https://terpconnect.umd.edu/~toh/spectrum/SegExpDeconv.m}{SegExpDeconv(x,y,tc)} divides x,y into several equal-length segments defined by the length of the vector ``tc'', then each segment is deconvoluted with an exponential decay of the form exp(-x./t) where ``t'' is the corresponding element of the vector ``tc''. Any number and sequence of t values can be used. \href{https://terpconnect.umd.edu/~toh/spectrum/SegExpDeconvPlot.m}{SegExpDeconvPlot.m} is the same except that it plots the original and deconvoluted signals and \textit{shows the divisions between the segments by vertical magenta lines} to make it easier to adjust the number and values of the segments. This is demonstrated by the script \href{https://terpconnect.umd.edu/~toh/spectrum/SegExpDeconvPlotExample.m}{SegExpDeconvPlotExample.m} (below). The inevitable noise increase can be moderated by segmented smoothing (page \pageref{ref-0399}). 

\InsImageInline{0.5}{l}{SegExpDeconvPlotExample.png}\href{https://terpconnect.umd.edu/~toh/spectrum/SegGaussDeconv.m}{SegGaussDeconv.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/SegGaussDeconvPlot.m}{SegGaussDeconvPlot.m} are the same except that they perform symmetrical (zero-centered) Gaussian deconvolution. \href{https://terpconnect.umd.edu/~toh/spectrum/SegDoubleExpDeconv.m}{SegDoubleExpDeconv.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/SegDoubleExpDeconvPlot.m}{SegDoubleExpDeconvPlot.m} perform symmetrical (zero-centered) exponential deconvolution. If the peak widths increase regularly across the signal, you can calculate a reasonable initial value for the vector ``tc'' by giving only the number of segments (``NumSegments''), the first value, ``start'', and the last value, ``endt'':

\texttt{tstep=(endt-startt)/NumSegments;}

\texttt{tc=startt:tstep:endt;} 

\subsection{Combination function\label{ref-0160}\label{ref-0161}}

The function \href{https://terpconnect.umd.edu/~toh/spectrum/convdeconv.m}{P=convdeconv(x,y,vmode,smode,vwidth,DAdd)}, for Matlab or Octave, performs Gaussian, Lorentzian, or exponential convolution and deconvolution of the signal in x,y. Set vmode=1 for convolution, 2 for deconvolution, smode=1 for Gaussian, 2 for Lorentzian, 3 for exponential; vwidth is the width of the convolution or deconvolution function, and DAdd is the constant denominator addition used to control ringing and noise resulting from deconvolution. For examples of the operation of this function, see \href{https://terpconnect.umd.edu/~toh/spectrum/ThreeGaussianSignalTrial.m}{ThreeGaussianSignalTrial.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/ThreeLorentzianSignalTrial.m}{ThreeLorentzianSignalTrial.m} (shown below). You can clearly see the substantial increase in peak height and decrease in peak width resulting from deconvolution. Although the peak heights are increased by deconvolution and the peak areas are slightly reduced by the denominator addition, this will not be a problem in \href{https://terpconnect.umd.edu/~toh/models/CalibrationCurve.html}{quantitative analysis by calibration curves} as long as the same deconvolution setting are used for all samples and standards. The random noise is also increased; nevertheless, the peak heights, positions, and widths can still be measured precisely by least squares methods. The noise can be reduced by applying a little subsequent smoothing (page \pageref{ref-0048}). That leaves the question of which values of vwidth and DAdd to use. Start with a value of vwidth somewhat smaller than the estimated width of the peaks in the signal, with DAdd set to zero or some small number like 0.001\%, then try larger values of vwidth to increase the peak sharpening. If noise and ringing start to obscure the signal, try larger values of DAdd. Excessively large values of either can distort the peaks. For peaks of Gaussian, Lorentzian, or a blend of the two, reasonable initial values of vwidth can be obtained from this empirical equation:


\begin{center}
vwidth = (0.91-PG*0.0017)*width,
\end{center}


where PG is the percent Gaussian character. 

\InsImageInline{0.5}{l}{DeconvLorFunction.png}

\texttt{-----------------------------------------}

\texttt{Parameters of the three Lorentzian peaks}

  \texttt{Peak Height FWHM Area}

  \texttt{1.0000 2.0000 2.0000 6.0929}

  \texttt{2.0000 3.0000 2.2000 10.0594}

  \texttt{3.0000 1.0000 2.2000 3.3407}

\texttt{Parameters of the three deconvoluted peaks}

  \texttt{Peak Height FWHM Area}

  \texttt{1.0000 5.6963 0.9624 5.9684}

  \texttt{2.0000 7.6741 1.0711 9.3350}

  \texttt{3.0000 2.7056 0.8623 3.0578}

\texttt{-----------------------------------------}

\subsection{Interactive deconvolution\label{ref-0162}\label{ref-0163}}

In my \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{\textbf{iSignal} version 8.3} and later (page \pageref{ref-0437}), you can press \textbf{Shift-V} to display the \href{https://terpconnect.umd.edu/~toh/spectrum/iSignalConvDeconvMenu.txt}{menu of Fourier convolution and deconvolution operations} that allow you to convolute or to deconvolute a Gaussian, Lorentzian or exponential function. It will ask you for the initial width or time constant of the deconvolution function (in X units), then you can use the \textbf{3} and \textbf{4} keys to decrease or increase the width by 10\% (or \textbf{Shift-3} and \textbf{Shift-4} to adjust by 1\%). This version of iSignal includes an additional way to reduce ringing and noise in the deconvoluted signal, by adding a constant to the denominator (reference 86) and adjusting it with the \textbf{5} and \textbf{6} keys to decrease or increase the constant by 10\% (or \textbf{Shift-5} and \textbf{Shift-6} to adjust by 1\%). The following is an application to a real experimental signal:

\InsImageInline{0.5}{l}{iSignalLorDeconvData658.png}In this example, the original signal is shown as the dotted green line and the result of deconvoluting it with a \textit{Lorentzian} deconvolution function is shown as the blue line. The deconvolution width was adjusted as large as possible without causing significant negative dips between the peaks, which for many types of experimental data, would be non-physical. (Recall that the mathematics of the deconvolution operation is structured so that the \textit{area under the peaks remains unchanged}, even though the widths are reduced, and the heights are increased). The zoomed-in close-up in the upper panel shows that several peaks with shoulders are resolved into distinct peaks, allowing their peak positions to be measured more accurately. Fortunately, the amplitude of those revealed peaks is greater than the small amount of noise remaining in the signal (thanks to the good signal-to-noise ratio of the original signal).

\chapter{Fourier filter\label{ref-0164}\label{ref-0165}\label{ref-0166}}

A Fourier filter is a type of filtering function that is based on manipulation of specific \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#Frequency}{frequency components} of a signal. It works by taking the \href{https://terpconnect.umd.edu/~toh/spectrum/HarmonicAnalysis.html}{Fourier transform} of the signal, then attenuating or amplifying specific frequencies or ranges of frequencies, and finally inverse transforming the result. \label{ref-0167}

In many scientific measurements, such as spectroscopy and chromatography, the signals are relatively smooth shapes that can be represented by a surprisingly small number of Fourier components. For example, the figure on the next page (\href{https://terpconnect.umd.edu/~toh/spectrum/GaussianFrequencyReconstruction3.m}{script}) shows in the top panel a signal of three smooth peaks, with peak heights of 1, 2, and 3, where the x-axis is time in seconds. The middle panel shows the first 50 frequencies of its Fourier spectrum, where the x-axis is frequency in Hz. The amplitude of the Fourier components is strongest at low frequencies and drops to near zero at 25 Hz. 

\InsImageInline{0.5}{l}{GaussianFrequencyReconstruction3.gif.png}The bottom panel shows the signal re-constructed by inverse transforming the first "n" Fourier components. A GIF \textit{animation} of this process (\href{https://terpconnect.umd.edu/~toh/spectrum/GaussianFrequencyReconstruction3.gif}{visible in Microsoft Word 365 or in any web browser}) shows the results of including the frequencies between 0 through 25 progressively. The reconstructed signal starts as a straight line (zero frequency, whose amplitude is the average value of the signal) and becomes progressively more complex as more frequencies are included, until it is visually indistinguishable from the original signal when all 26 frequencies are included. But notice what the reconstructed signal looks like when it gets to 16 frequencies. By that point, the amplitude of the frequencies has already dropped very low and there is relatively little amplitude in the remaining frequencies, so at that point the three peaks are rendered well. But the baseline has a small but distinct ripple, caused by the abrupt cut-off of the frequencies beyond that point. That can be avoided by including more frequencies or by using a filter with an adjustable filter shape that allows the cut-off rate to be controlled. This is illustrated on the next page.

The optimization of the Fourier filter for the signal-to-noise (SNR) ratio of peak signals faces the same compromise as conventional smoothing functions; namely, the optimum SNR is achieved when the peak height is less than the noiseless maximum. For example, the script \href{https://terpconnect.umd.edu/~toh/spectrum/GaussianSNRFrequencyReconstruction.m}{GaussianSNRFrequencyReconstruction.m} shows that for a Gaussian peak, the optimum SNR is reached when the peak height is about half the true value, but the peak area is the same. (White noise has \href{https://terpconnect.umd.edu/\%7Etoh/spectrum/SignalsAndNoise.html\#Frequency}{equal amplitude at all frequencies}, (page \pageref{ref-0032}) whereas most of the peak signal is concentrated in the first few frequencies). 

A more dramatic example is shown the figure below. In this case, the signal (top left) seems to be only \InsImageInline{0.5}{l}{FourierFilterDemo.png}random high-frequency noise, and its Fourier spectrum (top right) shows that high-frequency components dominate the spectrum over most of its frequency range (\href{https://terpconnect.umd.edu/~toh/spectrum/FourierFilterDemo.m}{script}). The bottom left panel shows the Fourier spectrum expanded in the X and Y directions to \InsImageInline{0.5}{l}{GaussianSNRFrequencyReconstructionFigure2.png}show the low-frequency region more clearly. There, the series of relatively smooth bumps, with peaks at the 1st, 20th, and 40th frequencies, are most likely the actual signal. Working on the hypothesis that the components above the 40th harmonic are increasingly dominated by noise, using a Fourier filter function (\href{https://terpconnect.umd.edu/\%7Etoh/spectrum/FouFilter.m}{FouFilter.m}) that can gradually reduce the higher harmonics and reconstruct the signal from the modified Fourier transform (red line). The result (bottom right) shows that the signal contains two partly overlapping Lorentzian peaks that were totally obscured by high-frequency noise in the original signal. 

\section{Computer software for Fourier Filtering\label{ref-0168}}

\textbf{MATLAB}. The simplest possible code for a Fourier simply cuts out all frequencies above a certain limit. To do this right, care must be taken to use both the sine \textit{and} cosine (or equivalently the frequency \textit{and} phase or the real \textit{and} imaginary) components of the Fourier transform. The operation must account for the mirror-image structure of the Matlab's Fourier transform: the lowest frequencies are at the extremes of the fft and the highest frequencies are in the \textit{center} portion. So, to pass the lowest \textit{n} frequencies, you must pass the first \textit{n} points \textit{and} the last \textit{n} points and \textit{zero out the others}.

\texttt{ffty=fft(y);} \texttt{\textcolor{color-9}{\% ffty is the fft of y}}

\texttt{lfft=length(ffty);} \texttt{\textcolor{color-9}{\% Length of the FFT}}

\texttt{ffty(n:lfft-n)=0;} \texttt{\textcolor{color-9}{\% Frequencies between n and lfft-n in the fft are set to zero.}}

\texttt{fy=real(ifft(ffty));} \texttt{\textcolor{color-9}{\% Real part of the inverse fft}}

The function form of this simple Fourier low pass filter is \href{https://terpconnect.umd.edu/~toh/spectrum/flp.m}{flp.m}. The is the minimal essence of a Fourier filter, but it is not really a practical filter, however, because its abrupt cutoff usually results in ringing on the baseline, as shown above. 

\InsImageInline{0.5}{l}{FourierFilterBandwidthOptimizationShape1.png}\InsImageInline{0.5}{l}{image109.png}\textbf{General-purpose Fourier filter function.} To make the Fourier filter more generally useful, we must add code not only low-pass filters, but also for high-pass, band-pass, and band-reject filter modes, plus a provision for more gentle and variable cut-off rates. The custom Matlab/\href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{Octave} function \href{https://terpconnect.umd.edu/~toh/spectrum/FouFilter.m}{FouFilter.m} is a more flexible Fourier filter that can serve as a low pass, high pass, bandpass, or band-reject (notch) filter \textit{with variable cut-off rate}. Has the form \texttt{[ry,fy,ffilter,ffy] = FouFilter (y,samplingtime,centerfrequency, frequencywidth,shape,mode)}, where y is the time-series signal vector, 'samplingtime' is the total duration of sampled signal in sec, millisec, or microsec; 'centerfrequency' and 'frequencywidth' are the center frequency and width of the filter in Hz, KHz, or MHz, respectively; 'Shape' determines the sharpness of the cut-off. If shape = 1, the filter is Gaussian; as shape increases the filter shape becomes more and more rectangular. Set mode = 0 for band-pass filter, mode = 1 for band-reject (notch) filter. FouFilter returns the filtered signal in 'ry'. It can handle signals of virtually any length, limited only by your computer’s memory. Here are two example scripts that call FouFilter.m: \href{https://terpconnect.umd.edu/~toh/spectrum/TestFouFilter.m}{TestFouFilter.m} demonstrates a Fourier bandpass filter applied to a noisy 100 Hz sine wave that appears in the middle third of the signal record, shown in the figure above. You can see that this filter is effective in extracting the signal from the noise, but that the response time is slow. The script \href{https://terpconnect.umd.edu/~toh/spectrum/TestFouFilter2.m}{TestFouFilter2.m} demonstrates a Fourier bandpass filter applied to a noisy 100 Hz sine wave signal with the filter center frequency swept from 50 to 150 Hz. Both require the FouFilter.m function in the Matlab/Octave path.

The figure on the right (\href{https://terpconnect.umd.edu/~toh/spectrumFourierFilterBandwidthOptimization.m}{Matlab script}) demonstrates the effect of the bandwidth of a Fourier low-pass filter applied to a typical peak signal with white noise. The graph shows the signal-to-noise ratio (red) and the percent errors in the peak parameters (height, width, and position) as a function of filter width. The signal to noise ratio is also shown. As usual, the results are poor if the bandwidth is either too low or too high, but in this case the signal-to-noise ratio is best at a relatively low bandwidth whereas most of the peak height and width measurements are most accurate at much higher values. \href{https://terpconnect.umd.edu/~toh/spectrum/FourierFilterBandwidthOptimizationShape2.png}{A slightly different result} is obtained with shape=2, for which the cut-off rate is faster. As usual, compromise is a must.

\InsImageInline{0.5}{l}{SSFexample3.png}\textbf{Segmented Fourier filter.} \href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedFouFilter.m}{SegmentedFouFilter.m} is a \textit{segmented} version of FouFilter.m, which applies different center frequencies and widths to different segments of the signal. The syntax is the same as FouFilter.m except that the two input arguments \textit{centerFrequency} and \textit{filterWidth} must be vectors with the values of \textit{centerFrequency} and \textit{filterWidth} for each segment. The function divides the signal into several equal-length segments determined by the length of \textit{centerFrequency} and \textit{filterWidth}, which must be equal in length. For help and examples, type ``help SegmentedFouFilter''. The figure on the left shows Example 2, which demonstrates a Fourier low-pass filter with decreasing bandwidth as the peak widths become wider from left to right.

\InsImageInline{0.5}{l}{iFilterExample3.png}Further Matlab-based applications using an \textit{interactive} Fourier filter, \href{https://terpconnect.umd.edu/~toh/spectrum/InteractiveFourierFilter.htm}{\textit{iFilter.m}}, shown on the right, allows you to adjust the filter parameters with keystrokes while observing the effect on your signal dynamically. This is described on page \pageref{ref-0441}. 

A demonstration of a \textit{real-time} Fourier filter is discussed on page \pageref{ref-0418}. \label{ref-0169}\label{ref-0170}\label{ref-0171}\label{ref-0172}\label{ref-0173}\label{ref-0174}

\chapter{Wavelets and wavelet denoising \label{ref-0175}}

Wavelets are literally ``little waves'', small oscillating waveforms that begin from zero, swell to a maximum, and then quickly decay to zero again. They can be contrasted to, for example, sine or cosine waves, which go on ``forever'', repeating out to positive and negative infinity. In the previous sections we have seen how useful it is to use the Fourier Transform of a signal, which expresses a signal as the sum of sine and cosine waves, allowing such useful operations as convolution, deconvolution, and Fourier filtering. But there is a downside to the Fourier Transform; it covers the entire signal duration, giving only the \textit{average} frequency content. We saw on pages 93-96 that it is possible to use segmented or time-resolved variations of the Fourier transform to overcome this difficulty. But a more sophisticated way to solve this limitation of Fourier analysis is to use wavelets as a basis set for representing signals rather than sine and cosine waves. Like sine waves, wavelets can be stretched or compressed along their ``x'' or time axis to cover different frequencies. But unlike sine waves, wavelets can be translated along the time axis of a signal to probe the time variations, because wavelets are of short duration compared with the signals they are used with. 

Wavelets were introduced by mathematicians and mathematical physicists in the early years of the 20\textsuperscript{th} century and the subsequent development has been highly mathematical. Many of the treatments of wavelets in the literature are aimed at the mathematical aspects, which have been ``worked out in excruciating detail'' (reference 82). The value system of mathematics \textendash{} rigorous proofs, exhaustive exploration, assumption of mathematical background, and the need for compact notation - makes it difficult for the non-specialists. Because of this, there are many ``easy'' introductions to the subject (references 79 - 82) that promise to soften the blow of mathematical abstraction. For that reason, I will not repeat all those mathematical details here. Rather, I will attempt to show what you can accomplish using wavelets \textit{without} understanding all the underlying mathematics. I am particularly interested in situations when wavelets work better than the best available conventional techniques, but also in the few situations where the conventional techniques remain superior. 

A \href{https://www.sciencedirect.com/topics/computer-science/wavelet-transforms}{wavelet transform} (WT) is a decomposition of a signal into a set of ``basis functions'' consisting of contractions, expansions, and translations of a wavelet function (reference 85). It can be computed by repeated convolution of the signal (page \pageref{ref-0140}) with the chosen wavelet as the wavelet is translated across the time dimension (to probe the time variation) and as it is stretched or compressed (to probe different frequencies). Because two dimensions are being probed, the result is naturally a 3D surface (time-frequency-amplitude) that can be conveniently displayed as a time-frequency \href{https://www.google.com/search?q=contour+plot&oq=contour+plot&aqs=chrome..69i57j0l7.2879j0j7&sourceid=chrome&ie=UTF-8}{\textit{contour plot}} with different colors representing the amplitudes at that time and frequency. Of course, you must expect that such calculations will require more complex algorithms and greater execution times, often taking about 5 to 20 times longer. That might have been a problem in the early days of computers, but with modern fast processors and great memory capacity, it is unlikely to be of concern now. 

Wavelets are used for the visualization, analysis, compression, and denoising of complex data. There are dozens of different wavelet shapes, which by itself is a big difference from sinewave-based Fourier analysis. The \href{https://en.wikipedia.org/wiki/Wavelet}{Wikipedia article on wavelets} mentions three of them, which are shown on the next page: the Meyer, the Morlet and the Mexican hat. Wavelets are conventionally constructed so that the areas under the curve is zero and the integral of their squares is 1.0. 

\InsImageInline{0.5}{l}{image112.png} \InsImageInline{0.5}{l}{image113.png} \InsImageInline{0.5}{l}{image114.png}

In Matlab, the easiest way to access these tools is to use the \textit{Wavelet Toolbox}, if that is included in your school or company campus Matlab site license or if you purchase it. This toolbox includes a graphical user interface (GUI) for a Wavelet Analyzer, Signal Multiresolution Analyzer, and a Wavelet Signal Denoiser, as well as an \href{https://terpconnect.umd.edu/~toh/spectrum/WaveletHelp.txt}{extensive collection of command-line wavelet functions}. Documentation is available at \url{https://www.mathworks.com/products/wavelet.html}. However, it is not necessary to have the Wavelet Toolbox; code has been published on the Internet in a variety of languages. For example, several papers (reference 84, 90) include or reference Matlab code that implement wavelets using only the inbuilt Matlab functions ``fft'', ``ifft'', and ``conv''. In this chapter will use all these software approaches to describe the properties and applications of wavelets to scientific measurement. 

\section{Visualization and analysis\label{ref-0176}}

Wavelets are quite effective at visualizing complicated signals and helping the scientist make sense of them. A good example is given in reference 84, which describes a signal sampled at 1000 Hz that consists of three overlapping components that are initially unknown to the experimenter. These components are shown in the figure below: (1) a swept sine wave (‘chirp’) going from 5 Hz to 20 Hz, (2) another simultaneous ‘chirp’ going in reverse from 20 Hz to 5 Hz, and finally (3) a Gaussian-modulated 20 Hz sine wave that peaks in the center of the signal. 

\InsImageInline{0.5}{l}{image115.png}When there three added are up, the resulting waveform, shown in the upper panel of the figure below (displayed in \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{iSignal.m}) is complicated and offers no hint of its underlying structure. The conventional Fourier transform spectrum, shown in the lower panel, offers no help. In fact, the Fourier spectrum is misleading; it suggests that there might be \textit{two} components, one at a higher frequency range than the other, with a small gap in between near 12 Hz. But there are in fact \textit{three} components, two of them covering a wide frequency range and the third one fixed at about 12 Hz. No help there.

\InsImageInline{0.5}{l}{image116.png}\InsImageInline{0.5}{l}{image117.png}In contrast, the wavelet-based time-frequency-amplitude contour shown below, which was computed using the Morlet wavelet by the \href{https://terpconnect.umd.edu/~toh/spectrum/Morlet.m}{Matlab code in reference} 86, helps to unravel the complexities, showing all three components clearly. In this display, \textit{yellow} corresponds to the greatest amplitudes and \textit{blue} to the lowest. 

(There is an interesting ambiguity concerning the two swept sine waves at the point where they cross in frequency in the middle of the signal and cancel out momentarily; do they keep going in the same direction, forming an ``X'', or do they both reverse direction, forming a ``V'' and its reflection? The two behaviors would result in the same final signal. The simplest assumption would be the former). 

Another example is closer to a typical scientific application: digging a signal out of an excess of noise and interference. This one is based on the ``buried peaks'' signal used before on page \pageref{ref-0130}. The signal (top panel in the iSignal screen below) has a pair of hidden Gaussian peaks that are totally buried in a much stronger interfering swept-frequency sine wave and random white noise. The Fourier spectrum, observed here in iSignal.m in the bottom panel, again offers no obvious hint of the underlying peaks. \InsImageInline{0.5}{l}{Example6iSignal.png}

\InsImageInline{0.5}{l}{image119.png}Here I applied the Morlet wavelet to this signal to create the time-frequency-amplitude matrix shown on the left (\href{https://terpconnect.umd.edu/~toh/spectrum/MorletExample2.m}{script} and \href{https://terpconnect.umd.edu/~toh/spectrum/mwavelet.m}{Morlet wavelet function)}. The big yellow diagonal stripe corresponds to the swept sine wave, but you can also see two weaker green humps at the bottom, the low-frequency end, near data points 5000 and 10000 that correspond to the two Gaussian peaks. On the basis of that observation, you would be justified to perform \href{https://terpconnect.umd.edu/~toh/spectrum/Example6iSignalSmooth.png}{smoothing} or \href{https://terpconnect.umd.edu/~toh/spectrum/Example6CurveFitResults.png}{curve-fitting} in that region, as we did before on page 94. (You can compare this graphic to the segmented Fourier spectrum display for this signal shown on that page, which is cruder but displays similar information; the wavelet is clearly a finer-grained tool).

\section{Wavelet denoising\label{ref-0177}\label{ref-0178}\label{ref-0179}\label{ref-0180}}

In the context of wavelets, ``denoising'' means reducing the noise as much as possible without distorting the signal. Denoising makes use of the time-frequency-amplitude matrix created by the wavelet transform. It assumes that the undesired noise will be separated from the desired signal by their frequency ranges. Most commonly in scientific measurements, the desired signal components are located at relatively \textit{low} frequencies and the noise is mostly at \textit{higher} frequencies. The process is controlled both by the selection of wavelet type and by a positive integer number called the wavelet ``level''; the higher the level, the lower is the frequency divider between signal and noise. (To that extent, the wavelet level is qualitatively like the smooth width of a smoothing operation). 

Again, Matlab’s Wavelet Toolbox provides some useful tools. First, there is the GUI app called the ``Wavelet Signal Denoiser''. The selection of the wavelet type and level are all selectable manually in that app. I used it to analyze the ``buried peaks'' signal described on the previous page, using the ``sym4'' wavelet at a relatively high level of 11, because lower levels allow too much of the interfering swept sine wave to come through and higher levels would damp out the Gaussian peaks too much. (The number in the wavelet name refers to the number of so-called ``\href{https://math.stackexchange.com/questions/128165/what-is-a-vanishing-moment}{vanishing moments}''. More vanishing moments means that it can represent more complex functions). The ``Approximation'' result (the dotted line in the graph below) is the \textit{low-frequency} information in the data, and you can clearly see that this is a ``denoised'' version of the original signal (shown in blue).  The two bumps at sample numbers 5000 and 10000 are the two Gaussian peaks. 

\InsImageInline{0.5}{l}{Example6WaveletDenoiseGUI.png}

So, both the sym4 wavelet in the Wavelet Signal Denoiser and the Morlet wavelet’s time-frequency-amplitude matrix give evidence of the hidden Gaussian peaks, but they display them in different ways. 

In addition to the GUI app, there is also a command-line denoising function called ``wdenoise'' (syntax: \texttt{wdenoise(noisydata,level,{\ldots})}. The selection of the wavelet type and level are set by including optional input arguments to this function. The advantage of a \textit{function}, compared to a GUI app, is that it is possible to write scripts that quickly and automatically compare many different wavelet settings, or that compare the results to several conventional noise reduction methods, or that automate the batch processing of large numbers of stored data sets (see page \pageref{ref-0411}). For example, the question of the optimal selection of wavelet level is answered by the script \href{https://terpconnect.umd.edu/~toh/spectrum/OptimizationOfWaveletLevel3peaks.m}{OptimizationOfWaveletLevel3peaks.m}, which creates a signal consisting of three noisy unit-height Gaussian (or Lorentzian) peaks with different peak widths, with added white noise, as in this figure. 

\InsImageInline{0.5}{l}{OptimizationOfWaveletLevel3peaks.png}The script uses the wdenoise.m function to denoise the signal with the ``coiflet'' wavelet from levels 1 to 11, measuring three quantities for each level: (a) the height of the peaks, (b) the signal-to-noise ratio improvement, and (c) the closeness to the noiseless underlying signal, as shown in the three plots below.

\InsImageInline{0.5}{l}{EffectOfWaveletLevel.png}We can see from these plots that a level of about 7 is optimum in this case. Above 7, the signal-to-noise ratio (center graph) continues to improve, but the results are unreliable and tend to scatter around too much. (Changing to Lorentzian peaks - line 28 of the script - yields similar results).

The script \href{https://terpconnect.umd.edu/~toh/spectrum/WaveletsComparison.m}{WaveletsComparison.m} compares five different wavelet types on the same signal: BlockJS, bior5.5, coif2, sym8, and db4, all at level 12 (\href{https://terpconnect.umd.edu/~toh/spectrum/WaveletsComparison.png}{graphic}). The results are similar but the sym8 has a slight edge. For most smooth peak shapes with additive white noise, the different wavelet types perform similarly. For signals with \textit{high}-frequency weighted noise, the \href{http://wavelets.pybytes.com/wavelet/bior5.5/}{bior5.5} wavelet works better than the others (\href{https://terpconnect.umd.edu/~toh/spectrum/WaveletsComparisonBlueNoise.m}{script}; \href{https://terpconnect.umd.edu/~toh/spectrum/WaveletsComparisonBlueNoise.png}{graphic}). For square pulses, the \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothVsWaveletsSquareWave.png}{Haar wavelet is clearly superior}. 

Another \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothVsWavelets2Gaussians.m}{script}, SmoothVsWavelets2Gaussians.m, compares five different non-wavelet smoothing techniques and two different wavelets, all using the same simulated signal consisting of two Gaussian peaks with a \textit{50-fold difference} in peak width, with additive white noise. For each method, the percent errors in the peak height, width, and area are measured, as well as the difference between the underlying noiseless signal and the denoised (or smoothed) noisy signal. This illustrates a significant advantage that wavelet denoising has over smoothing; \textit{it adapts much better to differences in peak width}. A summary of typical results is shown in this table. (Peak 1 is the narrow peak; peak 2 is 50 times wider).

\texttt{\textbf{\textcolor{color-6}{Typical results Percent errors of peak1{\textbar}peak2}}}

\texttt{\textbf{\textcolor{color-6}{Method Residuals Height Width Area}}} 

\texttt{\textbf{Original 9.88\% 6.29\%{\textbar}25.8\% 6.31\%{\textbar}-23.24\% -2.49\%{\textbar}0.86\%}}

\texttt{\textbf{Gaussian 2.53\% -3.34\%{\textbar}-3.79\% 5.72\%{\textbar}4.35\% -2.6\%{\textbar}0.82\%}}

\texttt{\textbf{Segmented 2.04\% -24.48\%{\textbar}3.09\% 37.8\%{\textbar}0.21\% -7.07\%{\textbar}0.85\%}}

\texttt{\textbf{Savitsky-Golay 2.93\% -2.08\%{\textbar}6.45\% 8.66\%{\textbar}-3.04\% -1.9\%{\textbar}0.86\%}}

\texttt{\textbf{RC filter 3.29\% -6.53\%{\textbar}9.58\% 16.06\%{\textbar}-5.45\% -11.76\%{\textbar}0.86\%}}

\texttt{\textbf{Hamming filter 2.91\% -5.12\%{\textbar}8.7\% 8.19\%{\textbar}-5.31\% -2.17\%{\textbar}0.86\%}}

\texttt{\textbf{\textcolor{color-12}{coif2 wavelet 1.02\% 1.78\%{\textbar}1.18\% -5.59\%{\textbar}0.22\% -6.54\%{\textbar}0.75\%}}}

\texttt{\textbf{db2 wavelet 1.17\% 11.47\%{\textbar}3.82\% 3.36\%{\textbar}2.38\% -5.34\%{\textbar}0.81\%}}

The ``Residuals'' are the percent differences between the underlying noiseless signal and the signal with random noise after denoising; it accounts for both the residual noise in the signal and distortion of the signal shape. As you can see, \textit{the coif2 wavelet comes out ahead by most measures}. This illustrates the most significant practical advantages of wavelet denoising: (1) it gives results that are at least as good as, and often better than, conventional smoothing methods; (2) it is easier to use because it automatically adapts to different peak widths; and (3) it is easier to optimize because in most cases only the level setting makes much difference in the practical results. 

However, there are a few situations where conventional methods are still better. For example, in calculating the second derivatives of noisy peaks of variable width, a segmented Gaussian-weighted smooth (page \pageref{ref-0399}) gives a signal-to-noise ratio better than that of a wavelet denoise (\href{https://terpconnect.umd.edu/~toh/spectrum/SmoothVsWaveletsDerivative2Segmented.m}{script}; \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothVsWaveletsDerivative2Segmented.png}{graphic}), especially if the signal-to-noise ratio is poor (\href{https://terpconnect.umd.edu/~toh/spectrum/SmoothVsWaveletsDerivative2Segmented10Xnoise.png}{graphic}), presumably because the frequency spectrum of the noise is so strongly high-frequency weighted. Also, wavelet denoising does not work at all if the amplitude of the noise is \textit{proportional} to the signal amplitude, rather than \textit{constant} (\href{https://terpconnect.umd.edu/~toh/spectrum/WaveletsComparisonProportionalNoise.m}{script}; \href{https://terpconnect.umd.edu/~toh/spectrum/WaveletsComparisonProportionalNoise.png}{graphic}). Sometimes, if the original signal-to-ratio is very poor, wavelet denoising produces narrow spike artifacts in the denoised signals, even when \href{https://www.mathworks.com/help/wavelet/examples/denoising-signals-and-images.html\#denoisingsignalsdemo-1}{soft thresholding} is used. These are special cases, however; there are many more situations where the wavelet denoise is really the method of choice, assuming that the slower speed of wavelet methods is not an issue.

The signal-to-noise ratio improvement performance of wavelet denoising is compared to traditional smoothing methods, as a function of smooth ratio, in the previous chapter on smoothing, page \pageref{ref-0075}. The wavelet method performs better but is 10 times slower than even the Savitzky-Golay smooth.

\chapter{Integration and peak area measurement\label{ref-0181}\label{ref-0182}}

Symbolic integration of functions and calculation of definite integrals are topics that are introduced in elementary Calculus courses. The numerical integration of digitized signals is applied in analytical signal processing as a method for measuring the areas under the curves of peak-type signals. 

Peak area measurements are very important in \href{https://en.wikipedia.org/wiki/Chromatography}{chromatography}, a class of chemical measurement techniques in which a mixture of components is made to flow through a chemically prepared tube or layer that allows some of the components in the mixture to travel faster than others, followed by a device called a \textit{detector} that measures and records the components after separation. Ideally, the \InsImageInline{0.5}{l}{EffectOfBroadening.png}components are sufficiently separated so that each one forms a distinct \textit{peak} in\href{https://terpconnect.umd.edu/~toh/spectrum/EffectOfBroadening.png}{} the detector signal. The magnitudes of the peaks are \href{http://terpconnect.umd.edu/~toh/models/CalibrationCurve.html}{calibrated }to the concentration of that component by measuring the peaks obtained from "standard solutions" of known concentration. In chromatography it is common to measure the \textit{area} under the detector peaks rather than the \textit{height} of the peaks because peak area is less sensitive to the influence of peak broadening (dispersion) mechanisms that cause the molecules of a specific substance to be diluted and spread out rather than being concentrated on one "plug" of material as it travels down the column. These dispersion effects, which arise from many sources, cause chromatographic peaks to become shorter, broader, and in some cases more unsymmetrical, but they have \textit{little effect on the total area under the peak}, if the total number of molecules remains the same. If the detector response is linear with respect to the concentration of the material, the peak \textit{area} remains proportional to the total quantity of substance passing into the detector, even though the peak \textit{height} is smaller. A graphical example is shown on the left (\href{https://terpconnect.umd.edu/~toh/spectrum/BroadenedPeak.m}{Matlab/Octave code)}, which plots detector signal vs time, where the \textbf{\textcolor{color-10}{blue curve}} represents the original signal and the \textbf{\textcolor{color-13}{red curve}} shows the effect of broadening by dispersion effects. The peak height is lower, and the width is greater, but the \textit{area} under the curve is almost the same. If the extent of broadening changes between the time that the \textit{standards} are run and the time that the unknown \textit{samples} are run, then \textit{peak area measurements will be more accurate and reliable} than peak height measurements. (Peak height will be proportional to the quantity of material only if the peak width and shape are constant). Another example with greater broadening: (\href{https://terpconnect.umd.edu/~toh/spectrum/GaussVsExpGauss.m}{script }and \href{https://terpconnect.umd.edu/~toh/spectrum/LastPeakTwoGaussiansPlotpub.png}{graphic}). 

Peak area measurements are occasionally used also in spectroscopy, for example in \href{http://ww2.chemistry.gatech.edu/class/analyt/fia.pdf}{flow injection} methods and in graphite furnace atomic absorption (\href{https://pubs.acs.org/doi/abs/10.1021/ac60358a039}{reference 87}). In that application, calibration curves based on area measurements are more linear than peak height measurements because most of the area of a peak is measured when the transient absorbance is less than maximum and where \href{https://terpconnect.umd.edu/~toh/models/BeersLaw.html}{Beer’s Law} is more strictly obeyed. 

On the other hand, peak height measurements are \textit{simpler to make} and are \textit{less prone to interference} by neighboring, overlapping peaks. And a further disadvantage of peak area measurement is that the peak start and stop points must be determined, which may be difficult especially if the multiple peaks overlap. In principle, \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{curve fitting} can measure the areas of peaks even then they overlap, but that requires that the shapes of the peaks be known at least approximately (however, see \href{https://terpconnect.umd.edu/~toh/spectrum/PeakShapeAnalyticalCurve.m}{PeakShapeAnalyticalCurve.m} described on page \pageref{ref-0403}). 

Chromatographic peaks are often described as a \href{https://terpconnect.umd.edu/~toh/models/DiscreteEquilib.html}{Gaussian function} or as a \href{https://terpconnect.umd.edu/~toh/spectrum/Convolution.html}{\textit{convolution} }of a Gaussian with an exponential function. A detailed quantitative comparison of peak height and peak area measurement is given in on page \pageref{ref-0371}\href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#HeightWidth}{: Why measure peak area rather than peak height?} (In spectroscopy, there are \href{http://www.phy.ohiou.edu/~mboett/astro401\_fall12/broadening.pdf}{other broadening mechanisms}, such as \href{https://en.wikipedia.org/wiki/Doppler\_broadening}{Doppler broadening} caused by thermal motion, which results in a \href{https://en.wikipedia.org/wiki/Gaussian\_function}{Gaussian broadening function}). 

Before computers, researchers used a variety of clever but archaic methods to compute peak areas: \label{ref-0183}

(\textbf{a}) plot the signal on a gridded paper chart, cut out the peak with scissors, then weigh the cutout piece on a micro-balance compared to a square section of a known area; 

(\textbf{b}) count the grid squares under a curve recorded on gridded graph paper

\textbf{(c)} use a mechanical \href{https://en.wikipedia.org/wiki/Ball-and-disk\_integrator}{ball-and-disk integrator}, 

(\textbf{d}) use a straight-edge and geometry to compute the area under a \href{https://terpconnect.umd.edu/~toh/spectrum/triangulation.png}{triangle constructed with its sides tangent to the sides of the peak}, or 

(\textbf{e}) compute the cumulative sum of the signal magnitude and measure the heights of the resulting steps (see figure below). This is a commonly used method in proton NMR spectroscopy, where the area under each peak or group of peaks is proportional to the number of equivalent hydrogen atoms responsible for that peak.

\InsImageInline{0.5}{l}{SignalIntegration.png}

Now that computing power is built into or connected to almost every measuring instrument, more accurate and convenient digital methods can be employed. No matter how it is measured, the \textit{units} of peak area are the \textit{product} of the x and y units. Thus, in a chromato-gram where the x is time in minutes and y is volts, the area is in volts-minute. In absorption spectra\index{absorption spectrum} where the x is nm (nanometers) and y is absorbance, the area is absorbance-nm. Because of this, the numerical magnitude of peak area will always be different from that of the peak height. If you are performing a quantitative analysis of unknown samples by means of a \href{https://terpconnect.umd.edu/~toh/models/CalibrationCurve.html}{calibration curve}, you must use the same method of measurement for both the standards and the samples, \textit{even if the measurements are inaccurate}, as long as the \textit{error is the same} for all standards and samples (which is why an approximate method like triangle construction works better than expected). 

The best method for calculating the area under a peak depends on whether the peak is isolated or overlapped with other peaks or superimposed on a non-zero baseline or not. For an isolated peak, Yuri Kalambet (reference 72) has shown that the \href{https://en.wikipedia.org/wiki/Trapezoidal\_rule}{trapezoidal rule} area is an efficient estimate of the full peak area with extraordinary low error, even if there are only a few data points across the width of the peak, whereas \href{https://en.wikipedia.org/wiki/Simpson\%27s\_rule}{Simpson's rule} is less efficient in full area integration. For a Gaussian peak, the trapezoidal rule requires only 0.62 points per standard deviation (2.5 points within the 4*sigma basewidth) to achieve an integration error of only 0.1\%. A \href{https://terpconnect.umd.edu/~toh/spectrum/IntegrationTest.m}{digital simulation} supports this result. For asymmetrical peaks, however, more data points are required. 

\section{Dealing with overlapping peaks\label{ref-0184}\label{ref-0185}}

The classical way to handle the overlapping peak problem is to draw two vertical lines from the left and right bounds of the peak down to the x-axis and then to measure the total area bounded by the signal curve, the x-axis (y=0 line), and the two vertical lines, shown the shaded area in the figure on the left. 


\begin{center}
\InsImageInline{0.5}{l}{Integration2.gif.png}\InsImageInline{0.5}{l}{Integration.gif.png}
\end{center}



\begin{center}
\textit{Peak area measurement for overlapping peaks, using the} \textbf{\textit{perpendicular drop method}} \textit{(left, shaded area)} \textit{and} \textbf{\textit{tangent skim method}} \textit{(right, shaded area).}
\end{center}


This is often called the \textbf{\textit{perpendicular drop}} method; it is an easy task for a computer, although tedious to do by hand. The left and right bounds of the peak are either taken as (a) the valleys (minima) between the peaks or (b) as the point half-way between the adjacent peak centers. The basic assumption is that the area missed by cutting off the feet of one peak is made up for by including the feet of the adjacent peak. This works well only of the peaks are symmetrical, not too overlapped, and not too different in height. In addition, the baseline must be zero; any extraneous background signal must be subtracted before measurement. Using this method, it is possible to estimate the area of the second peak in the example below to an accuracy of about 0.3\%. The last two peaks, however, give errors greater than 4\%, and that is only because the two peaks in this example have the same height and width; more generally, the error us much more if two peaks are this much overlapped.  As a rough rule, the valley between the peaks must be quite low, perhaps a quarter or a fifth of the adjacent peak height, for this method to be acceptable. Other geometrical methods exist that can reduce such errors in many cases. If there is no valley between the peaks you need to measure, you can apply peak sharpening (page \pageref{ref-0100}) to narrow the peaks before the perpendicular drop measurement; see \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningAreaMeasurementDemo.xlsm}{PeakSharpeningAreaMeasurementDemo.xlsm} (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningAreaMeasurementDemo.png}{screen image}). Moreover, \textit{asymmetrical} peaks that are the result of \href{https://pubs.acs.org/doi/abs/10.1021/ac50011a017}{exponential broadening} can be \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html\#Asymmetrical}{symmetrized by the weighted addition of its first derivative}, making the perpendicular drop areas \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningAreaMeasurementDemoEMG3.png}{more accurate} (page \pageref{ref-0189}). In both cases, it may be necessary to set the strength of sharpening higher than previously recommended, if it that is the only way to form a valley between peaks whose areas you want to measure. In the case where a single peak is superimposed on a straight or broadly curved baseline, you might use the \textbf{\textit{tangent skim method}}, which measures the area between the curve and a linear baseline drawn across the bottom of the peak (e.g., the \textit{shaded area} in the figure on the right, above). In general, the hardest part of the problem and the greatest source of uncertainty is determining the shape of the baseline under the peaks and determining when each peak begins and ends. Once those are determined, you subtract the baseline from each point between the start and endpoints, add them up, and multiply by the x-axis interval. Incidentally, smoothing a noisy signal does not change the areas under the peaks, but it may make the peak start and stop points easier to determine. The downside of smoothing is that increases peak width and the overlap between adjacent peaks. Numerical methods of peak sharpening, for example, \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html}{derivative sharpening} and \href{https://terpconnect.umd.edu/~toh/spectrum/Deconvolution.html}{Fourier deconvolution}, can help with the problem of peak overlap, and both of these techniques have the useful property that they do not change the total area under the peaks.

\textbf{Other methods}. Although the perpendicular drop method remains popular, there are other geometrical methods that can \InsImageInline{0.5}{l}{GeometricalHeightEqualization.png}work better in some cases. The "equalization" method, illustrated in the figure on the left, uses another method of locating the perpendicular drop point. A set of three straight-line segments is constructed that touches the estimated maxima of the two peaks, shown by the dotted red line labeled "cline" in the figure on the left. The quotient of the original signal, in blue, divided by this line, results in a temporarily normalized signal (the yellow line) that has more nearly equal peak heights. The effect of this treatment is to \textit{deepen the valley} between the peaks, so that it remains distinct for \href{https://terpconnect.umd.edu/~toh/spectrum/GeometricalHeightEqualization2.png}{lower values of the second peak height}. \textit{This is used only for the purpose of determining the separation point between the peaks}, shown as a vertical black line, and then is discarded. The perpendicular drop areas are then calculated on the \textit{original} observed signal (blue line). Note that this new separation point is not quite the same as the valley of the original signal, nor is it exactly the half-way point between the two peak positions. This need not be done by hand; software can do it easily, given only an initial \InsImageInline{0.5}{l}{SubtractionMethod.png}estimate for the two peak positions based on the observed signal.\label{ref-0186}

The "reflection/subtraction" method, shown on the right, is simpler, but it requires that the larger peak be symmetrical.  An estimate of the isolated larger peak is constructed by reflecting its left half and using it to replace the right half, resulting in the red dotted line in the figure. Then that estimated peak is simply subtracted from the entire signal to reveal the isolated second peak (dotted yellow line). The two areas are then separately calculated by the "trapz" function. This process is also easily automated, given only the peak position of the first peak. It works perfectly only if the larger peak is symmetrical and if the peak separation is sufficient so that the left-hand tail of the smaller peak does not significantly increase the height of the first peak. 

If the \textit{shape} of peaks is known, the best way to measure the areas of overlapping peaks is to use least-squares curve fitting, as is discussed starting on page \pageref{ref-0229}. If the peak positions, widths, and amplitudes are unknown, and only the fundamental peak shapes are known, then the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{iterative least-squares method} can be employed. In some cases, even the background can be \InsImageInline{0.5}{l}{image129.png}accounted for by curve fitting. 

\href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html\#Top}{}For gas chromatography and mass spectrometry specifically, \href{http://www.openchrom.net/main/content/index.php}{\textbf{Philip Wenig's OpenChrom}} is an \href{http://en.wikipedia.org/wiki/Open\_source}{\textit{open-source}} data system that can import binary and textual chromatographic data files directly. It includes methods to detect baselines and to measure peak areas in a chromatogram. Extensive documentation is available. It is available for Windows, Linux, Solaris and Mac OS X. A screenshot is shown on the left (click to enlarge). The author has regularly updated the program and its documentation.  Another freely-available open-source program for mass spectroscopy is "\href{https://skyline.ms/project/home/software/Skyline/begin.view}{Skyline}" from \href{https://skyline.ms/}{MacCoss Lab Softwar}e, which is specifically aimed at reaction monitoring. Tutorials and videos are available. There is also commercial software, such as \href{http://www.chromandspec.com/products/features/}{Ampersand’s Chrom\&Spec software} and \href{https://www.ssi.shimadzu.com/products/informatics/index.html}{Shimadzu’s LabSolutions}, which perform sophisticated factor analysis, peak deconvolution, etc.

\section{\textbf{Peak area measurement using spreadsheets.}\label{ref-0187}}

 \href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html}{EffectOfDx.xlsx} (\href{https://terpconnect.umd.edu/~toh/spectrum/EffectOFDx.png}{screen image}) demonstrates that the simple equation sum(y)*dx accurately measures the peak area of an isolated Gaussian peak if there are at least 4 or 5 points visibly above the baseline and as long as you include the points out to plus and minus at least 2 or 3 standard deviations of the Gaussian. It also shows that an exponentially broadened Gaussian needs to include more points on the tailing (right-hand, in this case) side to achieve the best accuracy. \href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html}{EffectOfNoiseAndBaseline.xlsx} (\href{https://terpconnect.umd.edu/~toh/spectrum/EffectOfNoiseAndBaseline.png}{screen image}) demonstrates the effect of random noise and non-zero baseline, showing that the area is more sensitive to a non-zero baseline than the same amount of random noise. \href{https://terpconnect.umd.edu/~toh/spectrum/CumulativeSum.xls}{CumulativeSum.xls} (\href{https://terpconnect.umd.edu/~toh/spectrum/CumulativeSum.png}{screen image}) illustrates the integration of a peak-type signal by normalized cumulative sum; you can paste your own data into columns A and B. \href{https://terpconnect.umd.edu/~toh/spectrum/CumulativeSumExample.xls}{CumulativeSumExample.xls} is an example with data.

The \textbf{Excel} and \textbf{Calc} spreadsheets \href{https://terpconnect.umd.edu/~toh/spectrum/https:/terpconnect.umd.edu/~toh/spectrum/PeakDetectionAndMeasurement.xlsx}{PeakDetectionAndMeasurement} and CurveFitter can measure the areas under partly overlapping Gaussian peaks in time-series data, using the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{findpeaks algorithm} and \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Fitting\_peaks}{iterative non-linear curve fitting} techniques, respectively. But neither is as versatile as using a dedicated chromatography program such as \href{http://www.openchrom.net/main/content/index.php}{OpenChrom}.

\subsection{Using sharpening for overlapping peak area measurements. \label{ref-0188}\label{ref-0189}\label{ref-0190}}

I have created a set of downloadable spreadsheets for perpendicular drop area measurements of overlapping peaks using \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html}{2\textsuperscript{nd} and 4\textsuperscript{th} derivative sharpening}. Sharpening the peaks reduces the degree of overlap and can greatly reduce the peak area measurement errors made by the perpendicular drop method. There is an empty template for you to paste your data into (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningAreaMeasurementTemplate.xlsm}{PeakSharpeningAreaMeasurementTemplate.xlsm}), an example version with some typical sample data and settings already entered (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningAreaMeasurementExample.xlsm}{PeakSharpeningArea-MeasurementExample.xlsm}), and a "demo" that creates and measures \textit{simulated data with known areas} (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningAreaMeasurementDemo.xlsm}{PeakSharpeningAreaMeasurementDemo.xlsm}) so you can see how sharpening effects area measurement accuracy. There are very brief instructions in row 2 of each of these. In addition, there are \textit{mouse-over pop-up notes} on many of the cells (indicated by a red marker in the upper right corner of the cell). All three have clickable ActiveX buttons for convenient interactive adjustment of the K2 and K4 factors by 1\% or by 10\% for each click. Of course, the problem is knowing what values of the 2\textsuperscript{nd} and 4\textsuperscript{th} derivative weighting factors (K1 and K2) to use. Those values depend on the peak separation, peak width, and the relative peak height of the two peaks, and you must determine them experimentally based on your preferred trade-off between extent of sharpening and extent of baseline upset. A good place to start for Gaussian peaks is (sigma\textasciicircum{}2)/30 for the 2\textsuperscript{nd} derivative factor and (sigma\textasciicircum{}4)/200 for the 4\textsuperscript{th} derivative factor, where sigma is the standard deviation of the Gaussian, then adjust to give the narrowest peaks without significant negative dips. Do not assume that increasing the K's until baseline resolution is achieved will always give the best area accuracy. The optimum values depend on the ratio of peak heights: at 1:1, with equal widths and shapes, the perpendicular drop method (page \pageref{ref-0184}) works perfectly with no sharpening, but if there is inequality in shapes, heights, or widths, increased K values give lower errors \textit{up to a point} but overdoing the sharpening can sacrifice accuracy. The two-screen images \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningAreaMeasurementDemo1.png}{screen1} and \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningAreaMeasurementDemo2.png}{screen2}, which use the \textit{same} K values, show that it is possible to find K values that give excellent accuracy for peak 2 over a range of relative peak heights, even when the smaller peak is quite small. \textit{Without} sharpening, accurate perpendicular drop area measurements are impossible because there is no valley between the peaks.

The template \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSymmetricalizationTemplate.xlsm}{PeakSymmetrizationTemplate.xlsm} (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSymmetricalizationExample.png}{graphic}) performs the symmetrization of exponentially broadened peaks by the weighted addition of the first derivative. See page \pageref{ref-0103}. \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSymmetricalizationTemplate.xlsm}{PeakSymmetrizationExample.xlsm} is an example application with sample data already typed in. The procedure here is first to adjust k1 to get the most symmetrical peak shapes (judged by equal but opposite slopes on the leading and trailing edges), then enter the start time, valley time, and end time from the graph for the pair of peaks you want to measure into cells B4, B5, and B6, and finally (optionally) adjust the second derivative sharpening factor k2. The perpendicular drop areas of those two peaks are reported in the table in columns F and G. These spreadsheets have Active-X clickable buttons to adjust the first derivative weighting factor (k1) in cell J4 and the second derivative sharpening factor k2 (cell J5). There is also a demo version that allows you to determine the accuracy of perpendicular drop peak areas under different conditions by internally generating overlapping peaks of known peak areas, with specified asymmetry (B6), relative peak height (B3), width (B4), and noise (B5): \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSymmetricalizationDemo.xlsm}{PeakSymmetrizationDemo.xlsm} (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningAreaMeasurementDemoEMG3.png}{graphic}). 

\section{\textbf{Peak area measurement using Matlab and} \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{\textbf{Octave}}\label{ref-0191}}

\textbf{Matlab and} \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{\textbf{Octave}} have built-in commands for the sum of elements (``sum'', and the cumulative sum ``cumsum'') and the trapezoidal numerical integration (``trapz''). For example, consider these three Matlab commands.

\texttt{{\textgreater}{\textgreater} x=-5:.1:5;} 

\texttt{{\textgreater}{\textgreater} y=exp(-(x).\textasciicircum{}2);}

\texttt{{\textgreater}{\textgreater} trapz(x,y)} 

These lines accurately compute the numerical value of the area under the curve of x,y, in this case an isolated Gaussian, whose area can be shown to be the square root of pi, which is equal to 1.7725:

 $\int _{-\infty }^{\infty }e^{-{x^{2}}}dx=\left[\int _{-\infty }^{\infty }e^{-{x^{2}}}dx\int _{-\infty }^{\infty }e^{-{y^{2}}}dy\right]^{1/2}=\left[\int _{0}^{2\pi }\int _{0}^{\infty }e^{-{r^{2}}}\textit{r\DifferentialD r\DifferentialD }\theta \right]^{1/2}=\left[\pi \int _{0}^{\infty }e^{-u}du\right]^{1/2}=\sqrt{\pi }$

If the interval between x values, dx, is \textit{constant}, then the area is simply \texttt{yi=sum(y).*dx}. Alternatively, the signal can be integrated using \texttt{yi=cumsum(y).*dx}, then the area of the peak will be equal to the \href{https://terpconnect.umd.edu/~toh/spectrum/iSignalArea1.png}{height of the resulting step}, \texttt{max(yi)-min(yi)=1.7725}. 

The area of a peak is proportional to the product of its height and its width, but the proportionality constant depends on the peak shape. A pure Gaussian peak with a peak height \textit{h} and \href{https://en.wikipedia.org/wiki/Full\_width\_at\_half\_maximum}{full-width at half-maximum} \textit{w} has a total area of 1.064467*\textit{h}*\textit{w}. A pure Lorentzian peak has a total area of (pi/2)*\textit{h}*\textit{w}. A Gaussian-Lorentzian blend with \textit{p} percent Gaussian character has an area of ((100- \textit{p})/100) * ((pi/2) *\textit{w}*\textit{h}) + (\textit{p} /100)*(1.064467*\textit{w}*\textit{h}). The graphic \href{https://terpconnect.umd.edu/~toh/spectrum/LorentzianVsGaussian.png}{LorentzianVsGaussian.png} compares Gaussian and Lorentzian peaks of the same height and width. The Lorentzian has more area in the outer wings, so if you measure the area of an unknown peak using trapz, you must measure over a very wide range on both sides of the peak. To get an area within 1\%, you need to expand that to 64 times the FWHM! (See \href{https://terpconnect.umd.edu/~toh/spectrum/LorentzianAreaProblem.m}{LorentzianAreaProblem.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/LorentzianAreaProblem.png}{graphic}). Many real signals in practice have too many peaks that are too close together to allow the theoretical areas to be measured directly by integration. If you really need an accurate area and the available measurement span is insufficient, it may be more accurate to measure the height and width and then calculate the area analytically using the above analytical expressions. 

 The peaks in real signals have some other complications: (a) The shapes of the peaks might not be known; (b) they may be superimposed on a baseline; and (c) they may be overlapped with other peaks.

These must be considered to measure accurate areas in experimental signals. Various Matlab/Octave functions have been developed to deal with these complications. 

\textbf{Perpendicular drop}. The following Matlab/Octave code measures the areas of two overlapping symmetrical peaks in the data vectors x,y by the perpendicular drop method. Variables m1 and m2 are the estimated positions of the two peaks, which are typically determined by some peak finding algorithm based on the first derivative. The "\href{https://terpconnect.umd.edu/~toh/spectrum/val2ind.m}{val2ind}" function returns the index number of the value in a vector that value matches the specified value. The third line finds the half-way point between the two peaks. The last two lines use the trapz function to measure the areas before and after the valley point.\label{ref-0192}

\texttt{index1=val2ind(x,m1);}

\texttt{index2=val2ind(x,m2);}

\texttt{valleyindex}\texttt{=val2ind(x,(m1+m2)/2)},

\texttt{PDMeasArea1=trapz(x(1:valleyindex),y(1:valleyindex));}

\texttt{PDMeasArea2=trapz(x(valleyindex:length(x)),y(valleyindex:length(x));}

Alternatively, you could replace ``valleyindex'' with \texttt{valleyy=min(y(index1:index2)); valleyindex=val2ind(y,valleyy);} which uses the \textit{minimum} between the peaks rather than the half-way point. But the half-way point method has the advantage that it works even when the two peaks are so close together or so different in height, or are so noisy, that there is not a discernable minimum between them. My function \href{https://terpconnect.umd.edu/~toh/spectrum/PerpDropAreas.m}{PerpDropAreas.m} uses the half-way point method to measure the areas of any number of overlapping peaks, given a list of their peak maxima positions. These methods work best if the peak \textit{widths} are equal or nearly so. Alternative methods incude \href{https://terpconnect.umd.edu/~toh/spectrum/EqualPerpDrop.m}{EqualPerpDrop.m}, which performs area measurements by the ``equalization'' method (page \pageref{ref-0186}), and \href{https://terpconnect.umd.edu/~toh/spectrum/EqualPerpDropTest.m}{EqualPerpDropTest.m}, which demonstrates the use of the function applied to the measurement of two simulated overlapping EMG (exponentially modified Gaussian) peaks. Matlab/Octave code for all these methods is contained in the script "\href{https://terpconnect.umd.edu/~toh/spectrum/OverlapAreaComparison.m}{OverlapAreaComparison.m}". For the case of Gaussian peak with a resolution of 0.7 and a height ratio of 1 to 0.5, the relative percent errors of the peak areas are:

  \texttt{\textbf{Peak 1 Peak 2}}

\texttt{Perpendicular drop, valley point: -6.44\% 12.89\%}

\texttt{Perpendicular drop, half-way point: 3.91\% -7.83\%}

\texttt{\textbf{Equalization method: 1.27\% -2.54\%}}

\texttt{Subtraction method: -2.12\% 4.25\%}

You can change the parameters in lines 5 through 10 to test with other peak separations and relative peak heights. The equalization method is often, but not always, the most accurate method. (Note: the script requires my \href{https://terpconnect.umd.edu/~toh/spectrum/val2ind.m}{val2ind.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/halfwidth.m}{halfwidth.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/ExpBroaden.m}{ExpBroaden.m}, and \href{https://terpconnect.umd.edu/~toh/spectrum/plotit.m}{plotit.m} functions to be in the path).

\InsImageInline{0.5}{l}{EffectOfResolutionOnOverlappingPeakAreaMeasurement.png}A more thorough investigation of these methods demonstrates the effect of changing the \href{https://terpconnect.umd.edu/~toh/spectrum/EffectOfPeakHeightR09T2N01t.png}{}peak resolution, shown on the left (\href{https://terpconnect.umd.edu/~toh/spectrum/EffectOfPeakPesolutionOnOverlappingPeakAreaMeasurement1.m}{script}, \href{https://terpconnect.umd.edu/~toh/spectrum/EffectOfResolutionOnOverlappingPeakAreaMeasurement.png}{graphic}) and of changing the height of the smaller peak, shown on the right (\href{https://terpconnect.umd.edu/~toh/spectrum/EffectOfPeakHeightOnOverlappingPeakAreaMeasurement.m}{script,} \href{https://terpconnect.umd.edu/~toh/spectrum/EffectOfPeakHeightR09T2N01t.png}{graphic}). These scripts include the effect of \textit{random noise} in the signal, because noise can influence the location of peak maxima and the separation point between the peaks, whether they are determined manually or by a computer algorithm (as it is here); the random noise is set by the variable "noise", which is the fractional random white noise added to the signal. In addition, these scripts include the effect of \textit{asym}\InsImageInline{0.5}{l}{EffectOfPeakHeightR09T2N01.png}\textit{metry} of the peak shapes, which can cause errors in area measurement by all these methods. After all, the very reason for measuring peak area rather than peak heights is to \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#HeightWidth}{reduce the effect of uncontrolled variations in peak broadening}. The asymmetry is set by the variable "TimeConstant", which is the time constant of the exponential convolution applied to the signal that reduces the height and stretches out the right-hand half. Both of those are zero in the above figures for simplicity and to show the best possible accuracy. For example, with a resolution of 1.0, a tau of 2, and noise set to 0.01 (1\%), the valley perpendicular drop and the equalization method outperform the other methods (\href{https://terpconnect.umd.edu/~toh/spectrum/EffectOfPeakHeightR10T2N01t.png}{graphic}).

Things are much easier and more forgiving in \textit{quantitative} analysis using a \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#Calibration}{calibration curve}, because in that case absolute area accuracy is not really necessary. Rather, it is really the \textit{reproducibility} of the areas that is key. \textit{Systematic} errors in the area measurement simply change the \textit{slope} of the calibration curve, and if the conditions are the same between calibration and analysis (always a requirement in any case), the error will cancel out exactly. For example, if you run the above scripts with very asymmetrical peaks (TimeConstant=3), poor resolution (resolution =0.68), and visible amounts of random noise=5\%, the \textit{systematic} area measurement \href{https://terpconnect.umd.edu/~toh/spectrum/EffectOfPeakHeightR068T3N05tx.png}{errors are quite large} (5\%-15\%), but nevertheless good linear calibration curves are produced by both the \href{https://terpconnect.umd.edu/~toh/spectrum/HPDR07T3N05t.png}{halfway point perpendicular drop} and the \href{https://terpconnect.umd.edu/~toh/spectrum/EPDR07T3N05t.png}{equalization method}, over the range of relative peak heights of 0.1 to 0.99, with correlation coefficients of 0.999. The calibration curve compensates for the systematic error and the area measurement reduces the effect of noise because it integrates over the width of the peak.

All of these methods can produce significant errors if the peaks are highly overlapped or asymmetrical. However, asymmetry that is the result of \textit{exponential broadening} can be symmetrized \textit{before} computing the areas using the \href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html\#broadening}{first derivative addition method}, which sharpens the peaks and removes the asymmetry \textit{without changing the peak areas}. Other methods of peak sharpening, especially self-deconvolution (page \pageref{ref-0156}), could also be used when the peak to be measured is too weak or too poorly resolved to allow easy measurement. Ultimately, in the most difficult cases, you may have to consider the use of \href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html\#curvefitting}{iterative curve fitting}, though it is admittedly more complex mathematically and is subject to its own limitations. 

\textbf{Automatic multiple peak detection}

\InsImageInline{0.5}{l}{HeightAndAreaTest.png}\href{https://terpconnect.umd.edu/~toh/spectrum/measurepeaks.m}{\textbf{Measurepeaks.m}} (The syntax is \texttt{M=measurepeaks (x,y, SlopeThreshold, AmpThreshold, SmoothWidth, FitWidth, plots)}) is a function that \textit{quickly and automatically} detects peaks in a s\href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest.png}{}ignal, using the derivative zero-crossing method \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html\#PeakDetection}{described previously}, and measures their areas using the perpendicular drop and tangent skim methods. It shares the first 6 input arguments with \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaks}{findpeaksG}. It returns a \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest.txt}{table} containing the peak number, peak\href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest2.png}{} position, absolute peak height, peak-valley difference, perpendicular drop area, \textit{and} the tangent skim area of each peak it detects. If the last input argument ('plots') is set to 1, it \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest.png}{plots the signal} with numbered peaks (shown on the left) and also \textit{plots the} \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest2.png}{\textit{individual peaks}} (in blue) with the maximum (red circles), valley points (magenta), and tangent lines (cyan) marked as shown on the right, below. The peak height and x-position are indicated by the red circles, the perpendicular drop area is the total area measured between the two magenta vertical lines down to zero, and the tangent skim area is the area between the cyan baseline and the blue peak (which compensates for a linear local baseline). Type ``\href{https://terpconnect.umd.edu/~toh/spectrum/measurepeaks.txt}{help measurepeaks}'' and try the seven examples there, or run \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndArea.m}{HeightAndArea.m} to test the \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaErrors.txt}{accuracy of peak height and area measurement} with signals that have multiple peaks with noise, background, and some peak overlap. Generally, the values for absolute peak height and perpendicular drop area are best for peaks that have no background, even if they are slightly overlapped, whereas the values for peak-valley difference and for tangential skim area are better for isolated peaks on a straight or slightly curved background. Note: this function uses \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothing} (specified by the \InsImageInline{0.5}{l}{HeightAndAreaTest2.png}SmoothWidth input argument) only for peak \textit{detection}; it performs measurements on the \textit{raw unsmoothed} y data. If the raw data are noisy, the location of the valleys may be uncertain, in which case it may be beneficial to smooth the y data yourself before calling measurepeaks.m, using any smooth function of your choice. (Smoothing does not change the peak area of an isolated peak).

\href{https://terpconnect.umd.edu/~toh/spectrum/autopeaks.m}{[M,A]=autopeaks.m} is basically a combination of autofindpeaks.m and measurepeaks.m. It has a similar syntax to measurepeaks.m, except that the peak detection parameters (SlopeThreshold, AmpThreshold, smoothwidth, peakgroup, and smoothtype) can be omitted and the function will calculate trial values in the manner of \href{https://terpconnect.umd.edu/~toh/spectrum/autofindpeaks.m}{autofindpeaks.m}. Using the simple syntax [M,A]=autopeaks(x, y) works well in some cases, but if not try [M,A]=autopeaks(x, y, \textit{n}), using different values of \textit{n} (roughly the number of peaks that would fit into the signal record) until it detects the peaks that you want to measure. Just like measurepeaks.m, it returns a \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest.txt}{table} M containing the peak number, peak position, absolute peak height, peak-valley difference, perpendicular drop area, and tangent skim area of each peak it detects, but is also can optionally return a vector A containing the peak detection parameters that it calculates (for use by other peak detection and fitting functions). For the most precise control over peak detection, you can specify all the peak detection parameters by typing M=autopeaks (x, y, SlopeThreshold, AmpThreshold, smoothwidth, peakgroup). The function \href{https://terpconnect.umd.edu/~toh/spectrum/autopeaksplot.m}{autopeaksplot.m} is the same but it also plots the signal and the individual peaks in the manner of measurepeaks.m (shown above). The script \href{https://terpconnect.umd.edu/~toh/spectrum/testautopeaks.m}{testautopeaks.m} runs all the examples in the autopeaks help file, with a 1-second pause between each one, printing out results in the command window and additionally plotting and numbering the peaks (Figure window 1) and each individual peak (Figure window 2); it requires \href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/fastsmooth.m}{fastsmooth.m} in the path. Autopeaks.m and autopeaksplot.m returns a matrix \textbf{M} that lists each peak detected in the rows and has the following peak measurements in the columns:

\texttt{\textbf{Peak Position PeakMax Peak-valley Perp drop Tan skim}}

\texttt{1 6.0000 1.3112 1.2987 1.7541 1.7121}

\texttt{2 . . . etc.}

For determining the effect of smoothing, peak sharpening, deconvolution, or other signal enhancement methods on the areas of overlapping peaks measured by the perpendicular drop method, the Matlab/ Octave function \href{https://terpconnect.umd.edu/~toh/spectrum/ComparePDAreas.m}{ComparePDAreas.m} uses \href{https://terpconnect.umd.edu/~toh/spectrum/autopeaks.m}{autopeaks.m} to measure the peak areas of original and processed signals, "orig" and "processed", and displays a scatter plot of original vs processed areas for each peak and returns the peak tables, P1 and P2 respectively, and the slope, intercept, and R\textsuperscript{2 }values, which should ideally be 1,0, and 1, if the processing has had no effect at all on peak area. 

The related functions \href{https://terpconnect.umd.edu/~toh/spectrum/wmeasurepeaks.m}{wmeasurepeaks.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/testwmeasurepeaks.m}{testwmeasurepeaks.m} utilize \textit{wavelet denoising} (page \pageref{ref-0178}) rather than smoothing, but that makes little difference, because the peak parameter measurements are based on least-squares fitting to the \textit{raw data}, not the \textit{smoothed} data, so the usual wavelet denoising advantage of avoiding smoothing distortion does not apply here.

The Matlab/Octave automatic peak-finding function \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaks}{findpeaksG.m} computes peak area assuming that the peak shape is Gaussian (or Lorentzian, for the variant \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksL.m}{findpeaksL.m}). The related function \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#triangle}{findpeaksT.m} uses the \textit{triangle construction method} to compute the peak parameters. Even for well-separated Gaussian peaks, the area measurements by the triangle construction method are not very accurate; the results are about 3\% below the correct values. (However, this method does perform better than findpeaksG.m when the peaks are noticeably asymmetric; see \href{https://terpconnect.umd.edu/~toh/spectrum/triangulationdemo.m}{triangulationdemo }for some \href{https://terpconnect.umd.edu/~toh/spectrum/TriangulationComparison.png}{examples}). In contrast, \href{https://terpconnect.umd.edu/~toh/spectrum/measurepeaks.m}{measurepeaks.m} makes no assumptions about the shape of the peak. 

\textbf{Peak sharpening} (page \pageref{ref-0099}) can often help in the measurement of the areas of overlapping peaks, by creating (or deepening) the valleys between peaks that are needed by the perpendicular drop method. \InsImageInline{0.5}{l}{SharpenedOverlapDemo2.png}\href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedOverlapDemo.m}{SharpenedOverlapDemo.m} is a script that automatically determines the optimum degree of even-derivative sharpening that minimizes \href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedOverlapDemo2.png}{}the errors of measuring peak areas of \href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedOverlapDemo.png}{two overlapping Gaussians} by the perpendicular drop method using the \href{https://terpconnect.umd.edu/~toh/spectrum/autopeaks.m}{autopeaks.m} function. It does this by applying different degrees of sharpening and plotting the area errors (percent difference between the true and measured errors) vs the sharpening factor, as shown on the right. It also shows the height of the valley between the peaks (yellow line). This demonstrates that:

(1) the optimum sharpening factor depends upon the width and separation of the two peaks and on their height ratio, 

(2) the degree of sharpening is not overly critical, often exhibiting a broad optimum region, 

(3) the optimum for the two peaks is not necessarily the same, and 

(4) the optimum for area measurement usually does not occur at the point where the valley is zero. (To run this script you must have \href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/derivxy.m}{derivxy.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/autopeaks.m}{autopeaks.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/val2ind.m}{val2ind.m}, and \href{https://terpconnect.umd.edu/~toh/spectrum/halfwidth.m}{halfwidth.m} in the path. Download these from \href{https://terpconnect.umd.edu/\%7Etoh/spectrum/}{https://terpconnect.umd.edu/\textasciitilde{}toh/spectrum/}).

\href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedOverlapCalibrationCurve.m}{SharpenedOverlapCalibrationCurve.m} is a script that simulates the construction and use of calibration curves of three overlapping Gaussian peaks (the blue lines in the signal plots on the left). Even-derivative sharpening (the red line in the signal plots) is used to improve the resolution of the peaks to allow perpendicular drop area measurement\href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedOverlapDemo.png}{}. A straight line is fit to the calibration curve and the R\textsuperscript{2 }is calculated, to demonstrate (1) the linearity of the response, and (2) in independence of the overlapping adjacent peaks. You can change the following parameters:

\textbf{1.} The resolution, Rs, by changing the peak width \textit{w} in line 15. Initially w=2, which makes Rs=0.55.

\InsImageInline{0.5}{l}{SharpenedOverlapDemo.png}\textbf{2.} The peak height ratios, by changing the minimum and maximum peaks in lines 21 and 22. (Default is 0.2 and 1.0, a 1:5 ratio range). Naturally, if peak 2 is \textit{too} small there will not be a valley between peaks.

\textbf{3.} The number of standards in the calibration curves, in line 24. Larger numbers give more reliable results. 

\textbf{4.} The number of simulated samples, in line 25. Larger numbers give more reliable average errors.

\href{https://terpconnect.umd.edu/~toh/spectrum/SymmetrizedOverlapCalibrationCurve.m}{SymmetrizedOverlapCalibrationCurve.m} does the same thing for symmetrization of overlapping exponentially modified Gaussian peaks by first-derivative addition. The critical variable is "factor" in line 27, which for best results should match or slightly exceed "tau", the exponential time constant in line 19. You must have \href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/derivxy.m}{derivxy.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/autopeaks.m}{autopeaks.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/val2ind.m}{val2ind.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/halfwidth.m}{halfwidth.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/fastsmooth.m}{fastsmooth.m}, and \href{https://terpconnect.umd.edu/~toh/spectrum/plotit.m}{plotit.m} in the path. 

\textbf{\InsImageInline{0.5}{l}{iSignalAreaAnimation.gif.png}}\href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{\textbf{\textit{iSignal}}} (page \pageref{ref-0434}) is a downloadable interactive Matlab function that performs various signal processsing functions described in this tutorial, including measurement of peak area using Simpson\index{Simpson}'s Rule and the perpendicular drop method. Click to view or \textbf{right-click {\textgreater} Save link as...} \href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{here}, or you can download the \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal7.zip}{ZIP file} with sample data for testing. It is shown on the left applying the perpendicular drop method to a series of four peaks of equal area. (Look at the bottom panel to see how the measurement intervals, marked by the vertical dotted magenta lines, are positioned at the valley \textit{minimum} on either side of each of the four peaks). You can see this animation if you read this within \textit{Microsoft Word 365}, otherwise click \href{https://terpconnect.umd.edu/~toh/spectrum/iSignalAreaAnimation.gif}{this link.}

The following lines of Matlab/Octave code creates four computer-synthesized Gaussian peaks that \textit{all have the same height} (1.000), \textit{width} (1.665), and \textit{area} (1.772) but with \textit{different degrees of peak overlap}, as in the figure on the right.

\texttt{x=[0:.01:18];}

\texttt{y=exp(-(x-4).\textasciicircum{}2) + exp(-(x-9).\textasciicircum{}2) + exp(-(x-12).\textasciicircum{}2) + exp(-(x-13.7).\textasciicircum{}2);}

\texttt{isignal(x,y);} 

To use \textbf{\textit{iSignal}} to measure the areas of each of these peaks by the perpendicular drop method, use the pan and zoom keys to position the two outer cursor lines (dotted magenta lines) in the valley on either side of the peak. The total of each peak area will be displayed below the upper window. 

\texttt{Peak \# Position Height Width Area}\index{Area}

  \texttt{1 4.00 1.00 1.661 1.7725}

  \texttt{2 9.001 1.0003 1.6673 1.77}

  \texttt{3 12.16 1.068 2.3 1.78}

  \texttt{4 13.55 1.0685 2.21 1.79}

 The area results are reasonably accurate in this example only because the perpendicular drop method roughly compensates for partial overlap between peaks, but only if the peaks are symmetrical, about equal in height, and have zero background. 

\textbf{\textit{iSignal}} includes an additional command (\textbf{J} key) that calls the \href{https://terpconnect.umd.edu/~toh/spectrum/autopeaksplot.m}{autopeaksplot }function, which automatically detects the peaks in the signal and measures their peak position, absolute peak height, peak-valley difference, perpendicular drop area, and tangent skim area. It asks you to type in the peak density (roughly the number of peaks that would fit into the signal record); the greater this number, the more sensitive it is to narrow peaks. It displays the measured peaks just as does the measurepeaks function described above. (To return to iSignal, press any cursor arrow key).

\subsection{Area\index{Area} measurement by iterative curve fitting\label{ref-0193}}

\texttt{\InsImageInline{0.5}{l}{PeakfitArea.png}}In general, the most flexible peak area measurements for overlapping peaks, assuming that the basic \textit{shape} of the peaks is known or can be guessed, are made with \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{iterative least-squares peak fitting}, for example using \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{\textbf{peakfit.m}}, shown below (for Matlab and \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{Octave}). This function can fit any number of overlapping peaks with model shapes selected from a list of different types. It uses the "trapz" function to calculate the area of each of the component model peaks. For example, using the \textbf{peakfit} function on the same data set as above, the results are much more accurate:

\texttt{{\textgreater}{\textgreater} peakfit([x;y],9,18,4,1,0,10,0,0,0)}

  \texttt{Peak \# Position Height Width Area}\index{Area}

  \texttt{1 4 1 1.6651 1.7725}

  \texttt{2 9 1 1.6651 1.7725}

  \texttt{3 12 1 1.6651 1.7725}

  \texttt{4 13.7 1 1.6651 1.7725}

The interactive function \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{\textbf{\textit{iPeak}}} (page \pageref{ref-0320}), can also be used to estimate peak areas. It has the advantage that it can detect and measure \textit{all the peaks in a signal in one operation.} The default area measurement method is by Gaussian estimation, by assuming that the peaks are Gaussian and fitting the top part of the peak. For example: 

\texttt{{\textgreater}{\textgreater} ipeak([x,y],10)}

  \texttt{Peak \# Position Height Width Area}\index{Area}

  \texttt{1 4 1 1.6651 1.7727}

  \texttt{2 9.0005 1.0001 1.6674 1.7754}

  \texttt{3 12.16 1.0684 2.2546 2.5644}

  \texttt{4 13.54 1.0684 2.2521 2.5615}

Peaks 1 and 2 are measured accurately by \textbf{\textit{iPeak}}, but the peak widths and areas for peaks 3 and 4 are not accurate because of the peak overlap. Fortunately, \textbf{iPeak} has a built-in "peakfit" function (activated by the \textbf{N} key) that uses these peak position and width estimates as its first guesses, resulting in good accuracy for all four peaks.

\texttt{Fitting Error 0.0002165\%} 

  \texttt{Peak\# Position Height Width Area}\index{Area} 

  \texttt{1 4 1 1.6651 1.7724}

  \texttt{2 9 1 1.6651 1.7725}

  \texttt{3 12 1 1.6651 1.7725}

  \texttt{4 13.7 0.99999 1.6651 1.7724}

So in this artificially ideal situation, the results are in perfect agreement with expectations.

\subsection{Correction for background/baseline\label{ref-0194}}

The presence of a baseline or background signal, on which the peaks are superimposed, will greatly influence the measured peak area if not corrected or compensated. \textbf{\textit{iSignal}}, \textbf{\textit{iPeak}}, \href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html\#measurepeaks}{\textbf{\textit{measurepeaks}}}, and \textbf{\textit{peakfit}} all have several different baseline correction modes, for flat, tilted linear, and curved baselines, and \textit{iSignal} and \textit{iPeak} additionally have a multipoint piece-wise linear baseline subtraction function allows the manually estimated background to be subtracted from the entire signal. If the baseline is caused by the edges of a strong overlapping adjacent peak, then it is possible to include that peak in the curve-fitting operation, as see in Example 22 on page \pageref{ref-0451}. 

Here is a Matlab/Octave experiment that compares several \textit{different methods of baseline correction} in peak area measurement. The simulated signal consists of two noiseless, slightly overlapping Gaussian peaks with theoretical peak heights of 2.00 and 1.00 and areas of 191.63 and 95.81 units, respectively. The baseline is tilted and linear, and slightly greater in magnitude than the peak heights themselves, but the most serious problem is that the \textit{signal never returns to the baseline} long enough to make it easy to distinguish the signal from the baseline. 

\texttt{{\textgreater}{\textgreater} x=400:1:800;y=2.*gaussian(x,500,90)+1.*gaussian(x,700,90)+2.*(x./400);} 

A straightforward application of iSignal, using baseline mode 1 and the perpendicular drop method, seriously underestimates both peak areas (168.6 and 81.78), because baseline mode 1 only works when the signal returns completely to the local baseline at the edges of the fitted range, which is not the case here.

\textbf{\InsImageInline{0.5}{l}{Autozero3.png}\InsImageInline{0.5}{l}{Autozero1.png}\InsImageInline{0.5}{l}{3peakLorentzian.png}\InsImageInline{0.5}{l}{3peakVariablelinear.png}}

An automated tangent skim measurement by \href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html\#measurepeaks}{measurepeaks} is not accurate in this case because the peaks do not go all the way down to the baseline at the edges of the signal and because of the slight overlap: 

\texttt{{\textgreater}{\textgreater} measurepeaks(x,y,.0001,.8,2,5,1)}

  \texttt{\textcolor{color-14}{Position PeakMax Peak-valley Perp drop Tan skim}}

\texttt{1 503.67 4.5091 1.895 672.29} \texttt{\textbf{171.44}}

\texttt{2 707.44 4.5184 0.8857 761.65} \texttt{\textbf{76.685}}

An attempt to use curve fitting with \textbf{peakfit.m} in the flat baseline correction mode 3 (\texttt{peakfit([x;y],0,0,2,1,0,1,0,3)}\texttt{,} above, top left-most figure) fails because the actual baseline is tilted, not flat. The linear baseline mode (\texttt{peakfit([x;y],0,0,2,1,0,1,0,1)},  top right figure) is not much better in this case (page \pageref{ref-0276}). A more accurate approach is set the baseline mode to \textit{zero} and to include a \textit{third} peak in the model to fit the baseline, for example with either a Lorentzian model \texttt{-} \texttt{peakfit([x;y],0,0,3,[1 1 2])}, bottom left figure - or with a "slope" model - shape 26 in peakfit version 6, bottom right figure. The latter method gives both the lowest fitting error (less than 0.01\%) and the most accurate peak areas (less than $\frac{1}{2}$\% error in peak area): 

\texttt{{\textgreater}{\textgreater} [FitResults,FitError]=peakfit([x;y],0,0,3,[1 1 26])}

\texttt{FitResults =} 

    \texttt{1 500 2.0001 90.005} \texttt{\textbf{190.77}} 

  \texttt{2 700 0.99999 89.998} \texttt{\textbf{95.373}}

  \texttt{3 5740.2 8.7115e-007 1 1200.1}

\texttt{FitError =0.0085798}

Note that in this last case the number of peaks is 3 and the shape argument is a vector [1 1 26] specifying two Gaussian components plus the "linear slope" shape 26. If the baseline seems to be non-linear, you might prefer to model it using a quadratic (shape 46; see example 38 on page \pageref{ref-0453}). If the baseline seems to be different on either side of the peak, try modeling the baseline with an S-shape (sigmoid), either an up-sigmoid, shape 10 \href{https://terpconnect.umd.edu/~toh/spectrum/UpSigmoidBaseline.png}{(click for graphic)}, \texttt{peakfit([x;y],0,0,2,[1 10],[0 0])}, or a down-sigmoid, shape 23 \href{https://terpconnect.umd.edu/~toh/spectrum/DownSigmoidBaseline.png}{(click for graphic),} \texttt{peakfit([x;y],0,0,2,[1 23],[0 0])}, in these examples leaving the peak modeled as a Gaussian.

\subsection{Asymmetrical peaks and peak broadening: perpendicular drop vs curve fitting\label{ref-0195}}

\InsImageInline{0.5}{l}{ExpBroadAreas.png}\href{https://terpconnect.umd.edu/~toh/spectrum/AsymmetricalAreaTest.m}{AsymmetricalAreaTest.m} is a Matlab/Octave script that compares the accuracy of peak area measurement methods for a single noisy asymmetrical peak measured by different methods: (A) Gaussian estimation, (B) triangulation, (C) perpendicular drop method, and curve fitting by (D) exponentially broadened Gaussian, and (E) two overlapping Gaussians. \href{https://terpconnect.umd.edu/~toh/spectrum/AsymmetricalAreaTest2.m}{AsymmetricalAreaTest2.m} is similar except that it compares the precision (standard deviation) of the areas. For a single peak with a baseline of zero, the perpendicular drop and curve fitting methods work equally well, both considerably better than Gaussian estimation or triangulation. The advantage of the curve fitting methods is that they can deal more accurately with peaks that overlap or that are superimposed on a baseline. 

Here's a Matlab/Octave experiment that creates a signal containing five Gaussian peaks with the \textit{same} initial peak \textit{height} (1.0) and \textit{width} (3.0) but which are subsequently broadened by \textit{increasing degrees of exponential broadening}, similar to the broadening of peaks commonly encountered in chromatography:

\texttt{{\textgreater}{\textgreater} x=5:.1:65;}

\texttt{{\textgreater}{\textgreater} y=modelpeaks2(x, [1 5 5 5 5], [1 1 1 1 1], [10 20 30 40 50], [3 3 3 3 3], [0 -5 -10 -15 -20]);}

\texttt{{\textgreater}{\textgreater} isignal(x,y)} ;

The theoretical area under these Gaussians is \textit{all the same}: 1.0645*Height*Width = 1*3*1.0645 = \textbf{3.1938}. A perfect area-measuring algorithm would return this number for all five peaks. 

As the broadening is increased from left to right, the peak height \textit{decreases} (by about 35\%) and peak width \textit{increases} (by about 32\%). Because the area under the peak is proportional to the \textit{product} of the peak height and the peak width, these two changes \textit{approximately cancel each other out} and the result is that the peak area is nearly independent of peak broadening (see the summary of results in \href{https://terpconnect.umd.edu/~toh/spectrum/5ExponentialBroadenedGaussianFit.xlsx}{5ExponentialBroadenedGaussianFit.xlsx}). The perpendicular drop method (page ~\ref{ref-0192}~\ref{ref-0192}\pageref{ref-0192}), PerpDropAreas (x, y, min(x), max(x), positions), where ``positions'' are the x-axis positions of the original peaks, gives the areas [3.1933 3.1926 3.1738 3.1006 3.3045], an average errors of 1.4\%, which is not quite perfect.

The Matlab/Octave peak-finding function \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaks}{\textbf{findpeaksG.m}}, finds all five peaks and measures their areas assuming a Gaussian shape; this works well for the unbroadened peak 1 (\href{https://terpconnect.umd.edu/~toh/spectrum/triangulationExp.m}{script}), but it underestimates the areas as the broadening increases in peaks 2-5: 

  \texttt{Peak Position Height Width Area}\index{Area}

  \texttt{1 10.0000 1.0000 3.0000 3.1938}

  \texttt{2 20.4112 0.9393 3.1819 3.1819}

  \texttt{3 30.7471 0.8359 3.4910 3.1066}

  \texttt{4 40.9924 0.7426 3.7786 2.9872}

  \texttt{5 51.1759 0.6657 4.0791 2.8910}

The {\hyperref[ref-0312]{triangle construction method}} (using \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#triangle}{\textbf{findpeaksT.m}}) underestimates even the area of the unbroadened peak 1 and is less accurate for the broadened peaks (\href{https://terpconnect.umd.edu/~toh/spectrum/triangulationExp.m}{script}; \href{https://terpconnect.umd.edu/~toh/spectrum/triangulationExp.png}{graphic}): 

\texttt{Peak Position Height Width Area}\index{Area}

  \texttt{1 10.0000 1.1615 2.6607 3.0905}

  \texttt{2 20.3889 1.0958 2.8108 3.0802}

  \texttt{3 30.6655 0.9676 3.1223 3.0210}

  \texttt{4 40.8463 0.8530 3.4438 2.9376}

  \texttt{5 50.9784 0.7563 3.8072 2.8795}

The automated function \href{https://terpconnect.umd.edu/~toh/spectrum/measurepeaks.m}{measurepeaks.m} gives better results using the perpendicular drop method (5th column of the table).

 \texttt{{\textgreater}{\textgreater} M=measurepeaks(x,y,0.0011074,0.10041,3,3,1)}

  Peak Position PeakMax Peak-val. \textbf{Perp drop}  Tan skim

  \texttt{1 10 1 .99047} \texttt{\textbf{3.1871}}   \texttt{3.1123}

  \texttt{2 20.4 .94018 .92897} \texttt{\textbf{3.1839}}   \texttt{3.0905} 

  \texttt{3 30.709 .83756 .81805} \texttt{\textbf{3.1597}}   \texttt{2.9794}

  \texttt{4 40.93 .74379 .70762} \texttt{\textbf{3.1188}}   \texttt{2.7634}

  \texttt{5 51.095 .66748 .61043} \texttt{\textbf{3.0835}}   \texttt{2.5151}

Using \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{\textbf{iSignal}} (page \pageref{ref-0434}) and the manual peak-by-peak perpendicular drop method yields areas of 3.193, 3.194, 3.187, 3.178, and 3.231, a mean of  3.1966 (pretty close to the theoretical value of 3.1938) and standard deviation of only 0.02 (0.63\%). Alternatively, integrating the signal, \texttt{cumsum(y).*dx)}, where dx is the difference between adjacent x-axis values (0.1 in this case), and then \href{https://terpconnect.umd.edu/~toh/spectrum/iSignalArea2.png}{measuring the heights of the resulting steps}, gives similar results: 3.19, 3.19, 3.18, 3.17, 3.23. By either method, the peak areas are not quite equal as they should be. 

But we can obtain a more accurate automated measurement of all five peaks, using \textbf{peakfit.m} with multiple shapes, one Gaussian and four exponentially modified Gaussians (shape 5) with different exponential factors (extra vector):

\texttt{{\textgreater}{\textgreater} [FitResults,FittingError]=peakfit([x;y],30,54,5,[1 5 5 5 5],[0 -5 -10 -15 -20],10, 0, 0)}

\texttt{FitResults =}

   \texttt{Peak\# Position Height Width Area}\index{Area}

  \texttt{1 9.9933 0.98051 3.1181 3.2541}

  \texttt{2 20.002 1.0316 2.8348 3.1128}

  \texttt{3 29.985 0.95265 3.233 3.2784}

  \texttt{4 40.022 0.9495 3.2186 3.2531}

  \texttt{5 49.979 0.83202 3.8244 3.2974}

\texttt{FittingError =} \texttt{2.184\%}

The fitting error is not much better than the simple Gaussian fit. Better results can be had using preliminary position and width results obtained from the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{findpeaks function} or by curve fitting with a simple Gaussian fit and using those results as the "start" vector (eight input argument):

\texttt{{\textgreater}{\textgreater} [FitResults,FittingError]=peakfit([x;y],30,54,5, [1 5 5 5 5], [0 -5 -10 -15 -20], 10, [10 3.5 20 3.5 31 3.5 41 3.5 51 3.5], 0)}

\texttt{FitResults =}

  \texttt{Peak\# Position Height Width Area}\index{Area}

  \texttt{1 9.9999 0.99995 3.0005 3.1936}

  \texttt{2 20 0.99998 3.001 3.1944}

  \texttt{3 30.001 1.0002 3.0006 3.1948}

  \texttt{4 40 0.99982 2.9996 3.1924}

  \texttt{5 49.999 1.0001 3.003 3.1243}

\texttt{FittingError =} \texttt{0.02\%}

Even more accurate results for area are obtained using peakfit with one Gaussian and four \textit{equal-width} exponentially modified Gaussians (shape 8):

\texttt{{\textgreater}{\textgreater} [FitResults,FittingError]=peakfit([x;y],30,54,5, [1 8 8 8 8], [0 -5 -10 -15 -20],10, [10 3.5 20 3.5 31 3.5 41 3.5 51 3.5],0)}

\texttt{FitResults =}

  \texttt{Peak\# Position Height Width Area}\index{Area}

  \texttt{1 10 1.0001 2.9995 3.1929}

  \texttt{2 20 0.99998 3.0005 3.1939}

  \texttt{3 30 0.99987 3.0008 3.1939}

  \texttt{4 40 0.99987 2.9997 3.1926}

  \texttt{5 50 1.0006 2.9978 3.1207}

\texttt{FittingError = 0.008}\texttt{\%}

The latter approach works because, although the \textit{broadened} peaks clearly have different widths (as shown in the simple Gaussian fit), the underlying pre-broadening peaks have all the \textit{same} width. In general, if you expect that the peaks should have equal widths or fixed widths, then it is better to use a \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Peak\_width\_constraints}{\textit{constrained model}} that fits that knowledge; you'll get better estimates of the measured unknown properties, even though the fitting error will be higher than for an unconstrained model. 

The \textit{disadvantages} of the exponentially broadened model are that:

(a) it may not be a perfect match to the actual physical broadening process; 

(b) it is slower than a simple Gaussian fit, and 

(c) it sometimes needs help, in the form of a start vector or equal-widths constraints, as seen above, to get the best results. 

Alternatively, if the objective is only to measure the peak \textit{areas}, and not the peak positions and widths, then it is not even necessary to model the physical peak-broadening of each peak. You can simply aim for a good fit using two (or more) closely spaced simple Gaussians for each peak and simply \textit{add up the areas} of the best-fit model. For example, the 5th peak in the above example (which is the most asymmetrical) can be fit very well with \href{https://terpconnect.umd.edu/~toh/spectrum/LastPeakTwoGaussians.png}{two overlapping Gaussians}, resulting in a total area of  1.9983 + 1.1948 = 3.1931, very close to the theoretical area of 3.1938. Even more overlapping Gaussians can be used if the peak shape is more complex. This is called the \href{https://en.wikipedia.org/wiki/Sum\_rule\_in\_integration}{"sum rule" in in}\href{https://en.wikipedia.org/wiki/Sum\_rule\_in\_integration}{tegral calculus}: the integral of a sum of two functions is equal to the sum of their integrals. As a demonstration, the script \href{https://terpconnect.umd.edu/~toh/spectrum/SumOfAreas.m}{SumOfAreas.m} shows that even drastically non-Gaussian peaks can be fitted with multiple Gaussian components and that the total area of the components approaches the area under the non-Gaussian peak as the number of components increases (\href{https://terpconnect.umd.edu/~toh/spectrum/SumOfAreas.png}{graphic}). When using this technique, it is best to set the number of trials (\textit{NumTrials}, the 7th input argument of the peakfit.m function) to 10 or more; additionally, if the peak of interest is on a baseline, you must add up the areas of only those peak that contribute to fitting the peak itself and \textit{not} those that are fitting the baseline.

\InsImageInline{0.5}{l}{triangulationExp2.png}An alternative to curve fitting with an exponentially broadened model is to use \href{https://terpconnect.umd.edu/~toh/spectrum/symmetrize.m}{symmetrize.m} or \href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{iSignal.m} version 7 on each peak to convert them into symmetrical peaks and then to fit them with an appropriate symmetrical model (in this case a Gaussian).

By making the peaks \textbf{closer together}, we can create a tougher and more realistic challenge. 



\texttt{{\textgreater}{\textgreater} y=modelpeaks2(x,[1 5 5 5 5],[1 1 1 1 1],[20 25 30 35 40],[3 3 3 3 3],[0 -5 -10 -15 -20]);}

In this case, the \href{https://terpconnect.umd.edu/~toh/spectrum/triangulationExp2.png}{triangle construction method} gives areas of 3.1294, 3.202 3.3958, 4.1563, and 4.4039, seriously overestimating the areas of the last two peaks, and measurepeaks.m using the perpendicular drop method (page \pageref{ref-0192}) gives areas of 3.233, 3.2108, 3.0884, 3.0647 3.3602, compared to the theoretical value of 3.1938, better but not perfect. The integration-step height method is almost useless because the steps are no longer clearly distinct. The peakfit function does better, again using the approximate result of \textbf{findpeaksG.m} as the 'start' value (8\textsuperscript{th} input argument) for peakfit.

\texttt{{\textgreater}{\textgreater}[FitResults,FittingError]=peakfit([x;y],30,54,5,[1 8 8 8 8],[0 -5 -10 -15 -20],10, [20 3.5 25 3.5 31 3.5 36 3.5 41 3.5],0)}

\texttt{FitResults =} 

         \texttt{\textcolor{color-15}{Peak\# Position Height Width Area}}\index{Area}

   \texttt{1 20 0.99999 3.0002 3.1935}

 \texttt{2 25 0.99988 3.0014 3.1945...}

  \texttt{3 30 1.0004 2.9971 3.1918}

  \texttt{4 35 0.9992 3.0043 3.1955}

  \texttt{5 40.001 1.0001 2.9981 3.1915}

\texttt{FittingError =} \texttt{0.01\%} 

Next, we make an \href{https://terpconnect.umd.edu/~toh/spectrum/iSignalAreaTougher.png}{\textit{even} }\href{https://terpconnect.umd.edu/~toh/spectrum/iSignalAreaTougher.png}{\textit{tougher} challenge} with different peak heights (1, 2, 3, 4 and 5, respectively) and a bit of \textit{added random noise}. The theoretical areas (Height*Width*1.0645) are 3.1938, 6.3876, 9.5814, 12.775, and 15.969. 

\texttt{{\textgreater}{\textgreater} y=modelpeaks2(x,[1 5 5 5 5],[1 2 3 4 5], [20 25 30 35 40], [3 3 3 3 3], [0 -5 -10 -15 -20])+.01*randn(size(x));}

\texttt{{\textgreater}{\textgreater} [FitResults,FittingError]=peakfit([x;y],30,54,5, [1 8 8 8 8], [0 -5 -10 -15 -20] ,20, [20 3.5 25 3.5 31 3.5 36 3.5 41 3.5],0)}

\texttt{FitResults =}

         \texttt{\textcolor{color-15}{Peak\# Position Height Width Area}}\index{Area}

  \texttt{1 19.999 1.0015 2.9978 3.1958}

  \texttt{2 25.001 1.9942 3.0165 6.4034}

  \texttt{3 30 3.0056 2.9851 9.5507}

  \texttt{4 34.997 3.9918 3.0076 12.78}

  \texttt{5 40.001 4.9965 3.0021 15.966}

\texttt{FittingError =} \texttt{0.2755}

The measured areas in this case (last column) are very close to the theoretical values, whereas all the other methods give substantially poorer accuracy. The more overlap between peaks, and the more \InsImageInline{0.5}{l}{5ebgwm.png}unequal are the peak heights, the poorer the accuracy of the perpendicular drop and triangle construction methods. If the peaks are so overlapped that separate maxima are not visible, both methods fail completely, whereas curve fitting can often retrieve a reasonable result, but \textit{only if you can provide approximate first-guess value}. 

Although curve fitting is generally the most powerful method for dealing with the combined effects of overlapping asymmetrical peaks superimposed on an irrelevant background, the simpler and computationally faster technique of \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html\#Assymetrical}{first derivative sharpening} (page \pageref{ref-0104}) can be useful as a method to reduce or eliminate the effects of exponential broadening, resulting in a simpler shape that is easier and faster to fit. As is the case with curve fitting, it is most effective is there is an isolated peak with the same \InsImageInline{0.5}{l}{image144.png}exponential broadening because that peak can be used to determine more easily the best value of the first derivative weighting factor. \href{https://terpconnect.umd.edu/~toh/spectrum/SymmetizedOverlapDemo.m}{SymmetizedOverlapDemo.m}, illustrated on the left, demonstrates the optimization of the first derivative symmetrization for the measurement of the areas of two overlapping exponentially broadened Gaussians. It plots and compares the original (blue) and sharpened peaks (red), then tries first-derivative weighting factors from +10\% to -10\% of the correct tau value in line 14 and plots absolute peak area errors vs factor values. You can change the resolution by changing either the peak positions in lines 17 and 18 or the peak width in line 13. Change the height in line 16. You must have derivxy.m, autopeaks.m, and halfwidth.m in the path. This method also easily deals with \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html\#DoubleExpSymm}{\textit{double} exponential broadening}, page \pageref{ref-0116}, which is not so easily handled by curve fitting alone.\label{ref-0196}\label{ref-0197}\label{ref-0198}\label{ref-0199}\label{ref-0200}

\chapter{Curve fitting A: Linear Least-squares\label{ref-0201}}

The objective of curve fitting is to find the parameters of a mathematical model that describes a set of (usually noisy) data in a way that minimizes the difference between the model and the data. The most common approach is the "linear least-squares" method, also called "polynomial least-squares", a well-known mathematical procedure for finding the coefficients of \href{http://en.wikipedia.org/wiki/Polynomial}{polynomial} equations that are a "best fit" to a set of X,Y data. A polynomial equation expresses the dependent variable Y as a weighted sum of a series of single-valued functions of the independent variable X, most commonly as a straight line (Y = \textbf{a} + \textbf{b}X, where \textbf{a} is the \textit{intercept} and \textbf{b} is the \textit{slope}), or a quadratic (Y = \textbf{a} + \textbf{b}X + \textbf{c}X\textsuperscript{2}), or a cubic (Y = \textbf{a} + \textbf{b}X + \textbf{c}X\textsuperscript{2} + \textbf{d}X\textsuperscript{3}), and so on to higher-order polynomials. Those coefficients (\textbf{a}, \textbf{b}, \textbf{c}, etc.) can be used to predict values of Y for each X. In all these cases, Y is a \textit{linear function} of the parameters \textbf{a}, \textbf{b}, \textbf{c}, and/or \textbf{d}\textit{.} \textbf{\textit{This}} \textit{is the reason we call it a "linear" least-squares fit,} \textbf{\textit{not}} \textit{because the plot of X vs Y is linear.} Only for the \textit{first}-order polynomial Y = \textbf{a} + \textbf{b}X is the plot of X vs Y linear. And if the model \textit{cannot} be described by a weighted sum of single-valued functions, then a different, more computationally laborious, "non-linear" least-squares method may be used, discussed on page \pageref{ref-0258}. 

``Best fit'' simply means that the differences between the actual measured Y values and the Y values predicted by the model equation are \textit{minimized}. It does \textit{not} mean a "perfect" fit; in most cases, a least-squares best fit \textit{does not go through all the points} in the data set. Above all, a least-squares fit \textit{must conform to the selected model} - for example, a straight line or a quadratic parabola - and there will almost always be some data points that do not fall exactly on the best-fit line, either because of random error in the data or because the model is not capable of describing the data exactly. 

Another thing: it is not correct to say "fit data to ..." a straight line or to some other model; it is the other way around: you are fitting a \textit{model} to the \textit{data}. The \textit{data} are not being modified in any way; it is the \textit{model} that is being adjusted to fit the data. 

Least-squares best fits can be calculated by some hand-held calculators, spreadsheets, and dedicated computer programs (see \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#MathDetails}{Math Details} below). Although it is possible to estimate the best-fit straight line by visual estimation and a straightedge, the least-square method is more objective and easier to automate. (If you were to give a plot of X, Y data to five different people and ask them to estimate the best-fit line visually, you would get five slightly different answers, but if you gave the data set to five different computer programs, you would get the exact same answer every time).

\section{Examples of polynomial fits\label{ref-0202}}

Here is a very simple example: the historical prices of different sizes of SD memory cards advertised in the February 19, 2012, issue of the New York Times. (Yes, I know, the prices are much lower now, but these really were the prices in a big-box store back in 2012).


\begin{table}
\begin{tabularx}{\textwidth}{|
p{\dimexpr 0.489\linewidth-2\tabcolsep-2\arrayrulewidth}|
p{\dimexpr 0.511\linewidth-2\tabcolsep-\arrayrulewidth}|} \hline 
\centering\arraybackslash{}Memory Capacity  \par in GBytes & \centering\arraybackslash{}Price in US dollars \\\hline 
\centering\arraybackslash{}2 & \centering\arraybackslash{}\$9.99 \\\hline 
\centering\arraybackslash{}4 & \centering\arraybackslash{}\$10.99 \\\hline 
\centering\arraybackslash{}8 & \centering\arraybackslash{}\$19.99 \\\hline 
\centering\arraybackslash{}16 & \centering\arraybackslash{}\$29.99 \\\hline 
\end{tabularx}
\end{table}
\InsImageInline{0.5}{l}{SDcards.gif.png}What is the relationship between memory capacity and cost? Naturally, we expect that the larger-capacity cards should cost more than the smaller-capacity ones, and if we plot cost vs capacity (graph on the left), we can see a rough straight-line relationship. A least-squares algorithm can compute the values of ``\textbf{a}'' (intercept) and ``\textbf{b}'' (slope) of the straight line that is a "best fit" to the data points. Using a linear least-squares calculation, where \textbf{X = capacity} and \textbf{Y = cost}, the straight-line mathematical equation that most simply describes these data (rounding to the nearest penny) is: 

\textbf{Cost = \$6.56 + Capacity * \$1.49}

So, \$1.49 is the \textit{slope} and \$6.56 is the \textit{intercept}. (The equation is plotted as the solid line that passes among the data points in the figure). Basically, this is saying that the cost of a memory card consists of a fixed cost of \$6.56 plus \$1.49 for each GBytes of capacity. How can we interpret this? The \$6.56 represents the costs that are the same regardless of the memory capacity: a reasonable guess is that it includes things like packaging (the different cards are the same physical size and are packaged the same way), shipping, marketing, advertising, and retail shop shelf space. The \$1.49 (1.49 dollars/Gbyte) represents the increasing retail price of the larger chips inside the larger capacity cards, mainly because they \textit{have more value for the consumer} but also probably cost more to make because they use more silicon, are more complex, and have a higher chip-testing rejection rate in the production line. So, in this case, the slope and intercept have real physical and economic meanings.

What can we do with this information?  First, we can see how closely the actual prices conform to this equation: approximately \textit{but not perfectly}. The line of the equation passes \textit{among} the data points but does not go exactly \textit{through} each one. That's because actual retail prices are also influenced by several factors that are unpredictable and random: local competition, supply, demand, and even rounding to the nearest "neat" number; all those factors constitute the "\href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html}{noise}" in these data. The least-squares procedure also calculates R\textsuperscript{2}, called the \textit{coefficient of determination} or the \textit{correlation coefficient}, which is an indicator of the "goodness of fit". R\textsuperscript{2} is exactly 1.0000 when the fit is perfect, less than that when the fit is imperfect. The closer to 1.0000 the better. An R\textsuperscript{2} value of 0.99 means a good fit; 0.999 is a very good fit. (The R\textsuperscript{2} value is calculated as shown on page \pageref{ref-0234}).

The second way we can use these data is to predict the likely prices of other card capacities, if they were available, by putting in the memory capacity into the equation and evaluating the cost. For example, a 12 Gbyte card would be expected to cost \$24.44 according to this model. And a 32 Gbyte card would be predicted to cost \$54.29, but \textit{that would be predicting beyond the range of the available data} - it is called ``extrapolation''\textit{- and it is very risky} because you do not really know what other factors may influence the data beyond the last data point. (You could also solve the equation for capacity as a function of cost and use it to predict how much capacity could be expected to be bought for a given amount of money if such a product were available).

As I said, there was the prices back in 2012. Why not do a little ``homework''? Look up and try fitting the \textit{current} prices and see how they compare. Did you get a lower slope, lower intercept, or both?

Here's another related example: the historical prices of standard high definition (not UHD) flat-screen LCD TVs as a function of screen size, as advertised on the Web in the spring of 2012. The prices of five selected models, \textit{similar except for screen size}, are plotted against the screen size in inches in the figure on the left and are fit to a first order (straight-line) model. As for the previous example, the fit is \InsImageInline{0.5}{l}{R2 9549.jpg}not perfect. The equation of the best-fit model is shown at the top of the graph, along with the R\textsuperscript{2 }value (0.9549) indicating that the fit is not very good. And you can see from the best-fit line that a 40-inch set would be predicted to have a \textit{negative cost}! That is crazy\textit{.} Would they \textit{pay} you to take these sets?  I do not think so. Clearly, something is wrong here. 

The goodness of fit is shown even more clearly in the little graph at the bottom of the figure, with the red dots. This shows the "residuals", the differences between each data point and the least-squares fit at that point. You can see that the deviations from zero are large (${\pm}$10\%), but more importantly, \textit{they are not completely random}; they form a \textit{clearly visible U-shaped curve}. This is a tip-off that the straight-line model we have used here may not be ideal and that we might get a better fit with another model. (Or it might be just \textit{chance}: the first and last points might be higher than expected because those were unusually expensive TVs for those sizes. How would you really know unless your data collection were very careful?) 

\InsImageInline{0.5}{l}{R2 9985.jpg}Least-squares calculations can fit not only straight-line data, but \textit{any set of data that can be described by a polynomial}, for example a second order (quadratic) equation (Y = \textbf{a} + \textbf{b}X + \textbf{c}X\textsuperscript{2}). Applying a second order fit to these data, we get the graph on the right. Now the R\textsuperscript{2 }value is higher, 0.9985, indicating that the fit is better (but again not perfect), and the residuals (the red dots at the bottom) are smaller and more random. This should not really be a surprise, because of the nature of these data. The size of a TV screen is always quoted as the \textit{length} of the diagonal, from one corner of the screen to its opposite corner, but the quantity of material, the difficulty of manufacture, the weight, and the power supply requirements of the screen should all scale with the \textit{screen} \textit{area}. Area\index{Area} is proportional to the square of the linear measure, so the inclusion of an X\textsuperscript{2 }term in the model is quite reasonable in this case. With this fit, the 40-inch set would be predicted to cost under \$500, which is more sensible than the linear fit. (The actual interpretation of the meaning of the best-fit coefficients \textbf{a}, \textbf{b}, and \textbf{c} is, however, impossible unless we know much more about the manufacture and marketing of TVs). The least-squares procedure allows us to model the data with a more-or-less simple polynomial equation. The point here is that a quadratic model is justified not just because it fits the data better, but in this case, it is justified because it is \textit{expected in principle} based on the relationship between length and area. (Incidentally, as you might expect, prices have dropped considerably since 2012; in 2021, a Visio 65" flat-screen HDTV 4K set was available at Costco for under \$500). 

In general, fitting \textit{any} set of data with a higher order polynomial, like a quadratic, cubic or higher, will reduce the fitting error and make the R\textsuperscript{2 }values closer to 1.000. That is because a higher-order model has more variable coefficients that the program can adjust to fit the data. \uline{For example, we} \uline{\textit{could}} \uline{fit the SD card price data to a} \uline{\textit{quadratic}} \uline{(}\href{https://terpconnect.umd.edu/~toh/spectrum/SDcardQuadratic.png}{graphic})\uline{, but} \uline{\textit{there is no reason to do so}} \uline{and the fit would only be slightly better.} The danger is that you could be "fitting the noise", that is, adjusting to the random noise in \textit{that data} set, whereas \textit{another} measurement with different random noise might give markedly different results. In fact, if you use a polynomial order that is \textit{one less than the number of data points}, the fit will be perfect and R\textsuperscript{2}=1.000. For example, the SD card data have only 4 data points, and if you fit those data to a 3\textsuperscript{rd} order (cubic) polynomial, you'll get a mathematically \textit{perfect fit} (\href{https://terpconnect.umd.edu/~toh/spectrum/SDcardCubic.png}{graphic}), but one that makes no sense in the real world (the price turns back down above x=14 Gbytes). It is meaningless and misleading - \textit{any} 4-point data would have fit a cubic model perfectly, \textit{even pure random noise}! The only justification for using a higher-order polynomial is if you have reason to believe, or have observed, that there is a \textit{consistent} non-linearity in the data set, as in the TV price example above.\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#basics}{}

\InsImageInline{0.5}{l}{LinearFit.GIF.png}The graph here shows a third example, taken from the field of analytical chemistry: it is a straight-line calibration data set where X = concentration and Y = the reading of some instrument that is supposed to be linearly proportional to the concentration X (in other words, Y = \textbf{a} + \textbf{b}X). If you are reading this online, you can \href{https://terpconnect.umd.edu/~toh/spectrum/DataExample.txt}{click to download that data}. The blue dots are the data points. They do not all fall in a perfect straight line because of random noise and measurement error in the instrument readings and possibly also volumetric errors in the concentrations of the standards (which are usually prepared in the laboratory by diluting a stock solution). For this set of data, the measured slope \textbf{b} is 9.7926 and the intercept \textbf{c} is 0.199. In analytical chemistry, the slope of the calibration curve is often called the "sensitivity". The intercept indicates the instrument reading that would be expected if the concentration were zero. Ordinarily, instruments are adjusted ("zeroed") by the operator to give a reading of zero for a concentration of zero, but random noise and instrument drift can cause the intercept to be non-zero for any calibration set. In this case, the data are in fact computer-generated, and the "true" value of the slope was exactly 10 and of the intercept was exactly zero before noise was added, and the noise was added by a zero-centered normally distributed random-number generator. The presence of the noise caused this measurement of the slope to be off by about 2\%. (Had there been a larger number of points in this data set, the calculated values of slope and intercept would almost certainly have been better. On average, the accuracy of measurements of slope and intercept improve with the \textit{square root of the number of points} in the data set). With this many data points, it is \textit{mathematically} possible to use an even higher polynomial degree, up to one less than the number of data points, but it is not \textit{physically} reasonable in most cases; for example, you could fit a 9\textsuperscript{th}-degree polynomial perfectly to these data, but the \href{https://terpconnect.umd.edu/~toh/spectrum/PolynomialDegree9.png}{result is pretty wild} (\href{https://terpconnect.umd.edu/~toh/spectrum/PolynomialDegree9.png}{graphic link}). No analytical instrument has a calibration curve that behaves like that. 

\InsImageInline{0.5}{l}{Residuals.GIF.png}A plot of the residuals for the calibration data (right) raises a question. Except for the 6th data point (at a concentration of 0.6), the other points seem to form a rough U-shaped curve, indicating that a quadratic equation might be a better model for those points than a straight line. Can we reject the 6th point as being an ``outlier'', perhaps caused by a mistake in preparing that solution standard or in reading the instrument for that point? Discarding that point would \href{https://terpconnect.umd.edu/~toh/spectrum/LLSErrorExample2.png}{improve the quality of fit} (R\textsuperscript{2}=0.992 instead of 0.986) especially if \href{https://terpconnect.umd.edu/~toh/spectrum/QLSErrorExample2.png}{a quadratic fit were used} (R\textsuperscript{2}=0.998). The only way to know for sure is to repeat that standard solution preparation and calibration and see if that U shape persists in the residuals. Many instruments do give a very linear calibration response, while others may show a slightly non-linear response under some circumstances (\href{https://terpconnect.umd.edu/~toh/models/BeersLawCurveFit.html}{for example}). But in fact, the calibration data used for \textit{this} example were computer-generated to be \textit{perfectly linear,} with normally distributed random numbers added to simulate noise. So that 6th point is \textit{not an outlier} and the underlying data are not really curved, but \textit{you would not know that in a real application}. It would have been a mistake to discard that 6th point and use a quadratic fit in this case. Moral: do not throw out data points just because they seem a little off, unless you have a good reason, and do not use higher-order polynomial fits just to get better fits if the instrument is known to give linear response under those circumstances. Even perfectly normally distributed random errors can occasionally give individual deviations that are quite far from the average and might tempt you into thinking that they are outliers. Do not be fooled. (\textit{Full disclosure}: I obtained the above example by ``\href{https://en.wikipedia.org/wiki/Cherry\_picking\_\%28fallacy\%29}{cherry-picking}'' from among dozens of randomly generated linear data sets with added random noise, in order to find one that, although actually random, \textit{seemed} to have an outlier). 

\textbf{Solving the calibration equation for concentration.} Once the calibration curve is established, it can be used to determine the concentrations of unknown samples that are measured on the same instrument, for example by \textit{solving the equation for concentration as a function of instrument reading}. The result for the linear case is that the concentration of the sample Cx is given by Cx = (Sx - \textit{intercept})/\textit{slope}, where Sx is the signal given by the sample solution, and "\textit{slope}" and "\textit{intercept}" are the results of the least-squares fit. If a quadratic fit is used, then you must use the more complex "\href{https://en.wikipedia.org/wiki/Quadratic\_equation}{quadratic equation}" to solve for concentration, but the problem of solving the calibration equation for concentration becomes \href{https://math.vanderbilt.edu/schectex/courses/cubic/}{forbiddingly complex for higher-order polynomial fits}. (The concentration and the instrument readings can be recorded in any convenient units if the same units are used for calibration and for the measurement of unknowns).

\section{Reliability of curve fitting results\label{ref-0203}\label{ref-0204}\label{ref-0205}\label{ref-0206}}

How reliable are the slope, intercept and other polynomial coefficients obtained from least-squares calculations on experimental data? The single most important factor is the appropriateness of the model chosen; it is critical that the model (e.g., linear, quadratic, gaussian, etc.) be a good match to the actual underlying shape of the data. You can do that either by choosing a model based on the known and expected behavior of that system (like using a linear calibration model for an instrument that is known to give linear response under those conditions) or by choosing a model that always gives randomly scattered residuals that do not exhibit a regular shape. But even with a perfect model, the least-squares procedure applied to repetitive sets of measurements will not give the same results every time because of random error (noise) in the data. If you were to repeat the entire set of measurements many times and do least-squares calculations on each data set, the standard deviations of the coefficients would vary directly with the standard deviation of the noise and inversely with the square root of the number of data points in each fit, all else being equal. The problem, obviously, is that it is not always possible to repeat the entire set of measurements many times. You may have only \textit{one} set of measurements, and each experiment may be very expensive to repeat. So, it would be great if we had a short-cut method that would let us \textit{predict} the standard deviations of the coefficients from a \textit{single measurement} of the signal, without repeating the measurements. 

Here I will describe three general ways to predict the standard deviations of the polynomial coefficients: \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Algebraic}{algebraic propagation of errors}, \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Monte}{Monte Carlo simulation}, and the\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Bootstrap}{ bootstrap sampling method}.

\subsection{\textbf{Algebraic Propagation of errors}\label{ref-0207}\label{ref-0208}}

\InsImageInline{0.5}{l}{image150.png}\InsImageInline{0.5}{l}{image151.png}The classical way is based on the \href{https://terpconnect.umd.edu/~toh/spectrum/ErrorPropagation.pdf}{rules for mathematical error propagation}. The propagation of errors of the entire curve-fitting method can be described in \href{http://www.chem.hope.edu/~polik/Chem345-2000/errorpropagation.htm}{closed-form algebra} by breaking down the method into a series of simple differences, sums, products, and ratios, and applying the \href{https://terpconnect.umd.edu/~toh/spectrum/ErrorPropagation.pdf}{rules for error propagation }to each step. The results of this procedure for a first order (straight line) least-squares fit are shown in the last three lines of the set of equations in Math Details, on page \pageref{ref-0234}. Essentially, these equations make use of the deviations from the least-squares line (the "residuals") to estimate the standard deviations of the slope and intercept, based on the assumption that the noise in that single data set is \textit{random} and is representative of the noise that would be obtained upon repeated measurements. \textit{Because these predictions are based only on a single data set, they are good only insofar as that data set is typical of others that might be obtained in repeated measurements.} If your random errors happen to be \textit{small} when you acquire your data set, you will get a deceptively \textit{good-looking} fit, but then your estimates of the standard deviation of the slope and intercept will be too \textit{low}, on average. If your random errors happen to be \textit{large} in that data set, you will get a deceptively \textit{bad-looking} fit, but then your estimates of the standard deviation will be too \textit{high}, on average. This problem becomes worse when the number of data points is small. This is not to say that it is not worth the trouble to calculate the predicted standard deviations of slope and intercept, but keep in mind that these predictions are accurate only if the number of data points is large (and only if the noise is random and normally distributed). Beware: if the deviations from linearity in your data set are \textit{systematic} and not \textit{random} - for example, if try to fit a straight line to a smooth curved data set  (left), then the estimates the standard deviations of the slope and intercept by these last two equations \textit{will be too high}, because they assume the deviations are caused by random noise that varies from measurement to measurement, whereas in fact a smooth curved data set without random noise (right) will give the \textit{same} slope and intercept from measurement to measurement.

In the application to analytical calibration, the concentration of the sample Cx is given by Cx = (Sx - \textit{intercept})/\textit{slope}, where Sx is the signal given by the sample solution. The uncertainty of all three terms contribute to the uncertainty of Cx. The standard deviation of Cx can be estimated from the standard deviations of the slope, intercept, and Sx using the \href{https://terpconnect.umd.edu/~toh/spectrum/ErrorPropagation.pdf}{rules for mathematical error propagation}. But the problem is that, in analytical chemistry, the labor and cost of preparing and running large numbers of standards solution often limits the number of standards to a rather small set, by statistical standards, so these estimates of standard deviation are often poor. 

\InsImageInline{0.5}{l}{LLSErrorExample.png}A spreadsheet that performs these error-propagation calculations for your own first-order (linear) analytical calibration data can be downloaded from \href{https://terpconnect.umd.edu/~toh/spectrum/CalibrationLinear.xls}{http://terpconnect.umd.edu/\textasciitilde{}toh/models/CalibrationLinear.xls}). For example, the linear calibration example just given in the previous section, where the "true" value of the slope was 10 and the intercept was zero, this spreadsheet (whose screenshot shown on the right) predicts that the slope is 9.8 with a standard deviation 0.407 (4.2\%) and that the intercept is 0.197 with a standard deviation 0.25 (128\%), both well within two standard deviations of the true values. This spreadsheet also performs the propagation of error calculations for the calculated concentrations of each unknown in the last two columns on the right. In the example in this figure, the instrument readings of the standards are taken as the unknowns, showing that the predicted percent concentration errors range from about 5\% to 19\% of the true values of those standards. (Note that the standard deviation of the concentration is greater at high concentrations than the standard deviation of the slope, and considerably greater at low concentrations because of the greater influence of the uncertainty in the intercept). For further discussion and some examples, see "\href{https://terpconnect.umd.edu/~toh/models/Bracket.html\#Cal\_curve\_linear}{The Calibration Curve Method with Linear Curve Fit}". My Matlab/Octave \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#plotit}{plotit.m} function uses the algebraic method to compute the standard deviations of least-squares coefficients for any polynomial order.

\subsection{Monte Carlo simulation\label{ref-0209}\label{ref-0210}\label{ref-0211}}

The second way of estimating the standard deviations of the least-squares coefficients is to perform a random-number simulation (a type of \href{http://en.wikipedia.org/wiki/Monte\_Carlo\_method}{Monte Carlo simulation}). This requires that you know (by previous measurements) the average standard deviation of the random noise in the data. Using a computer, you construct a model of your data over the normal range of X and Y values (e.g. Y = \textbf{intercept} + \textbf{slope*}X + \textbf{noise}, where \textbf{noise} is the n\href{https://terpconnect.umd.edu/~toh/spectrum/MonteCarloAnimation.gif}{}oise in the data), compute the slope and intercept of each simulated noisy data set, then repeat that process many times (usually a few thousand) with different sets of random noise, and finally compute the standard deviation of all the resulting slopes and intercepts. This is ordinarily done with normally distributed random noise (e.g., the RANDN function that many programming languages have). These random number generators produce "white" noise, but \href{https://terpconnect.umd.edu/~toh/spectrum/functions.html\#Signal\_processing}{other noise colors can be derived}. If the model is good and the noise in the data is well-characterized in terms of frequency distribution and signal amplitude dependence, the results will be a very good estimate of the expected standard deviations of the least-squares coefficients. (If the noise is not constant, but rather varies with the X or Y values, or if the noise is not white or is not normally distributed, then that behavior must be included in the simulation). 

An \href{https://terpconnect.umd.edu/~toh/spectrum/MonteCarloDemo.m}{animated example} is shown on the right (which you can view if you read this within \textit{Microsoft Word 365}, otherwise click \href{https://terpconnect.umd.edu/~toh/spectrum/MonteCarloAnimation.gif}{this} link), for the case of a 100-point \InsImageInline{0.5}{l}{MonteCarloAnimation.gif.png}straight-line data set with slope=1, intercept=0, and standard deviation of the added noise equal to 5\% of the maximum value of y. For each repeated set of simulated data, the fit coefficients (least-squares measured slope and intercept) are slightly different because of the noise. 

Obviously, this method involves programming a computer to compute the model and is not as convenient as evaluating a simple algebraic expression. But there are two important advantages to this method: (1) is has great generality; it can be applied to curve fitting methods that are too complicated for the classical closed-form algebraic propagation-of-error calculations, even \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{iterative non-linear methods}; and (2) its predictions are based on the average noise in the data, not the noise in just a single data set. For that reason, it gives more reliable estimations, particularly when the number of data points in each data set is small. Nevertheless, you cannot always apply this method because you do not always know the average standard deviation of the random noise in the data. You can do this type of computation easily in Matlab/Octave and in spreadsheets (page \pageref{ref-0232}). 

You can download a Matlab/Octave script that compares the Monte Carlo simulation to the algebraic method above from \href{https://terpconnect.umd.edu/~toh/spectrum/LinearFiMC.m}{http://terpconnect.umd.edu/\textasciitilde{}toh/spectrum/LinearFiMC.m}. By running this script with different sizes of data sets ("NumPoints" in line 10), you can see that the standard deviation predicted by the algebraic method fluctuates a lot from run to run when NumPoints is small (e.g., 10), but the Monte Carlo predictions are much more steady. When NumPoints is large (e.g., 1000), both methods agree very well.

\subsection{The Bootstrap method\label{ref-0212}\label{ref-0213}\label{ref-0214}\label{ref-0215}\label{ref-0216}\label{ref-0217}\label{ref-0218}\label{ref-0219}\label{ref-0220}\label{ref-0221}\label{ref-0222}}

The third method is the \href{http://www.stat.rutgers.edu/home/mxie/RCPapers/bootstrap.pdf}{"bootstrap"} method, a procedure that involves choosing random sub-samples with replacement from a single data set and analyzing each sample the same way (e.g. by a least-squares fit). Every sample is returned to the data set after sampling, so that (a) a particular data point from the original data set could appear multiple times in each sample, and (b) the number of elements in each bootstrap sub-sample equals the number of elements in the original data set. As a simple example, consider a data set with 10 x,y pairs assigned the letters \textit{a} through \textit{j.} The original data set is represented as [\textit{a b c d e f g h i j}], and some typical bootstrap sub-samples might be [\textit{a b b d e f f h i i}] or [\textit{a a c c e f g g i j}]. Each bootstrap sample contains the same number of data points, but with about a third of the data pairs skipped, a third duplicated, and a third left alone. (This is equivalent to weighting a third of the data pairs by a factor of 2, a third by 0, and leaving a third unweighted). You would use a computer to generate hundreds or thousands of bootstrap samples like that and to apply the calculation procedure under investigation (in this case a linear least-squares) to each set. 

If there were \textit{no} noise in the data set, and if the model were properly chosen, then all the points in the original data set and in all the bootstrap sub-samples would fall exactly on the model line, with the result that the least-squares results would virtually be the \textit{same for every subsample}. 

However, if there \textit{is} noise in the data set, each set would give a slightly different result (e.g., the least-squares polynomial coefficients), because each sub-sample has a different subset of the random noise, as the animation on the right demonstrates. 

The process is illustrated by the animation on the right (which you can view if you read this within \textit{Microsoft Word 365}, otherwise click \href{https://terpconnect.umd.edu/~toh/spectrum/BootStrap.gif}{this} link), for the same 100-point straight-line data set used above. \InsImageInline{0.5}{l}{BootStrap.gif.png}You can see that the variation in the best-fit coefficients between sub-samples is the same as for the Monte Carlo simulation above. The greater the amount of random noise in the data set, the greater would be the range of results from sample to sample in the bootstrap set. This enables you to estimate the uncertainty of the quantity you are estimating, just as in the Monte-Carlo method above. The difference is that the Monte-Carlo method assumes that the noise is known, random, and can be accurately simulated by a random number generator on a computer, whereas the bootstrap method uses the \textit{actual noise in the data set} at hand, like the algebraic method, except that it does not need an algebraic solution of error propagation. The bootstrap method thus shares its generality with the Monte Carlo approach but is limited by the assumption that the noise in that (possibly small) single data set is representative of the noise that would be obtained upon repeated measurements. The bootstrap method cannot, however, correctly estimate the parameter errors resulting from \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#accuracy}{poor model selection}. The method is examined in detail in its \href{http://scholar.google.com/scholar?hl=en&as\_sdt=0,21&q=bootstrap+statistics}{extensive literature}. This type of bootstrap computation is easily done in \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Matlab\_Bootstrap}{Matlab/Octave} and can also be done (with greater difficulty) in \href{https://terpconnect.umd.edu/~toh/spectrum/CalibrationQuadraticB.xlsx}{spreadsheets}.

\subsection{Comparison of error prediction methods. \label{ref-0223}}

The Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/TestLinearFit.m}{TestLinearFit.m} compares \textit{all three} of these methods (Monte Carlo simulation, the algebraic method, and the bootstrap method) for a 100-point first-order linear least-squares fit. Each method is repeated on different data sets with the same average slope, intercept, and random noise, then the standard deviation (SD) of the slopes (\texttt{SDslope}) and intercepts (\texttt{SDint}) were compiled and are tabulated below. 

\texttt{NumPoints = 100 SD of the Noise = 9.236 x-range = 30}

            \texttt{\textbf{Simulation Algebraic equation Bootstrap method}}

          \texttt{\textbf{SDslope SDint SDslope SDint SDslope SDint}}

\texttt{\textbf{Mean SD:}}   \texttt{0.1140 4.1158 0.1133 4.4821 0.1096 4.0203}

 (You can download this script from \href{https://terpconnect.umd.edu/~toh/spectrum/TestLinearFit.m}{http://terpconnect.umd.edu/\textasciitilde{}toh/spectrum/TestLinearFit.m}). On average, the mean standard deviations ("Mean SD") of the three methods agree very well, but the algebraic and bootstrap methods fluctuate more than the Monte Carlo simulation each time this script is run, because they are based on the noise in one \textit{single} 100-point data set, whereas the Monte Carlo simulation reports the average of many data sets. Naturally, the algebraic method is simpler and faster to compute than the other methods. However, an algebraic propagation of error solution is not always possible to obtain, whereas the Monte Carlo and bootstrap methods do not depend on an algebraic solution and can be applied readily to more complicated curve-fitting situations, such as \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{non-linear iterative least-squares}, as will be seen later.

\subsection{Effect of the number of data points on least-squares fit precision\label{ref-0224}}

 \href{https://terpconnect.umd.edu/~toh/spectrum/EffectOfNumberOfPoints.jpg}{}The spreadsheets \href{https://terpconnect.umd.edu/~toh/spectrum/EffectOfSampleSize.ods}{EffectOfSampleSize.od}s or \href{https://terpconnect.umd.edu/~toh/spectrum/EffectOfSampleSize.xlsx}{EffectOfSampleSize.xlsx}, which collect the results of many runs of \href{https://terpconnect.umd.edu/~toh/spectrum/TestLinearFit.m}{TestLinearFit.m} with different numbers of data points ("NumPoints"), demonstrates that \InsImageInline{0.5}{l}{EffectOfNumberOfPoints.jpg}the standard deviation of the slope and the intercept \textit{decrease} if the number of data points is \textit{increased}; on average, the \textit{standard deviations are inversely proportional to the square root of the number of data points,} which is consistent with the observation that the slope of a log-log plot is roughly 1/2. 

These plots really dramatize the problem of small sample sizes, but this must be balanced against the cost of obtaining more data points. For example, in analytical chemistry calibration, a larger number of calibration points could be obtained either by preparing and measuring more standard solutions or by reading each of a smaller number of standards repeatedly. The former approach accounts for both the volumetric errors in preparing solutions and the random noise in the instrument readings, but the labor and cost of preparing and running large numbers of standard solutions, and safely disposing of them afterward, is limiting. The latter approach is less expensive but is less reliable because it accounts only for the random noise in the instrument readings. Overall, it better to refine the laboratory techniques and instrument settings to minimize error than to attempt to compensate by taking lots of readings. 

\textit{It is very important that the noisy signal not be} \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#NOT\_smooth}{\textit{smoothed}} \textit{before the least-squares calculations}, because doing so will \textit{not} improve the reliability of the least-squares results, but it will cause both the algebraic propagation-of-errors and the bootstrap calculations to \textit{seriously underestimate} the standard deviation of the least-squares results. You can demonstrate using the most recent version of the script \href{https://terpconnect.umd.edu/~toh/spectrum/TestLinearFit.m}{TestLinearFit.m} by setting SmoothWidth in line 10 to something higher than 1, which will smooth the data before the least-squares calculations. This has no significant effect on the \textit{actual} standard deviation as calculated by the Monte Carlo method, but it does significantly reduce the \textit{predicted} standard deviation calculated by the algebraic propagation-of-errors and (especially) the bootstrap method. For similar reasons, if the noise is \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#Frequency}{pink rather than white}, the bootstrap error estimates will also be too low. Conversely, if the noise is \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#Frequency}{blue}, as occurs in processed signals that have been subjected to some sort of \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html}{differentiation }process or that have been \href{https://terpconnect.umd.edu/~toh/spectrum/Deconvolution.html}{deconvoluted }from some blurring process, then the errors predicted by the algebraic propagation-of-errors and the bootstrap methods will be \textit{high}. (You can prove this to yourself by running \href{https://terpconnect.umd.edu/~toh/spectrum/TestLinearFit.m}{TestLinearFit.m} with pink and blue noise modes selected in lines 23 and 24). Bottom line: error prediction works best for \textit{white} noise. 

\subsection{Transforming non-linear relationships\label{ref-0225}\label{ref-0226}\label{ref-0227}\label{ref-0228}}

In some cases, a fundamentally non-linear relationship can be transformed into a form that is amenable to polynomial curve fitting by means of a coordinate transformation (e.g., taking the log or the reciprocal of the data), and then the least-squares method can be applied to the resulting linear equation. For example, the signal in the figure below is from a simulation of exponential decay that has the mathematical form Y = \textbf{a} exp(\textbf{b}X), where X=time, Y=signal intensity, \textbf{a} is the Y-value at X=0 and \textbf{b} is the decay constant. This is a fundamentally non-linear problem because Y is a non-linear function of the parameter \textbf{b}. However, by taking the natural log of both sides of the equation, we obtain ln(Y)=ln(\textbf{a}) + \textbf{b}X. In this equation, Y is a \textit{linear} function of both parameters ln(\textbf{a}) and \textbf{b}, so it can be fit by the least-squares method to estimate ln(\textbf{a}) and \textbf{b}, from which you get \textbf{a} by computing exp(ln(\textbf{a})). In this example, the "true" values of the coefficients are \textbf{a} =1 and \textbf{b} = -0.9, but random noise has been added to each data point, with a standard deviation equal to 10\% of the value of that data point, to simulate a typical experimental measurement in the laboratory. An estimate of the values of ln(\textbf{a}) and \textbf{b}, given only the noisy data points, can be determined by the least-squares curve fitting of ln(Y) vs X. 


\begin{center}
\InsImageInline{0.5}{l}{expexample.png}
\end{center}



\begin{center}
\textit{An exponential least-squares fit (solid line) applied to a noisy data set (points)} 
\end{center}



\begin{center}
\textit{to estimate the decay constant.}
\end{center}


The best-fit equation, shown by the green solid line in the figure, is Y =0.959 exp(- 0.905 X), that is, \textbf{a} = 0.959 and \textbf{b} = -0.905, which are reasonably close to the expected values of 1 and -0.9, respectively. Thus, even in the presence of substantial random noise (10\% relative standard deviation), it is possible to get reasonable estimates of the parameters of the underlying equation (to within about 4\%). The most important requirement is that the model must be good, that is, that the equation selected for the model accurately describes the underlying behavior of the system (except for noise). Often that is the most difficult aspect because the underlying models are not always known with certainty. In Matlab and Octave, is fit can be performed in a single line of code: \texttt{polyfit(x,log(y),1)}, which returns \texttt{[b log(a)]}. (In Matlab and Octave, "log" is the natural log, "log10" is the base-10 log).

Another example of the linearization of an exponential relationship is explored on page \pageref{ref-0390}: \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#StockMarket}{Signal and Noise in the Stock Market}. 

Other examples of non-linear relationships that can be linearized by coordinate transformation include the logarithmic (Y = \textbf{a} ln(\textbf{b}X)) and power (Y=\textbf{a}X\textsuperscript{\textbf{b}}) relationships. Methods of this type used to be very common back in the days before computers, when fitting anything but a straight line was difficult. It is still used today to extend the range of functional relationships that can be handled by common linear least-squares routines available in spreadsheets and hand-held calculators. (My Matlab/Octave function \href{https://terpconnect.umd.edu/~toh/spectrum/trydatatrans.m}{trydatatrans.m} tries eight different simple data transformations on any given x,y data set and fits the transformed data to a straight line or polynomial). Only a few non-linear relationships can be handled by simple data transformation, however. To fit \textit{any} arbitrary custom function, you may have to resort to the \textit{iterative} curve fitting method, which will be treated in \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{Curve Fitting C}. 

\subsection{Fitting Gaussian and Lorentzian peaks\label{ref-0229}\label{ref-0230}}

An interesting example of the use of transformation to convert a non-linear relationship into a form that is amenable to polynomial curve fitting is the use of the natural log (ln) transformation to convert a positive \href{http://en.wikipedia.org/wiki/Gaussian\_function}{Gaussian} peak, which has the fundamental functional form exp(-x\textsuperscript{2}), into a parabola of the form -x\textsuperscript{2}, which can be fit with a second-order polynomial (quadratic) function (y = \textbf{a} + \textbf{bx} + \textbf{c}x\textsuperscript{2}). The equation for a Gaussian peak is y = \textbf{h}*exp(-((x-\textbf{p})./(1/(2*sqrt(ln(2)))*\textbf{w})) \textasciicircum{}2)), where \textbf{h} is the peak height, \textbf{p} is the x-axis location of the peak maximum, \textbf{w} is the full width of the peak at half-maximum. The natural log of \textit{y} \href{http://www.wolframalpha.com/input/?i=expand+log\%28+h*exp\%28-\%28\%28x-p\%29\%2F\%281\%2F\%282*sqrt\%28ln\%282\%29\%29\%29*w\%29\%29\%5E2\%29\%29}{can be shown to be} log(\textbf{h})-(4 \textbf{p}\textasciicircum{}2 log(2))/\textbf{w}\textasciicircum{}2+(8 \textbf{p} \textit{x} log(2))/\textbf{w}\textasciicircum{}2-(4 \textit{x}\textasciicircum{}2 log(2))/\textbf{w}\textasciicircum{}2, which is a quadratic form in the independent variable x because it is the sum of x\textasciicircum{}2, x, and constant terms. Expressing each of the peak parameters \textbf{h}, \textbf{p}, and \textbf{w} in terms of the three quadratic coefficients, \href{http://www.wolframalpha.com/input/?i=solve+a\%3Dlog\%28h\%29-\%284*log\%282\%29p\%5E2\%29\%2Fw\%5E2\%2Cb\%3D\%288*log\%282\%29*p\%29\%2Fw\%5E2\%2C+c\%3D-\%284*log\%282\%29\%29\%2Fw\%5E2+for+h\%2Cp\%2Cw}{a little algebra} (courtesy of \href{http://www.wolframalpha.com/about.html}{Wolfram Alpha\index{Wolfram Alpha}}) will show that all three parameters of the peak (height, maximum position, and width) can be calculated from the three quadratic coefficients \textbf{a}, \textbf{b}, and \textbf{c}. The peak height is given by exp(\textbf{a}-\textbf{c}*(\textbf{b}/(2*\textbf{c}))\textasciicircum{}2), the peak position by -\textbf{b}/(2*\textbf{c}), and the peak half-width by 2.35482/(sqrt(2)*sqrt(-\textbf{c})). This is called "Caruana's Algorithm"; see \textit{Streamlining Digital Signal Processing: A "Tricks of the Trade" Guidebook}, Richard G. Lyons, ed., \href{http://books.google.com/books?id=fmua6jWDgtUC&pg=PA298&lpg=PA298&dq=natural+log+Gaussian+quadratic+parabola&source=bl&ots=aCBUx2gYnn&sig=l3kiACEWGfPaOgpKDlk28JpYJIo&hl=en&sa=X&ei=CbZeUcbCFOvA4AP3\_YCABA&ved=0CD4Q6AEwAg\#v=onepage&q&f=false}{page 298}. The area under the Gaussian peak can be shown to be 1.064467*height*width.

\InsImageInline{0.5}{l}{QuadFitToGaussian.png}One advantage of this type of Gaussian curve fitting, as opposed to simple visual estimation, is illustrated in the figure below. The signal is a synthesized Gaussian peak with a true peak height of exactly 100 units, a true peak position of 100 units, and a true half-width of 100 units, but it is \textit{sparsely sampled} only every 31 units on the x-axis. The \href{https://terpconnect.umd.edu/~toh/spectrum/ResultiingDataSet.txt}{resulting data set}, shown by the red points in the upper left, has only 6 data points on the peak itself. If we were to take the maximum of those 6 points (the 3rd point from the left, with x=87, y=95) as the peak maximum, we would get only a rough approximation to the true values of peak position (100) and height (100). If we were to take the distance between the 2\textsuperscript{nd} the 5\textsuperscript{th} data points as the peak width, we would get only 3*31=93, compared to the true value of 100. If we were to attempt to estimate the \textit{area} under the peak from those measurements, we would get 1.064467*95*93=9404.6, much lower than the theoretical width of 1.064467*height*width=10644.67.

However, taking the \textit{natural log} of the data (upper right) produces a \textit{parabola} that can be fitted with a quadratic least-squares fit (shown by the blue line in the lower left). From the three coefficients of the quadratic fit, we can calculate much more accurate values of the Gaussian peak parameters, shown at the bottom of the figure: height=100.93; position=99.11; width=99.25; area= 10663. The plot in the lower right shows the resulting Gaussian fit (in blue) displayed with the original data (red points). The accuracy of those peak parameters (about 1\% in this example) is limited only by the noise in the data. 

This figure was created in Matlab (or Octave), using \href{https://terpconnect.umd.edu/~toh/spectrum/QuadFitToGaussian.m}{this script}. (The Matlab/Octave function \href{https://terpconnect.umd.edu/~toh/spectrum/gaussfit.m}{gaussfit.m} performs the calculation for an x,y data set. You can also download a spreadsheet that does the same calculation; it is available in OpenOffice Calc (\href{https://terpconnect.umd.edu/~toh/spectrum/GaussianLeastSquares.ods}{Download link}, \href{https://terpconnect.umd.edu/~toh/spectrum/GaussianLeastSquares.GIF}{Screenshot}) and \href{https://terpconnect.umd.edu/~toh/spectrum/GaussianLeastSquares.xls}{Excel} formats). The method is simple and very fast, but for this method to work properly, the data set \textit{must not contain any zeros or negative points}; if the signal-to-noise ratio is very poor, it may be useful to skip those points or to pre-smooth the data slightly to reduce this problem. Moreover, the original Gaussian peak signal must be a single isolated peak with a zero baseline, that is, must tend to zero far from the peak center. In practice, this means that any non-zero baseline must be subtracted from the data set before applying this method. (A more general approach to fitting Gaussian peaks, which works for data sets with zeros and negative numbers and also for data with multiple overlapping peaks, is the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{non-linear iterative curve fitting} method, which will be treated later). 

A similar method can be derived for a \href{http://en.wikipedia.org/wiki/Cauchy\_distribution}{Lorentzian} peak, \href{https://terpconnect.umd.edu/~toh/spectrum/LorentzianLeastSquares.gif}{}which has the fundamental form y=\textbf{h}/(1+((x-\textbf{p})/(0.5*\textbf{w}))\textasciicircum{}2), by fitting a quadratic to the \href{http://www.wolframalpha.com/input/?i=expand+\%281\%2B\%28\%28x-p\%29\%2F\%280.5*w\%29\%29\%5E2\%29\%2Fh}{\textit{reciprocal} of y}. As for the Gaussian peak, all three parameters of the peak (height \textbf{h}, maximum position \textbf{p}, and width \textbf{w}) can be calculated from the three quadratic coefficients \textbf{a}, \textbf{b}, and \textbf{c} of the quadratic fit: \textbf{h}=4*\textbf{a}/((4*\textbf{a}*\textbf{c})-\textbf{b}\textasciicircum{}2), \textbf{p}= -\textbf{b}/(2*\textbf{a}), and \textbf{w}= sqrt(((4*\textbf{a}*\textbf{c})-\textbf{b}\textasciicircum{}2)/\textbf{a})/sqrt(\textbf{a}). Just as for the Gaussian case, the data set must not contain any zero or negative y values. The Matlab/Octave function \href{https://terpconnect.umd.edu/~toh/spectrum/lorentzfit.m}{lorentzfit.m} performs the calculation for an x,y data set, and the Calc and Excel spreadsheets \href{https://terpconnect.umd.edu/~toh/spectrum/LorentzianLeastSquares.ods}{LorentzianLeastSquares.ods} and \href{https://terpconnect.umd.edu/~toh/spectrum/LorentzianLeastSquares.xls}{LorentzianLeastSquares.xls} \InsImageInline{0.5}{l}{LorentzianLeastSquares.gif.png}perform the same calculation. 

(By the way, a quick way to test either of the above methods is to use this \textit{simple peak data set}: x=5, 20, 35 and y=5, 10, 5, which has a height, position, and width equal to 10, 20, and 30, respectively, for a single isolated symmetrical peak of any shape, assuming only a baseline of zero). 

To apply the above methods to signals containing \textit{two or more} Gaussian or Lorentzian peaks, it is necessary to locate all the peak maxima first, so that the proper groups of points centered on each peak can be processed with the algorithms just discussed. That is discussed on page \pageref{ref-0288}.

However, there is a downside to using coordinate transformation methods to convert non-linear relationships into simple polynomial form, and that is that the noise is also affected by the transformation, with the result that the \href{http://en.wikipedia.org/wiki/Propagation\_of\_uncertainty}{propagation of error} from the original data to the final results is often difficult to predict. For example, in the method just described for measuring the peak height, position, and width of Gaussian or Lorentzian peaks, the results depend not only on the amplitude of noise in the signal, but also on how many points across the peak are taken for fitting. As you take more points far from the peak center, where the y-values approach zero, the natural log of those points approaches negative infinity as y approaches zero. The result is that the noise of those low-magnitude points is unduly magnified and has a disproportional effect on the curve fitting. This runs counter the usual expectation that the quality of the parameters derived from curve fitting improves with the square root of the number of data points (\href{https://terpconnect.umd.edu/CurveFittingC.html\#Noise}{page} \pageref{ref-0277}). A reasonable compromise in this case is to take \textit{only the points in the top half of the peak}, with Y-values down to one-half of the peak maximum. If you do that, the error propagation (predicted by a \href{http://en.wikipedia.org/wiki/Monte\_Carlo\_method}{Monte Carlo simulation} with constant normally-distributed random noise) shows that the relative standard deviations of the measured peak parameters are directly proportional to the noise in the data and inversely proportional to the square root of the number of data points (as expected), but that the proportionality constants differ: \begin{enumerate}[{(a)}]


\item the relative standard deviation of the peak height = 1.73*\textit{noise}/sqrt(\textit{N}), 

\item the relative standard deviation of the peak position = \textit{noise}/sqrt(\textit{N}), 

\item the relative standard deviation of the peak width = 3.62*\textit{noise}/sqrt(\textit{N}), 

\end{enumerate}
\InsImageInline{0.5}{l}{MonteCarlo.gif.png}where \textit{noise} is the standard deviation of the noise in the data and \textit{N} is the number of data points taken for the least-squares fit. You can see from these results that the measurement of peak \textit{position} is most precise, followed by the peak \textit{height}, with the peak \textit{width} being the least precise. If one were to include points far from the peak maximum, where the signal-to-noise ratio is very low, the results would be poorer than predicted. These predictions depend on knowledge of the noise in the signal; if only a single sample of that noise is available for measurement, there is no guarantee that sample is a representative sample, especially if the total number of points in the measured signal is small; the standard deviation of small samples is notoriously variable. Moreover, these predictions are based on a simulation with \textit{constant normally distributed white} noise; had the actual noise varied with signal level or with x-axis value, or if the probability distribution had been something other than normal, those predictions would not necessarily have been accurate. In such cases, the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Bootstrap}{bootstrap method} (page \pageref{ref-0213}) has the advantage that it samples the actual noise in the signal. 

You can download the Matlab/Octave code for this Monte Carlo simulation from \url{http://terpconnect.umd.edu/~toh/spectrum/GaussFitMC.m}; view \href{https://terpconnect.umd.edu/~toh/spectrum/MonteCarlo.gif}{screen capture}. A similar simulation (\url{http://terpconnect.umd.edu/~toh/spectrum/GaussFitMC2.m}, view \href{https://terpconnect.umd.edu/~toh/spectrum/GaussFitMC2.gif}{screen capture}) compares this method to fitting the entire Gaussian peak with the iterative method in \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{Curve Fitting 3}, finding that the precision of the results is only slightly better with the (slower) iterative method.

\textbf{Note 1:} If you are reading this online, you can right-click on any of the m-file links above and select ``\textbf{Save Link As...}'' to download them to your computer for use within Matlab/Octave. 

\textbf{Note 2:} In the curve fitting techniques described here and in the next two chapters, there is no requirement that the x-axis interval between data points be uniform, as is the assumption in many of the other signal processing techniques previously covered. Curve fitting algorithms typically accept a set of arbitrarily spaced x-axis values and a corresponding set of y-axis values.

\section{Math and software details for linear least squares\label{ref-0231}\label{ref-0232}\label{ref-0233}}

The least-squares best fit for an x,y data set can be computed using only basic arithmetic. Here are the relevant equations for computing the slope and intercept of the first order best-fit equation, y = intercept + slope*x, as well as the predicted standard deviation of the slope and intercept, and the coefficient of determination, R\textsuperscript{2}, which is an indicator of the "goodness of fit". (R\textsuperscript{2} is 1.0000 if the fit is perfect and less than that if the fit is imperfect).


\begin{table}
\begin{tabularx}{\textwidth}{|
p{\dimexpr 1\linewidth-2\tabcolsep-2\arrayrulewidth}|} \hline 
n = number of x,y data points  \par sumx = ${\Upsigma}$x  \par sumy = ${\Upsigma}$y  \par sumxy = ${\Upsigma}$x*y  \par sumx2 = ${\Upsigma}$x*x  \par meanx = sumx / n  \par meany = sumy / n  \par \textbf{slope} = (n*sumxy - sumx*sumy) / (n*sumx2 - sumx*sumx)  \par \textbf{intercept} = meany-(slope*meanx)  \par ssy = ${\Upsigma}$(y-meany)\textasciicircum{}2  \par ssr = ${\Upsigma}$(y-intercept-slope*x)\textasciicircum{}2  \par \textbf{R}\textsuperscript{\textbf{2}} = 1-(ssr/ssy)\label{ref-0234} \par \textbf{Standard deviation of the slope} = SQRT(ssr/(n-2))*SQRT(n/(n*sumx2 - sumx*sumx)) \par \textbf{Standard deviation of the intercept} = SQRT(ssr/(n-2))*SQRT(sumx2/(n*sumx2 - sumx*sumx)) \\\hline 
\end{tabularx}
\end{table}
(In these equations, ${\Upsigma}$ represents summation; for example, ${\Upsigma}$x means the sum of all the x values, and ${\Upsigma}$x*y means the sum of all the x*y products, etc.)

The last two lines predict the standard deviation of the slope and the intercept, based only on that data sample, assuming that the deviations from the line are random and normally distributed. These are estimates of the variability of slopes and intercepts you are likely to get if you repeated the data measurements over and over multiple times under the same conditions, assuming that the deviations from the straight line are due to \textit{random variability} and not a systematic error caused by non-linearity. If the deviations are random, they will be slightly different from time to time, causing the slope and intercept to vary from measurement to measurement, with a standard deviation predicted by these last two equations. However, if the deviations are caused by systematic non-linearity, they will be the same from measurement to measurement, in which case the prediction of these last two equations will not be relevant, and you might be better off using a polynomial fit such as a quadratic or cubic. 

The reliability of these standard deviation estimates depends on the assumption of random deviations and on the number of data points in the curve fit; they improve with the square root of the number of points. A \href{https://terpconnect.umd.edu/~toh/models/CalibrationQuadraticEquations.txt}{slightly more complex set of equations} can be written to fit a second-order (quadratic or parabolic) equations to a set of data; instead of a slope and intercept, three coefficients are calculated, \textbf{a}, \textbf{b}, and \textbf{c}, representing the coefficients of the quadratic equation \textbf{a}x\textsuperscript{2}+\textbf{b}x+\textbf{c}. 

These calculations could be performed step-by-step by hand, with the aid of a calculator or a spreadsheet, with a \href{https://terpconnect.umd.edu/~toh/spectrum/LeastSquaresCode.txt}{program} written in any programming language, such as a \href{https://terpconnect.umd.edu/~toh/spectrum/LeastSquaresMatlab.txt}{Matlab or Octave script}. 

\textbf{Web sites}: \href{http://www.wolframalpha.com/}{Wolfram Alpha\index{Wolfram Alpha}} includes some capabilities for least-squares \href{http://www.wolframalpha.com/examples/RegressionAnalysis.html}{regression analysis}, including linear, polynomial, exponential, and logarithmic fits.  \href{http://statpages.org/index.html}{Statpages.org} can perform a huge range of statistical calculations and tests, and there are several Web sites that specialize in plotting and data visualization that have curve-fitting capabilities, including most notably \href{https://plotly.com/matlab/}{Plotly} and \href{https://mycurvefit.com/}{MyCurveFit}. Web sites such as these can be very handy when working from a smartphone, tablet, or a computer that does not have suitable computational software. If you are reading this online, you can \textbf{Ctrl-Click} on these links to open them in your browser.

\subsection{Spreadsheets for linear least squares\label{ref-0235}\label{ref-0236}}

\InsImageInline{0.5}{l}{LeastSquares.GIF.png}Spreadsheets can perform the math described above easily. The spreadsheets pictured below (\href{https://terpconnect.umd.edu/~toh/spectrum/LeastSquares.xls}{LeastSquares.xls} and \href{https://terpconnect.umd.edu/~toh/spectrum/LeastSquares.odt}{LeastSquares.odt} for linear fits and \href{https://terpconnect.umd.edu/~toh/spectrum/QuadraticLeastSquares.xls}{QuadraticLeastSquares.xls} and \href{https://terpconnect.umd.edu/~toh/spectrum/QuadraticLeastSquares.ods}{QuadraticLeastSquares.ods} for quadratic fits), utilize the expressions given above to compute and plot linear and quadratic (parabolic) least-squares fit, respectively. The advantage of spreadsheets is that they are highly customizable for your application and can be deployed on mobile devices such as tablets or smartphones. For straight-line fits, you can use the convenient built-in functions \textit{slope} and \textit{intercept}.

\textit{Animation of spreadsheet template data entry for first order (linear) least-squares fit (\url{https://terpconnect.umd.edu/~toh/spectrum/LeastSquares.GIF})}



\InsImageInline{0.5}{l}{QuadraticLeastSquares.GIF.png}

\textit{Animation of spreadsheet template data entry for second order (quadratic) least-squares fit. ( \url{https://terpconnect.umd.edu/~toh/spectrum/QuadraticLeastSquares.GIF})}



 

\textbf{The LINEST function}. Modern spreadsheets also have \textit{built-in} facilities for computing polynomial least-squares curve fits of \textit{any} order. For example, the LINEST function in both \href{http://www.google.com/url?sa=t&ct=res&cd=1&url=http\%3A\%2F\%2Fwww.colby.edu\%2Fchemistry\%2FPChem\%2Fnotes\%2Flinest.pdf&ei=DTl\_SMnUDIye8gTV-vjTCw&usg=AFQjCNGNAoinemLK1XD9VxkN0SlQvlseFg&sig2=cxJZu6DsHOSjcBWjaSbsiw}{Excel} and \href{http://wiki.services.openoffice.org/wiki/Documentation/How\_Tos/Calc:\_LINEST\_function}{OpenOffice Calc} can be used to compute polynomial and other curvilinear least-squares fits. In addition to the best-fit polynomial coefficients, the LINEST function also calculates at the same time the standard error values, the determination coefficient (\href{http://wiki.services.openoffice.org/wiki/Documentation/How\_Tos/Calc:\_RSQ\_function}{R\textsuperscript{2}}), the standard error value for the \textit{y} estimate, the F statistic, the number of degrees of freedom, the regression sum of squares, and the residual sum of squares. A significant inconvenience of LINEST, compared to working out the math using the series of mathematical expressions described above, is that it is more difficult to adjust to a variable number of data points and to remove suspect data points or to change the order of the polynomial. LINEST is an \textit{array function}, which means that when you enter the formula in one cell, multiple cells will be used for the output of the function. \textit{You cannot edit a LINEST function just like any other spreadsheet function.} To specify that LINEST is an array function, do the following. Highlight the entire formula, including the "=" sign. On the Macintosh, next, hold down the ``apple'' key and press "return." On the PC hold down the ``Ctrl'' and ``Shift'' keys and press ``Enter.'' Excel adds "\{ \}" brackets around the formula, to show that it is an array. Note that you cannot type in the "\{ \}" characters yourself; if you do Excel will treat the cell contents as characters and not a formula. \textit{Highlighting the full formula and typing the ``apple'' key or ``Ctrl'','' Shift'' and "return" is the only way to enter an array formula.} This instruction sheet from Colby College may help:  \url{http://www.colby.edu/chemistry/PChem/notes/linest.pdf}. 

\textbf{Practical Note}: If you are working with a template that uses the LINEST function, and you wish to change the number of data points, the easiest way to do that is to select the rows or columns containing the data, right-click on the row or column \textit{heading} (1,2,3 or a, b, c, etc.) and use the \textbf{Insert} or \textbf{Delete} in the right-click menu. If you do it that way, the LINEST function referring to those rows or columns will be adjusted \textit{automatically}. That is easier than trying to edit the LINEST function directly. (If you are inserting rows or columns, you must drag-copy the formulas from the older rows or columns into the newly inserted empty ones). See \href{https://terpconnect.umd.edu/CalibrationSpreadsheets\%20/CalibrationCubic5Points.xls}{CalibrationCubic5Points.xls} for an example. 

\subsection{Application to analytical calibration and measurement\label{ref-0237}}

There are \href{https://terpconnect.umd.edu/~toh/models/CalibrationCurve.html}{specific versions of these spreadsheets} that also calculate the concentrations of the unknowns (download complete set as \href{http://terpconnect.umd.edu/~toh/models/CalibrationSpreadsheets.zip}{CalibrationSpreadsheets.zip}. Of course, these spreadsheets can be used for just about any measurement calibration application; just change the labels of the columns and axes to suit your application. A typical application of these spreadsheet templates to XRF (X-ray fluorescence) analysis is shown in this YouTube video:\href{https://www.youtube.com/watch?v=U3kzgVz4HgQ}{ https://www.youtube.com/watch?v=U3kzgVz4HgQ}

There is also another \href{http://terpconnect.umd.edu/~toh/models/Bracket.html}{set of spreadsheets} that perform \href{https://en.wikipedia.org/wiki/Monte\_Carlo\_method}{Monte Carlo simulations} of the calibration and measurement process using several widely-used analytical calibration methods, including first-order (straight line) and second-order (curved line) least-squares fits. Typical systematic and random errors in both signal and in volumetric measurements are included, for the purpose of demonstrating how non-linearity, interferences, and random errors combine to influence the result (the so-called "propagation of errors").

For fitting \textit{peaks}, \href{https://terpconnect.umd.edu/~toh/spectrum/GaussianLeastSquares.ods}{GaussianLeastSquares.}\href{http://terpconnect.umd.edu/~toh/spectrum/GaussianLeastSquares.ods}{odt}, is an OpenOffice spreadsheet that fits a quadratic function to the natural log of y(x) and computes the height, position, and width of the Gaussian that is the best fit to y(x). There is also an Excel version (\href{https://terpconnect.umd.edu/~toh/spectrum/GaussianLeastSquares.xls}{GaussianLeastSquares.xls}). \href{https://terpconnect.umd.edu/~toh/spectrum/LorentzianLeastSquares.ods}{LorentzianLeastSquares.ods} and \href{https://terpconnect.umd.edu/~toh/spectrum/LorentzianLeastSquares.xls}{LorentzianLeastSquares.xls} fits a quadratic function to the reciprocal of y(x) and computes the height, position, and width of the Lorentzian that is a best fit to y(x). Note that for either of these fits, the data may not contain zeros or negative points, and the baseline (the value that y approaches far from the peak center) must be zero. See \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#FittingPeaks}{Fitting Peaks}, above.

\subsection{Matlab and Octave\label{ref-0238}\label{ref-0239}}

\href{http://en.wikipedia.org/wiki/MATLAB}{\textbf{Matlab}} and \textbf{Octave} have simple built-in functions for least-squares curve fitting: \href{https://terpconnect.umd.edu/~toh/spectrum/polyfit.txt}{polyfit} and \href{https://terpconnect.umd.edu/~toh/spectrum/polyval.txt}{polyval}. For example, if you have a set of x,y data points in the vectors "x" and "y", then the coefficients for the least-squares fit are given by \texttt{coef=polyfit(x,y,n)}, where "n" is the order of the polynomial fit: n = 1 for a straight-line fit, 2 for a quadratic (parabola) fit, etc. The polynomial coefficients 'coef" are given in decreasing powers of x. For a straight-line fit (n=1), \texttt{coef(1}) is the slope ("b") and \texttt{coef(2)} is the intercept ("a"). For a quadratic fit (n=2), \texttt{coef(1)} is the x\textsuperscript{2} term ("c"), \texttt{coef(2)} is the x term ("b") and \texttt{coef(3)} is the constant term ("a"). 

The fit equation can be evaluated using the function \href{https://terpconnect.umd.edu/~toh/spectrum/polyval.txt}{polyval}, for example, \texttt{fity=polyval(coef,x)}. This works for any order of polynomial fit ("n"). You can plot the data and the fitted equation together using the \texttt{plot} function: \texttt{plot(x,y,'ob',x,polyval(coef,x),'-r')}, which plots the data as blue circles and the fitted equation as a red line. You can plot the residuals by writing \texttt{plot(x,y-polyval(coef,x))}. 

When the number of data points is small, you might notice that the fitted curve is displayed as a series of straight-line segments, which can look ugly. You can get a smoother plot of the fitted equation, evaluated at more finely divided values of x, by defining \texttt{xx=}\href{https://terpconnect.umd.edu/~toh/spectrum/linspace.txt}{\texttt{linspace}}\texttt{(min(x),max(x));} and then using xx rather than x to evaluate and plot the fit: \texttt{plot(x,y,'ob',xx,polyval(coef,xx),'-r')}. 

\texttt{[coef,S] = polyfit(x,y,n)} returns the polynomial coefficients coef and a \href{http://www.mathworks.com/help/matlab/structures.html}{structure }'S' used to obtain \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Algebraic}{error estimates}. 

\texttt{{\textgreater}{\textgreater} [coef,S]=polyfit(x,y,1)}

\texttt{coef =}

  \texttt{1.4913 6.5552}

\texttt{S =} 

  \texttt{R: [2x2 double]}

  \texttt{df: 2}

  \texttt{normr: 2.2341}

\texttt{{\textgreater}{\textgreater} S.R}

\texttt{ans =}

  \texttt{-18.4391 -1.6270}

  \texttt{0 -1.1632}

The vector of standard deviations of the coefficients \href{http://www.mathworks.com/matlabcentral/newsreader/view\_thread/103667}{can be computed from S by} the expression \texttt{sqrt(diag(inv(S.R)*inv(S.R')).*S.normr.\textasciicircum{}2./S.df)'}, in the same order as the coefficients. 

\textbf{Matrix Method}

Alternatively, you may perform the polynomial least-squares calculations for the row vectors x,y \textit{without} using the Matlab/Octave built-in polyfit function by using the \href{https://en.wikipedia.org/wiki/Linear\_least\_squares\_\%28mathematics\%29}{matrix method }with the Matlab "/" symbol, meaning "right matrix divide". The coefficients of a first-order fit are given by \texttt{y/[x;ones(size(y}\texttt{))]} and a second-order (quadratic) fit by \texttt{y/[x.\textasciicircum{}2;x;ones(size(y))]} . For higher-order polynomials, just add another row to the denominator matrix, for example, a third-order fit would be \texttt{y/[x.\textasciicircum{}3;x.\textasciicircum{}2;x;ones(size(y))]} and so on. The coefficients are returned in the same order as polyfit, in decreasing powers of x (e.g., for a first-order fit, \textit{slope} first (the x\textasciicircum{}1 term) then \textit{intercept} (the x\textasciicircum{}0 term). Using the example of the first-order fit to the SD memory card prices:

\texttt{{\textgreater}{\textgreater}x=[2 4 8 16];}

\texttt{{\textgreater}{\textgreater} y=[9.99 10.99 19.99 29.99];}

\texttt{{\textgreater}{\textgreater} polyfit(x,y,1)}

\texttt{ans =}

  \texttt{1.4913 6.5552}

\texttt{{\textgreater}{\textgreater} y/[x;ones(size(y))]}

\texttt{ans =}

  \texttt{1.4913 6.5552}

which shows that the \textit{slope and intercept results for the polyfit function and for the matrix method are the same}. (The slope and intercept results are the same, but the polyfit function has the advantage that it also can compute the \textit{error estimates} with little extra effort, as described above (page \pageref{ref-0203}). 

\textbf{The plotit.m} \textbf{function}

The graph on the next page was generated by my Matlab/ Octave function \href{https://terpconnect.umd.edu/~toh/spectrum/plotit.m}{plotit(data)} or \href{https://terpconnect.umd.edu/~toh/spectrum/plotit.m}{plotit(data,polyorder)}, that uses \textit{all the techniques mentioned in the previous} \InsImageInline{0.5}{l}{polyfitdemo.GIF.png}\textit{paragraph}. It accepts 'data' in the form of a single vector, or a pair of vectors "x" and "y", or a 2xn or nx2 matrix with x in the first row or column and y in the second, and plots the data points as red dots.

If the optional input argument "polyorder" is provided, plotit fits a polynomial of order "polyorder" to the data and plots the fit as a green line and displays the fit coefficients and the goodness-of-fit measure R\textsuperscript{2} in the upper left corner of the graph.

Here is a Matlab/Octave example of the use of plotit.m to perform the coordinate transformation described, on page \pageref{ref-0227}, to fit an exponential relationship, showing both the original exponential data and the transformed data with a linear fit in the \href{https://terpconnect.umd.edu/~toh/spectrum/CoordinateTransformation2.png}{figure(2)} and \href{https://terpconnect.umd.edu/~toh/spectrum/CoordinateTransformation1.png}{figure(1)} windows, respectively. If you are reading this online, \href{https://terpconnect.umd.edu/~toh/spectrum/expexample.m}{click to download}. 

\texttt{x=1:.1:2.6;} 

\texttt{a=1;}

\texttt{b=-.9;}

\texttt{y=a.*exp(b.*x);}

\texttt{y=y+y.*.1.*rand(size(x));} 

\texttt{figure(1)}

\texttt{[coeff,R2]=plotit(x,log(y),1);}

\texttt{ylabel('}\texttt{\textcolor{color-16}{ln(y)}}\texttt{');}

\texttt{title(}\texttt{\textcolor{color-16}{'Plot of x vs the natural log (ln) of y}}\texttt{')}

\texttt{aa=exp(coeff(2));}

\texttt{bb=coeff(1);}

\texttt{yy= aa.*exp(bb.*x);}

\texttt{figure(2)}

\texttt{plot(x,y,'}\texttt{\textcolor{color-16}{r.}}\texttt{',x,yy,'}\texttt{\textcolor{color-16}{g}}\texttt{')}

\texttt{xlabel('x');}

\texttt{ylabel('y');}

\texttt{title(['y = a*exp(b*x)} \texttt{\textcolor{color-16}{a =}} \texttt{' num2str(aa) '} \texttt{\textcolor{color-16}{b =}} \texttt{' num2str(bb)} \texttt{\textcolor{color-16}{' R2 =}} \texttt{' num2str(R2) ] ) ;}

In version 5 or 6 the syntax of plotit can be [\texttt{coef, RSquared, StdDevs] =plotit(x,y,}n\texttt{)}. It returns the best-fit coefficients '\texttt{coeff}', in decreasing powers of x, the standard deviations of those coefficients '\texttt{StdDevs'} in the same order, and the R-squared value. To compute the \textit{relative} standard deviations, just type \texttt{StdDevs}\texttt{./coef}\texttt{.} For example, the following script computes a straight line with five data points and a slope of 10, an intercept of zero, and noise equal to 1.0. It then uses plotit.m to plot and fit the data to a first-order linear model (straight line) and compute the estimated standard deviation of the slope and intercept, if you run this repeatedly, you will observe that the measured slope and intercept are usually within two standard deviations of 10 and zero respectively. Try it with different values of Noise. 

\texttt{NumPoints=5;}

\texttt{slope=10;}

\texttt{Noise=1;}

\texttt{x=round(10.*rand(size(1:NumPoints)));}

\texttt{y=slope*x+Noise.*randn(size(x));}

\texttt{[coef,RSquared,StdDevs]=plotit(x,y,1)} 

\textbf{Comparing two data sets}. Plotit can also be used to compare to two different dependent variable vectors (e.g., y1 and y2) \textit{if they share the same independent variables x}, for example to determine the similarity of two different spectra measured over the same wavelengths as was done on page \pageref{ref-0003}: \texttt{[coeff,R2]=plotit(y1,y2,1);} 

R\textsubscript{2} is a measure of similarity. The closer R\textsuperscript{2} is to 1.000, the more similar they are. If y1 and y2 are two measurements of the \textit{same} signal with different random noise, the plot will show a random scatter of points along a straight line with a slope, coeff(1), of 1.00. If the y1 and y2 are the same signal with different amplitudes, the slope of the line will equal their average ratio. If the data points are curved and loop around, the difference between the two y vectors is greater than the random noise.

In \textbf{version 6} the syntax can be optionally \texttt{plotit(x,y,}\texttt{n,datastyle,fitstyle}\texttt{)}, where \texttt{datastyle} and \texttt{fitstyle} are optional strings specifying the line and symbol style and color, in standard Matlab convention. The strings, in single quotes, are made from one element from any or all the following 3 columns:

\texttt{b blue . point - solid}

\texttt{g green o circle : dotted}

\texttt{r red x x-mark -. dashdot} 

\texttt{c cyan + plus -- dashed} 

\texttt{m magenta * star (none) no line}

\texttt{y yellow s square}

\texttt{k black d diamond}

\texttt{w white v triangle (down)}

    \texttt{\textasciicircum{} triangle (up)}

        \texttt{{\textless} triangle (left)}

  \texttt{{\textgreater} triangle (right)}

  \texttt{p pentagram}

    \texttt{h hexagram}

For example, \texttt{plotit(x,y,3,'or','-g')} plots the data as red circles and the fit as a green solid line (the default is red dots and a blue line, respectively). 

You can use plotit.m in Matlab to linearize and plot other \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Transforming}{\textbf{nonlinear relationships}}, such as: 

y = \textbf{a} exp(\textbf{b}x) \texttt{:} \texttt{[coeff,R2]=plotit(x,log(y),1);} \texttt{\textbf{a}}\texttt{=exp(coeff(2))}\texttt{\textbf{; b}}\texttt{=coeff(1);} 

y = \textbf{a} ln(\textbf{b}x) : \texttt{[coeff,R2]=plotit(log(x),y,1);} \texttt{\textbf{a}}\texttt{=coeff(1)}\texttt{\textbf{; b}}\texttt{=log(coeff(2));}

y=\textbf{a}x\textsuperscript{\textbf{b}} : \texttt{[coeff,R2]=plotit(log(x),log(y),1)}\texttt{\textbf{; a}}\texttt{=exp(coeff(2))}\texttt{\textbf{; b}}\texttt{=coeff(1);}

y=\textbf{start}(1+\textbf{rate})\textsuperscript{x}: \texttt{[coeff,R2]=plotit(x,log(y),1); start=exp(coeff(2)); rate=exp(coeff(1))-1;} 

This last one is the expression for \textit{compound interest,} covered on page \pageref{ref-0391}: \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#StockMarket}{Signal and Noise in the Stock Market}. 

Do not forget that in Matlab/Octave, "log" means \textit{natural log}; the \textit{base-10} log is denoted by "log10".

\textbf{Estimating the coefficient errors}. The \href{https://terpconnect.umd.edu/~toh/spectrum/plotit.m}{plotit }function version 2 also has a built-in \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Bootstrap}{\textit{bootstrap routine}} that computes coefficient error estimates by the bootstrap method (Standard deviation STD and relative standard deviation RSD) and returns the results in the matrix "BootResults" (of size 5 x polyorder+1). You can change the number of bootstrap samples in line 101. The calculation is triggered by including a 4th \textit{output} argument, e.g. 

\texttt{[coef, RSquared, StdDevs, BootResults]= plotit(x,y,polyorder)}. 

This works for any polynomial order. For example:

\texttt{{\textgreater}{\textgreater} x=0:100;}

\texttt{{\textgreater}{\textgreater} y=100+(x*100)+100.*randn(size(x));}

\texttt{{\textgreater}{\textgreater} [FitResults, GOF, baseline, coeff, residual, xi, yi, BootResults] = plotit(x,y,1);} 

\InsImageInline{0.5}{l}{BootStrapQuadratic.gif.png}The above statements compute a straight line with an intercept and slope of 100, plus random noise with a standard deviation of 100, then fits a straight line to that data and prints out a table of bootstrap error estimates, with the slope in the first column and the intercept in the second column: 

\texttt{Bootstrap Results}

\texttt{Mean: 100.359 88.01638}

\texttt{STD: 0.204564 15.4803}

\texttt{STD (IQR): 0.291484 20.5882}

\texttt{\% RSD: 0.203832 17.5879}

\texttt{\% RSD (IQR): 0.290441 23.3914}

The variation \href{https://terpconnect.umd.edu/~toh/spectrum/plotfita.m}{plotfita} \textit{animates} the bootstrap process for instructional purposes, \href{https://terpconnect.umd.edu/~toh/spectrum/BootStrapQuadratic.gif}{as shown in the animation on the right} for a quadratic fit. You must include the output arguments, for example:

\texttt{[coef, RSquared, BootResults]=plotfita([1 2 3 4 5 6],[1 3 4 3 2 1],2);}

\InsImageInline{0.5}{l}{trypoly.png}The variation \href{https://terpconnect.umd.edu/~toh/spectrum/logplotfit.m}{logplotfit} plots and fits log(x) vs log(y), for data that follows a \href{http://en.wikipedia.org/wiki/Power\_law}{power-law relationship} or that covers a very wide numerical range. 

\textbf{Comparing polynomial orders.} My function \href{https://terpconnect.umd.edu/~toh/spectrum/trypoly.m}{trypoly(x,y)} fits the data in x,y with a series of polynomials of degree 1 through length(x)-1 and returns the coefficients of determination (R2) of each fit as a vector, showing that, for \textit{any} data, the coefficient of determination R\textsuperscript{2 }approaches 1 as the polynomial order approaches length(x)-1. The variant \href{https://terpconnect.umd.edu/~toh/spectrum/trypolyplot\%28x,y\%29.m}{trypolyplot(x,y)} creates a bar graph such as shown on the left.

\textbf{Comparing data transformations.} The function \href{https://terpconnect.umd.edu/~toh/spectrum/trydatatrans.m}{trydatatrans(x, y, polyorder)} tries 8 different simple data transformations on the data x,y, fits the transformed data to a polynomial of order 'polyorder', displays results \href{https://terpconnect.umd.edu/~toh/spectrum/trydatatrans.png}{graphically in 3 x 3 array of small plots} and returns the R\textsuperscript{2 }values in a vector. In the example below, for polyorder=1, it is the 5\textsuperscript{th} one that is best \textendash{} x vs ln(y).

\InsImageInline{0.5}{l}{image165.png}\subsection{Fitting Single Gaussian and Lorentzian peaks\label{ref-0240}}

A simple user-defined Matlab/Octave function that fits a single Gaussian function to an x,y signal is \href{https://terpconnect.umd.edu/~toh/spectrum/gaussfit.m}{gaussfit.m}, which implements the x vs ln(y) quadratic fitting method \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#FittingPeaks}{described above}. It takes the form \texttt{[Height,Position,Width]=gaussfit(x,y)}. For example, 

\texttt{{\textgreater}{\textgreater} x=50:150;}

\texttt{{\textgreater}{\textgreater} y=100.*gaussian(x,100,100)+10.*randn(size(x));}

\texttt{{\textgreater}{\textgreater} [Height,Position,Width]=gaussfit(x,y)} 

returns [Height,Position,Width] clustered around 100,100,100. A similar function for Lorentzian peaks is \href{https://terpconnect.umd.edu/~toh/spectrum/lorentzfit.m}{lorentzfit.m}, which takes the form 

\texttt{[Height,Position,Width]=lorentzfit(x,y)}.

An expanded variant of the gaussfit.m function is \href{https://terpconnect.umd.edu/~toh/spectrum/bootgaussfit.m}{bootgaussfit.m}, which does the same thing but also optionally plots the data and the fit and computes estimates of the random error in the height, width, and position of the fitted Gaussian function by the bootstrap sampling method. For example:

\texttt{{\textgreater}{\textgreater} x=50:150;}

\texttt{{\textgreater}{\textgreater} y=100.*gaussian(x,100,100)+10.*randn(size(x));}

\texttt{{\textgreater}{\textgreater} [Height, Position, Width, BootResults]=bootgaussfit(x,y,1);}

This does the same as the previous example but also displays error estimates in a table and returns the 3x5 matrix \texttt{BootResults}. Type "help bootgaussfit" for help.

  \texttt{Height Position Width} 

\texttt{Bootstrap Mean: 100.84 101.325 98.341}

\texttt{Bootstrap STD: 1.3458 0.63091 2.0686}

\texttt{Bootstrap IQR: 1.7692 0.86874 2.9735}

\texttt{Percent RSD: 1.3346 0.62266 2.1035}

\texttt{Percent IQR: 1.7543 0.85737 3.0237}

\textit{It is important that the noisy signal not be} \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#NOT\_smooth}{\textit{smoothed}} if the bootstrap error predictions are to be accurate. Smoothing causes the bootstrap method to seriously underestimate the precision of the results.

The gaussfit.m and lorentzfit.m functions are simple and easy, but they do not work well with very noisy peaks or for multiple overlapping peaks. As a demonstration, \href{https://terpconnect.umd.edu/~toh/spectrum/OverlappingPeaks.m}{OverlappingPeaks.m} is a demo script that shows how to use gaussfit.m to measure \href{https://terpconnect.umd.edu/~toh/spectrum/OverlappingPeaks.png}{two overlapping partially Gaussian peaks}. It requires careful selection of the optimum data regions around the top of each peak. Try changing the relative position and height of the second peak or adding noise (line 3) and see how it affects the accuracy. This function needs the gaussian.m, gaussfit.m, and peakfit.m functions in the path. The script also performs measurements by the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{iterative method} (page \pageref{ref-0258}) using peakfit.m, which is \href{https://terpconnect.umd.edu/~toh/spectrum/OverlappingPeaks.txt}{\textit{more accurate} but \textit{takes about times longer} to compute}.

My Matlab-only functions \href{https://terpconnect.umd.edu/~toh/spectrum/isignal.html}{iSignal.m} (page \pageref{ref-0434}) and \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{ipf.m} (page \pageref{ref-0461}), whose principal functions are fitting \textit{peaks}, also have a function for fitting \textit{polynomials} of any order (\textbf{Shift-o}).

Recent versions of Matlab have a convenient tool for interactive manually-controlled (rather than programmed) polynomial curve fitting in the Figure window. If you are reading this online, click for a video example: \href{http://www.youtube.com/watch?v=EypwejBhN34}{(external link to YouTube)}.

The \textit{Matlab Statistics Toolbox} includes two types of bootstrap functions, "\href{https://terpconnect.umd.edu/~toh/spectrum/bootstrp.txt}{bootstrp}" and "\href{https://terpconnect.umd.edu/~toh/spectrum/jackknife.txt}{jackknife}". To open the reference page in Matlab's help browser, type "doc bootstrp" or "doc jackknife".\label{ref-0241}

\chapter{Curve fitting B: Multicomponent Spectroscopy\label{ref-0242}\label{ref-0243}\label{ref-0244}}

The spectroscopic analysis of mixtures, when the spectrum of the mixture is the simple sum of the spectra of known components that may overlap but are not identical, can be performed using special calibration methods based on a type of linear least-squares called \textit{multiple linear regression}. This method is widely used in multi-wavelength instruments such as diode-array, Fourier transform, and digitally controlled scanning spectrometers (because perfect wavelength reproducibility is a key requirement). To understand the required math, it is helpful to do a little basic \href{http://en.wikipedia.org/wiki/Matrix\_\%28mathematics\%29}{matrix algebra} (a.k.a., linear algebra), which is just a shorthand notation for dealing with signals expressed as equations with one term for each point. Because that area of math may not be a part of everyone's math background, I will do some elementary matrix math derivations in this section. 

\textbf{Definitions:} 

n = number of distinct chemical components in the mixture

s = number of samples

s1, s2 = sample 1, sample 2, etc.

c = molar concentration

c1, c2 = component 1, component 2, etc.

w = number of wavelengths at which signal is measured

w1, w2 = wavelength 1, wavelength 2, etc.

Ɛ = analytical sensitivity (slope of a plot of A vs c)

A = analytical signal

\textbf{M}\textsuperscript{T} = matrix transpose of matrix \textbf{M} (rows and columns switched).

\textbf{M}\textsuperscript{-1} = \href{http://mathworld.wolfram.com/MatrixInverse.html}{matrix inverse} of matrix \textbf{M}.

\textbf{Assumptions:} 

The analytical signal, A (such as absorbance in absorption spectroscopy, fluorescence intensity in fluorescence spectroscopy, and reflectance in reflectance spectroscopy) is directly proportional to concentration, c. The proportionality constant, which is the slope of a plot of A vs c, is Ɛ.


\begin{center}
A = Ɛc
\end{center}


The total signal is the sum of the signals for each component in a mixture: 

A\textsubscript{total} = A\textsubscript{c1} + A\textsubscript{c2} + ... for all \textit{n} components.

\section{Classical Least-squares (CLS) multivariate calibration\label{ref-0245}\label{ref-0246}\label{ref-0247}\label{ref-0248}\label{ref-0249}}

This method is applicable to the quantitative analysis of a mixture of components you can measure the spectra of the individual components and where the total signal of the mixture is simply the sum of the signals for each component in the mixture. Measurement of the spectra of known concentrations of the separate components allows their analytical sensitivity Ɛ at each wavelength to be determined. Then it follows that:

A\textsubscript{w1}=Ɛ\textsubscript{c1,w1} c\textsubscript{c1} + Ɛ\textsubscript{c2,w1} c\textsubscript{c2} + Ɛ\textsubscript{c3,w1} \textbf{c}\textsubscript{c3} + ... for all \textit{n} components.

A\textsubscript{w2}=Ɛ\textsubscript{c1,w2} \textbf{c}\textsubscript{c1} + Ɛ\textsubscript{c2,w2} \textbf{c}\textsubscript{c2} + Ɛ\textsubscript{c3,w2} \textbf{c}\textsubscript{c3} + ...

and so on for all wavelengths - w3, w4, etc. It is messy to write out all these individual terms, especially because there may be \textit{hundreds} of wavelengths in modern array-detector spectrometers. Moreover, despite the mass of raw data, these are just nothing more than linear equations and so the calculations required here are rather simple and certainly very easy for a computer to do. So, \textit{we really need a correspondingly simple notation} that is more compact. To do this, it is conventional to use \textbf{bold-face letters} to represent a \textit{vector} (like a column or row of numbers in a spreadsheet) or a \textit{matrix} (like a \textit{block} of numbers in a spreadsheet). For example, \textbf{A} could represent the list of absorbances at each wavelength in an absorption spectrum\index{absorption spectrum}. So, this big set of linear equations above can be written:

 \textbf{A} = \textbf{ƐC}

where \textbf{A} is the \textit{w}-length vector of measured signals (i.e., the signal spectrum) of the mixture, \textbf{Ɛ} is the \textit{n} ${\times}$ \textit{w} rectangular matrix of the known \textbf{Ɛ}-values for each of the \textit{n} components at each of the \textit{w} wavelengths, and \textbf{C} is the \textit{n}-length vector of concentrations of all the components. \textbf{ƐC} means that \textbf{Ɛ} ``pre-multiplies'' \textbf{C}; that is, \textit{each column of} \textbf{\textit{Ɛ}} \textit{is multiplied point-by-point} by the vector \textbf{C.} 

If you have a sample solution containing unknown amounts of components those n components, you measure its spectrum \textbf{A} and seek to calculate the concentration vector of concentrations \textbf{C}. To solve the above matrix equation for \textbf{C}, the number of wavelengths \textit{w} must be equal to or greater than the number of components \textit{n}. If \textit{w} = \textit{n}, then we have a system of \textit{n} equations in \textit{n} unknowns which can be solved by pre-multiplying both sides of the equation by \textbf{Ɛ}\textsuperscript{-1}, the \href{http://mathworld.wolfram.com/MatrixInverse.html}{matrix inverse} of \textbf{Ɛ}, and using the property that any matrix times its inverse is unity:


\begin{center}
 \textbf{C} = \textbf{Ɛ}\textsuperscript{${-}$1}\textbf{A} 
\end{center}


Because real experimental spectra are subject to random noise (e.g., photon noise and detector noise), the solution will be more precise if the signals at a larger number of wavelengths are used, i.e., if \textit{w} {\textgreater} \textit{n}. This is easily done with no increase in labor by using a modern \href{https://www.google.com/search?ei=RR\_2W\_jwPKSu5wK8h7f4AQ&q=array-detector+spectrophotometer&oq=array-detector+spectrophotometer&gs\_l=psy-ab.3..0i22i30.10797559.10797559..10798015...0.0..0.53.53.1......0....1j2..gws-wiz.......0i71.\_4UZ6NMhc\_Q}{\textit{array-detector} spectrophotometer}. But then the equation cannot be solved by simple matrix inversion, because the \textbf{Ɛ} matrix is a \textit{w} ${\times}$ \textit{n} matrix and \textit{a matrix inverse exists only for square matrices}. However, a solution can be obtained in this case by pre-multiplying both sides of the equation by the expression (\textbf{Ɛ}\textsuperscript{T}\textbf{Ɛ})\textsuperscript{-1}\textbf{Ɛ}\textsuperscript{T}:


\begin{center}
(\textbf{Ɛ}\textsuperscript{T}\textbf{Ɛ})\textsuperscript{-1}\textbf{Ɛ}\textsuperscript{T}\textbf{A} = (\textbf{Ɛ}\textsuperscript{T}Ɛ)\textsuperscript{-1}\textbf{Ɛ}\textsuperscript{T}\textbf{ƐC} = (\textbf{Ɛ}\textsuperscript{T}\textbf{Ɛ})\textsuperscript{-1}(\textbf{Ɛ}\textsuperscript{T}\textbf{Ɛ})\textbf{C}
\end{center}


where \textbf{Ɛ}\textsuperscript{T }is the \textit{transpose} of \textbf{Ɛ} (rows and columns switched). But the quantity (\textbf{Ɛ}\textsuperscript{T}\textbf{Ɛ})\textsuperscript{-1}(\textbf{Ɛ}\textsuperscript{T}\textbf{Ɛ}) is a matrix multiplied by its inverse and is therefore unity. Thus, we can simplify the result to:


\begin{center}
\textbf{C} = (\textbf{Ɛ}\textsuperscript{T}\textbf{Ɛ})\textsuperscript{-1}\textbf{Ɛ}\textsuperscript{T}\textbf{A}
\end{center}


In this expression, \textbf{Ɛ}\textsuperscript{T}\textbf{Ɛ} is a square matrix of order \textit{n}, the number of components. In most practical applications, \textit{n}, the number of chemical components, is relatively small, perhaps only 2 to 5. The vector \textbf{A} is of length \textit{w}, the number of wavelengths. That can be quite large, perhaps several hundred; in general, the more wavelengths are used, the more effectively the random noise will be averaged out (although it will not help to use wavelengths in spectral regions where none of the components produce analytical signals). The determination of the optimum wavelength region must usually be determined empirically. All components that contribute to the spectrum must be accounted for and included in the \textbf{Ɛ} matrix.

Three extensions of the CLS method are commonly made:\begin{enumerate}[{(a)}]


\item If you have s multiple unknown samples to measure, you can compute them all at once using the same notation as above, by combining their spectra into a \textit{w} ${\times}$ \textit{s} matrix \textbf{A}, which will result in an \textit{n} ${\times}$ \textit{s} matrix \textbf{C}. (This is explored in the Appendix: "Spectroscopy and chromatography combined: time-resolved Classical Least-squares" on page \pageref{ref-0427}). 

\item  To account for the baseline shift caused by drift, background, and light scattering, a column of 1s is added to the \textbf{Ɛ} matrix. This has the effect of introducing into the solution an additional component with a flat spectrum; this is referred to as ``background correction''. \label{ref-0250}

\item  To account for the fact that the precision of measurement may vary with wavelength, it is common to perform a \textit{weighted} least-squares solution that de-emphasizes wavelength regions where precision is poor:

\item  \textbf{C} = (\textbf{Ɛ}\textsuperscript{T} \textbf{Ɛ}\textsuperscript{-1}\textbf{Ɛ})\textsuperscript{-1} \textbf{Ɛ}\textsuperscript{T} \textbf{V}\textsuperscript{-1 }\textbf{A}

\end{enumerate}
where \textbf{V} is a \textit{w} ${\times}$ \textit{w} diagonal matrix of variances at each wavelength. In absorption spectroscopy, where the precision of measurement is poor in spectral regions where the absorbance is very high (and the light level and signal-to-noise ratio therefore low), it is common to use the transmittance T or its square T\textsuperscript{2} as weighting factors. 

The method is in principle applicable to any number of overlapping components. Its accuracy is limited by how accurately the spectra of the individual components are known, the amount of noise in the signal, the extent of overlap of the spectra, and the linearity of the analytical curves of each component (the extent to which the signal amplitudes are proportional to concentration). In practice, the method does not work well with old-style instruments with manual wavelength control, because of insufficient wavelength reproducibility. Specifically, many measurements are made on the \textit{sides} of spectral bands, where \textit{even small failures in the reproducibility of wavelength settings between measurements would result in large intensity changes}. However, it works with automated computer-controlled scanning instruments and is especially well suited to diode-array and Fourier transform instruments, which have extremely good wavelength reproducibility. The method also depends on the linearity of analytical signal with respect to concentration. The well-known \href{http://terpconnect.umd.edu/~toh/models/BeersLaw.html}{deviations from analytical curve linearity} in absorption spectrophotometry set a limit to the performance to this method, but that can be avoided by applying iterative \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{curve fitting} (page \pageref{ref-0258}) and \href{https://terpconnect.umd.edu/~toh/spectrum/Convolution.html}{Fourier convolution} (page \pageref{ref-0139}) to the transmission spectra, an idea that idea will be developed later, \href{https://terpconnect.umd.edu/~toh/spectrum/TFit.html}{on} page \pageref{ref-0333}.

\section{Inverse Least-squares (ILS) calibration\label{ref-0251}\label{ref-0252}}

ILS is a method that can be used to measure the concentration of an analyte in samples in which the spectrum of the analyte in the sample is not known beforehand. Whereas the classical least-squares method models the signal at each wavelength as the sum of the concentrations of the analyte times the analytical sensitivity, the inverse least-squares methods use the inverse approach and models the analyte concentration c in each sample as the sum of the signals A at each wavelength times calibration coefficients m that express how the concentration of that component is related to the signal at each wavelength:

 c\textsubscript{s1} = m\textsubscript{w1}A\textsubscript{s1,w1} + m\textsubscript{w2}A\textsubscript{s1,w2}+ m\textsubscript{w3}A\textsubscript{s1,w3} + ... for all \textit{w} wavelengths.

 c\textsubscript{s2} = m\textsubscript{w1}A\textsubscript{s2,w1} + m\textsubscript{w2}A\textsubscript{s2,w2} + m\textsubscript{w3}A\textsubscript{s2,w3} + ...

and so on for all \textit{s} samples. In matrix form


\begin{center}
\textbf{C} = \textbf{AM}
\end{center}


where \textbf{C} is the \textit{s}-length vector of concentrations of the analyte in the \textit{s} samples, \textbf{A} is the \textit{w} ${\times}$ \textit{s} matrix of measured signals at the \textit{w} wavelengths in the \textit{s} samples, and \textbf{M} is the \textit{w}-length vector of calibration coefficients. 

Now, suppose that you have a set of standard samples that are typical of the type of sample that you wish to be able to measure and which contain a range of analyte concentrations that span the range of concentrations expected to be found in other samples of that type. This will serve as the \textit{calibration set.} You measure the spectrum of each of the samples in this calibration set and put these data into a \textit{w} ${\times}$ \textit{s} matrix of measured signals \textbf{A}. You then measure the analyte concentrations in each of the samples \textit{by some reliable and independent analytical method} and put those data into a \textit{s}-length vector of concentrations \textbf{C.} Together these data allow you to calculate the calibration vector \textbf{M} by solving the above equation. If the number of samples in the calibration set is greater than the number of wavelengths, the least-squares solution is:

\textbf{M} = (\textbf{A}\textsuperscript{T}\textbf{A})\textsuperscript{${-}$1}\textbf{A}\textsuperscript{T}\textbf{C} 

(Note that \textbf{A}\textsuperscript{T}\textbf{A} is a square matrix of size \textit{w}, the number of wavelengths, which must be less than \textit{s}). This calibration vector can be used to compute the analyte concentrations of other samples which are similar to those in the calibration set, from the measured spectra of the samples:


\begin{center}
\textbf{C} = \textbf{AM}
\end{center}


Clearly, this will work well only if the analytical samples are similar in composition to the calibration set. The advantage of this method is that the spectrum of an unknown sample can be measured much more quickly and cheaply than the more laborious standard reference methods that are used to measure the calibration set, but if the unknowns are similar enough to the calibration set, the concentrations calculated by the above equation will be accurate enough for many purposes.

\section{Computer software for multiwavelength spectroscopy\label{ref-0253}}

\subsection{Spreadsheets\label{ref-0254}\label{ref-0255}}

Most modern spreadsheets have basic matrix manipulation capabilities and can be used for multi-component calibration, for example \href{http://www.stanford.edu/~wfsharpe/mia/mat/mia\_mat4.htm\#operations}{Excel} and \href{http://www.openofficetips.com/blog/archives/2004/10/array\_formulas.html}{OpenOffice Calc}. The spreadsheets \href{https://terpconnect.umd.edu/~toh/spectrum/RegressionDemo.xls}{RegressionDemo.xls} and \href{https://terpconnect.umd.edu/~toh/spectrum/RegressionDemo.ods}{RegressionDemo.ods} (for Excel and Calc, respectively) demonstrate the classical least-squares procedure for a simulated spectrum of a 5-component mixture measured at 100 wavelengths. A screenshot is shown below. The matrix calculations described above that solves for the concentration of the components on the unknown mixture:


\begin{center}
\textbf{C} = (\textbf{Ɛ}\textsuperscript{T}\textbf{Ɛ})\textsuperscript{-1}\textbf{Ɛ}\textsuperscript{T}\textbf{A}
\end{center}


are performed in these spreadsheets by the TRANSPOSE (matrix transpose), MMULT (matrix multiplication), and MINVERSE (matrix inverse) array functions, laid out step-by-step in \href{https://terpconnect.umd.edu/~toh/spectrum/RegressionSteps.jpg}{rows 123 to 158 of this spreadsheet}. Alternatively, all these array operations may be combined into one big scary cell equation: 

\textbf{C} = MMULT (MMULT (MINVERSE (MMULT (TRANSPOSE (\textbf{E});\textbf{E})); TRANSPOSE (\textbf{E}));\textbf{A})

where \textbf{C} is the vector of the 5 concentrations of all the components in the mixture, \textbf{E} is the 5 ${\times}$ 100 rectangular matrix of the known sensitivities (e.g. absorptivities) for each of the 5 components at each of the 100 wavelengths, and \textbf{A} is the vector of measured signals at each of the 100 wavelengths (i.e. the signal spectrum) of the unknown mixture. (Note: spreadsheet array functions like this must be entered by typing \textbf{Ctrl-Shift-Enter}, not just \textbf{Enter} as usual. See "\href{https://support.office.com/en-us/article/guidelines-and-examples-of-array-formulas-7d94a64e-3ff3-4686-9372-ecfd5caa57c7}{Guidelines and examples of array formulas}".


\begin{center}
\InsImageInline{0.5}{l}{RegressionDemoSpreadsheet.gif.png}.
\end{center}



\begin{center}
\textit{OpenOffice Calc spreadsheet demonstrating the CLS procedure for the measurement of a 5-component unknown mixture at 100 wavelengths.}
\end{center}


Alternatively, you can skip over all the details above and use the built-in \textbf{LINEST} function, in both \href{http://office.microsoft.com/en-001/excel-help/linest-HP005209155.aspx}{Excel} or \href{https://wiki.openoffice.org/wiki/Documentation/How\_Tos/Calc:\_LINEST\_function}{OpenOffice Calc}, which performs this type of calculation in a single function statement. This is illustrated in \href{https://terpconnect.umd.edu/~toh/spectrum/RegressionTemplate.xls}{RegressionTemplate.xls}, in cell Q23. (A slight modification of the function syntax, shown in cell Q32, performs a \textit{baseline-corrected} calculation). A significant advantage of the LINEST function is that it can automatically compute the standard errors of the coefficients and the R\textsuperscript{2} value in the same operation; using Matlab or Octave, which would require some extra work. (LINEST is also an array function that must also be entered by typing \textbf{Ctrl-Shift-Enter}, not just \textbf{Enter}). Note that this is the \textit{same} LINEST function that was previously used for \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html}{polynomial least-squares} (page \pageref{ref-0200}) the difference is that in polynomial least-squares, the multiple columns of x values are \textit{computed}, for example by taking the powers (squares, cubes, etc.) of the x's, whereas, in the multicomponent CLS method, the multiple columns of x values are \textit{experimental} spectra of the different standard solutions. The \textit{math} is the same, but the \textit{origin of the x data} is very different. 

A template for performing a 5-component analysis on your own data, with step-by-step instructions, is available as \href{https://terpconnect.umd.edu/~toh/spectrum/RegressionTemplate.xls}{RegressionTemplate.xls} and \href{https://terpconnect.umd.edu/~toh/spectrum/RegressionTemplate.ods}{RegressionTemplate.ods} (\href{https://terpconnect.umd.edu/~toh/spectrum/RegressionTemplate.png}{Graphic o}n the next page) from the demo above). Paste your own data into columns B - G. You must adjust the formulas if your number of data points or of components is different from this example. The easiest way to add more wavelengths is to select an entire row anywhere between row 40 and the end, right-click on the row number on the left and select \textbf{Insert}. That will insert a new blank row and will automatically adjust all the cell formulas (including the LINEST function) and the graph to include the new row. Repeat that as many times as needed. Finally, select the entire row just before the insertion (that is, the last non-blank row) and drag-copy in down to fill in all the new blank rows. Changing the number of components is more difficult: it involves inserting or deleting columns between C and G and between H and L, and adjusting the formulas in rows 15 and 16 and also in Q29-U29. 

Spreadsheets of this type, though easy to use once constructed, must be carefully modified for different applications that have different numbers of components or different numbers of wavelengths, which is inconvenient and can be error prone. However, it is possible to construct these spreadsheets in such a way that they \textit{automatically} adjust to any number of components or wavelengths. This is done by using two new functions: 

(a) the \href{https://support.office.com/en-us/article/count-function-a59cd7fc-b623-4d93-87a4-d23bf411294c}{COUNT }function in cells B18 and F18, which counts the number of wavelengths in column A and the number of components in row Q22-U22, respectively, and 

(b) the \href{https://support.office.com/en-us/article/indirect-function-474b3a3a-8a26-4f44-b491-92b6306fa261}{INDIRECT} function (see page \pageref{ref-0420}) in cell Q23 and in rows 12 and 13, which allows the address of a cell or range of cells to be \textit{calculated within the spreadsheet} (based on the number of wavelengths and components just counted) rather than using a fixed address range. \label{ref-0256}

This technique is used in \href{https://terpconnect.umd.edu/~toh/spectrum/RegressionTemplate2.xls}{RegressionTemplate2.xls} and in two examples showing the \textit{same template} with data entered for different numbers of wavelengths and for mixtures of 5 components at 100 wavelengths (\href{https://terpconnect.umd.edu/~toh/spectrum/RegressionTemplate2Example.xls}{RegressionTemplate2Example.xls}) and for 2 components at 59 wavelengths (\href{https://terpconnect.umd.edu/~toh/spectrum/RegressionTemplate3Example.xls}{RegressionTemplate3Example.xls}). If you inspect the LINEST functions in cell Q23, you will see that it is the same in both of those two example templates, even though the number of wavelengths and the number of components is different. You will still have to adjust the graph, however, to cover the desired x-axis range. See page \pageref{ref-0420}.


\begin{center}
\InsImageInline{0.5}{l}{RegressionTemplate.gif.png}
\end{center}



\begin{center}
\textit{Excel template applied to the measurement of a 5-component unknown mixture at 100 wavelengths}\textbf{\textit{.}}
\end{center}




\subsection{\textbf{Matlab and Octave } \label{ref-0257}}

Matlab and Octave are really the natural languages for multicomponent analysis because they handle all types of matrix math so easily, compactly, and quickly, and they readily adapt to any number of wavelengths or number of components without any special tricks. In these languages, the notation is very compact: the transpose of matrix A is A', the inverse of A is inv(A), and matrix multiplication is designated by an asterisk (*). Thus, the solution to the classical least-squares method above is written in Matlab/Octave notation as 


\begin{center}
\texttt{C = inv(E'*E)*E'*A}
\end{center}


where E is the rectangular matrix of sensitivities at each wavelength for each component and A is the observed spectrum of the mixture. Note that the Matlab/Octave notation is not only shorter than the spreadsheet notation, but also closer to the traditional mathematical notation. Even more compactly, you can write C = A/E, using the Matlab \href{http://matlab.izmiran.ru/help/techdoc/ref/mldivide.html}{forward slash or "right divide" operator}, which yields the same results but is in principle more accurate with respect to the numerical precision of the computer (usually negligible compared to the noise in the signal; see page \pageref{ref-0405}.

\InsImageInline{0.5}{l}{RegressionDemo.GIF.png}The script \href{http://www.wam.umd.edu/~toh/spectrum/RegressionDemo.m}{RegressionDemo.m} (for Matlab or Octave) demonstrates the classical least-squares procedure for a simulated absorption spectrum\index{absorption spectrum} of a 5-component mixture at 100 wavelengths, illustrated above. Most of this script is just signal generation and plotting; the actual least-squares regression is performed on one line:


\begin{center}
MeasuredAmp = ObservedSpectrum*A'*inv(A*A')
\end{center}


where different symbols are used for the variables: "A" is a matrix containing the spectrum of each of the components in each of its rows and "ObservedSprectum" is the observed spectrum of the unknown mixture. In this example, the dots represent the observed spectrum of the mixture (with noise) and the five colored bands represent the five components in the mixture, whose spectra are known but whose concentrations in the mixture are unknown. The black line represents the "best fit" to the observed spectrum calculated by the program. In this example, the concentrations of the five components are measured to an accuracy of about 1\% relative (limited by the noise in the observed spectrum). Comparing \href{http://www.wam.umd.edu/~toh/spectrum/RegressionDemo.m}{RegressionDemo.m} to its spreadsheet equivalent, \href{https://terpconnect.umd.edu/~toh/spectrum/RegressionDemo.ods}{RegressionDemo.ods}, both running the same computer, you can see that the Matlab/Octave code computes and plots the results quicker than the spreadsheet, although both take no more than a fraction of a second for this example. 

\textbf{Extensions}:

 \textbf{(a)} The extension to \textbf{multiple unknown samples}, each with its own "ObservedSpectrum" is straightforward in Matlab/Octave. If you have "s" samples, just assemble their observed spectra onto a \textit{matrix} with "s" rows and "w" columns ("w" is the number of wavelengths), then use the same formulation as before:

MeasuredAmp = ObservedSpectrum*A'*inv(A*A')

The resulting "MeasuredAmp" will be an "s" ${\times}$ "n" \textit{matrix} rather than an n-length vector ("n" is the number of measured components). This is a great example of the convenience of the vector/matrix nature of this language. (\href{https://terpconnect.umd.edu/~toh/spectrum/RegressionDemoMultipleSamples.m}{RegressionDemoMultipleSamples.m} demonstrates this).

\textbf{(b)} The extension to \textbf{"background correction"} is easily accomplished in Matlab/Octave by adding a column of 1s to the \textbf{A} matrix containing the absorption spectrum\index{absorption spectrum} of each of the components:

\texttt{background=ones(size(ObservedSpectrum));}

\texttt{A=[background A1 A2 A3];} 

where A1, A2, A3... are the absorption spectra vectors of the individual components.

\textbf{(c)} Performing a \textbf{T-weighted regression} is also readily performed:

\texttt{MeasuredAmp=([T T] .* A)\textbackslash (ObservedSpectrum .* T);}

where T is the transmission spectrum vector. Here, the matrix division backslash "\textbackslash " is used as a short-cut to the classical least-squares matrix solution (c.f. \url{http://www.mathworks.com/help/techdoc/ref/mldivide.html}). 



\textbf{The} \href{https://terpconnect.umd.edu/~toh/spectrum/cls.m}{\textbf{cls.m}} \textbf{function}: Ordinarily, the calibration matrix \textbf{M} is assembled from the experimentally measured signals (e.g. spectra) of the individual components of the mixture, but it is also possible to fit a computer-generated model of basic peak shapes (e.g. Gaussians, Lorentzians, etc.) to a signal to determine if that signal can be represented as the weighted sum of overlapping basic peak shapes. The function \href{https://terpconnect.umd.edu/~toh/spectrum/cls.m}{cls.m} computes such a model consisting of the sum of any number of peaks of \textit{known shape, width, and position}, but of \textit{unknown height}, and fit it to noisy x,y data sets. The syntax is 

\texttt{heights=cls(x, y, NumPeaks, PeakShape, Positions, Widths, extra)}

where x and y are the vectors of measured data (e.g. x might be wavelength and y might be the absorbance at each wavelength), 'NumPeaks' is the number of peaks, 'PeakShape' is the peak shape number (1=Gaussian, 2=Lorentzian, 3=logistic, 4=Pearson, 5=exponentially broadened Gaussian; 6=equal-width Gaussians; 7=Equal-width Lorentzians; 8=exponentially broadened equal-width Gaussian, 9=exponential pulse, 10=sigmoid, 11=Fixed-width Gaussian, 12=Fixed-width Lorentzian; 13=Gaussian/Lorentzian blend; 14=BiGaussian, 15=BiLorentzian), 'Positions' is the vector of peak positions on the x axis (one entry per peak), 'Widths' is the vector of peak widths in x units (one entry per peak), and 'extra' is the additional shape parameter required by the exponentially broadened, Pearson, Gaussian/Lorentzian blend, BiGaussian and BiLorentzian shapes. Cls.m returns a vector of measured peak heights for each peak.

\InsImageInline{0.5}{l}{image169.png}The demonstration script \href{https://terpconnect.umd.edu/~toh/spectrum/clsdemo.m}{\textbf{clsdemo.m}} (\href{https://terpconnect.umd.edu/~toh/spectrum/CLSdemo.png}{on} the right) creates some noisy model data, fits it with cls.m, computes the accuracy of the measured heights, then repeats the calculation \textit{by iterative non-linear least-squares peak fitting} (INLS, covered on page \pageref{ref-0258}) using my Matlab/ Octave function \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit.m}, making use of the known peak positions and widths only as \textit{starting guesses} ("start"). You can see that CLS is faster and (usually) more accurate, especially if the peaks are highly overlapped. (This script requires the functions cls.m, modelpeaks.m, and peakfit.m in the Matlab/Octave path).

\texttt{Figure window(1): Classical Least-squares (multilinear regression)}

\texttt{Elapsed time is 0.012 seconds.}

\texttt{Average peak height accuracy = 0.9145\%}

\texttt{Figure window(2): Iterative non-linear peak fitting with peakfit.m}

\texttt{Elapsed time is 0.39 seconds.}

\texttt{Average peak height accuracy = 1.6215\%}

On the other hand, INLS can be better than CLS if there are \textit{unsuspected shifts} in peak position and/or peak width between calibration and measurement (for example caused by drifting spectrometer calibration or by changing temperature, pressure, or solution variables) because INLS can track and compensate for changes in peak position and width. You can test this by changing the variable "PeakShift" (line 16) to a non-zero value in \href{https://terpconnect.umd.edu/~toh/spectrum/clsdemo.m}{clsdemo.m}. 

The \href{https://terpconnect.umd.edu/~toh/spectrum/cls2.m}{cls2.m} function is similar to \href{https://terpconnect.umd.edu/~toh/spectrum/cls.m}{cls.m}, except that it also measures the baseline (assumed to be flat), using the extension to "background correction" described above, and returns a vector containing the background B and measured peak heights H for each peak 1,2,3, e.g., [B H1 H2 H3...].

\textbf{Weighted linear regression:} My Matlab/Octave function "\href{https://terpconnect.umd.edu/~toh/spectrum/tfit.m}{tfit.m}" simulates the measurement of the absorption spectrum\index{absorption spectrum} of a mixture of three components by \textit{weighted} linear regression (on line 61), demonstrates the effect of the amount of noise in the signal, the extent of overlap of the spectra, and the linearity of the analytical curves of each component. This function also compares the results to \href{https://terpconnect.umd.edu/~toh/spectrum/TFit.html}{a more advanced method} described later (line 66) that applies curve fitting to the \textit{transmission} spectra rather than to the \textit{absorbance} spectra (page \pageref{ref-0334}). 

\InsImageInline{0.5}{l}{WheatILScalibration.GIF.png}The \textbf{Inverse Least-squares (ILS)} technique is demonstrated in Matlab by \href{https://terpconnect.umd.edu/~toh/spectrum/wheatILS.zip}{this script} and the graph above. The mathematics, described above on page \pageref{ref-0251}, is similar to the Classical Least-squares method and can be done by any of the Matlab/Octave or spreadsheet methods described in this section. This example is a real data set derived from the \href{http://en.wikipedia.org/wiki/Near-infrared\_spectroscopy}{near-infrared (NIR) reflectance spectroscopy} of agricultural wheat paste samples analyzed for protein content. In this example, there are 50 calibration samples measured at 6 wavelengths. These calibration samples had already been analyzed for \href{http://people.umass.edu/~mcclemen/581Proteins.html}{protein content} by a reliable (but slow) \href{http://en.wikipedia.org/wiki/Kjeldahl\_method}{reference method}. The purpose of this calibration is to establish whether near-infrared reflectance spectroscopy correlates to their protein content as determined by the (usually laborious and time-consuming) reference method. These results indicate that it does, at least for this set of 50 wheat samples, and therefore is it likely that near-infrared spectroscopy should do a pretty good job of estimating the protein content of similar unknown samples. \textit{The key is that the unknown samples must be similar to the calibration samples} (except for the protein content), but this is a very common analytical situation in industrial and agricultural \textit{quality control}, where many samples of products or crops of a similar predictable type must often be tested quickly and cheaply, often in the field using simple portable instruments. It may take a good bit of time and effort to calibrate the instrument initially, but once that is done, it can be applied quickly and cheaply to the type of sample for which it was calibrated. 

It is worth noting that the above method, specifically near infrared methods for agricultural samples, was pioneered by Karl Norris at the Beltsville Agricultural Research Center in Maryland. The early history is reviewed by Dr. Norris himself in \url{https://journals.sagepub.com/doi/10.1255/jnirs.941}.

\textbf{Note:} If you are reading this online, you can right-click on any of the m-file links above and select \textbf{Save Link As...} to download them to your computer for use within Matlab. 

\chapter{Curve fitting C: Non-linear Iterative Curve Fitting \label{ref-0258}\label{ref-0259}}

Least-squares curve fitting, described in "\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html}{Curve Fitting A}" on page \pageref{ref-0196}, is simple and fast, but it is limited to situations where the dependent variable can be modeled as a polynomial with linear coefficients. We saw that in some cases a non-linear situation can be converted into a linear one by a coordinate transformation, but this is possible only in some special cases, it may restrict the range of allowable dependent data values, and, in any case, the resulting coordinate transformation of the noise in the data can result in imprecision in the parameters measured in this way. 

\InsImageInline{0.5}{l}{FittingAnimation.gif.png}The most general way of fitting any model to a set of data is the \href{http://en.wikipedia.org/wiki/Iterative\_method}{iterative method}, (a.k.a. "spectral deconvolution" or "peak deconvolution"). It is a kind of "trial and error" procedure in which the parameters of the model are adjusted in a systematic fashion until the equation fits the data as close as required. This sounds like a brute-force approach, and it is. In fact, in the days before computers, this method was not widely used. But its great generality, coupled with advances in computer speed and algorithm efficiency, means that iterative methods are more widely used now than ever before. 

An iterative method proceeds in the following general way: 

(1) Select a model for the data; 

(2) Make first guesses of all the non-linear parameters; (i.e. position and width; there is no need to guess the peak heights)

(3) A computer program computes the model and compares it to the data set, calculating a fitting error; 

(4) If the fitting error is greater than the required fitting accuracy, the program systematically changes the parameters and loops back around to the previous step and repeats until the required fitting accuracy is achieved or the maximum number of iterations is reached. 

This continues until the fitting error is less than the specified error. One popular technique for doing this is called the \href{http://en.wikipedia.org/wiki/Nelder-Mead\_method}{\textit{Nelder-Mead Modified Simplex}}. This is \href{https://terpconnect.umd.edu/~toh/spectrum/FittingAnimation.gif}{}essentially a way of organizing and optimizing the changes in parameters (step 4, above) to shorten the time required to fit the function to the required degree of accuracy. It might sound complicated, but with contemporary personal computers, the entire process typically takes only a fraction of a second to a few seconds, depending on the complexity of the model and the number of independently adjustable parameters in the model. The animation shown above (\href{https://terpconnect.umd.edu/~toh/spectrum/Demofitgauss2animated.m}{script}) shows the working of the iterative process for a 2-peak unconstrained Gaussian fit to a small set of x,y data. This model has \textit{four nonlinear variables} (the positions and width of the two Gaussians, which are determined by iteration) and \textit{two linear variables} (the peak heights of the two Gaussians, which are determined directly by \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html}{regression} for each trial iteration). To allow the process to be observed in action, this animation is \textit{slowed down artificially} by (a) plotting step-by-step, (b) making a bad initial guess, and (c) adding a "\texttt{pause()}" statement. Without all that slowing it down, the process normally takes only about 0.05 seconds on a standard desktop PC, depending mainly on the number of nonlinear variables that must be iterated. This even works if the peaks are so overlapped that they bend into a single peak, as shown by \href{https://terpconnect.umd.edu/~toh/spectrum/Demofitgauss2AnimatedBlended.m}{Demofitgauss2AnimatedBlended.m}, but it may take more iterations and it is more sensitive to any random noise in the data (which you can set in line 13).

Note: the script that created \href{https://terpconnect.umd.edu/~toh/spectrum/FittingAnimation.gif}{this animation}, \href{https://terpconnect.umd.edu/~toh/spectrum/Demofitgauss2animated.m}{Demofitgauss2animated.m}, requires the \href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/fitgauss2animated.m}{fitgauss2animated.m}, and \href{https://terpconnect.umd.edu/~toh/spectrum/FMINSEARCH.txt}{fminsearch}.m functions to be in the Matlab/Octave path. As an instructional aid, a modified version of this script, \href{https://terpconnect.umd.edu/~toh/spectrum/Demofitgauss2animatedError.m}{Demofitgauss2animatedError.m}, plots the fitting error vs iteration number, showing that a poor initial guess (start value) can be detrimental, \href{https://terpconnect.umd.edu/~toh/spectrum/Bad\%20guess.png}{requiring many more iterations} (and more time) to find a good fit, which is not very efficient from the point of view of a human observer like yourself who can visually estimate the peak parameters much more intelligently. A good guess requires \href{https://terpconnect.umd.edu/~toh/spectrum/Good\%20guess.png}{fewer iterations}. One way to get very good start values for peak-type signals is to precede the curve fitting with a fast, single-pass \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaksb}{peak detector algorithm} to determine the number of peaks and their approximate positions and widths (as will be covered in a later section), but that will only work when the peaks to be fit have distinct maxima and are not \href{https://terpconnect.umd.edu/~toh/spectrum/BlendedPeak.png}{blended into a single peak}. As you might expect, a \href{https://terpconnect.umd.edu/~toh/spectrum/Demofitgauss3animatedError.m}{3-peak model}, which has \textit{six} non-linear parameters to optimize, will require \href{https://terpconnect.umd.edu/~toh/spectrum/Demofitgauss3animatedError.png}{even more iterations}.

The main difficulty of the iterative methods is that they sometimes fail to converge at an overall optimum solution in difficult cases, especially if given a bad initial starting guess. To understand how that could happen, think of a physical analogy: you deposit a blind man on a hilly landscape and ask him to walk to the highest peak, given only a stick to probe his immediate surroundings to test if the ground is higher or lower on each side. If it is higher, he walks there and probes around that area. He stops when all the areas around him are lower than where he is. This works well if there is one smooth hill. But what if there are multiple small hills, or if the ground is rocky and uneven?  He is very likely to stop at one of the smaller hills, not knowing that there might be an even higher hill nearby. To walk the \textit{entire} landscape in a grid to probe for all the hills would take a lot of time.

The standard approach to handle this is to restart the process with random variations of the first guesses, repeat several times, and take the one with the lowest fitting error. That entire process is automated in the peak fitting functions \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Interactive\_Peak\_Fitter\_}{described on} page \pageref{ref-0446}. If that is not enough, we can make our own starting guesses. With all this, it is no surprise that iterative curve fitting takes longer than linear regression - with typical personal computers, an iterative fit might take fractions of a second where a regression would take fractions of a millisecond. Still, that’s fast enough for many purposes.

The \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Reliability}{precision} of the model parameters measured by iterative fitting (page \pageref{ref-0267}), like classical least-squares fitting, depends strongly on a good model, accurate compensation for the background/baseline, the signal-to-noise ratio, and the number of data points across the feature that you wish to measure. It is not practical to predict the standard deviations of the measured model parameters using the algebraic approach, but both the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Reliability}{Monte Carlo simulation and bootstrap methods} (page \pageref{ref-0213}) are applicable.

\textbf{Note}: the term "spectral deconvolution" or "band deconvolution" is often used to refer to this technique, but in this book, "deconvolution" specifically means \textit{Fourier} deconvolution, an independent concept that is treated on page \pageref{ref-0148}. In Fourier deconvolution, the underlying peak shape is \textit{unknown,} but the broadening function is assumed to be \textit{known}; whereas, in iterative least-squares curve fitting, it is just the reverse: the peak shape must be known but the width of the broadening process, which determines the width and shape of the peaks in the recorded data, is unknown. Thus, the term "spectral deconvolution" is ambiguous: it might mean the Fourier deconvolution of a response function from a spectrum, or it might mean the decomposing of a spectrum into its separate additive peak components. These are different processes; do not get them confused. See page \pageref{ref-0361}.

\section{Spreadsheets and stand-alone programs\label{ref-0260}\label{ref-0261}}

Both \textit{Excel} and \textit{OpenOffice Calc} have a "\href{http://www.excel-easy.com/data-analysis/solver.html}{Solver}" capability that will change specified cells in an attempt to produce a \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Basics}{}specified goal; this can be used in peak fitting to minimize the fitting error between a set of data and a proposed calculated model, such as a set of overlapping Gaussian bands. The latest version includes \href{http://blogs.office.com/2009/09/21/new-and-improved-solver/}{three different solving methods}. \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitterStart4x100.xlsx}{This Excel spreadsheet example} (\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter4componentsStart.png}{screenshot}) demonstrates how this is used to fit four Gaussian components to a sample set of x,y data that has already been entered into columns A and B, rows 22 to 101 (you could type or paste in your own data there). 

After entering the data, you do a visual estimate of how many Gaussian peaks it might take to represent the data, and their locations and widths, and type those estimated values into the 'Proposed model' table. The spreadsheet calculates the best-fit values for the peak \textit{heights} by \href{http://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html\#cls}{multilinear regression} (page \pageref{ref-0246}) in the 'Calculated amplitudes' table and plots the data and the fit. It also plots the "residuals", which are the point-by-point \textit{differences} between the data and the model; ideally, the residuals would be zero, or at least small. (Adjust the x-axis scale of these graphs to fit your data).


\begin{center}
\InsImageInline{0.5}{l}{CurveFittingStart.png}
\end{center}


The next step is to use \textit{Solver} function to "fine-tune" the position and width of each component to minimize the \% fitting error (in red) and to make the residuals plot as random as possible: click \textbf{Data} in the top menu bar, click \textbf{Solver} (upper right) to open the Solver box, into which you type "C12" into "Set Objective", click "min", select the cells in the "Proposed Model" that you want to optimize, add any desired constraints in the "Subject to the Constraints" box, and click the \textbf{Solve} button. Solver automatically optimizes the position, width, and amplitude of all the components and \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter4componentsDone.png}{best fit is displayed}. (You can see that the Solver has changed the selected entries in the proposed model table, reduced the fitting error (cell C12, in red), and made the residuals smaller and more random). If the fit fails, change the starting values, click \textbf{Solver}, and click the \textbf{Solve} button. You can automate the above process and reduce it to a single function-key press by using \textit{macros}, as described on page \pageref{ref-0374}.

So, how many Gaussian components does it take to fit the data? One way to tell is to look at the plot of the residuals (which shows the point-by-point difference between the data and the fitted model), and add components until the residuals are \textit{random, not wavy}, but this works only if the data are \textit{not smoothed} before fitting. Here's an example - a set of real data that are fitted with an increasing sequence of \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter\_2\_fit2components.png}{two Gaussians}, \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter\_2\_fit3components.png}{three Gaussians}, \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter\_2\_fit4components.png}{four Gaussians}, and \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter\_2\_fit5components.png}{five Gaussians}. As you look at this sequence of screenshots, you will see the percent fitting error decrease, the R\textsuperscript{2} value become closer to 1.000, and the residuals become smaller and more random. (Note that in the 5-component fit, the first and last components are not \textit{peaks} within the 250-600 x-range of the data, but rather they account for the \textit{background}). There is no need to try a \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter6componentsDone.png}{6-component fit} because the residuals are already random at 5 components and more components than that would just "fit the noise" and would likely be unstable and give a very different result with another sample of that signal with different noise. 

There are a number of downloadable non-linear iterative curve fitting add-ons and macros for \href{http://www.google.com/search?q=nelder+mead+simplex+excel&ie=utf-8&oe=utf-8&aq=t&rls=org.mozilla:en-US:official&client=firefox-a}{Excel} and \href{http://www.google.com/search?q=nelder+mead+simplex+\%22open+office\%22&ie=utf-8&oe=utf-8&aq=t&rls=org.mozilla:en-US:official&client=firefox-a}{OpenOffice}. For example, \href{http://www.chem.qmul.ac.uk/staff/nix/}{Dr. Roger Nix} of Queen Mary University of London has developed a very nice \href{http://www.chem.qmul.ac.uk/software/eXPFit.htm}{Excel/VBA spreadsheet} for curve fitting X-ray photoelectron spectroscopy (XPS) data, but it could be used to fit other types of spectroscopic data. A 4-page instruction sheet is also provided. 

There are also many examples of stand-alone \href{http://www.google.com/search?hl=en&client=firefox-a&rls=org.mozilla\%3Aen-US\%3Aofficial&q=nelder+mead+simplex+freeware&btnG=Search}{freeware} and commercial programs, including \href{http://www.sigmaplot.co.uk/products/peakfit/peakfit.php}{PeakFit}, \href{http://www.datamaster2003.com/}{Data Master 2003}, \href{https://www.mycurvefit.com/}{MyCurveFit}, \href{http://www.curveexpert.net/}{Curve Expert}, \href{http://www.originlab.com/}{Origin}, \href{https://www.ndcurvemaster.com/}{ndcurvemaster}, and the \href{https://cran.r-project.org/}{R language}.

If you use a spreadsheet for this type of curve fitting, you have to build a custom spreadsheet for each problem, with the right number of rows for the data and with the desired number of components. For example, my \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter5components.xlsx}{CurveFitter.xlsx} template is only for a 100-point signal and a 5-component Gaussian model. It is easy to extend to a larger number of data points by inserting rows between 22 and 100, columns A through N, and drag-copying the formulas down into the new cells (e.g.\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter2.xlsx}{ CurveFitter2.xlsx} is extended to 256 points). To handle other numbers of components or model shapes you would have to insert or delete columns between C and G and between Q and U and edit the formulas, as has been done in this set of templates for \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter2Gaussian.xlsx}{2 Gaussians}, \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter3Gaussian.xlsx}{3 Gaussians}, \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter4Gaussian.xlsx}{4 Gaussians}, \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter\_2\_fit5components.png}{5}\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter5Gaussian.xlsx}{Gaussians}, and \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter6Gaussian.xlsx}{6 Gaussians}. 

If your peaks are superimposed on a baseline, you can \textit{include a model for the baseline} as one of the components. For instance, if you wish to fit 2 Gaussian peaks on a linear tilted slope baseline, select a \textit{3-component} spreadsheet template and change one of the Gaussian components to the equation for a straight line (y=\textit{m}x+\textit{b}, where \textit{m} is the slope and \textit{b} is the intercept). A template for that particular case is \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter2GaussianBaseline.xlsx}{CurveFitter2GaussianBaseline.xlsx} (\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter2GaussiansPlusBaseline.png}{graphic}); do not click "Make Unconstrained Variables Non-Negative" in this case, because the baseline model may well need negative variables, as it does in this particular example. If you want to use another peak shape or another baseline shape, you'd have to modify the equation in row 22 of the corresponding columns C through G and drag-copy the modified cell down to the last row, as was done to change the Gaussian peak shape into a Lorentzian shape in \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter6Lorentzian.xlsx}{CurveFitter6Lorentzian.xlsx}. Or you could make columns C through G contain equations for \textit{different} peak or baseline shapes. For exponentially broadened Gaussian peak shapes, you can use \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter2ExpGaussianTemplate.xlsx}{CurveFitter2ExpGaussianTemplate.xlsx} for two overlapping peaks (\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter2ExpGaussianExample.png}{screen graphic}). In this case, each peak has \textit{four} parameters: height, position, width, and lambda (which determines the asymmetry - the extent of exponential broadening).

In some cases, it is useful to add \textit{constraints} to the variables determined by iteration, for example, to constrain them to be greater than zero, or constrained between two limits, or equal, etc. Doing so will force the solutions to adhere to known expectations and avoid nonphysical solutions. This is especially important for complex shapes such as the exponentially broadened Gaussian just discussed in the previous paragraph. You can do this by adding those constraints using the "Subject to the Constraints:" box in the center of the "Solver Parameters" box (see the graphic on the previous page). For details, see \url{https://www.solver.com/excel-solver-add-change-or-delete-constraint?}

The point is that you can do - in fact, you \textit{must} do - a lot of custom editing to get a spreadsheet template that fits your data. In contrast, my Matlab/Octave \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit.m function} (page \pageref{ref-0448}) \textit{automatically} adapts to any number of data points and is easily set to over 40 different model peak shapes (graphic on page \pageref{ref-0469}) and any number of peaks simply by changing the input arguments. Using my \textit{Interactive Peak Fitter} function \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{ipf.m} in Matlab (page \pageref{ref-0461}), you can \textit{press a single keystroke} to instantly change the peak shape, the number of peaks, the baseline mode (page \pageref{ref-0276}), or to re-calculate the fit with a different start value or with a bootstrap subset of the data (to estimate the peak parameters errors). That is far quicker and easier than the spreadsheet. But on the other hand, a \textit{real advantage of spreadsheets} in this application is that it is relatively easy to add your own \textit{custom shape functions and constraints}, even complicated ones, using standard spreadsheet cell formula construction. And if you are hiring help, it is probably easier to find an experienced spreadsheet programmer than a Matlab programmer. So, if you are not sure which to use, my advice is to try both methods and decide for yourself. 

\section{Matlab and Octave \label{ref-0262}}

\textbf{Matlab} and \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{\textbf{Octave}} have a convenient and efficient function called "\href{http://www.mathworks.com/help/techdoc/ref/fminsearch.html}{fminsearch}" that uses the Nelder-Mead method. It was originally designed for finding the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Basics}{}minimum values of functions, but it can be applied to least-squares curve fitting by creating a so-called \href{http://en.wikipedia.org/wiki/Anonymous\_fun}{anonymous function} (a.k.a. "\textit{lambda"} function) that computes the model, compares it to the data, and returns the fitting error. For example, writing \texttt{parameters = fminsearch(@(lambda)(fitfunction(lambda, x, y)),start)} performs an iterative fit of the data in the vectors x,y to a model described in a previously-created function called \texttt{fitfunction}, using the first guesses in the vector "start". The parameters of the best-fit model are returned in the vector "parameters", in the same order that they appear in "start". 

Note: for \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{Octave} users, the fminsearch function is contained in the "Optim" add-on package (use the latest version 1.2.2 or later), downloadable from \href{http://downloads.sourceforge.net/octave/optim-1.2.1.tar.gz?download}{Octave Forge}. It is a good idea to install \textit{all} these add-on packages just in case they are needed; follow the instructions on the \href{http://octave.sourceforge.net/packages.php}{Octave Forge} web page. For Matlab users, fminsearch is a built-in function, although there are many other optimization routines in the optional Optimization Toolbox, which is not needed for the examples and programs in this document.

A simple example is fitting the \href{http://en.wikipedia.org/wiki/Blackbody\#Planck.27s\_law\_of\_black-body\_radiation}{blackbody equation} to the spectrum of an incandescent body for the purpose of estimating its color temperature. In this case, there is only \textit{one} nonlinear parameter, temperature. The script \href{https://terpconnect.umd.edu/~toh/spectrum/BlackbodyDataFit.m}{BlackbodyDataFit.m} demonstrates the technique, placing the experimentally measured spectrum in the vectors "wavelength" and "radiance" and then calling fminsearch with the fitting function \href{https://terpconnect.umd.edu/~toh/spectrum/fitblackbody.m}{fitblackbody.m}. Incandescent lightbulbs, of the type that used to be common in household lighting before LEDs, are examples of blackbody radiators. (If a blackbody source is not thermally homogeneous, it may be possible to model it as the \textit{sum} of two or more regions of different temperature, as in example 3 of \href{https://terpconnect.umd.edu/~toh/spectrum/fitshape1.m}{fitshape1.m}). 

Another application is demonstrated by Matlab's built-in demo \href{https://terpconnect.umd.edu/~toh/spectrum/fitdemo.txt}{fitdemo.m} and its corresponding fitting function \href{https://terpconnect.umd.edu/~toh/spectrum/fitfun.txt}{fitfun.m,} which model the sum of two exponential decays. To see this, just type "fitdemo" in the Matlab command window. (Octave does not have this demo function). 

\section{Fitting peaks\label{ref-0263}\label{ref-0264}}

Many instrumental methods of measurement produce signals in the form of peaks of various shapes; a common requirement is to measure the positions, heights, widths, and/or areas of those peaks, even when they are noisy or overlapped with one another. This cannot be done by linear least-squares methods, because such signals cannot be modeled as polynomials with linear coefficients (the positions and widths of the peaks are not linear functions), so iterative curve fitting techniques are used instead, often using Gaussian, Lorentzian, or some other fundamental simple peak shapes as a model.

The Matlab/\href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{Octave} demonstration script \href{https://terpconnect.umd.edu/~toh/spectrum/Demofitgauss.m}{Demofitgauss.m} demonstrates fitting a Gaussian function to a set of data, using the \href{https://terpconnect.umd.edu/~toh/spectrum/Demofitgauss2.GIF}{}fitting function \href{https://terpconnect.umd.edu/~toh/spectrum/fitgauss2.m}{fitgauss.m}. In this case, there are two non-linear parameters: peak position and peak width (the peak height is a linear parameter and is determined by regression in a single step in line 9 of the fitting function \href{https://terpconnect.umd.edu/~toh/spectrum/fitgauss2.m}{fitgauss.m} and is returned in the global variable "c"). Compared to the simpler polynomial least-squares methods for measuring peaks (page \pageref{ref-0229}), the iterative method has the advantage of using all the data points across the entire peak, including zero and negative points, and it can be applied to multiple overlapping peaks as demonstrated the script \href{https://terpconnect.umd.edu/~toh/spectrum/Demofitgauss2.m}{Demofitgauss2.m}.

To accommodate the possibility that the baseline may shift, we can add a column of 1s to the A matrix, just as was done in the CLS method (page \pageref{ref-0245}). This has the effect of introducing into the model an additional component that is simply a flat line; its amplitude is returned along with the peak heights in the global vector ``c''; \href{https://terpconnect.umd.edu/~toh/spectrum/Demofitgaussb.m}{Demofitgaussb.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/fitgauss2b.m}{fitgauss2b.m} illustrates this addition. (\href{https://terpconnect.umd.edu/~toh/spectrum/Demofitlorentzianb.m}{Demofitlorentzianb.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/fitlorentzianb.m}{fitlorentzianb.m} for Lorentzian peaks).

This peak fitting technique is easily extended to any number of overlapping peaks of the same type using the \textit{same} fitting function fitgauss.m, which easily adapts to any number of peaks, depending on the length of the first-guess "start" vector \textit{lambda} that is passed to the function as input arguments, along with the data vectors \textit{t} and \textit{y}:

1 function err = fitgauss(lambda, t,y)

2 \% \textcolor{color-9}{Fitting functions for a Gaussian band spectrum.}

3 \% \textcolor{color-9}{T. C. O'Haver. Updated to Matlab 6, March 2006}

4 global c

5 A = zeros(length(t),round(length(lambda)/2));

6 for j = 1:length(lambda)/2,

7 A(:,j) = gaussian(t, lambda(2*j-1),lambda(2*j))';

8 end

9 c = A\textbackslash y'; \% c = abs(A\textbackslash y') for positive peak heights only

10 z = A*c;

11 err = norm(z-y');

If there are \textit{n} peaks in the model, then the length of lambda is 2\textit{n}, one entry for each iterated variable ([position1 width1 position2 width2....etc.]). The "for" loop (lines 5-7) constructs an \textit{n} ${\times}$ length(t) matrix containing the model for each peak separately, using a user-defined peak shape function (in this case \href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian.m}), then it computes the \textit{n}-length peak height vector \textit{c} by least-squares regression in line 9, using the \href{http://www.mathworks.com/help/matlab/ref/mldivide.html}{Matlab shortcut "\textbackslash " notation}. (To constrain the fit to \textit{positive} values of peak height, replace A\textbackslash y' with abs(A\textbackslash y') in line 9). The resulting peak heights are used to compute \textit{z}, the sum of all \textit{n} model peaks, by \href{http://en.wikipedia.org/wiki/Matrix\_multiplication}{matrix multiplication} in line 10, and then "err", the root-mean-square difference between the model \textit{z} and the actual data \textit{y}, is computed in line 11 by the Matlab 'norm' function and returned to the calling function ('fminsearch'), which repeats the process many times, trying different values of the peak positions and the peak widths until the value of "err" is low enough.

This fitting function above would be called by Matlab's fminsearch function like so\texttt{:} 

\texttt{params=fminsearch(@(lambda)(fitgauss(lambda, x,y)),[50 20])}

where the square brackets contain a vector of first guesses for position and width for each peak ([position1 width1 position2 width2....etc.]). The output argument 'params' returns a 2 ${\times}$ \textit{n} matrix of best-fit values of position and width for each peak, and the peak heights are contained in the \textit{n}-length global variable vector c. Similar fitting functions can be defined for other peak shapes simply by calling the corresponding peak shape function, such as \href{https://terpconnect.umd.edu/~toh/spectrum/lorentzian.m}{lorentzian.m }in line 7. (Note: for this and other scripts like Demofitgauss.m or Demofitgauss2.m to work on your version of Matlab, all the functions that they call must be loaded into Matlab beforehand, in this case fitgauss.m and gaussian.m. Scripts that call sub-functions must have those functions in the Matlab path. Functions, on the other hand, can have all their required sub-functions defined within the main function itself and thus can be self-contained, as are the next two examples).

The function \href{https://terpconnect.umd.edu/~toh/spectrum/fitshape2.m}{fitshape2.m} (syntax: \texttt{[Positions, Heights, Widths, FittingError] = fitshape2(x, y, start)}) pulls all of this together into a simplified general-purpose Matlab/ Octave \textit{function} for fitting multiple overlapping model shapes to the data contained in the vector variables x and y. The model is the weighted sum of any number of basic peak shapes which are defined mathematically as a function of x, with two variables that the program will independently determine for each peak - positions and widths - in addition to the peak heights (i.e., the weights of the weighted sum). You must provide the first guess starting vector 'start', in the form [position1 width1 position2 width2 ...etc.], which specifies the first-guess position and width of each component (one pair of position and width for each peak in the model). The function returns the parameters of the best-fit model in the vectors \texttt{Positions, Heights, Widths}, and computes the percent error between the data and the model in \texttt{FittingError}. It also plots the data as dots and the fitted model as a line. 

 The interesting thing about this function is that \textit{the only part that defines the shape of the model is the last line}. In fitshape2.m, that line contains the expression for a \textit{Gaussian peak of unit height}, but you could change that to \textit{any other expression or algorithm} that computes \textit{g} as a function of \textit{x} with two unknown parameters 'pos' and 'wid' (position and width, respectively, for peak-type shapes, but they could represent anything for other function types, such as the exponential pulse, sigmoidal, etc.); everything else in the fitshape.m function can remain the same. It is all about the bottom line. This makes fitshape.m a good platform for experimenting with different mathematical expressions as proposed models to fit data. There are also two other variations of this function for models with \textit{one} iterated variable plus peak height (\href{https://terpconnect.umd.edu/~toh/spectrum/fitshape1.m}{fitshape1.m}) and \textit{three} iterated variables plus peak height (\href{https://terpconnect.umd.edu/~toh/spectrum/fitshape3.m}{fitshape3.m}). Each has illustrative examples in the help file (type "help fitshape...").

\textbf{\textit{Variable shape types}}, such as the \href{https://terpconnect.umd.edu/~toh/spectrum/voigt.m}{Voigt profile}, Pearson, \href{https://terpconnect.umd.edu/~toh/spectrum/BWF.m}{Breit-Wigner-Fano}, Gauss-Lorentz blend, and the exponentially-broadened Gaussian and Lorentzian, are defined not only by the peak position, height, and width, but also by \textit{an additional parameter that fine-tunes the shape of the peak}. If that parameter is \textit{equal and known} for all peaks in a group, it can be passed as an additional input argument to the shape function, as shown in the demo function \href{https://terpconnect.umd.edu/~toh/spectrum/VoigtFixedAlpha.m}{VoigtFixedApha.m} for the Voigt profile, which is calculated as a convolution of Gaussian and Lorentzian components with different widths. If the shape parameter (\textit{alpha}, the ratio of the two widths) is allowed to be \textit{different} for each peak in the group and is to be determined by iteration (just as is position and width), then the routine must be modified to accommodate \textit{three, rather than two}, iterated variables, as shown in the demo function \href{https://terpconnect.umd.edu/~toh/spectrum/VoigtVariableAlpha.m}{VoigtVariableAlpha.m}. Although the fitting error is lower with variable alphas, the execution time is longer and the alpha values so determined are not very stable, with respect to noise in the data and the starting guess values, especially for multiple peaks. (These are self-contained functions). Version 9.5 of the general-purpose Matlab/Octave function \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit.m} includes fixed and variable shape types for the Pearson, ExpGaussian, Voigt, and Gaussian/Lorentzian blend, as well as the 3-parameter logistic or Gompertz function (whose three parameters are labeled Bo, Kh, and L, rather than position, width, and shape factor). The script \href{https://terpconnect.umd.edu/\%7Etoh/spectrum/VoigtShapeFittingDemonstration.m}{VoigtShapeFittingDemonstration.m} uses the peakfit.m version 9.5 to fit a single Voigt profile and to calculate the Gaussian width component, Lorentzian width component, and alpha. It computes the theoretical Voigt profile with added random noise for realism. The script \href{https://terpconnect.umd.edu/\%7Etoh/spectrum/VoigtShapeFittingDemonstration2.m}{VoigtShapeFittingDemonstration2.m} does the same for \textit{two} overlapping Voigt profiles, using both fixed alpha and variable alpha models (shape numbers 20 and 30). (Requires voigt.m, halfwidth.m, and peakfit.m in the path).

If you do \textit{not} know the shape of your peaks, you can use peakfit.m or ipf.m (page \pageref{ref-0461}) to try different shapes to see if one of the standard shapes included in those programs fits the data; try to find a peak in your data that is typical, isolated, and that has a good signal-to-noise ratio. For example, the Matlab functions \href{https://terpconnect.umd.edu/~toh/spectrum/ShapeTestS.m}{ShapeTestS.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/ShapeTestA.m}{ShapeTestA.m} test the data in its input arguments x,y, assumed to be a single isolated peak, fits it with \textit{different candidate model peak shapes} using peakfit.m, plots each fit in a separate figure window, and prints out a table of fitting errors in the command window. \href{https://terpconnect.umd.edu/~toh/spectrum/ShapeTestS.m}{ShapeTestS.m} tries seven different candidate \textit{symmetrical} model peaks, and \href{https://terpconnect.umd.edu/~toh/spectrum/ShapeTestS.m}{ShapeTestA.m} tries six different candidate \textit{asymmetrical} model peaks. The one with the lowest fitting error (and R\textsuperscript{2 }closest to 1.000) is presumably the best candidate. \href{https://terpconnect.umd.edu/~toh/spectrum/ShapeTestResults.txt}{Try the examples} in the help files for each of these functions. But beware, if there is too much noise in your data, the results can be misleading. For example, even if the actual peak shape is something other than a Gaussian, the multiple Gaussians model is likely to fit slightly better because it has more degrees of freedom and can "fit the noise". The Matlab function peakfit.m has many more built-in shapes to choose from, but it is still a \textit{finite} list and there is always the possibility that the actual underlying peak shape is not available in the software you are using or that it is simply not describable by a mathematical function.

Signals with \textit{peaks of different shape types in one signal} can be fit by the fitting function \href{https://terpconnect.umd.edu/~toh/spectrum/fitmultiple.m}{fitmultiple.m}, which takes as input arguments a vector of peak types and a vector of shape variables. The sequence of peak types and shape parameters must be determined beforehand. To see how this is used, see \href{https://terpconnect.umd.edu/~toh/spectrum/Demofitmultiple.m}{Demofitmultiple.m}.

You can create your own fitting functions for any purpose; they are \textit{not} limited to single algebraic expressions but can be arbitrarily complex multiple-step algorithms. For example, in \href{https://terpconnect.umd.edu/~toh/spectrum/TFit.html}{the TFit method for quantitative absorption spectroscopy} (page \pageref{ref-0332}), a model of the instrumentally-broadened transmission spectrum is fit to the observed transmission data, using a \href{https://terpconnect.umd.edu/~toh/spectrum/fitM.m}{fitting function} that performs \href{https://terpconnect.umd.edu/~toh/spectrum/Convolution.html}{Fourier convolution} (page \pageref{ref-0138}) of the transmission spectrum model with the known slit function of the spectrometer. The result is an alternative method of calculating absorbance that allows the optimization of signal-to-noise ratio and extends the dynamic range and calibration linearity of absorption spectroscopy far beyond the normal limits.

\section{\href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{\label{ref-0265}\label{ref-0266}Peak Fitters for Matlab and Octave}\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Basics}{}}

\InsImageInline{0.5}{l}{image173.png}Here I describe more complete peak fitting functions that have additional capabilities: a built-in set of basic peak shapes that you can select, provision for estimating the ``start'' vector if you do not provide one, provision for handling baselines, ability to estimate peak parameter uncertainties, etc. These functions accept signals of any length, including those with non-integer and non-uniform x-values, and can fit any number of peaks with Gaussian, equal-width Gaussian, fixed-width Gaussian, exponentially-broadened Gaussian, exponentially-broadened equal-width Gaussians, bifurcated Gaussian, Lorentzian, fixed-width Lorentzians, equal-width Lorentzians, exponentially-broadened Lorentzian, bifurcated Lorentzian, logistic distribution, logistic function, triangular, alpha function, Pearson 7, exponential pulse, up sigmoid, down sigmoid, Gaussian/ Lorentzian blend, Breit-Wigner-Fano, and Voigt profile shapes. (\href{https://terpconnect.umd.edu/~toh/spectrum/ShapeDemo.png}{A graphic} that illustrates the basic peak shapes available is on page \pageref{ref-0469}). 

There are two different versions, a \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#command}{command-line version} called \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m}, for Matlab or \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{Octave}, and a \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{keypress operated interactive version} called \href{https://terpconnect.umd.edu/~toh/spectrum/ipf.m}{ipf.m}, for Matlab only (page \pageref{ref-0461}). For adding as an element on your own programs and for automating the fitting of large numbers of signals, peakfit.m is better; ipf.m is best for exploring a few signals to determine the best fitting range, peak shapes, number of peaks, baseline correction mode, etc. Both functions allow for simplified operation by providing default values for any unspecified input arguments; for example, the starting values, if not specified in the input arguments, are estimated by the program based on the length and x-axis interval of the data. Compared to the fitshape.m function described above, \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit.m} has a large number of built-in peak shapes, it does not require (although it can be given) the first-guess position and width of each component, and it has features for background correction and other useful features to improve the quality and reliability of fits.

These functions can optionally estimate the expected standard deviation and interquartile range of the peak parameters using the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Bootstrap}{bootstrap sampling method} (page \pageref{ref-0220}). See \href{https://terpconnect.umd.edu/~toh/spectrum/DemoPeakfitBootstrap.m}{DemoPeakfitBootstrap} for a self-contained demonstration of this function. 

\InsImageInline{0.5}{l}{BootstrapIterativeFit.gif.png}\textbf{The effect of random noise on the uncertainty of the peak parameters} determined by iterative least-squares fitting is readily estimated by the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Bootstrap}{bootstrap sampling method} (introduced on page \pageref{ref-0213}). A simple demonstration of bootstrap estimation of the variability of an iterative least-squares fit to a single noisy Gaussian peak is given by my Matlab/ \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{Octave} function "\href{https://terpconnect.umd.edu/~toh/spectrum/BootstrapIterativeFit.m}{BootstrapIterativeFit.m}", which creates a single x,y data set consisting of a single noisy Gaussian peak, extracts bootstrap samples from that data set, performs an iterative fit to the peak on each of the bootstrap samples, and plots the distributions (histograms) of peak height, position, and width of the bootstrap samples. The syntax is \texttt{BootstrapIterativeFit(TrueHeight, TruePosition, TrueWidth, NumPoints, Noise, NumTrials)} where \texttt{TrueHeight} is the true peak height of the Gaussian peak, \texttt{TruePosition} is the true x-axis value at the peak maximum, \texttt{TrueWidth} is the true half-width (FWHM) of the peak, NumPoints is the number of points taken for the least-squares fit, Noise is the standard deviation of (normally-distributed) random noise, and NumTrials is the number of bootstrap samples. 

A typical example for \texttt{BootstrapIterativeFit( 100,100,100,20,10,100}); is displayed in the figure on the right. The results, displayed in the command window, are:

\texttt{{\textgreater}{\textgreater} BootstrapIterativeFit(100,100,100,20,10,100);}

  \texttt{Peak Height Peak Position Peak Width}

\texttt{mean: 99.27028 100.4002 94.5059}

\texttt{STD: 2.8292 1.3264 2.9939}

\texttt{IQR: 4.0897 1.6822 4.0164}

\texttt{IQR/STD Ratio: 1.3518}

A similar demonstration function for \textit{two} overlapping Gaussian peaks is available in "\href{https://terpconnect.umd.edu/~toh/spectrum/BootstrapIterativeFit2.m}{BootstrapIterativeFit2.m}". Type "help BootstrapIterativeFit2" for more information. In both these simulations, the standard deviation (STD), as well as the \href{http://en.wikipedia.org/wiki/Interquartile\_range}{interquartile range} (IQR) of each of the peak parameters, are computed. This is done because the interquartile range is much less influenced by \textit{outliers}. The distribution of peak parameters measured by iterative fitting is often non-normal, exhibiting a greater fraction of large deviations from the mean than is expected for a normal distribution. This is because the iterative procedure sometimes converges on an abnormal result, especially for multiple peak fits with many variable parameters. (You may be able to see this in the histograms plotted by these simulations, especially for the weaker peak in \href{https://terpconnect.umd.edu/~toh/spectrum/BootstrapIterativeFit2.m}{BootstrapIterativeFit2}). In those cases, the standard deviation will be too high because of the outliers, and the IQR/STD ratio will be much less than the value of 1.34896 that is expected for a normal distribution. In that case, a better estimate of the standard deviation of the central portion of the distribution (without the outliers) is IQR/1.34896. 

It is important to emphasize that the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Reliability}{bootstrap method} predicts only the effect of random noise on the peak parameters for a fixed fitting model. It does not consider the possibility of peak parameter inaccuracy caused by using a non-optimum data range, or choosing an imperfect model, or by inaccurate compensation for the background/baseline, all of which are at least partly subjective and thus beyond the range of influences that can easily be treated by random statistics. If the data have relatively little random noise or have been smoothed to reduce the noise, then it is likely that model selection and baseline correction will be the major sources of peak parameter inaccuracy, which are not well predicted by the bootstrap method. 

For the quantitative measurement of peaks, it is instructive to compare the iterative least-squares method with simpler, less computationally intensive, methods. For example, the measurement of the peak height of a single peak of uncertain width and position could be done simply by taking the maximum of the signal in that region. If the signal is noisy, a more accurate peak height will be obtained if the signal is \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothed }beforehand (page \pageref{ref-0048}). But smoothing can distort the signal and reduce peak heights. Using an iterative peak fitting method, assuming only that the peak shape is known, can give the best possible accuracy \textit{and} precision, without requiring smoothing even under high noise conditions, e.g. when the signal-to-noise ratio is 1, as in the demo script \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothVsFit.m}{SmoothVsFit.m}: 

\texttt{True peak height = 1 NumTrials = 100 SmoothWidth = 50}

  \texttt{Method Maximum y Max Smoothed y Peakfit} 

\texttt{Average peak height 3.65 0.96625 1.0165}

\texttt{Standard deviation 0.36395 0.10364 0.1157}

If peak \textit{area} is measured rather than peak \textit{height}, smoothing is unnecessary (unless to locate the peak beginning and end) but peak fitting still yields the best precision. See \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothVsFitArea.m}{SmoothVsFitArea.m}. 

It is also instructive to compare the iterative least-squares method with \textit{classical least-squares curve fitting}, discussed on page \pageref{ref-0242}, which can also fit peaks in a signal. The difference is that in the classical least-squares method, the positions, widths, and shapes of all the individual components are all known beforehand; the \textit{only} unknowns are the amplitudes (e.g., peak heights) of the components in the mixture. In non-linear iterative curve fitting, on the other hand, the positions, widths, and heights of the peaks a\textit{re all unknown} beforehand; the \textit{only} thing that is known is the fundamental underlying \textit{shape} of the peaks. The non-linear iterative curve fitting is more difficult to do (for the computer, anyway) and more prone to error, but it is necessary if you need to track shifts in peak position or width or to decompose a complex overlapping peak signal into fundamental components knowing only their shape. The Matlab/Octave script ``\href{https://terpconnect.umd.edu/~toh/spectrum/CLSvsINLS.m}{CLSvsINLS.m}'' compares the classical least-squares (CLS) method with three different variations of the iterative method (INLS) method for measuring the peak heights of three Gaussian peaks in a noisy test signal on a standard Windows PC, demonstrating that the fewer the number of unknown parameters, the faster and more accurate is the peak height calculation.

\texttt{Method Positions Widths Execution time \% Accuracy}

  \texttt{CLS known known 0.00133 0.30831}

  \texttt{INLS unknown unknown 0.61289 0.6693}

  \texttt{INLS known unknown 0.16385 0.67824}

  \texttt{INLS unknown known 0.24631 0.33026}

  \texttt{INLS unknown known (equal) 0.15883 0.31131}

Another comparison of multiple measurement techniques is presented in \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#D}{Case Study D} (page \pageref{ref-0352}).

\textbf{Note:} If you are reading this book online, you can right-click on any of the m-file links and select \textbf{Save Link As...} to download them to your computer, then place them in the Matlab path for use within Matlab.

\section{Accuracy and precision of peak parameters \label{ref-0267}\label{ref-0268}}

Iterative curve fitting is often used to measure the position, height, and width of peaks in a signal, especially when they overlap significantly. There are four major sources of error in measuring these peak parameters by iterative curve fitting. This section makes use of my \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m} function. Instructions are \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{here} or type "help peakfit". (Once you have peakfit.m in your path, you can copy and paste, or drag and drop, any of the following single-line or multi-line code examples into the Matlab or Octave editor or into the command line and press \textbf{Enter} to execute it). 

\subsection{a. Model errors. \label{ref-0269}\label{ref-0270}\label{ref-0271}\label{ref-0272}}

\InsImageInline{0.5}{l}{PerfectFit.png}
Peak shape. If you have the wrong model for your peaks, the results cannot be expected to be accurate; for instance, if your actual peaks are Lorentzian in shape, but you fit them with a Gaussian model or \textit{vice versa}. For example, a single isolated Gaussian peak at x=5, with a height of 1.000 fits a Gaussian model virtually perfectly, using the Matlab user-defined \href{http://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#command}{peakfit} function (page \pageref{ref-0448}), as shown on the right. (The 5th input argument for the peakfit function specifies the shape of peaks to be used in the fit; "1" means Gaussian).

\texttt{{\textgreater}{\textgreater} x=[0:.1:10];y=exp(-(x-5).\textasciicircum{}2);} 

\texttt{{\textgreater}{\textgreater} [FitResults,MeanFitError]=peakfit([x' y'],5,10,1,}\texttt{\textbf{1}}\texttt{)}

\texttt{\textcolor{color-17}{Peak\# Position Height Width Area}}\index{Area}

\texttt{1 5 1 1.6651 1.7725}

\texttt{MeanFitError = 7.8579e-07 R2= 1}

The "FitResults" are, from left to right, peak number, peak position, peak height, peak width, and peak area. The MeanFitError, or just "fitting error", is the square root of the sum of the squares of the differences between the data and the best-fit model, as a percentage of the maximum signal in the fitted region. Recent versions of peakfit return also the R2, the "R-squared" or coefficient of determination, which is exactly 1 for a perfect fit. Note the agreement between the area (1.7725) with the \textit{theoretical} area under the curve of exp(-x\textsuperscript{2}), which is the \href{http://en.wikipedia.org/wiki/Gaussian\_function\#Properties}{square root of pi}, If you are reading this online, click for \href{http://www.wolframalpha.com/input/?i=integral+of+y\%3Dexp\%28-x\%5E2\%29+from+-inf+to+inf}{Wolfram Alpha solution}.

But this same peak, when fitted with the incorrect model (a \textit{Logistic} model, peak shape number 3), gives a fitting error of 1.4\% and height and width errors of 3\% and 6\%, respectively. However, the peak area error is only 1.7\%, because the height and width errors partially cancel out. So, you do not have to have a perfect model to get a decent area measurement.

\texttt{{\textgreater}{\textgreater} [FitResults,MeanFitError]=peakfit([x' y'],5,10,1,}\texttt{\textbf{3}}\texttt{)}

\texttt{\textcolor{color-17}{Peak\# Position Height Width Area}}\index{Area}

\texttt{FitResults =}

\texttt{1 5.0002 0.96652 1.762 1.7419}

\texttt{MeanFitError =1.4095}

When fit with an even more incorrect \textit{Lorentzian} model (peak shape 2, shown on the right), this peak gives a 6\% fitting error and height, width and area errors of 8\%, 20\%, and 17\%, respectively. 

\texttt{{\textgreater}{\textgreater} [FitResults,MeanFitError]=peakfit([x' y'],5,10,1,}\texttt{\textbf{2}}\texttt{)}

\InsImageInline{0.5}{l}{LvsG.GIF.png}\texttt{\textcolor{color-17}{Peak\# Position Height Width Area}}\index{Area}

\texttt{1 5 1.0876 1.3139 2.0579}

\texttt{MeanFitError =5.7893}

But, practically speaking, it is unlikely that your estimate of a model will be that far off; it is much more likely that your actual peaks are some unknown combination of peak shapes, such as Gaussian with a little Lorentzian mixed in or vice versa, or some slightly asymmetrical modification of a standard symmetrical shape. So, if you use an available model that is at least \textit{close} to the actual shape, the parameter errors may not be so bad and may, in fact, be better than other measurement methods.

\InsImageInline{0.5}{l}{image177.png}So clearly the larger the fitting errors, the larger are the parameter errors, but the parameter errors are of course not \textit{equal} to the fitting error (that would be \textit{too} easy). Also, the peak \textit{height} and \textit{width} are the parameters most susceptible to errors. The peak \textit{positions} are measured accurately even if the model is wrong, if the peak is symmetrical and is not highly overlapping with other peaks. 

\textit{A good fit is not by itself proof that the shape function you have chosen is the correct one}. In some cases, the wrong function can give a fit that looks perfect. For example, the graph on the left shows \href{https://terpconnect.umd.edu/~toh/spectrum/GoodFitWrongModel.png}{a fit of a real data set} to a 5-peak Gaussian model that exhibits a low percent fitting error residuals that look random - usually an indicator of a good fit. But in fact\textit{, in this case, the model is wrong}; that data came from an experimental domain where the underlying shape is fundamentally non-Gaussian but, in some cases, can look very like a Gaussian. As another example, a data set consisting of peaks with a \href{https://terpconnect.umd.edu/~toh/spectrum/voigt.m}{Voigt profile} peak shape can be fitted with a \href{https://terpconnect.umd.edu/~toh/spectrum/2GL.png}{weighted sum of a Gaussian and a Lorentzian} almost as well as with an actual \href{https://terpconnect.umd.edu/~toh/spectrum/2voigt.png}{Voigt model}, even though those models are \textit{not the same mathematically}; the difference in fitting error is so small that it would likely be \href{https://terpconnect.umd.edu/~toh/spectrum/2voigtNoisy.png}{obscured by the random noise} if it were a real experimental signal. The same thing can occur in sigmoidal signal shapes: a pair of simple 2-parameter logistic functions seems to fit \href{https://terpconnect.umd.edu/~toh/spectrum/WrongModel.png}{this example data} pretty well, with a fitting error of less than 1\%; you would have no reason to doubt the goodness of fit unless the random noise is low enough so you can see that the residuals are wavy. In fact, a \textit{3-parameter} logistic (\href{https://en.wikipedia.org/wiki/Gompertz\_function}{Gompertz} function) \href{https://terpconnect.umd.edu/~toh/spectrum/RightModel.png}{fits much better}, and the residuals are random, not wavy. In such cases you cannot depend solely on what \textit{looks} like a good fit to determine whether the fit is model is optimum; sometimes you need to know more about the peak shape expected in that kind of experiment, especially if the data are noisy. At best, if you do get a good fit with random non-wavy residuals, you can claim only that the data \textit{are consistent with} the proposed model. 

Sometimes the accuracy of the model is \textit{not} so important. Take the example of \textit{quantitative analysis applications}, where the peak heights or areas measured by curve fitting is used to determine the concentration of the substance that created the peak by constructing a \href{https://terpconnect.umd.edu/~toh/models/CalibrationCurve.html}{\textit{calibration curve}} (page \pageref{ref-0490}) based on laboratory prepared standards solutions of known concentrations. In that case, the necessity of using the exact peak model is lessened, if the shape of the unknown peak is \textit{constant and independent of concentration}. If the wrong model shape is used, the R\textsuperscript{2} \textit{for curve fitting} will be poor (much less than 1.000) and the peak heights and areas measured by curve fitting will be inaccurate, but the \textit{error will be the same} for the unknown samples and the known calibration standards, so the error will cancel out. As a result, the R\textsuperscript{2} \textit{for the calibration curve} can be very high (0.9999 or better) and the \textit{measured concentrations will be no less accurate than they would have been with a perfect peak shape model}. Even so, it is useful to use as accurate a model peak shape as possible, because the R\textsuperscript{2} for curve fitting will work better as a warning indicator if something unexpected goes wrong during the analysis (such as an increase in the noise or the appearance of an interfering peak from a foreign substance). See \href{https://terpconnect.umd.edu/~toh/spectrum/PeakShapeAnalyticalCurve.m}{PeakShapeAnalyticalCurve.m} for a Matlab/Octave demonstration.

\textbf{Number of peaks.} Another source of model error occurs if you have the wrong \textit{number of peaks} in your model, for example if the signal actually has two peaks but you try to fit it with only one peak. In the example below, a line of Matlab code generates a simulated signal with of two Gaussian peaks at x=4 and x=6 with peaks heights of 1.000 and 0.5000 respectively and widths of 1.665, plus random noise with a standard deviation 5\% of the height of the largest peak (a signal-to-noise ratio of 20): 

\texttt{{\textgreater}{\textgreater} x=[0:.1:10];}

\texttt{{\textgreater}{\textgreater} y=exp(-(x-6).\textasciicircum{}2)+.5*exp(-(x-4).\textasciicircum{}2)+.05*randn(size(x));}

In a real experiment, you would not usually know the peak positions, heights, and widths; you would be using curve fitting to \textit{measure} those parameters. Let us assume that, based on previous experience or some preliminary trial fits, you have established that the optimum peak \textit{shape} model is Gaussian, but you do not know for sure how many peaks are in this group. If you start out by fitting this signal with a \textit{single}-peak Gaussian model, you get:

\texttt{{\textgreater}{\textgreater} [FitResults,MeanFitError]=peakfit([x' y'],5,10,}\texttt{\textbf{1}}\texttt{,1)}

\texttt{FitResults =}

 \texttt{\textcolor{color-17}{Peak\# Position Height Width Area}}\index{Area}

  \texttt{1 5.5291 0.86396 2.9789 2.7392}

\texttt{MeanFitError = 10.467}

\texttt{\InsImageInline{0.5}{l}{1peak.png}}The residual plot (bottom panel) shows a "wavy" structure that is clearly visible in the random scatter of points due to the random noise in the signal. This means that the fitting error is not limited by the random noise; it is a clue that the model is not quite right. 

However, a fit with \textit{two} peaks yields much better results (The 4\textsuperscript{th} input argument for the peakfit function specifies the number of peaks to be used in the fit).

\texttt{{\textgreater}{\textgreater} [FitResults,MeanFitError]=peakfit([x' y'],5,10,}\texttt{\textbf{2}}\texttt{,1)}

\texttt{FitResults =}

      \texttt{\textcolor{color-17}{Peak\# Position Height Width Area}}\index{Area}

  \texttt{1 4.0165 0.50484 1.6982 0.91267}

  \texttt{2 5.9932 1.0018 1.6652 1.7759}

\texttt{MeanFitError = 4.4635}

\InsImageInline{0.5}{l}{2peak.png}Now the residuals have a random scatter of points, as would be expected if the signal had been accurately fit except for the random noise. Moreover, the fitting error is much lower (less than half) of the error with only one peak. In fact, the fitting error is just about what we would expect in this case based on the 5\% random noise in the signal (estimating the relative standard deviation of the points in the baseline visible at the edges of the signal). Because this is a simulation in which we know beforehand the true values of the peak parameters (peaks at x=4 and x=6 with peaks heights of 1.0 and 0.50 respectively and widths of 1.665), we can calculate the parameter errors (the difference between the real peak positions, heights, and widths and the measured values). Note that they are quite accurate (in this case within about 1\% relative on the peak height and 2\% on the widths), which is better than the 5\% random noise in this signal because of the averaging effect of fitting to multiple data points in the signal. 

However, if going from one peak to two peaks gave us a better fit, why not go to \textit{three} peaks? If there were no noise in the data, and if the underlying peak shape were perfectly matched by the model, then the fitting error would have already been essentially zero with two model peaks. Adding a third peak to the model would yield a vanishingly small height for that third peak. But in our examples here, as in real data, there is always some random noise, and the result is that the third peak height will not be zero. Changing the number of peaks to three gives these results: 

\texttt{\InsImageInline{0.5}{l}{3peak.png}}\texttt{{\textgreater}{\textgreater} [FitResults,MeanFitError]=peakfit([x' y'],5,10,}\texttt{\textbf{3}}\texttt{,1)}

\texttt{FitResults =}

          \texttt{\textcolor{color-17}{Peak\# Position Height Width Area}}\index{Area}

  \texttt{1 4.0748 0.51617 1.7874 0.98212}

  \texttt{2 6.7799 0.089595 2.0455 0.19507}

  \texttt{3 5.9711 0.94455 1.53 1.5384}

\texttt{MeanFitError = 4.3878}

The fitting algorithm has now tried to fit an additional low-amplitude peak (numbered peak 2 in this case) located at x=6.78. The fitting error is lower than for the 2-peak fit, but only slightly lower, and \textit{the residuals are no less visually random} that with a 2-peak fit. So, knowing nothing else, a 3-peak fit might be rejected on that basis alone. In fact, there is a serious downside to fitting more peaks than are present in the signal: it increases the parameter measurement errors of the peaks that are present. Again, we can prove this because we know beforehand the true values of the peak parameters: clearly, the peak positions, heights, and widths of the two real peaks than are in the signal (peaks 1 and 3) are significantly less accurate than those of the 2-peak fit. 

Moreover, if we repeat that fit with the \textit{same signal} but with a \textit{different} sample of random noise (simulating a repeat measurement of a stable experimental signal in the presence of random noise), the additional third peak in the 3-peak fit will bounce around all over the place (because the third peak is fitting the random \textit{noise}, not an actual peak in the signal). 

\texttt{{\textgreater}{\textgreater} x=[0:.1:10];}

\texttt{{\textgreater}{\textgreater} y=exp(-(x-6).\textasciicircum{}2)+.5*exp(-(x-4).\textasciicircum{}2)+.05*randn(size(x));}

\texttt{{\textgreater}{\textgreater} [FitResults,MeanFitError]=peakfit([x' y'],5,10,3,1)}

\texttt{FitResults =}

      \texttt{\textcolor{color-17}{Peak\# Position Height Width Area}}\index{Area}

  \texttt{1 4.115 0.44767 1.8768 0.89442}

  \texttt{2 5.3118 0.09340 2.6986 0.26832}

  \texttt{3 6.0681 0.91085 1.5116 1.4657}

\texttt{MeanFitError = 4.4089}

With this new set of data, two of the peaks (numbers 1 and 3) have roughly the same position, height, and width, but peak number 2 has changed substantially compared to the previous run. Now we have an even more compelling reason to reject the 3-peak model: the 3-peak solution \textit{is not stable}. And because this is a simulation in which we know the right answers, we can verify that the accuracy of the peak heights is substantially poorer (about 10\% error) than expected with this level of random noise in the signal (5\%). If we were to run a 2-peak fit on the same new data, we get much better measurements of the peak heights.

\texttt{{\textgreater}{\textgreater} [FitResults,MeanFitError]=peakfit([x' y'],5,10,2,1)}

\texttt{FitResults =}

        \texttt{\textcolor{color-17}{Peak\# Position Height Width Area}}\index{Area}

          \texttt{1 4.1601 0.49981 1.9108 1.0167}

  \texttt{2 6.0585 0.97557 1.548 1.6076}

\texttt{MeanFitError = 4.4113}

If this is repeated several times, it turns out that the peak parameters of the peaks at x=4 and x=6 are, on average, more accurately measured by the 2-peak fit. In practice, the best way to evaluate a proposed fitting model is to fit several repeat measurements of the same signal (if that is practical experimentally) and to compute the standard deviation of the peak parameter values. 

In real experimental work, of course, you usually do not \textit{know} the right answers beforehand, so that is why it is important to use methods that work well when you \textit{do} know. Here’s an example of a set of real data that was fit with a succession of \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter\_2\_fit2components.png}{\textbf{2}}, \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter\_2\_fit3components.png}{\textbf{3}}, \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter\_2\_fit4components.png}{\textbf{4}} and \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter\_2\_fit5components.png}{\textbf{5}} Gaussian models, until the residuals became random. With each added component, the fitting error becomes smaller and the residuals become more random. But beyond 5 components point, there is little to be gained by adding more peaks to the model. Another way to determine the minimum number of models peaks needed is to plot the fitting error vs the number of model peaks; the point at which the fitting error reaches a minimum, and increases afterward, would be the fit with the "\href{http://en.wikipedia.org/wiki/Coefficient\_of\_determination\#Adjusted\_R2}{ideal combination of having the best fit without excess/unnecessary terms}". The Matlab/Octave function \href{https://terpconnect.umd.edu/~toh/spectrum/testnumpeaks.m}{testnumpeaks.m} (\texttt{R = testnumpeaks(x, y, peakshape, extra, NumTrials, MaxPeaks)}) applies this idea by fitting the x,y data to a series of models of shape \texttt{peakshape} containing 1 to \texttt{MaxPeaks} model peaks. The correct number of underlying peaks is either the model with the lowest fitting error, or, if two or more models have about the same fitting error, the model with the \textit{least} number of peaks. The Matlab/Octave demo script \href{https://terpconnect.umd.edu/~toh/spectrum/NumPeaksTest.m}{NumPeaksTest.m} uses this function with noisy computer-generated signals containing a user-selected 3, 4, 5 or 6 underlying peaks. With very noisy data, however, the technique is not always reliable.

\textbf{Peak width constraints.} Finally, there is one more thing that we can do that might improve the peak parameter measurement accuracy, and it concerns the peak widths. In all the above simulations, the basic assumption that \textit{all} the peak parameters were unknown and independent of one another. In some types of measurements, however, the peak widths of each group of adjacent peaks are all expected to be equal, on the basis of first principles or previous experiments. This is a common situation in analytical chemistry, especially in atomic spectroscopy and in chromatography, where the peak widths are determined largely by instrumental factors. 

In the current simulation, the true widths of both peaks are in fact equal to 1.665, but all the results \texttt{\InsImageInline{0.5}{l}{2peakFixedWidth.png}}above show that the \textit{measured} peak widths are close but not quite equal, due to random noise in the signal. The unequal peak widths are a consequence of the random noise, not real differences in peak width. But we can introduce an \textit{equal-width} constraint into the fit by using peak shape 6 (Equal-width Gaussians) or peak shape 7 (Equal-width Lorentzian). Using peak shape 6 on the same set of data as the previous example:

\texttt{{\textgreater}{\textgreater} [FitResults,MeanFitError] =peakfit([x' y'],5,10,2,}\texttt{\textbf{6}}\texttt{)}

\texttt{FitResults =}

           \texttt{\textcolor{color-17}{Peak\# Position Height Width Area}}\index{Area}

  \texttt{1 4.0293 0.52818 1.5666 0.8808}

  \texttt{2 5.9965 1.0192 1.5666 1.6997}

\texttt{MeanFitError =} \texttt{4.5588}

This "equal width" fit forces all the peaks within one group to have exactly the same width, but that width is determined by the program from the data. The result is a \textit{slightly higher} fitting error (in this case 4.5\% rather than 4.4\%), but - perhaps surprisingly - the peak parameter measurements are usually \textit{more accurate} and \textit{more reproducible} (Specifically, the relative standard deviations are on average lower for the equal-width fit than for an unconstrained-width fit to the same data, assuming of course that the true underlying peak widths are equal). \textit{This is an exception} to the general expectation that lower fitting errors result in lower peak parameter errors. It is an illustration of the general rule that \textit{the more you know about the nature of your signals, and the closer your chosen model adheres to that knowledge, the better the results}. In this case we knew that the peak shape was Gaussian (although we could have verified that choice by trying other candidate peaks shapes). We determined that the number of peaks was 2 by inspecting the residuals and fitting errors for 1, 2, and 3 peak models. And then we introduced the constraint of equal peak widths within each group of peaks, based on prior knowledge of the experiment rather than on inspection of residuals and fitting errors. Here's another example, with real experimental data from a measurement where the \textit{adjacent peak widths are expected to be equal}, showing the result of an \href{https://terpconnect.umd.edu/~toh/spectrum/Unconstrained4G.png}{unconstrained fit} and an \href{https://terpconnect.umd.edu/~toh/spectrum/EqualWidth4G.png}{equal width fit}; the fitting errors are slightly larger for the equal-width fit, but that is to be preferred in this case. \textit{Not every experiment can be expected to yield peaks of equal width, but when it does, it is better to make use of that constraint.} 

\textbf{Fixed-width shapes.} Going one step beyond \textit{equal widths} (in peakfit version 7.6 and later), you can also specify a \textit{fixed-width} shapes (shape numbers 11, 12, 34-37), in which the \textit{widths of the peaks are known beforehand}, but are not necessarily equal, and are specified as a \textit{vector} in input argument 10, one element for each peak, rather than being determined from the data as in the equal-width fit above. Introducing this constraint onto the previous example, and supplying an accurate width as the 10th input argument:

 \texttt{{\textgreater}{\textgreater} [FitResults,MeanFitError]=peakfit([x' y'],0,0,2,}\texttt{\textbf{11}}\texttt{,0,0,0,0,}\texttt{\textbf{[1.6661.666]}}\texttt{)}

\texttt{FitResults =}

      \texttt{\textcolor{color-17}{Peak\# Position Height Width Area}}\index{Area}

  \texttt{1 3.9943 0.49537 1.666 0.8785}

  \texttt{2 5.9924 0.98612 1.666 1.7488}

\texttt{MeanFitError = 4.8128}

Comparing to the previous equal-width fit, the fitting error of 4.8\% is larger here (because there are fewer degrees of freedom to minimize the error), but the parameter errors, particularly the peak heights, are \textit{more accurate} because the width information provided in the input argument was more accurate (1.666) than the width determined by the equal-width fit (1.5666). Again, not every experiment yields peaks of known width, but when it does, it is better to make use of that constraint. For example, see \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Examples}{Example 35} (page ~\ref{ref-0454}\pageref{ref-0452}) and the Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/WidthTest.m}{WidthTest.m} (typical results for a Gaussian/Lorentzian blend shape shown below, showing that the more constraints, the greater the fitting error but the lower the parameter errors, if the constraints are accurate).


\begin{table}
\begin{tabularx}{\textwidth}{|
p{\dimexpr 0.438\linewidth-2\tabcolsep-2\arrayrulewidth}|
p{\dimexpr 0.134\linewidth-2\tabcolsep-\arrayrulewidth}|
p{\dimexpr 0.153\linewidth-2\tabcolsep-\arrayrulewidth}|
p{\dimexpr 0.139\linewidth-2\tabcolsep-\arrayrulewidth}|
p{\dimexpr 0.136\linewidth-2\tabcolsep-\arrayrulewidth}|} \hline 
\centering\arraybackslash{}\textbf{Relative percent error} & \centering\arraybackslash{}\textbf{Fitting error} & \centering\arraybackslash{}\textbf{Position Error} & \centering\arraybackslash{}\textbf{Height Error} & \centering\arraybackslash{}\textbf{Width Error} \\\hline 
\centering\arraybackslash{}Unconstrained shape factor and widths: shape 33 & \centering\arraybackslash{}0.78\% & \centering\arraybackslash{}0.39\% & \centering\arraybackslash{}0.80\% & \centering\arraybackslash{}1.66\% \\\hline 
\centering\arraybackslash{}Fixed shape factor and variable widths: shape 13 & \centering\arraybackslash{}0.79\% & \centering\arraybackslash{}0.25\% & \centering\arraybackslash{}1.3\% & \centering\arraybackslash{}0.98\% \\\hline 
\centering\arraybackslash{}Fixed shape factor and fixed widths: shape 35 & \centering\arraybackslash{} 0.8\% & \centering\arraybackslash{}0.19\% & \centering\arraybackslash{}0.695 & \centering\arraybackslash{}0.0\% \\\hline 
\end{tabularx}
\end{table}
\textbf{Multiple linear regression (peakfit version 9).} Finally, note that if the peak \textit{positions} are also known, and only the peak \textit{heights} are unknown, you do not even need to use the iterative fitting method at all; you can use the easier and faster \textit{multilinear regression} technique (also called \textit{classical least-squares}, page \pageref{ref-0242}) which is implemented by the function \href{https://terpconnect.umd.edu/~toh/spectrum/cls.m}{cls.m} and by version 9 of \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit.m} as shape number 50. Although multilinear regression results in fitting error slightly \textit{greater} (and R\textsuperscript{2 }lower), the errors in the measured peak heights are often \textit{less}, as in this example from \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit9demo.m}{peakfit9demo.m}, where the true peak heights of the \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit9.png}{three overlapping Gaussian peaks} are 10, 30, and 20. 

\texttt{Multilinear regression results (known position and width):}

  \texttt{Peak Position Height Width Area}\index{Area}

  \texttt{1 400 9.9073 70 738.22}

  \texttt{2 500 29.995 85 2714}

  \texttt{3 560 19.932 90 1909.5}

\texttt{\%fitting error=1.3048 R2= 0.99832 \%MeanHeightError=0.427}

\texttt{Unconstrained iterative non-linear least-squares results:}

  \texttt{Peak Position Height Width Area}\index{Area}

  \texttt{1 399.7 9.7737 70.234 730.7}

  \texttt{2 503.12 32.262 88.217 3029.6}

  \texttt{3 565.08 17.381 86.58 1601.9}

\texttt{\%fitting error=1.3008 R2= 0.99833 \%MeanHeightError=7.63}

This demonstrates dramatically how different measurement methods can \textit{look} the same, and give fitting errors almost the same, and yet differ greatly in parameter measurement accuracy. (The similar script \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit9demoL.m}{peakfit9demoL.m} is the same thing with Lorentzian peaks).

\href{https://terpconnect.umd.edu/~toh/spectrum/SmallPeak.m}{SmallPeak.m} is a demonstration script comparing all these techniques applied to the challenging problem of measuring the height of a small peak that is closely overlapped with, and completely obscured by, a much larger peak. It compares unconstrained, equal-width, and fixed-position iterative fits (using peakfit.m) with a classical least-squares fit in which \textit{only} the peak heights are unknown (using \href{https://terpconnect.umd.edu/~toh/spectrum/cls.m}{cls.m}). It helps to spread out the four figure windows, so you can observe the dramatic difference in the stability of the different methods. A final table of relative percent peak height errors shows that \textit{the more the constraints, the better the results} (but only if the constraints are \textit{justified}). The real key is to know which parameters can be relied upon to be constant and which must be allowed to vary. 

Here's a screen video (\href{https://terpconnect.umd.edu/~toh/spectrum/MorePeaksLowerFittingError.mp4}{MorePeaksLowerFittingError.mp4}) of a real-data experiment using the interactive peak fitter \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{ipf.m} (page \pageref{ref-0461}) with a complex experimental signal in which several different fits were performed using models from 4 to 9 variable-width, equal-width, and fixed-width Gaussian peaks. The fitting error gradually \textit{decreases from 11\% initially to 1.4\%} as more peaks are used, but \textit{is that really justified?} If the objective is simply to get a good fit, then do whatever it takes. But if the objective is to extract some useful information from the model peak parameters, then more specific knowledge about that experiment is needed: how many peaks are really expected; are the peak widths really expected to be constrained? Note that in this case the residuals (bottom panel) are \textit{not random} and always have a distinct "wavy" character, suggesting that the data \textit{may have been smoothed} before curve fitting (not a good idea: see \url{http://wmbriggs.com/blog/?p=195}). Thus, there is a real possibility that some of those 9 peaks are simply "fitting the noise", as will be discussed further on page \pageref{ref-0347}.\label{ref-0273}

\label{ref-0274}

\textbf{b. Background correction}\label{ref-0275}

The peaks that are measured in many scientific instruments are sometimes superimposed on a non-specific background or baseline. Ordinarily, the experimental protocol is designed to minimize the background or to compensate for the background, for example by subtracting a "\href{https://en.wikipedia.org/wiki/Blank\_\%28solution\%29}{blank}" signal from the signal of an actual specimen. But even so, there is often a residual background that cannot be eliminated completely experimentally. The origin and shape of that background depend on the specific measurement method, but often this background is a broad, tilted, or curved shape, and the peaks of interest are comparatively narrow features superimposed on that background. In some cases, the baseline may be another interfering peak that overlaps the peaks of interest. The presence of the background has relatively little effect on the peak \textit{positions}, but it is impossible to measure the peak heights, width, and areas accurately unless something is done to account for the background. 

 Various methods are described in the literature for estimating and subtracting the background in such cases. The simplest assumption is that the background can be approximated as a simple function in the local group of peaks being fit together, for example as a constant (flat), straight-line (linear) or curved line (quadratic). This is the basis of the "BaselineMode" modes in the \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{ipf.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.m}{iSignal.m}, and \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{iPeak.m} functions, which are selected by the \textbf{T} key to cycle through \textit{OFF}, \textit{linear}, \textit{quadratic}, \textit{flat}, and \textit{mode(y)} modes. In the \textit{flat} mode, a constant baseline is included in the curve fitting calculation. In \textit{linear} mode, a straight-line baseline connecting the two ends of the signal segment in the upper panel will be automatically subtracted \textit{before the iterative curve fitting}. In \textit{quadratic} mode, a parabolic baseline is subtracted. In the last two modes, you must adjust the pan and zoom controls to isolate the group of overlapping peaks to be fit, so that the signal returns to the local background at the left and right ends of the window. In the \textit{mode(y)} mode, the most common value is subtracted from all points. \label{ref-0276}


\begin{center}
\InsImageInline{0.5}{l}{Baseline.GIF.png}\InsImageInline{0.5}{l}{BaselineSubtracted.GIF.png}\InsImageInline{0.5}{l}{BaselineSubtractedFit.GIF.png}\textit{Example of an experimental chromatographic signal in ipf.m. From left to right, (1) Raw data with weak peaks on a tilted baseline. The peaks of interest are selected using the pan and zoom controls, adjusted so that the signal returns to the local background at the edges of the segment displayed in the upper window; (2) The linear baseline is subtracted when BaselineMode set to 1 in ipf.m by pressing the} \textbf{\textit{T}} \textit{key; (3) The selected region is fit with a three-peak Gaussian model, activated by pressing} \textbf{\textit{3}}\textit{,} \textbf{\textit{G}}\textit{,} \textbf{\textit{F}} \textit{(meaning} \textbf{\textit{3}} \textit{peaks,} \textbf{\textit{G}}\textit{aussian,} \textbf{\textit{F}}\textit{it). Press} \textbf{\textit{R}} \textit{to print out a peak table.}
\end{center}


Alternatively, it may be better to subtract the background from the \textit{entire} signal first, before further operations are performed. As before, the simplest assumption is that the background is piece-wise linear, that is, can be approximated as a series of small straight-line segments. This is the basis of the multiple-point background subtraction mode in \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{ipf.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{iPeak.m}, and in \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{iSignal.}  The user enters the number of points that is thought to be sufficient to define the baseline, then click where the baseline is thought to be along the entire length of the signal in the lower whole-signal display (e.g. on the valleys between the peaks). After the last point is clicked, the program interpolates between the clicked points and subtracts the piece-wise linear background from the original signal.


\begin{center}
\InsImageInline{0.5}{l}{ChromB.gif.png} \InsImageInline{0.5}{l}{ChromBC.gif.png}
\end{center}



\begin{center}
  \textit{From left to right, (1) Raw data with peaks superimposed on the baseline. (2) Background subtracted from the entire signal using the multipoint background subtraction function in} \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{\textit{iPeak.m}} \textit{(ipf.m and iSignal.m have the same function).}
\end{center}


Sometimes, even without an actual baseline present, the peaks may overlap enough so that the signal never returns to the baseline, making it seem that there is a baseline to be corrected. This can occur especially with peaks shapes that have gradually sloping sides, such as the Lorentzian, as \href{https://terpconnect.umd.edu/~toh/spectrum/Demofindpeaksb3Large.png}{shown in this example}. Curve fitting \textit{without} baseline correction will work in that case. 

\InsImageInline{0.5}{l}{4peaks.png}In many cases, the background may be modeled as a broad peak whose maximum falls \textit{outside} of the range of data acquired, as in the real-data example on the left. It may be possible to fit the off-screen peak simply by including \textit{an extra peak in the model to account for the baseline}. In the example on the left, there are three clear peaks visible, superimposed on a tilted baseline. In this case, the signal was fit nicely with four, rather than three, variable-width Gaussians, with an error of only 1.3\%. The additional broad Gaussian, with a peak at x = -38.7, serves as the baseline. (Obviously, you should not use the equal-width shapes for this, because the background peak is broader than the other peaks). \href{https://terpconnect.umd.edu/~toh/spectrum/FiveLorentzianBackground.png}{Another real-data example} exhibits four on-screen peaks of very different heights and widths on a broad baseline. Such a signal can be difficult to fit because the starting point for most iterative fits is that all peaks have about the same width. So, in some cases, assigning a custom ``start'' vector may be necessary. Using the \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{ipf.m} (page \pageref{ref-0464}) ‘C’ and ‘W’ keys can help.

In another real-data example of an \href{https://terpconnect.umd.edu/~toh/spectrum/GaussianBaseline.png}{experimental spectrum}, the linear baseline subtraction ("BaselineMode") mode described above is used in conjunction with a 5-Gaussian model, with one Gaussian component fitting the broad peak that may be part of the background and the other four fitting the sharper peaks. This fits the data very well (0.5\% fitting error), but a fit like this can be difficult to get, because there are so many other solutions with slightly higher fitting errors; it may take several trials. It can help if you specify the \textit{start values} for the iterated variables, rather than using the default choices; all the software programs described here have that capability. 

The Matlab/Octave function \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m} will accept a peak shape that is a \textit{vector of different shape numbers,} which can be useful for baseline correction. As an example, consider a weak Gaussian peak on a sloped straight-line baseline, using a 2-component fit with one Gaussian component and one variable-slope straight line ('slope', shape 26), specified by using the vector ([1 26]) as the shape argument:

\texttt{x=8:.05:12;y=x+exp(-(x-10).\textasciicircum{}2);} 

\texttt{[FitResults,GOF]= peakfit([x;y],0,0,2,[1 26],[1 1],1,0)}

\texttt{FitResults =} 

  \texttt{1 10 1 1.6651 1.7642}

  \texttt{2 4.485 0.22297 0.05 40.045}

\texttt{GOF = 0.0928 0.9999}

If the baseline seems to be curved rather than straight, you can model the baseline with a \textit{quadratic} (shape 46) rather than a linear slope (peakfit \textit{version 8} and later). 

If the baseline seems to be different on either side of the peak, you can try to model the baseline with an \textit{S-shape} (sigmoid), either an up-sigmoid, shape 10 \href{https://terpconnect.umd.edu/~toh/spectrum/UpSigmoidBaseline.png}{(click for graphic)}, \texttt{peakfit([x;y],0,0,2,[1 10],[0 0]}, or a down-sigmoid, shape 23 \href{https://terpconnect.umd.edu/~toh/spectrum/DownSigmoidBaseline.png}{(click for graphic),} \texttt{peakfit([x;y],0,0,2,[1 23],[0 0]}, in these examples leaving the peak modeled as a Gaussian.

If the signal is very weak compared to the baseline, the fit can be helped by adding rough first guesses ('start') using the 'polyfit' function to generate automatic first guesses for the sloping baseline. For example, with \textit{two} overlapping signal peaks and a 3-peak fit with peakshape=[1 1 26]. 

\texttt{x=4:.05:16;}

\texttt{y=x+exp(-(x-9).\textasciicircum{}2)+exp(-(x-11).\textasciicircum{}2)+.02.*randn(size(x));}

\texttt{start=[8 1 10 1 polyfit(x,y,1)];}

\texttt{peakfit([x;y],0,0,3,[1 1 26],[1 1 1],1,start)}

 A similar technique can be employed in a \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Spreadsheets}{spreadsheet}, as demonstrated in \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter2GaussianBaseline.xlsx}{CurveFitter2GaussianBaseline.xlsx} (\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter2GaussiansPlusBaseline.png}{graphic}). 

The downside to including the baseline as a variable component is that it increases the number of degrees of freedom, increases the execution time, and increases the possibility of unstable fits. Specifying start values can help.

\subsection{\textbf{c. Random noise in the signal. } \label{ref-0277}\label{ref-0278}}

Any experimental signal has a certain amount of random noise, which means that the individual data points scatter randomly above and below their true values. The assumption is ordinarily made that the scatter is equally above and below the true signal so that the long-term average approaches the true mean value; the noise "averages to zero" as it is often said. The practical problem is that any given recording of the signal contains only one finite sample of the noise. If another recording of the signal is made, it will contain another independent sample of the noise. These noise samples are not infinitely long and therefore do not represent the true long-term nature of the noise. This presents two problems: (1) an individual sample of the noise will not "average to zero" and thus the parameters of the best-fit model will not necessarily equal the true values, and (2) the magnitude of the noise during one sample might not be typical; the noise might have been randomly greater or smaller than average during that time. This means that the mathematical "propagation of error" methods, which seek to estimate the likely error in the model parameters based on the noise in the signal, will be subject to error (\textit{underestimating} the error if the noise happens to be \textit{lower} than average and \textit{overestimating} the errors if the noise happens to be \textit{larger} than average). 

A better way to estimate the parameter errors is to record multiple samples of the signal, fit each of those separately, compute the model parameters from each fit, and calculate the standard error of each parameter. However, if that is not practical, it is possible to simulate such measurements by adding random noise to a model with known parameters, then fitting that simulated noisy signal to determine the parameters, then repeating the procedure repeatedly with different sets of random noise. This is exactly what the script \href{https://terpconnect.umd.edu/~toh/spectrum/DemoPeakfit.m}{DemoPeakfit.m} (which requires the \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m} function) does for simulated noisy peak signals such as those illustrated below. It is easy to demonstrate that, as expected, the average fitting error precision and the relative standard deviation of the parameters increases directly with the random noise level in the signal. But the precision and the accuracy of the measured parameters \textit{also} depend on which parameter it is (peak positions are always measured more accurately than their heights, widths, and areas) and on the peak height and extent of peak overlap (the two left-most peaks in this example are not only weaker but also more overlapped than the right-most peak, and therefore exhibit poorer parameter measurements). In this example, the fitting error is 1.6\% and the percent relative standard deviation of the parameters ranges from 0.05\% for the peak position of the largest peak to 12\% for the peak area of the smallest peak.

 \InsImageInline{0.5}{l}{1peak.GIF.png}\InsImageInline{0.5}{l}{2peaks.GIF.png}\InsImageInline{0.5}{l}{3peaks.GIF.png}

\textbf{\textit{Overlap matters}}\textit{: The errors in the values of peak parameters measured by curve fitting depend not only on the characteristics of the peaks in question and the signal-to-noise ratio but also upon other peaks that are overlapping it. From left to right:}

\textbf{\textit{(1)}} \textit{a single peak at x=100 with a peak height of 1.0 and width of 30 is fitted with a Gaussian model, yielding a relative fit error of 4.9\% and relative standard deviation of peak position, height, and width of 0.2\%, 0.95\%, and 1.5\%, respectively.} 

\textbf{\textit{(2)}} \textit{The same peak, with the same noise level but with another peak overlapping it,} \textbf{\textit{reduces}} \textit{the relative fit error to 2.4\% (because the addition of the second peak increases overall signal amplitude). However, it} \textbf{\textit{increases}} \textit{the relative standard deviation of peak position, height, and width to 0.84\%, 5\%, and 4\% -} \textbf{\textit{a seemingly better fit, but with poorer precision}} \textit{for the first peak.}

\textbf{\textit{(3)}} \textit{The addition of a third (larger but non-overlapping) peak reduces the fit error further to 1.6\%, but the relative standard deviation of peak position, height, and width of the first peak are about the same as with two peaks, because} \textbf{\textit{the third peak does not overlap}} \textit{the first one significantly.}

\InsImageInline{0.5}{l}{Bootstrap3peak.gif.png}If the average noise in the signal is not known or its probability distribution is uncertain, it is possible to use the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Bootstrap}{bootstrap sampling method} (page \pageref{ref-0213}) to estimate the uncertainty of the peak heights, positions, and widths, as illustrated on the left and as described in detail on page \pageref{ref-0218}. The latest version of the keypress operated interactive function \href{https://terpconnect.umd.edu/~toh/spectrum/ipf.m}{ipf.m} (page \pageref{ref-0461}) has added a function (activated by the 'v' key) that estimates the expected standard deviation of the peak parameters using this method. Click on the figure to open a GIF animation.

One way to reduce the effect of noise is to take more data. If the experiment makes it possible to reduce the x-axis interval between points, or to take multiple readings at each x-axis values, then the resulting increase in the number of data points in each peak should help reduce the effect of noise. As a demonstration, using the script \href{https://terpconnect.umd.edu/~toh/spectrum/DemoPeakfit.m}{DemoPeakfit.m} to create a simulated overlapping peak signal like that shown above left, it is possible to change the interval between x values and thus the total number of data points in the signal. With a noise level of 1\% and 75 points in the signal, the fitting error is 0.35 and the average parameter error is 0.8\%. With 300 points in the signal and the same noise level, the fitting error is essentially the same, but the average parameter error drops to 0.4\%, suggesting that the accuracy of the measured parameters varies inversely with the square root of the number of data points in the peaks. 

\InsImageInline{0.5}{l}{udx10noise.png}The figure on the right illustrates the importance of sampling interval and data density. You can download the data file "udx" in \href{https://terpconnect.umd.edu/~toh/spectrum/udx.txt}{TXT} format or in Matlab \href{https://terpconnect.umd.edu/~toh/spectrum/udx.mat}{MAT} format. The signal consists of two Gaussian peaks, one located at x=50 and the second at x=150. Both peaks have a peak height of 1.0 and a peak half-width of 10, and normally distributed random white noise with a standard deviation of 0.1 has been added to the entire signal. The x-axis sampling interval, however, is different for the two peaks; it is 0.1 for the first peak and 1.0 for the second peak. This means that the first peak is characterized by ten times more points than the second peak. When you fit these peaks separately to a Gaussian model (e.g., using peakfit.m or ipf.m), you will find that all the parameters of the first peak are measured more accurately than the second, even though the fitting error is not much different:

\textbf{First peak: Second peak:}

\texttt{Percent Fitting Error=7.6434\% Percent Fitting Error=8.8827\%}

\texttt{Peak\# Position Height Width Peak\# Position Height Width} 

\texttt{1 49.95 1.0049 10.111 1 149.64 1.0313 9.941}

So far, this discussion has applied to white noise. But other noise colors (page \pageref{ref-0032}) have different effects. Low-frequency weighted (``pink'') noise has a \textit{greater} effect on the accuracy of peak parameters measured by curve fitting, and, in a nice symmetry, high-frequency ``blue'' noise has a \textit{smaller} effect on the accuracy of peak parameters that would be expected on the basis of its standard deviation, because the information in a smooth peak signal is concentrated at low frequencies (page \pageref{ref-0124}). An example of this occurs when you apply curve fitting is to a signal that has been \href{https://terpconnect.umd.edu/~toh/spectrum/Deconvolution.html}{deconvoluted }(page \pageref{ref-0149}) to remove a broadening effect. This is the reason smoothing before curve fitting does not help (page \pageref{ref-0286}) because the peak signal information is concentrated in the \textit{low-}frequency range but smoothing reduces mainly the noise in the \textit{high-}frequency range. 

Sometimes you may notice that the residuals in a curve-fitting operation are structured into bands or lines rather than being completely random. This can occur if either the \href{https://terpconnect.umd.edu/~toh/spectrum/QuantizationX.png}{independent variable} or the \href{https://terpconnect.umd.edu/~toh/spectrum/QuantizationY.png}{dependent variable} is \textit{quantized} into discrete steps rather than continuous. It may look strange, but it has little effect on the results if the random noise is larger than the steps.

When there is noise in the data (in other words, pretty much always), the exact results will depend on the region selected for the fit - for example, the results will vary slightly with the pan and zoom setting in ipf.m, and the more noise, the greater the effect. 

\subsection{\textbf{d. Iterative fitting errors}\label{ref-0279}}

\textbf{\textnormal{\InsImageInline{0.5}{l}{DemoPeakfitErrorTest2.GIF.png}}}Unlike multiple linear regression, curve fitting, iterative methods may not always converge on the exact same model parameters each time the fit is repeated with slightly different starting values (first guesses). The \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{Interactive Peak Fitter} ipf.m (page \pageref{ref-0461}) makes it easy to test this, because it uses slightly different starting values each time the signal is fit (by pressing the \textbf{F} key in \href{http://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{ipf.m}, for example). Even better, by pressing the \textbf{X} key, the ipf.m function silently computes 10 fits with different starting values and takes the one with the lowest fitting error. A basic assumption of any curve fitting operation is that the fitting error (the root-mean-square difference between the model and the data) is minimized; the parameter errors (the difference between the actual parameters and the parameters of the best-fit model) will also be minimized. This is generally a good assumption, as demonstrated by the graph to the right, which shows typical percent parameters errors as a function of fitting error for the left-most peak in one sample of the simulated signal generated by \href{https://terpconnect.umd.edu/~toh/spectrum/DemoPeakfit.m}{DemoPeakfit.m} (shown in the previous section). The variability of the fitting error here is caused by random small variations in the first guesses, rather than by random noise in the signal. In many practical cases, there is enough random noise in the signals that the iterative fitting errors within one sample of the signal are small compared to the random noise errors between samples. 

Remember that the variability in measured peak parameters from fit to fit of a single sample of the signal is \textit{not} a good estimate of the precision or accuracy of those parameters, for the simple reason that those results represent only one sample of the signal, noise, and background. The sample-to-sample variations are likely to be much greater than the within-sample variations due to the iterative curve fitting. (In this case, a "sample" is a single recording of signal). To estimate the contribution of random noise to the variability in measured peak parameters when only a single sample if the signal is available, the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Bootstrap}{\textit{bootstrap} \textit{method}} can be used (page \pageref{ref-0213}).

\textbf{Selecting the optimum data region of interest.} When you perform a peak fitting using \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{ipf.m} (page \pageref{ref-0461}), you have control over data region selected by using the pan and zoom controls (or, using the command-line function peakfit.m, by setting the ``center'' and ``window'' input arguments). Changing these settings usually changes the resulting fitted peak parameters. If the data were perfect, say, a mathematically perfect peak shape with no random noise, then the pan and zoom settings would make no difference at all; you would get the exact same values for peak parameters at all settings, assuming only that the model you are using matches the actual shape. But of course, in the real world, data are never mathematically perfect and noiseless. The greater the amount of random noise in the data, or the greater the discrepancy between your data and the model you select, the more the measured parameters will vary if you fit different regions using the pan and zoom controls. This is simply an indication of the un\InsImageInline{0.5}{l}{image193.png}\InsImageInline{0.5}{l}{Overlapping.png}certainty in the measured parameters.\label{ref-0280}

\textbf{A difficult case}. As a dramatic example of the ideas in the previous sections, consider this simulated example signal, consisting of two Gaussian peaks of equal height = 1.00, shown on the left, that overlap closely enough so that their sum, shown on the right, is a single symmetrical peak that \textit{looks very much like} a \href{https://terpconnect.umd.edu/~toh/spectrum/DifficultCase.png}{single Gaussian}.

\texttt{{\textgreater}{\textgreater} x=[0:.1:10]';}

\texttt{{\textgreater}{\textgreater} y=exp(-(x-5.5).\textasciicircum{}2)+exp(-(x-4.5).\textasciicircum{}2);}

Attempts to fit this with a \textit{single} Gaussian yield a fit with roughly a 1\% fitting error and noticeably wavy but smooth residual, suggesting that there is no random noise in the data but that the model is not right. 

\InsImageInline{0.5}{l}{image195.png}\texttt{{\textgreater}{\textgreater} peakfit([x y],5,19,1,1)}

          \texttt{\textcolor{color-17}{Peak\# Position Height Width Area}}\index{Area}

           \texttt{1 4.5004 1.001 1.6648 1.773}

           \texttt{2 5.5006 0.99934 1.6641 1.770}

If there were no noise in the signal, the iterative curve fitting (peakfit.m or ipf.m) routines could easily extract the two equal Gaussian components to an accuracy of 1 part in 1000. But in the presence of even a little noise (for example, 1\% RSD), the results are uneven; one peak is almost always significantly higher than the other:

 \texttt{{\textgreater}{\textgreater} y=exp(-(x-5.5).\textasciicircum{}2)+exp(-(x-4.5).\textasciicircum{}2)+.01*randn(size(x))}

 \texttt{{\textgreater}{\textgreater} peakfit([x y],5,19,2,1)}

   \texttt{\textcolor{color-17}{Peak\# Position Height Width Area}}\index{Area}

 \texttt{1 4.4117 0.83282 1.61 1.43}

 \texttt{2 5.4022 1.1486 1.734 2.1}2

The fit is stable with any one sample of noise, if \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit.m} is run again with slightly different starting values, for example by pressing the \textbf{F} key several times in \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{ipf.m} (page \pageref{ref-0461}). So, the problem is \textit{not} iterative fitting errors caused by different starting values. The problem is the \textit{noise}: although the signal is completely symmetrical, any sample of the noise is not perfectly symmetrical (e.g., the first half of the noise invariably averages either slightly higher or slightly lower than the second half, resulting in an asymmetrical fit result). The surprising thing is that the error in the peak heights is much larger (about 15\% relative, on average) than the random noise in the data (1\% in this example). So even though \textit{the fit looks good} - the fitting error is low (less than 1\%) and the residuals are random and unstructured -\textit{the model parameters can still be very far off.} If you were to simulate another measurement (i.e., generate another independent set of noise), the results would be different but still inaccurate (the first peak has an equal chance of being larger or smaller than the second). Unfortunately, the expected error is not accurately predicted by the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#bootstrap}{\textit{bootstrap method}} (page \pageref{ref-0219}), which seriously underestimates the standard deviation of the peak parameters with repeated measurements of independent signals (because a bootstrap sub-sample of asymmetrical noise is likely to remain asymmetrical). A \href{http://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Monte}{\textit{Monte Carlo simulation}} (page \pageref{ref-0210}) would give a more reliable estimation of uncertainty in such cases. 

Better results can be obtained in cases where the peak widths are expected to be equal, in which case you can use peak shape 6 (equal-width Gaussian) instead of peak shape 1: 

\texttt{peakfit([x y],5,19,2,6)}. 

It also helps to provide decent first guesses (start) and to set the number of trials (NumTrials) to a number above 1): 

\texttt{peakfit([x,y],5,10,2,6,0,10,[4 2 5 2],0,0).} 

The best case will be if the shape, position, and width of the two peaks are known accurately, and if the \textit{only} unknown is their heights. Then the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html}{Classical Least-squares (multiple regression)} technique can be employed and the results will be much better. 

For an even more challenging example like this, where the two closely overlapping peaks are very different in height, see page \pageref{ref-0386}.

So, to sum up, we can make the following observations about the accuracy of model parameters: (1) the parameter errors depend on the accuracy of the model chosen and on number of peaks; (2) the parameter errors are directly proportional to the noise in the data (and worse for low-frequency or pink noise); (3) all else being equal, parameter errors are proportional to the fitting error, but a model that fits the underlying reality better, e.g. equal or fixed widths or shapes) often gives lower parameter errors even if the fitting error is larger; (4) the errors are typically least for peak position and worse for peak width and area; (5) the errors depend on the \textit{data density} (number of independent data points in the width of each peak) and on the \textit{extent of peak overlap} (the parameters of isolated peaks are easier to measure than highly overlapped peaks); (6) if only a single signal is available, the effect of noise on the standard deviation of the peak parameters in many cases can be predicted approximately by the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#bootstrap}{bootstrap method}, but if the overlap of the peaks is too great, the error of the parameter measurements can be much greater than predicted.

\href{https://terpconnect.umd.edu/~toh/spectrum/AsymmetricalOverlappingPeaks.m}{AsymmetricalOverlappingPeaks.m} illustrates one way to deal with the problem of excessive peak overlap in a multi-step script that uses first-derivative symmetrization as a pre-process performed before iterative least-squares curve fitting to analyze a complex signal consisting of multiple asymmetric overlapping peaks. See page \pageref{ref-0429} for details.

\section{Fitting signals that are subject to exponential broadening.\label{ref-0281}\label{ref-0282}\label{ref-0283}}

\InsImageInline{0.5}{l}{DataMatrix2.png}\href{https://terpconnect.umd.edu/~toh/spectrum/DataMatrix2.mat}{DataMatrix2} (figure on the left) is a computer-generated test signal consisting of 16 symmetrical Gaussian peaks with random whi\href{https://terpconnect.umd.edu/~toh/spectrum/DataMatrix2.png}{}te noise added. The peaks occur in groups of 1, 2, or 3 overlapping peaks, but the peak maxima are located at exactly integer values of x from 300 to 3900 (on the 100's) and the peak widths are always exactly \InsImageInline{0.5}{l}{DataMatrix3.png}60 units. The peak heights vary from 0.06 to 1.85. The standard deviation of the noise is 0.01. You can use this signal to test curve-fitting programs and to determine the accuracy of their measurements of peak parameters. Right-click and select "Save" to download this signal, put it in the Matlab path, then type "load \href{https://terpconnect.umd.edu/Tom\%27s\%20\%20Documents/FTP/SPECTRUM/DataMatrix2.mat}{DataMatrix2}" at the command prompt to load it into the Matlab workspace.

\href{https://terpconnect.umd.edu/~toh/spectrum/DataMatrix3.mat}{DataMatrix3} (figure on the left) i\href{https://terpconnect.umd.edu/~toh/spectrum/DataMatrix3.png}{}s an \href{http://www.google.com/search?sourceid=chrome&ie=UTF-8&q=peak+fitting\#pq=exponentially\%20broadening&hl=en&cp=11&gs\_id=4y&xhr=t&q=exponential+broadening&qe=ZXhwb25lbnRpYWwgYnJvYWRlbmluZw&qesig=rpyGatby7ykxTM-8OxiO5w&pkc=AFgZ2tl2Ieg3rRvLTcbPqB51UwlrLtXTdFBlToOwdA\_HoPjrkbwj\_Lpz9v4UatehiFvZJ53EMN9GaTydNbPJGFVeOGIv9qJ\_VQ&pf=p&sc}{exponentially broadened} version of DataMatrix2, with a "decay constant", also called "time constant", of 33 points on the x-axis. The result of the exponential broadening is that all the peaks in this signal are asymmetrical, their peak maxima are shifted to longer x values, and their peak heights are smaller, and their peak widths are larger than the corresponding peaks in DataMatrix2. Also, the random noise is damped in this signal compared to \href{https://terpconnect.umd.edu/~toh/spectrum/SignalWithWhiteNoise.png}{the original} and is \href{https://terpconnect.umd.edu/~toh/spectrum/SignalWithLPFilteredNoise.png}{no longer "white"}, as a consequence of the broadening. This type of effect is common in physical measurements and often arises from some physical or electrical effect in the measurement system that is apart from the fundamental peak characteristics. In such cases, it is usually desirable to compensate for the effect of the broadening, either by \href{http://terpconnect.umd.edu/~toh/spectrum/Deconvolution.html}{deconvolution} or by curve fitting, in an attempt to measure what the peak parameters would have been \textit{before} the broadening (and also to measure the broadening itself). This can be done for Gaussian peaks that are exponentially broadened by using the "ExpGaussian" peak shape in peakfit.m and ipf.m (page \pageref{ref-0461}), or the "ExpLorentzian", if the underlying peaks are Lorentzian. Right-click and select "Save" to download this signal, put it in the Matlab path, then type "load \href{https://terpconnect.umd.edu/~toh/spectrum/DataMatrix3.mat}{DataMatrix3}" to load it into the Matlab workspace.\href{https://terpconnect.umd.edu/~toh/spectrum/GfitDataMatrix3.gif}{}

\InsImageInline{0.5}{l}{GfitDataMatrix3.gif.png}The example illustrated on the right focuses on the single isolated peak whose "true" peak position, height, width, and area in the original unbroadened signal, are 2800, 0.52, 60, and 33.2 respectively. (The relative standard deviation of the noise is 0.01/0.52=2\%.) In the broadened signal, the peak is visibly asymmetrical, the peak maximum is shifted to larger x values, and it has a shorter height and larger width, as demonstrated by the attempt to fit a normal (symmetrical) Gaussian to the broadened peak. (The peak \textit{area}, on the other hand, is not much affected by the broadening). 

\texttt{{\textgreater}{\textgreater} load} \texttt{DataMatrix3}

\texttt{{\textgreater}{\textgreater} ipf(DataMatrix3);}

\texttt{Peak Shape = Gaussian}

\texttt{BaselineMode ON}

\texttt{Number of peaks = 1}

\texttt{Fitted range = 2640 - 2979.5 (339.5) (2809.75)} 

\texttt{Percent Error = 1.2084}

\texttt{Peak\# Position Height Width Area}\index{Area}

 \texttt{1 2814.832 0.451005 68.4412 32.8594}

\InsImageInline{0.5}{l}{ExpGfitDataMatrix3.gif.png}The large "wavy" residual in the plot above is a tip-off that the model is not quite right. Moreover, the fitting error (1.2\%) is larger than expected for a peak with a half-width of 60 points and a 2\% noise RSD (approximately 2\%/sqrt(60)=0.25\%).



Fitting to an exponentially-broadened Gaussian (pictured on the right) gives a much lower fitting error ("Percent error") and a more nearly random residual plot. But the interesting thing is that it also \textit{recovers the original peak position, height, and width to an accuracy of a fraction of 1\%}. In performing this fit, the decay constant ("extra") was experimentally determined from the broadened signal by adjusting it with the A and Z keys to give the lowest fitting error; that also results in a reasonably good measurement of the broadening factor (32.6, vs the actual value of 33). Had the original signal been noiser, these measurements would not be so accurate. Note: When using peakshape 5 (fixed decay constant exponentially broadened Gaussian) you must give it a reasonably good value for the decay constant ('extra'), the input argument right after the peakshape number. If the value is too far off, the fit may fail completely, returning all zeros. A little trial and error suffice. (Also, \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m version 8.4} has two forms of unconstrained variable decay constant exponentially broadened Gaussian, shape numbers 31 and 39, that will \textit{measure} the decay constant as an iterated variable. Shape 31 (\href{https://terpconnect.umd.edu/~toh/spectrum/expgaussian.m}{expgaussian.m}) creates the shape by performing a Fourier convolution of a specified Gaussian by an exponential decay of specified decay constant, whereas shape 39 (\href{https://terpconnect.umd.edu/~toh/spectrum/expgaussian2.m}{expgaussian2.m}) uses a mathematical expression for the final shape so produced. Both result in the \textit{same peak shape} but are parameterized differently. Shape 31 reports the peak height and position as that of the \textit{original} Gaussian before broadening, whereas shape 39 reports the peak height of the broadened \textit{result}. Shape 31 reports the width as the FWHM (full width at half maximum) and shape 39 reports the standard deviation (sigma) of the Gaussian. Shape 31 reports the exponential factor and the \textit{number of data points,} and shape 39 reports the \textit{reciprocal of the decay constant} in time units. (See the script \href{https://terpconnect.umd.edu/~toh/spectrum/DemoExpgaussian.m}{DemoExpgaussian.m} for a more detailed numerical example). For multiple-peak fits, both shapes usually require a reasonable first guess ('start") vector for best results. If the exponential decay constant of each peak is expected to be different and you need to measure those values, use shapes 31 or 39, but the decay constant of all the peaks is expected to be the same, use shape 5, and determine the decay constant by fitting an isolated peak. For example:

\texttt{Peak Shape = Exponentially-broadened Gaussian}

\texttt{BaselineMode ON}

\texttt{Number of peaks = 1}

\texttt{Extra = 32.6327}

\texttt{Fitted range = 2640 - 2979.5 (339.5) (2809.75)} 

\texttt{Percent Error = 0.21696}

\texttt{Peak\# Position Height Width Area}\index{Area}

 \texttt{1 2800.130 0.518299 60.08629 33.152429}

Comparing the two methods, the exponentially broadened Gaussian fit recovers all the underlying peak parameters quite accurately:


\begin{table}
\begin{tabularx}{\textwidth}{|
p{\dimexpr 0.444\linewidth-2\tabcolsep-2\arrayrulewidth}|
p{\dimexpr 0.135\linewidth-2\tabcolsep-\arrayrulewidth}|
p{\dimexpr 0.149\linewidth-2\tabcolsep-\arrayrulewidth}|
p{\dimexpr 0.135\linewidth-2\tabcolsep-\arrayrulewidth}|
p{\dimexpr 0.136\linewidth-2\tabcolsep-\arrayrulewidth}|} \hline 
 & Position & Height & Width & Area\index{Area} \\\hline 
Actual peak parameters & 2800 & 0.52 & 60 & 33.2155 \\\hline 
Gaussian fit to broadened signal & 2814.832 & 0.45100549 & 68.441262 & 32.859436 \\\hline 
ExpGaussian fit to broadened signal & 2800.1302 & 0.51829906 & 60.086295 & 33.152429 \\\hline 
\end{tabularx}
\end{table}
\InsImageInline{0.5}{l}{ExpGfit2DataMatrix3.gif.png}Other peaks in the same signal, under the broadening influence of the same decay constant, can be fitted with similar settings, for example, the set of three overlapping peaks near x=2400. As before, the peak positions are recovered almost exactly and even the width measurements are reasonably accurate (1\% or better). If the exponential broadening decay constant is \textit{not} the same for all the peaks in the signal, for example, if it gradually increases for larger x values, then the decay constant setting can be optimized for each group of peaks. 

The smaller fitting error evident here is just a reflection of the larger peak heights in this group of peaks - the noise is the same everywhere in this signal. 

\texttt{Peak Shape = Exponentially-broadened Gaussian}

\texttt{BaselineMode OFF}

\texttt{Number of peaks = 3}

\texttt{Extra = 31.9071}

\texttt{Fitted range = 2206 - 2646.5 (440.5) (2426.25)} 

\texttt{Percent Error = 0.11659}

\texttt{Peak\# Position Height Width Area}\index{Area}

\texttt{1 2300.2349 0.83255884 60.283214 53.422354}

\texttt{2 2400.1618 0.4882451 60.122977 31.24918}

\texttt{3 2500.3123 0.85404245 60.633532 55.124839}

The residual plots in both examples still have some "wavy" character, rather than being completely random and "white". The exponential broadening smooths out any white noise in the original signal that is introduced \textit{before} the exponential effect, acting as a low-pass filter in the time domain and resulting in a low-frequency dominated "pink" noise, which is what remains in the residuals after the broadened peaks have been fit as well as possible. On the other hand, white noise that is introduced \textit{after} the exponential effect would continue to appear white and random on the residuals. In real experimental data, both types of noise may be present in varying amounts. 

\InsImageInline{0.5}{l}{ipf69.png}\href{https://terpconnect.umd.edu/~toh/spectrum/Peakfit21.png}{}One final caveat: peak asymmetry such as exponential broadening could possibly be the result of a pair of closely spaced peaks of different peak heights. In fact, a single exponential broadened Gaussian peak can sometimes be fitted with two symmetrical Gaussians to a fitting error at least as low as a \InsImageInline{0.5}{l}{Peakfit21.png}single exponential broadened Gaussian fit. This makes it hard to distinguish between these two models based on fitting error alone. However, you can decide that by inspecting the other peaks in the signal: in most experiments, exponential broadening applies to every peak in the signal, and the broadening is either constant or changes gradually over the length of the signal. If only one or a few of the peaks exhibit asymmetry, and the others are symmetrical, it is most likely that the asymmetry is due to closely spaced peaks of different peak heights. If \textit{all} peaks have the same or similar asymmetry, it is more likely to be a broadening factor that applies to the entire signal. The two figures here provide an example from real experimental data. On the left, three asymmetrical peaks are each fitted with two symmetrical Gaussians (six peaks total). On the right, those same three peaks are fitted with one exponentially broadened Gaussian each (three peaks total). In this case, the three asymmetrical peaks all have the same asymmetry and can be fitted with the same decay constant ("extra"). Moreover, the fitting error is slightly lower for the three-peak exponentially broadened fit. \textit{Both observations argue for the three-peak exponentially broadened fit rather than the six-peak fit.}

Note: if your peaks are trailing off to the left, rather than to the right as in the above examples, simply use a \textit{negative} value for the decay constant; to do that in ipf.m (page \pageref{ref-0461}), press Shift-X and type a negative value.

An alternative to this type of curve fitting for exponentially broadened peaks is to use the \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html\#Asymmetrical}{first-derivative addition technique} (page \pageref{ref-0107}) to remove the asymmetry and then fit the resulting peak with a symmetrical model. This is faster in terms of computer execution time, especially for signals with many peaks, but it requires that the exponential time constant is known or estimated experimentally beforehand. 

\section{The Effect of Smoothing before least-squares analysis\label{ref-0284}\label{ref-0285}\label{ref-0286}\label{ref-0287}}

\InsImageInline{0.5}{l}{SmoothOptimization.png}In general, it is not advisable to \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#Optimization}{smooth} a signal before applying least-squares fitting, because doing so might distort the signal, can make it hard to evaluate the residuals properly, and might bias the results of bootstrap sampling estimations of precision, causing it to underestimate the between-signal variations in peak parameters (page \pageref{ref-0213}). \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothOptimization.m}{SmoothOptimization.m} is a Matlab/\href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{Octave} script that compares the effect of smoothing on the measurements of peak height of a Gaussian peak with a half-width of 166 points, plus white noise with a signal-to-noise ratio (SNR) of 10. The script uses three different methods: 

(a) simply taking the single point at the center of the peak as the peak height; 

(b) using the ``\href{https://terpconnect.umd.edu/~toh/spectrum/gaussfit.m}{gaussfit}'' method to fit the top half of the peak (see page \pageref{ref-0225}), and 

(c) fitting the entire signal with a Gaussian using the iterative method. 

The results of 150 trials with independent white noise samples are shown on the left: a typical raw signal is shown in the upper left. The other three plots show the effect of the SNR of the measured peak height vs the smooth ratio (the ratio of the smooth width to the half-width of the peak) for those three measurement methods. The results show that the simple single-point measurement is indeed much improved by smoothing, as is expected; however, the optimum SNR (which improves by roughly the square root of the peak width of 166 points) is achieved only when the smooth ratio approaches 1.0, and that much smoothing distorts the peak shape significantly, reducing the peak height by about 40\%. The curve-fitting methods are much less affected by smoothing and the iterative method hardly at all. So, the bottom line is that you should not smooth prior to curve-fitting, because it will distort the peak and will not gain any significant SNR advantage. The only situations where it might be advantageous so smooth before fitting are when the noise in the signal is high-frequency weighted (i.e. \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html}{"blue" noise}), where low-pass filtering will make the \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#H}{peaks easier to see} for the purpose of setting the starting points for an iterative fit, or if the signal is contaminated with high-amplitude narrow spike artifacts, in which case a \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#Matlab}{median-based pre-filter} can remove the spikes without much change to the rest of the signal. And, in another application altogether, if you want to fit a curve joining the successive peaks of a modulated wave (called the "envelope"), then you can smooth the absolute value\index{absolute value} of the wave before fitting the envelope. 

\chapter{Peak Finding and Measurement\label{ref-0288}\label{ref-0289}\label{ref-0290}\label{ref-0291}\label{ref-0292}\label{ref-0293}\label{ref-0294}\label{ref-0295}\label{ref-0296}\label{ref-0297}}

A common requirement in scientific data processing is to detect peaks in a signal and to measure their positions, heights, widths, and/or areas. One way to do this is to make use of the fact that the first \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html}{derivative} of a peak has a downward-going \href{http://en.wikipedia.org/wiki/Zero\_crossing}{zero-crossing} at the peak maximum (page \pageref{ref-0086}). However, the presence of random noise in real experimental signal will cause many false zero-crossing \InsImageInline{0.5}{l}{DemoFindPeaksb.gif.png}simply due to the noise. To avoid this problem, the technique described here \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smooths} the first derivative of the signal, then looks for downward-going zero-crossings, and then it takes only those zero-crossings whose slope exceeds a certain predetermined minimum (called the ``\textit{slope threshold}'') at a point where the original signal exceeds a certain minimum (called the ``\textit{amplitude threshold}''). By adjusting the smooth width, slope threshold, and amplitude threshold, it is possible to detect only the desired peaks and ignore most peaks that are too small, too wide, or too narrow. Moreover, this technique can be extended to estimate the position, height, and width of each peak by \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html}{least-squares curve-fitting} of a segment of the \textit{original unsmoothed signal} in the vicinity of the zero-crossing. Thus, even if heavy smoothing of the first derivative is necessary to provide reliable discrimination against noise peaks, the peak parameters extracted by curve fitting are \textit{not distorted by the smoothing}, and the effect of random noise in the signal is reduced by curve fitting over multiple data points in the peak. This technique can measure peak positions and heights quite accurately, but the measurements of peak widths and areas are most accurate if the peaks are Gaussian in shape (or Lorentzian, in the variant findpeaksL). For the most accurate measurement of other peak shapes, or of highly overlapped peaks, or of peak superimposed on a baseline, the related functions \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaksb}{findpeaksb.m}, findpeaksb3.m, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaksfit}{findpeaksfit.m} utilize \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{non-linear iterative curve fitting} with selectable peak shape models and baseline correction modes.

The routine is now available in several different versions that are described below: 

(1) a set of command-line functions for Matlab or Octave, each linked to its description: \href{https://terpconnect.umd.edu/~toh/spectrum/peaksat.m}{peaksat.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaksx}{findpeaksx}, \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksxw.m}{findpeaksxw,} \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaks}{findpeaksG}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#Valleys}{findvalleys}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaksL}{findpeaksL}, \href{https://terpconnect.umd.edu/~toh/spectrum/measurepeaks.m}{measurepeaks}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaksG2d}{findpeaksGd}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaksb}{findpeaksb}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaksb3}{findpeaksb3}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaksplot}{findpeaksplot}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaksplot}{findpeaksplotL}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#peakstats}{peakstats}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#PeakSNR}{findpeaksE}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#PeakStartAndEnd}{findpeaksGSS}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#PeakStartAndEnd}{findpeaksLSS}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#triangle}{findpeaksT}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaksfit}{findpeaksfit}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#autofindpeaks}{autofindpeaks}, and \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#autopeaks}{autopeaks}. These can be used as components in creating your own custom scripts and functions. Do not confuse with the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaks\_vs\_findpeaks}{"findpeaks" function in Matlab's \textit{Signal Processing Toolbox}}; that's a completely different algorithm.

(2) an interactive \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#ipeak}{keypress-operated function}, called \textbf{\textit{iPeak}} (\href{https://terpconnect.umd.edu/~toh/spectrum/ipeak.m}{ipeak.m}), (page \pageref{ref-0320}), for adjusting the peak detection criteria interactively to optimize for any particular peak type (Matlab only). \textit{iPeak} runs in the Figure window and use a simple set of keystroke commands to reduce screen clutter, minimize overhead, and maximize processing speed. 

(3) A set of \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#Spreadsheet}{spreadsheet}s, available in \textit{Excel} and in \textit{OpenOffice} formats. 

(4) \textit{Real-time} peak detection in Matlab is discussed on page \pageref{ref-0418}.

\href{https://terpconnect.umd.edu/~toh/spectrum/PeakFinder.zip}{Click here to download the ZIP file "PeakFinder.zip"}, which includes findpeaksG.m and its variants, ipeak.m, and a sample data file and demo scripts for testing. You can also download \textit{iPeak} and other programs of mine from the \href{http://www.mathworks.com/matlabcentral/fileexchange/11755-peak-finding-and-measurement}{Matlab File Exchange}.

\section{Simple peak detection\label{ref-0298}\label{ref-0299}}

\href{https://terpconnect.umd.edu/~toh/spectrum/allpeaks.m}{allpeaks.m}, \texttt{P=}\texttt{allpeaks(x,y),} returns a matrix with the peak number, x value, and y value of every \textit{y} value that has lower \textit{y} values on both sides; \href{https://terpconnect.umd.edu/~toh/spectrum/allvalleys.m}{allvalleys.m} is the same for valleys, lists every \textit{y} value that has \textit{higher y} values on both sides.

\href{https://terpconnect.umd.edu/~toh/spectrum/peaksat.m}{peaksat.m}, `` Peaks Above Threshold'', \texttt{P=peaksat(x,y,threshold)}, detects every y value that (a) has lower y values on both sides and (b) is above the specified threshold. Returns a 2 by \textit{n} matrix P with the x and y values of each peak, where \textit{n} is the number of detected peaks.

\texttt{P=findpeaksx(x, y, SlopeThreshold, AmpThreshold, SmoothWidth, PeakGroup, smoothtype)}

findpeaksx.m is a  Matlab/Octave command-line functions to \textit{locate and count} the positive peaks in noisy data sets. They detect peaks by looking for downward zero-crossings in the smoothed first derivative that exceed SlopeThreshold and peak amplitudes that exceed AmpThreshold and returns a list (in matrix \textbf{P}) containing the peak number and the measured position and height of each peak (and for the variant \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksxw.m}{findpeaksxw}, the \href{https://en.wikipedia.org/wiki/Full\_width\_at\_half\_maximum}{full width at half maximum}, determined by calling the \href{https://terpconnect.umd.edu/~toh/spectrum/halfwidth.m}{halfwidth.m} function). It can find and count over 10,000 peaks per second in very large signals. The data are passed to the findpeaksx function in the vectors x and y (x = independent variable, y = dependent variable). The other parameters are user-adjustable:

\textbf{SlopeThreshold} - Slope of the smoothed first derivative that is taken to indicate a peak. This discriminates based on peak width. Larger values of this parameter will neglect the broad features of the signal. A reasonable initial value for Gaussian peaks is 0.7*WidthPoints\textasciicircum{}-2, where WidthPoints is the \textit{number of data points} in the half-width (\href{https://en.wikipedia.org/wiki/Full\_width\_at\_half\_maximum}{FWHM}) of the peak. 

\textbf{AmpThreshold} - Discriminates based on peak height. Any peaks with height less than this value are ignored. 

\textbf{SmoothWidth} - Width of the smooth function that is applied to data before the slope is measured. Larger values of SmoothWidth will neglect small, sharp features. A reasonable value is typically about equal to 1/2 of \textit{the number of data points} in the half-width of the peaks. 

\textbf{PeakGroup} - The number of data points around the "top part" of the (unsmoothed) peak that are taken to estimate the peak heights. If the value of PeakGroup is 1 or 2, the maximum y value of the 1 or 2 points at the point of zero-crossing is taken as the peak height value; if PeakGroup \textbf{n} is 3 or greater, the \textit{average} of the next \textbf{n} points is taken as the peak height value. For spikes or very narrow peaks, keep PeakGroup=1 or 2; for broad or noisy peaks, make PeakGroup larger to reduce the effect of noise. 

\textbf{Smoothtype} determines the smoothing algorithm (page \pageref{ref-0050})

  If smoothtype=1, rectangular (sliding-average or boxcar) 

  If smoothtype=2, triangular (2 passes of sliding-average)

  If smoothtype=3, p-spline (3 passes of sliding-average)

Basically, higher values yield a greater reduction in high-frequency noise, at the expense of slower execution. For a comparison of these smoothing types, see page \pageref{ref-0078}.

The demonstration scripts \href{https://terpconnect.umd.edu/~toh/spectrum/demofindpeaksx.m}{\textbf{demofindpeaksx.m}} and \href{https://terpconnect.umd.edu/~toh/spectrum/demofindpeaksxw.m}{\textbf{demofindpeaksxw.m}} finds, numbers, plots, and measures noisy peaks with unknown random positions. (Note that if two peaks overlap too much, the reported width will be the width of the \textit{blended} peak; in that case, it is better to use \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksG.m}{findpeaksG.m}.

\textbf{Speed demonstration}, comparing the peaksat.m and findpeaksx.m functions, in Matlab, on a typical desktop PC. Note: Matlab’s ``tic'' and ``toc'' functions are used to determine elapsed time.

\textbf{peaksat.m}: \texttt{{\textgreater}{\textgreater} x=[0:.01:500]'; y=x.*sin(x.\textasciicircum{}2).\textasciicircum{}2; tic; P=peaksat(x,y,0); toc; NumPeaks=length(P)}

Elapsed time is 0.025 on a Dell XPS i7 3.5Ghz desktop, which is \textit{523,000 peaks per second}. 

\textbf{findpeaksx.m:} \texttt{{\textgreater}{\textgreater} x=[0:.01:500]'; y=x.*sin(x.\textasciicircum{}2).\textasciicircum{}2; tic; P=findpeaksx(x,y,0,0,3,3); toc; NumPeaks=length(P)}

Elapsed time is 0.11 on a Dell XPS i7 3.5Ghz desktop, which is \textit{110,000 peaks per second}.

\subsection{Gaussian peak measurement\label{ref-0300}\label{ref-0301}\label{ref-0302}\label{ref-0303}\label{ref-0304}\label{ref-0305}}

\texttt{P=findpeaksG(x, y, SlopeThreshold, AmpThreshold, SmoothWidth, FitWidth, smoothtype)}

\texttt{P=findpeaksL(x, y, SlopeThreshold, AmpThreshold, SmoothWidth, FitWidth, smoothtype)}

These Matlab/Octave functions locate the positive peaks in a noisy data set, perform a \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#FittingPeaks}{\textit{least-squares curve-fit}} of a Gaussian or Lorentzian function to the top part of the peak, and compute the position, height, and width (\href{https://en.wikipedia.org/wiki/Full\_width\_at\_half\_maximum}{FWHM}) of each peak from that least-squares fit. (The 6\textsuperscript{th} input argument, FitWidth, is the number of data points around each peak top that is fit). The other arguments are the same as findpeaksx. It returns a list (in matrix P) containing the peak number and the estimated \textit{position, height, width, and area} of each peak. It can find and curve-fit over 1800 peaks per second in very large signals. (This is useful primarily for signals that have several data points in each peak, not for spikes that have only one or two points, for which \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaksx}{findpeaksx }is better). 

\texttt{{\textgreater}{\textgreater} x=[0:.01:50]; y=(1+cos(x)).\textasciicircum{}2; P=findpeaksG(x,y,0,-1,5,5); plot(x,y)}

\texttt{P =}

  \texttt{1 6.2832 4 2.3548 10.028}

  \texttt{2 12.566 4 2.3548 10.028}

  \texttt{3 18.85 4 2.3548 10.028...}

The function \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksplot.m}{findpeaksplot.m} is a simple variant of findpeaksG.m that also \textit{plots} the x,y data and numbers the peaks on the graph (if any are found). The function \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksplotL.m}{findpeaksplotL.m} does the same thing optimized for Lorentzian peak.

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSG.m}{\textbf{findpeaksSG.m}} is a \textit{segmented}\index{\textcolor{color-3}{segmented}} variant of the findpeaksG function, with the same syntax, except that the four peak detection parameters can be \textit{vectors}, dividing up the signal into regions that you can optimize for peaks of different widths. You can declare any number of segments, based on the length of the third (SlopeThreshold) input argument. (Note: you only need to enter vectors for those parameters that you want to vary between segments; to allow any of the other peak detection parameters to remain \textit{unchanged} across all segments, simply enter a \textit{single scalar value} for that parameter; only the SlopeThreshold must be a vector). (\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSL.m}{FindpeaksSL.m} is the same thing for Lorentzian peaks.) The following example declares two segments, with AmpThreshold remaining the same in both segments.

\texttt{SlopeThreshold=[0.001 .0001];}

\texttt{AmpThreshold=.2;}

\texttt{SmoothWidth=[5 10];} 

\texttt{FitWidth=[10 20];}

\texttt{P=findpeaksSG(x, y, SlopeThreshold, AmpThreshold, SmoothWidth, FitWidth,3);}

In the graphic example shown on the right, the demonstration script \href{https://terpconnect.umd.edu/~toh/spectrum/TestPrecisionFindpeaksSG.m}{TestPrecisionFindpeaksSG.m} creates a noisy signal with three peaks of widely \InsImageInline{0.5}{l}{TestPrecisionFindpeaskSG.png}different widths, detects and measures the peak positions, heights and widths of each peak using findpeaksSG, then prints out the percent relative standard deviations of parameters of the three peaks in 100 measurements with independent random noise. With 3-segment peak detection parameters, findpeaksSG reliably detects and accurately measures all three peaks. In contrast, findpeaksG, when tuned to the middle peak (using line 26 instead of line 25), measures the first and last peaks poorly, because the peak detection parameters are far from optimum for those peak widths. You can also see that the \textit{precision} of peak height measurements gets progressively \textit{better} (smaller relative standard deviation) the \textit{larger} the peak widths, simply because there are \textit{more data points} in wider peaks. (You can change any of the variables in lines 10-18).

A related function is \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSGw.m}{\textbf{findpeaksSGw.m}} which is like the above except that is uses \textit{wavelet denoising} (page \pageref{ref-0179}) instead of smoothing. It takes the wavelet ``level'' rather than the smooth width as an input argument. The script \href{https://terpconnect.umd.edu/~toh/spectrum/TestPrecisionFindpeaksSGvsW.m}{TestPrecisionFindpeaksSGvsW.m} compares the precision and accuracy for peak position and height measurement for both the regular \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSGw.m}{findpeaksSG.m} and the wavelet-based \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSGw.m}{findpeaksSGw.m} functions, finding that there is little to be gained in most cases by using the wavelet denoise instead of smoothing. That is mainly because in either case the peak parameter measurements are based on least-squares fitting to the \textit{raw}, not the \textit{smoothed}, data at each detected peak location, so the usual wavelet denoising advantage of avoiding smoothing distortion does not apply here.

One difficulty with the above peak finding functions it is annoying to have to estimate the values of the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#PeakDetectionParameters}{peak detection parameters} that you need to use for your signals. A quick way to estimate these is to use \href{https://terpconnect.umd.edu/~toh/spectrum/autofindpeaks.m}{\textbf{autofindpeaks.m}}, which is similar to findpeaksG.m except that \textit{you can optionally leave out the peak detection parameters} and just write ``autofindpeaks(x, y)'' or ``autofindpeaks(x, y, \textit{n})'', where \textit{n} is the "peak capacity", roughly the number of peaks that would fit into that signal record (greater \textit{n} looks for many narrow peaks; smaller \textit{n} looks for fewer wider peaks and neglects the fine structure). Simply, \textit{n} allows you to \textit{quickly adjust all the peak detection parameters at once} just by changing a single number. In addition, if you do leave out the explicit peak detection parameters, autofindpeaks will \textit{print out the numerical input argument list} that it uses in the command window, so you can copy, paste, and edit for use with any of the findpeaks... functions. If you call autofindpeaks with the \textit{output} arguments [P,A]=autofindpeaks(x,y,\textit{n}), it returns the calculated peak detection parameters as a 4-element row vector A, which you can then pass on to other functions such as measurepeaks, effectively giving that function the ability to calculate the peak detection parameters from a single number \textit{n}. For example, in the following signal, a visual estimate indicates about 20 peaks, so you use 20 as the 3rd argument: 

\texttt{x=[0:.1:50];} 

\texttt{y=10+10.*sin(x).\textasciicircum{}2+randn(size(x));} 

\texttt{[P,A]=autofindpeaks(x,y,20);}

Then you can use A as the peak detection parameters for other peak detection functions, such as \texttt{P=findpeaksG(x,y,A(1),A(2),A(3),A(4),1)} or \texttt{P=}\href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html\#measurepeaks}{\texttt{measurepeaks}}\texttt{(x,y,A(1),A(2),A(3),A(4),1).} You will probably want to fine-tune the \textit{amplitude} threshold A(2) manually for your own needs, but that is the one that is easiest to know.

Type "help autofindpeaks" and run the examples there. (The function \href{https://terpconnect.umd.edu/~toh/spectrum/autofindpeaksplot.m}{autofindpeaksplot.m} is the same but also plots and numbers the peaks). The script \href{https://terpconnect.umd.edu/~toh/spectrum/testautofindpeaks.m}{testautofindpeaks.m} runs all the examples in the help file, plots the data and numbers the peaks (like autofindpeaksplot.m), with a 1-second pause between each example (If you are reading this online, click for the \href{https://terpconnect.umd.edu/~toh/spectrum/testautofindpeaks.gif}{animated graphic}).

\section{Optimization of peak finding\label{ref-0306}}

Finding peaks in a signal depends on distinguishing between legitimate peaks and other features like noise and baseline changes. Ideally, a peak detector should detect all the legitimate peaks and ignore all \InsImageInline{0.5}{l}{OnePeakOrTwo.png}the other features. This requires that a peak detector be "tuned" or optimized for the desired peaks. For example, the Matlab/Octave demonstration script \href{https://terpconnect.umd.edu/~toh/spectrum/OnePeakOrTwo.m}{OnePeakOrTwo.m} creates\href{https://terpconnect.umd.edu/~toh/spectrum/OnePeakOrTwo.png}{} a signal (shown on the right) that might be interpreted as either \textit{one peak} at x=3 on a curved baseline or as \textit{two peaks} at x=.5 and x=3, depending on context. The peak finding algorithms described here have input arguments that allow some latitude for adjustment. In this example script, the "SlopeThreshold" argument is adjusted to detect just one or both of those peaks. The findpeaks... functions allow \textit{either} interpretation, depending on the peak detection parameters. The optimum values of the input arguments for findpeaksG and related functions depend on the signal and on which features of the signal are important for your work. Rough values for these parameters can be estimated based on the width of the peaks that you wish to detect, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#PeakDetectionParameters}{as described above}, but for the greatest control it will be best to fine-tune these parameters for your particular signal. A simple way to do that is to use \href{https://terpconnect.umd.edu/~toh/spectrum/autopeakfindplot.m}{autopeakfindplot}(x, y, \textit{n}) and adjust \textit{n} until it finds the peak you want; it will print out the numerical input argument list so you can copy, paste, and edit for use with \textit{any} of the findpeaks... functions. A more flexible way, if you are using Matlab, is to use the \textit{interactive peak detector iPeak} (page \pageref{ref-0320}), which allows you to adjust all of these parameters \href{https://terpconnect.umd.edu/~toh/spectrum/AmpThresholdBothPeaks.png}{}individually by simple keypresses and displays the results graphically and instantly. The script \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#FindpeaksComparison}{FindpeaksComparison} shows how findpeakG compares to the other peak detection functions when applied to a computer-generated signal with multiple peaks with variable types and amounts of baseline and random noise. By itself, autofindpeaks.m, findpeaksG and findpeaksL do \textit{not} correct for a non-zero baseline; if your peaks are superimposed on a baseline, you should subtract the baseline first or use \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#FindpeaksComparison}{the other peak detection functions} that do correct for the baseline.\InsImageInline{0.5}{l}{AmpThresholdBothPeaks.png} In the example shown on the left (using the interactive peak detector \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#ipeak}{\textit{iPeak}} program described on page \pageref{ref-0320}), suppose that the important parts of the signal are two broad peaks at x=4 and x=6, the second one half the height of the first. The small, jagged features are just random noise. We want to detect the two peaks but ignore the noise. (The detected peaks are numbered 1,2,3,...in the lower panel of this graphic). This is what it looks like if the \textit{AmpThreshold} is \href{https://terpconnect.umd.edu/~toh/spectrum/AmpThresholdTooLow.png}{too small} or \href{https://terpconnect.umd.edu/~toh/spectrum/AmpThresholdTooLarge.png}{too large}, if the \textit{SlopeThreshold} is \href{https://terpconnect.umd.edu/~toh/spectrum/AllSettingsTooLow.png}{too small} or \href{https://terpconnect.umd.edu/~toh/spectrum/SlopeThresholdTooLarge.png}{too large}, if the \textit{SmoothWidth} is \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothWidthTooSmall.png}{too small} or \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothWidthTooLarge.png}{too large}, and if the \textit{FitWidth} is \href{https://terpconnect.umd.edu/~toh/spectrum/FitWidthTooSmall.png}{too small} or \href{https://terpconnect.umd.edu/~toh/spectrum/FitWidthTooLarge.png}{too large}. If these parameters are within the optimum range for this measurement objective, the findpeaksG functions will return something like this (although the exact values will vary with the noise and with the value of FitWidth):

 \texttt{Peak\# Position Height Width Area}\index{Area} 

  \texttt{1 3.9649 0.99919 1.8237 1.94} 

  \texttt{2 5.8675 0.53817 1.6671 0.955}  

\section{How is 'findpeaksG' different from 'max' in Matlab or 'findpeaks' in the \textit{Signal Processing Toolkit}?\label{ref-0307}}

\InsImageInline{0.5}{l}{NoisySine.png}The 'max' function simply returns the largest \textit{single} value in a vector. \href{https://terpconnect.umd.edu/~toh/spectrum/FindpeaksSPThelp.txt}{Findpeaks }in the \textit{Signal Processing Toolbox} can be used to find the values and indices of all the peaks that are higher than a specified peak height and are separated from their neighbors by a specified minimum distance. My version of findpeaks (\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksG.m}{findpeaksG}) accepts both an independent variable (x) and dependent variable (y) vectors, finds the places where the average curvature over a specified region is concave down, fits that region with a least-squares fit and returns the peak position (in x units), height, width, and area, of any peak that exceeds a specified height. 

For example, let us create a noisy series of peaks (plotted on the right) and apply both findpeaks functions to the resulting data.

\texttt{x=[0:.1:100];}

\texttt{y=5+5.*sin(x)+randn(size(x));}

\texttt{plot(x,y)}

Now, most people just looking at this plot of data would count \textit{16 peaks}, with peak heights averaging about 10 units. Every time the statements above are run, the random noise is different, but you would still count the 16 peaks because the signal-to-noise ratio is 10, which is not that bad. But the findpeaks function in the \textit{Signal Processing Toolbox} counts anywhere from 11 to 20 peaks, with an average height (\texttt{PKS}) of 11.5. 

\texttt{[PKS,LOCS]=findpeaks(y,'MINPEAKHEIGHT',5,'MINPEAKDISTANCE',11)}

In contrast, my findpeaksG function \texttt{findpeaksG(x,y,0.001,5,11,11,3)} counts 16 peaks every time, with an average height of 10 ${\pm}$0.3, which is much more reasonable. It also measures the width and area, assuming the peaks are Gaussian (or Lorentzian, in the variant findpeaksL). To be fair, \href{https://terpconnect.umd.edu/~toh/spectrum/FindpeaksSPThelp.txt}{f}\href{https://terpconnect.umd.edu/~toh/spectrum/FindpeaksSPThelp.txt}{indpeaks} in the \textit{Signal Processing Toolbox}, or my even faster \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaksx}{findpeaksx.m} function, works better for peaks that have only 1-3 data points on the peak; my function is better for peaks that have more data points. 

The demonstration script \href{https://terpconnect.umd.edu/~toh/spectrum/FindpeaksSpeedTest.m}{FindpeaksSpeedTest.m} compares the speed of the four peak detectors on the same large test signal: Signal Processing Toolkit \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSPT.m}{findpeaks}, \href{https://terpconnect.umd.edu/~toh/spectrum/peaksat.m}{peaksat}, \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksx.m}{findpeaksx}, and \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksG.m}{findpeaksG}:

  \texttt{\textbf{Number Elapsed Peaks per}} 

 \texttt{\textbf{Function of peaks time, sec second}}

\texttt{findpeaks (SPT) 160 0.012584 12715}

\texttt{peaksat 999 0.0012912 773699}

\texttt{findpeaksx 158 0.001444 109418}

\texttt{findpeaksG 157 0.011005 14267} 

Finding valleys

There is also a similar function for finding \textit{valleys} (minima), called \href{https://terpconnect.umd.edu/~toh/spectrum/findvalleys.m}{\textbf{findvalleys.m}}, which works the same way as findpeaksG.m, except that it locates \textit{minima} instead of \textit{maxima}. Only valleys above the AmpThreshold (that is, more positive or less negative) are detected; if you wish to detect valleys that have negative minima, then AmpThreshold must be set more negative than that.

\texttt{{\textgreater}{\textgreater} x=[0:.01:50];y=cos(x);P=findvalleys(x,y,0,-1,5,5)}

\texttt{P =}

  \texttt{1.0000 3.1416 -1.0000 2.3549 0}

  \texttt{2.0000 9.4248 -1.0000 2.3549 0}

  \texttt{3.0000 15.7080 -1.0000 2.3549 0}

  \texttt{4.0000 21.9911 -1.0000 2.3549 0....}

\texttt{....}

\section{\textbf{Accuracy of the measurements} of peaks  \label{ref-0308}}

\textbf{\textnormal{The accuracy of the measurements}} of peak position, height, width, and area by the findpeaksG function depends on the shape of the peaks, the extent of peak overlap, the strength of the background, and the signal-to-noise ratio. The width and area measurements particularly are strongly influenced by peak overlap, noise, and the choice of FitWidth. Isolated peaks of Gaussian shape are measured most accurately. For \textit{Lorentzian} peaks, use \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksL.m}{findpeaksL.m} instead (the only difference is that the reported peak heights, widths, and areas will be more accurate if the peaks are Lorentzian). See "ipeakdemo.m" below for an accuracy trial for Gaussian peaks. For highly overlapping peaks that do not exhibit distinct maxima, use \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m} or the \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{Interactive Peak Fitter} (\href{https://terpconnect.umd.edu/~toh/spectrum/ipf.m}{ipf.m}, page \pageref{ref-0461}). \label{ref-0309}

For a direct comparison of the accuracy of findpeaksG vs peakfit, run the demonstration script \href{https://terpconnect.umd.edu/~toh/spectrum/peakfitVSfindpeaks.m}{peakfitVSfindpeaks.m}. This script generates four very noisy peaks of different heights and widths, then \InsImageInline{0.5}{l}{peakfitVSfindpeaks.png}measures them in two different ways: first with findpeaksG.m (figure on the left) and then with \href{https://terpconnect.umd.edu/~toh/spectrum/peakfitExample.png}{peakfit.m}, and compares the results. The peaks detected by findpeaksG are labeled "Peak 1", "Peak 2", etc. If you run this script several times, it will generate the same peaks but with \textit{independent samples of the random noise each time}. You will find that both methods work well most of the time, with peakfit giving smaller errors in most cases (because it uses \textit{all} the points in each peak, not just the top part), but occasionally findpeaksG will miss the first (lowest) peak and rarely it will detect a 5th peak that is not there. On the other hand, peakfit.m is constrained to fit 4 and only 4 peaks each time.

The demonstration script \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#FindpeaksComparison}{FindpeaksComparison} compares the accuracy of findpeaksG and findpeaksL to several peak detection functions when applied to signals with multiple peaks and variable types and amounts of baseline and random noise. 



\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksb.m}{\textbf{findpeaksb.m}} is a variant of findpeaksG.m that more accurately measures peak parameters by using \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{\textit{iterative least-square curve fitting}} based on my \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit.m} function. This yields better peak parameter values than findpeaksG alone for three reasons: 

(1) it can be set for different peak shapes with the input argument 'PeakShape'; 

(2) it fits the \textit{entire} peak, not just the top part; and 

(3) it has provision for background subtraction (when the input argument "BaselineMode" is set to 1, 2, or 3 - linear, quadratic, or flat, respectively). 

This function works best with isolated peaks that do not overlap. For version 3, the syntax is \texttt{P = findpeaksb(x,y, SlopeThreshold, AmpThreshold, smoothwidth, peakgroup, smoothtype, windowspan, PeakShape, extra, BASELINEMODE}). The first seven input arguments are the same as for the findpeaksG.m function; if you have been using findpeaksG or \textit{iPeak} to find and measure peaks in your signals, you can use those same input argument values for findpeaksb.m. The remaining four input arguments are for the \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit }function: 
\begin{itemize}
\item "\texttt{windowspan}" specifies the number of data points over which each peak is fit to the model shape (This is the hardest one to estimate; in BaselineMode 1 and 2, 'windowspan' must be large enough to cover the entire single peak and get down to the background on both sides of the peak, but not so large as to overlap neighboring peaks);"\texttt{PeakShape}" specifies the model peak shape: 1=Gaussian, 2=Lorentzian, etc (type 'help findpeaksb' for a list),

\item "\texttt{extra}" is the shape modifier variable that is used for the Voigt, Pearson, exponentially broadened Gaussian and Lorentzian, Gaussian/Lorentzian blend, and bifurcated Gaussian and Lorentzian shapes to fine-tune the peak shape;

\item "\texttt{BASELINEMODE}" is 0, 1, 2, or 3 for no, linear, quadratic, or flat background subtraction.


\end{itemize}
The peak table returned by this function has a 6th column listing the percent fitting errors for each peak. Here is a simple example with three Gaussians on a linear background, comparing plain \textit{findpeaksG, findpeaksb} without background subtraction (\texttt{BASELINEMODE=0)}, and to \textit{findpeaksb} with background subtraction (\texttt{BASELINEMODE=1)}. 

\texttt{x=1:.2:100;Heights=[1 2 3];Positions=[20 50 80];Widths=[3 3 3];}

\texttt{y=2-(x./50)+}\href{https://terpconnect.umd.edu/~toh/spectrum/modelpeaks.m}{\texttt{modelpeaks}}\texttt{(x,3,1,Heights,Positions,Widths)+.02*randn(size(x));}

\texttt{plot(x,y);}

\texttt{disp(' Peak Position Height Width Area}\index{Area}  \texttt{\% error')}

\texttt{PlainFindpeaks=findpeaksG(x,y,.00005,.5,30,20,3)} 

\texttt{NoBackgroundSubtraction=findpeaksb(x,y,.00005,.5,30,20,3,150,1,0,0)}

\texttt{LinearBackgroundSubtraction=findpeaksb(x,y,.00005,.5,30,20,3,150,1,0,1)}

The demonstration script \href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindPeaksb.m}{DemoFindPeaksb.m} shows how findpeaksb works with multiple peaks on a curved background, and \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#FindpeaksComparison}{FindpeaksComparison} shows how findpeaksb compares to the other peak detection functions when applied to signals with multiple peaks and variable types and amounts of baseline and random noise. 

\textbf{Segmented peak finder.} \InsImageInline{0.5}{l}{findpeaksSb.png}\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSb.m}{\textbf{findpeaksSb.m}} \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSb.png}{}is a \textit{segmented}\index{\textcolor{color-3}{segmented}} variant of findpeaksb.m. It has the same syntax as findpeaksb.m, except that the input arguments SlopeThreshold, AmpThreshold, smoothwidth, peakgroup, window, width, PeakShape, extra, NumTrials, BaselineMode, and fixedparameters, can all optionally be scalars \textit{or vectors with one entry for each segment}, in the same manner as \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSG.m}{findpeaksSG.m}. It returns a matrix P listing the peak number, position, height, width, area, percent fitting error and "R2" of each detected peak. In the example on the right, the two peaks have the same height above baseline (1.00) but different shapes (the first Lorentzian and the second Gaussian), very different widths, and different baselines. So, using findpeaksG or findpeaksL or findpeaksb, it would be impossible to find one set of input arguments that would be optimum for both peaks. However, using findpeaksSb.m, different settings can apply to different regions of the signal. In this simple example, there are only \textit{two} segments, defined by SlopeThreshold with 2 different values, and the other input arguments are either the same or are different in those two segments. The result is that the peak height of both peaks is measured accurately. First, we define the values of the peak detection parameters, then call findpeaksSb.

\texttt{{\textgreater}{\textgreater} SlopeThreshold=[.001 .00005]; AmpThreshold=.6; smoothwidth=[5 120]; peakgroup=[5 120];smoothtype=3; window=[30 200]; PeakShape=[2 1]; extra=0; NumTrials=1; BaselineMode=[3 0];}

\texttt{{\textgreater}{\textgreater} findpeaksSb(x,y, SlopeThreshold, AmpThreshold, smoothwidth, peakgroup, smoothtype, window, PeakShape, extra, NumTrials, BaselineMode)}

\texttt{Peak \# Position Height Width Area}\index{Area}

  \texttt{1 19.979 0.9882 1.487 1.565}

  \texttt{2 79.805 1.0052 23.888 25.563}

\href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindPeaksSb.m}{DemoFindPeaksSb.m} demonstrates the findpeaksSG.m function by creating a random number of Gaussian peaks whose widths increase by a factor of 25-fold over the x-axis range and that are superimposed on a curved baseline with random white noise that increases gradually; four segments are used in this example, changing the peak detection and curve fitting values so that all the peaks are measured accurately. \href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindPeaksSbLarge.png}{Graphic}. \href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindpeaksSb.txt}{Printout}.

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksb3.m}{\textbf{findpeaksb3.m}} is a more ambitious variant of findpeaksb.m that fits each detected peak \textit{along with the previous and following peaks} found by findpeaksG.m, to deal better with the \textit{overlap of the adjacent overlapping peaks}. The syntax is 

\texttt{FPB=findpeaksb3(x,y, SlopeThreshold, AmpThreshold, smoothwidth, peakgroup, smoothtype, PeakShape, extra, NumTrials, BASELINEMODE, ShowPlots)}. 

\InsImageInline{0.5}{l}{Demofindpeaksb3Small.png}

The demonstration script \href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindPeaksb3.m}{DemoFindPeaksb3.m} shows how findpeaksb3 works with irregular clusters of overlapping Lorentzian peaks, as in the example on the left (type "help findpeaksb3") for more. The demonstration script \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#FindpeaksComparison}{FindpeaksComparison} shows how findpeaksb3 compares to the other peak detection functions when applied to signals with multiple peaks and variable types and amounts of baseline and random noise. 





\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksfit.m}{\textbf{findpeaksfit}}\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksfit.m}{\textbf{.m}} is essentially a serial combination of findpeaksG.m and \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit.m}. It uses the number of peaks found and the peak positions and widths that are the output of the findpeaksG function as the input for the peakfit.m function, which then fits the \textit{entire signal} with the specified peak model. This combination yields better values than findpeaksG alone, because peakfit fits the entire peak, not just the top part, and it deals with non-Gaussian and overlapped peaks. However, it fits only those peaks that are found by findpeaksG, so you must make sure that all the peaks in the data are found. The syntax is: 

\texttt{function [P, FitResults, LowestError, BestStart, xi, yi] =} \texttt{findpeaksfit (x, y, SlopeThreshold, AmpThreshold, smoothwidth, peakgroup, smoothtype, peakshape, extra, NumTrials, BaselineMode, fixedparameters, plots)}

\InsImageInline{0.5}{l}{findpeaksfit.gif.png}The first seven input arguments are exactly the same as for the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaks}{findpeaksG.m} function; if you have been using \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksG.m}{findpeaksG} or \href{https://terpconnect.umd.edu/~toh/spectrum/ipeak.m}{\textit{iPeak}} to find and measure peaks in your signals, you can use those same input argument values for findpeaksfit.m. The remaining six input arguments of findpeaksfit.m are for the peakfit function; if you have been using peakfit.m or \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{ipf.m} (page \pageref{ref-0461}) to fit the peaks in your signals, then you can use those same input argument values for findpeaksfit.m. The demonstration script \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksfitdemo.m}{findpeaksfitdemo.m}, shows findpeaksfit automatically finding and fitting the peaks in a set of 150 signals, each of which may have 1 to 3 noisy Lorentzian peaks in variable locations, \textit{artificially slowed down} with the "pause" function so you can see it better. Requires the \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksfit.m}{findpeaksfit.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/lorentzian.m}{lorentzian.m} functions installed. This script was used to generate \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksfit.gif}{the GIF animation} shown on the right. Type "help findpeaksfit" for more information. 

\section{Comparison of peak finding functions\label{ref-0310}}

\InsImageInline{0.5}{l}{FindpeaksComparison.png}The demonstration script \href{https://terpconnect.umd.edu/~toh/spectrum/FindpeaksComparison.m}{FindpeaksComparison.m} compares the peak parameter accuracy of findpeaksG/L, findpeaksb, findpeaksb3, and findpeaksfit applied to a computer-generated signal with multiple peaks plus variable types and \href{https://terpconnect.umd.edu/~toh/spectrum/FindpeaksComparison2large.png}{}amounts of baseline and random noise. (Requires those four functions, plus gaussian.m, lorentzian.m, modelpeaks.m findpeaksG.m, findpeaksL.m, pinknoise.m, and propnoise.m, in the Matlab/ Octave path). Results are displayed graphically in figure windows \href{https://terpconnect.umd.edu/~toh/spectrum/FindpeaksComparison2.png}{1}, \href{https://terpconnect.umd.edu/~toh/spectrum/FindpeaksComparison2.png}{2}, and \href{https://terpconnect.umd.edu/~toh/spectrum/ErrorBars.png}{3} and printed out in a table of parameter accuracy and elapsed time for each method, as shown below. You may change the lines in the script marked by {\textless}{\textless}{\textless} to modify the number and character and amplitude of the signal peaks, baseline, and noise. (Make the signal like yours to discover which method works best for your type of signal). The best method depends mainly on the shape and amplitude of the baseline and on the extent of peak overlap. Type "\href{https://terpconnect.umd.edu/~toh/spectrum/FindpeaksComparison.txt}{help FindpeaksComparison}" for details. (Elapsed times updated for Matlab 2020 running on Dell XPS i7 3.5Ghz).  \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#Top}{}

\texttt{Average absolute percent errors of all peaks}

 \texttt{Position error Height error Width error Elapsed time, sec}

\texttt{findpeaksG 0.35955\% 38.573\% 25.797\% 0.005768}

\texttt{findpeaksb 0.38828\% 8.5024\% 14.329\% 0.069061}

\texttt{findpeaksb3 0.27187\% 3.7445\% 3.0474\% 0.49538 findpeaksfit 0.51930\% 8.0417\% 24.035\% 0.27363}

Note: \textbf{findpeaksfit.m} differs from \textbf{findpeaksb.m} in that \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksfit.m}{findpeaksfit.m} \textit{fits all the found peaks at one time} with a single multi-peak model, whereas \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksb.m}{findpeaksb.m} \textit{fits each peak separately} with a single-peak model, and \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksb3.m}{findpeaksb3.m} fits each detected peak \textit{along with the previous and following peaks.} As a result, \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksfit.m}{findpeaksfit.m} works better with a relatively small number of peak that all overlap, whereas \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksb.m}{findpeaksb.m} works better with a large number of isolated non-overlapping peaks, and \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksb3.m}{findpeaksb3.m} works for large numbers of peaks that overlap at most one or two adjacent peaks. \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksg.m}{FindpeaksG}/\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksL.m}{L} is simple and fast, but it does not perform baseline correction; \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksfit.m}{findpeaksfit }can perform flat, linear, or quadratic baseline correction, but it works only over the entire signal at once; in contrast, \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksb.m}{findpeaksb} and \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksb3.m}{findpeaksb3 }perform \textit{local} baseline correction, which often works well if the baseline is curved or irregular. 

\textbf{\InsImageInline{0.5}{l}{TestFindpeaksG2d.png}Other related functions}

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksG2d.m}{\textbf{findpeaksG2d.m}} is a variant of findpeaksG that can be used to locate the positive peaks \textit{and shoulders} in a noisy x-y time series data set. Detects peaks in the \textit{negative of the second derivative} of the signal, by looking for downward slopes in the third derivative that exceed SlopeThreshold. See \href{https://terpconnect.umd.edu/~toh/spectrum/TestFindpeaksG2d.m}{TestFindpeaksG2d.m}.

\href{https://terpconnect.umd.edu/~toh/spectrum/autopeaks.m}{\textbf{[M,A]=autopeaks.m}} is a peak detector for peaks of arbitrary shape; it is basically a combination of \href{https://terpconnect.umd.edu/~toh/spectrum/autofindpeaks.m}{autofindpeaks.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/measurepeaks.m}{measurepeaks.m}. It has similar syntax to \href{https://terpconnect.umd.edu/~toh/spectrum/measurepeaks.m}{measurepeaks.m}, except that the peak detection parameters (SlopeThreshold, AmpThreshold, smoothwidth, peakgroup, and smoothtype) can be omitted and the function will calculate trial values in the manner of \href{https://terpconnect.umd.edu/~toh/spectrum/autofindpeaks.m}{autofindpeaks.m}. Using the simple syntax [M,A]=autopeaks(x, y) works well in some cases, but if not try [M,A]=autopeaks(x, y, n), using different values of n (roughly the number of peaks that would fit into the signal record) until it detects the peaks that you want to measure. Like measurepeaks, it returns a table M containing the peak number, peak position, absolute peak height, peak-valley difference, perpendicular drop area (page \pageref{ref-0192}), and tangent skim area of each peak it detects (page \pageref{ref-0184}), but is also can optionally return a vector A containing the peak detection parameters that it calculates (for use by other peak detection and fitting functions). For the most precise control over peak detection, you can specify all the peak detection parameters by typing M=autopeaks(x,y, SlopeThreshold, AmpThreshold, smoothwidth, peakgroup). \href{https://terpconnect.umd.edu/~toh/spectrum/autopeaksplot.m}{[M,A]=autopeaksplot.m} is the same but it also plots the signal and the individual peaks in the manner of measurepeaks.m (shown above). The script \href{https://terpconnect.umd.edu/~toh/spectrum/testautopeaks.m}{testautopeaks.m} runs all the examples in the autopeaks help file, with a 1-second pause between each one, printing out results in the command window and additionally plotting and numbering the peaks (Figure window 1) and each individual peak (Figure window 2); it requires \href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/fastsmooth.m}{fastsmooth.m} in the path.



The function \href{https://terpconnect.umd.edu/~toh/spectrum/peakstats.m}{\textbf{peakstats.m}} uses the same algorithm as findpeaksG, but it computes and returns a table \InsImageInline{0.5}{l}{PeakstatsHistograms.png}of summary statistics of the peak intervals (the x-axis interval between adjacent detected peaks), heights, widths, and areas, listing the maximum, minimum, average, and percent standard deviation of each, and optionally plotting the x,y data with numbered peaks in figure window 1, printing the table of peak statistics in the command window, and plotting the \href{http://www.mathsisfun.com/data/histograms.html}{histograms} of the peak intervals, heights, widths, and areas in the four quadrants of figure window 2. Type "help peakstats". The syntax is the same as findpeaksG, with the addition of an 8th input argument to control the display and plotting. Version 2, March 2016, adds median and mode. Example:

\texttt{x=[0:.1:1000];y=5+5.*cos(x)+randn(size(x));}

\texttt{PS=peakstats(x,y,0,-1,15,23,3,1);}

\texttt{Peak Summary Statistics}

\texttt{158 peaks detected}

  \texttt{Interval Height Width Area}\index{Area}

\texttt{Maximum 6.6428 10.9101 5.6258 56.8416}

\texttt{Minimum 6.0035 9.1217 2.5063 28.2559}

\texttt{Mean 6.283 9.9973 3.3453 35.4737}

\texttt{\% STD 1.8259 3.4265 15.1007 12.6203}

\texttt{Median 6.2719 10.0262 3.2468 34.6473}

\texttt{Mode 6.0035 9.1217 2.5063 28.2559}

With the last input argument omitted or equal to zero, the plotting and printing in the command window are omitted; the numerical values of the peak statistics table are returned as a 4x4 array, in the same order as the example above.

\href{https://terpconnect.umd.edu/~toh/spectrum/tablestats.m}{\textbf{tablestats.m}} (\texttt{PS=tablestats(P,displayit)}) is similar to peakstats.m except that it accepts as input a peak table P such as generated by findpeaksG.m, findvalleys.m, findpeaksL.m, findpeaksb.m, findpeaksplot.m, findpeaksnr.m, findpeaksGSS.m, findpeaksLSS.m, or findpeaksfit.m - any of the functions that return a table of peaks with at least 4 columns listing peak number, height, width, and area. Computes the peak intervals (the x-axis interval between adjacent detected peaks) and the maximum, minimum, average, and percent standard deviation of each, and optionally displaying the histograms of the peak intervals, heights, widths, and areas in figure window 2. Set the optional last argument displayit = 1 if the histograms are to be displayed, otherwise not. Example: 

\texttt{x=[0:.1:1000];y=5+5.*cos(x)+.5.*randn(size(x));}

\texttt{figure(1);P=findpeaksplot(x,y,0,8,11,19,3);tablestats(P,1);}

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksE.m}{\textbf{FindpeaksE.m}} is a variant of findpeaksG.m that additionally estimates the percent relative fitting error of each peak (assuming a Gaussian peak shape) and returns it in the 6th column of the peak table. 

Example: 

\texttt{{\textgreater}{\textgreater} x=[0:.01:5];}

\texttt{{\textgreater}{\textgreater} y=x.*sin(x.\textasciicircum{}2).\textasciicircum{}2+.1*whitenoise(x);}

\texttt{{\textgreater}{\textgreater} P=findpeaksE(x,y,.0001,1,15,10)}

\texttt{P =}

  \texttt{1 1.3175 1.3279 0.25511 0.36065 5.8404}

  \texttt{2 1.4245 1.2064 0.49053 0.62998 10.476}

  \texttt{3 2.1763 2.1516 0.65173 1.4929 3.7984}

  \texttt{4 2.8129 2.8811 0.2291 0.70272 2.3318...}

\textbf{Peak start and end}\label{ref-0311}

Defining the "start" and "end" of the peak (the x-values where the peak begins and ends) is a bit arbitrary because typical peak shapes approach the baseline asymptotically far from the peak maximum. You might define the peak start and end points as the x values where the y value is some small fraction, say 1\%, of the peak height, but then the random noise on the baseline will be a larger fraction of the signal amplitude at that point. Smoothing to reduce noise is likely to distort and broaden peaks, effectively changing their start and end points. Overlap of peaks also greatly complicates the issue. One solution is to fit each peak to a model shape (page \pageref{ref-0229}), then calculate the peak start and end from the model expression. That method minimizes the noise problem by fitting the data over the entire peak, and it can handle overlapping peaks, but it works only if the peaks can be modeled by available fitting programs. For example, Gaussian peaks \href{http://www.wolframalpha.com/input/?i=solve+exp\%28-\%28\%28x\%29\%2F\%281\%2F\%282+sqrt\%28log\%282\%29\%29\%29+w\%29\%29\%5E2\%29+\%3D+a\%2C+w\%3E0+for++x}{can be shown} to reach a fraction \textbf{\textit{a}} of the peak height at x = \textit{p} ${\pm}$ sqrt(\textit{w}\textasciicircum{}2 log(1/\textbf{\textit{a}}))/(2 sqrt(log(2))) where \textit{p} is the peak position and \textit{w} is the peak width (full width at half maximum). So, for example if \textbf{\textit{a}} = .01, x =\textit{p} ${\pm}$ \textit{w}*sqrt((log(2)+log(5))/(2 log(2))) = 1.288784*\textit{w}. Lorentzian peaks \href{http://www.wolframalpha.com/input/?i=solve+1\%2F\%281\%2B\%28\%28x\%29\%2F\%280.5*w\%29\%29\%5E2\%29\%3Da+for+w\%3E0\%2C+x}{can be shown} to reach a fraction \textbf{\textit{a}} of the peak height at x = \textit{p} ${\pm}$ sqrt[(\textit{w}\textasciicircum{}2 - \textbf{\textit{a}} \textit{w}\textasciicircum{}2)/\textit{a}]/2. If \textbf{\textit{a}} = .01, x = \textit{p} ${\pm}$ (3/2 sqrt(11)*\textit{w}) = 4.97493*\textit{w}. The findpeaksG variants \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksGSS.m}{\textbf{findpeaksGSS.m}} and \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksLSS.m}{\textbf{findpeaksLSS.m}}, for Gaussian and Lorentzian peaks respectively, compute the peak start and end positions in this manner and return them in the 6th and 7th columns of the peak table \textbf{P}. Uncertainty in the measured peak position \textit{p} and especially in the peak width \textit{w} will make the results less certain. 

The problem with this method is that it requires an analytical peak model, expressed as a closed-form expression that can be solved algebraically for their start and end points. A more versatile method is to fit a model to the peak data by \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{iterative curve fitting} (page \pageref{ref-0258}), and then use the best-fit model to locate the start and stop points by interpolation. For complex peak shapes, the model \textit{need not be limited to a single peak}; complex, asymmetrical peak shapes can often be modeled as the sum of simple shapes, such as Gaussians. An example of this method is demonstrated in the script \href{https://terpconnect.umd.edu/~toh/spectrum/StartAndEnd.m}{StartAndEnd.m}, which simulates a noisy, asymmetrical peak and then applies this method using my \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#command}{peakfit.m} function (page \pageref{ref-0448}). You can select the start/stop cut-off point as a fraction of the peak height in line 8 \InsImageInline{0.5}{l}{StartAndEnd.gif.png}of this script, the amount of random noise in line 7, and the number of model peaks in line 9. At the cut-off points, the signal-to-noise ratio is very poor, so a direct measurement of x where y equals the cut-off is impractical.  Nevertheless, the start and end points can be calculated surprisingly precisely by computing a least-squares best-fit model (contained in the output arguments xi and yi of the peakfit function), which averages out the noise over the entire signal (the more data points the better). The graphic on the left shows the method in operation for 50 repeat measurements with different random noise samples, first with 1\% noise and then with 10\% noise. If the animation is not visible, click \href{https://terpconnect.umd.edu/~toh/spectrum/StartAndEnd.gif}{this link}. Despite the poor signal-to-noise ratio at the cut-off points, the relative standard deviation of the measured start and end points (marked by the vertical lines) is only about 0.2\%. Even when the noise is increased 10-fold (line 7), the relative standard deviation is still under 1\%. (If you have a different number of data point per peak, the precision will be inversely proportional to the square root of the number of points).

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksT.m}{\textbf{findpeaksT.m}} and \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksTplot.m}{\textbf{findpeaksTplot.m}} are variants of findpeaksG that measure the peak parameters by \InsImageInline{0.5}{l}{triangulation.png}\textit{constructing a triangle around each peak} with its sides tangent to the sides of the peak, as shown on the right (a \href{https://terpconnect.umd.edu/~toh/spectrum/triangulationG.m}{script that generated this graphic}). This method mimics the geometric construction method that was formerly used to measure peak parameters manually before the age of computers. Peak height is taken as the apex of the triangle, which is slightly higher than the peak of the underlying curve. The \href{https://terpconnect.umd.edu/~toh/spectrum/TriangulationMethodAccuracy.txt}{performance of this method} is poor when the signals are very noisy or if the peaks overlap, but in a few special circumstances, the triangle construction method can be more accurate for the measurement of peak area than the Gaussian method if the peaks are \textit{asymmetric} or of \textit{uncertain shape.} (For some specific examples, see the demo function triangulationdemo.m: \href{https://terpconnect.umd.edu/~toh/spectrum/triangulationdemo.png}{click for the graphic}). \label{ref-0312}

\textbf{\InsImageInline{0.5}{l}{findsteps.png}Locating sharp steps}. The function \href{https://terpconnect.umd.edu/~toh/spectrum/findsteps.m}{\textbf{findsteps.m}}, syntax: P=findsteps(x, y, SlopeThreshold, AmpThreshold, SmoothWidth, peakgroup), locates positive transient steps in noisy x-y time series data, by computing the first derivative of y that exceed ``SlopeThreshold'', computes the step height as the difference between the maximum and minimum y values over a number of data point equal to "Peakgroup", and returns list P with step number, x position, y position, and the step height of each step detected. "SlopeThreshold" and "AmpThreshold" control step sensitivity; higher values will neglect smaller features. Increasing "SmoothWidth" reduces small sharp false steps caused by random noise or by "glitches" in the data acquisition. See \href{https://terpconnect.umd.edu/~toh/spectrum/findsteps.png}{findsteps.png} for a real example. And \href{https://terpconnect.umd.edu/~toh/spectrum/findstepsplot.m}{findstepsplot.m} plots and numbers the peaks.

\InsImageInline{0.5}{l}{findsquarepulse.png}\textbf{Rectangular pulses} (square waves) require a different approach, based on amplitude discrimination rather than differentiation. The function "\href{https://terpconnect.umd.edu/~toh/spectrum/findsquarepulse.m}{findsquarepulse.m}" (syntax \texttt{S=findsquarepulse(t,y, threshold}) locates the rectangular pulses in the signal t,y that exceed a y-value of "threshold" and determines their start time, average height (relative to the baseline) and width. \href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindsquare.m}{DemoFindsquare.m} creates a test signal (with a true height of 2636 and a width of 750) and calls findsquarepulse.m to demonstrate. If the signal is very noisy, some preliminary rectangular \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothing} (e.g. using \href{https://terpconnect.umd.edu/~toh/spectrum/fastsmooth.m}{fastsmooth.m}) before calling findsquarepulse.m may be helpful to eliminate false peaks.

\href{https://terpconnect.umd.edu/~toh/spectrum/NumAT.m}{\textbf{NumAT(m,threshold)}}: "\textbf{Num}bers \textbf{A}bove \textbf{T}hreshold": Counts the number of adjacent elements in the vector ``m'' that are greater than or equal to the scalar value 'threshold'. It returns a matrix listing each group of adjacent values, their starting index, the number of elements in each group, the sum of each group, and the average (mean) of each group. Type "\texttt{help NumAT}" and try the example.\href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#Top}{}

\section{Using the peak table\label{ref-0313}\label{ref-0314}\label{ref-0315}}

All these peak finding functions return a peak table as a matrix, with one row for each peak that it detects and with several columns listing, for example, the peak number, position, height, width, and area in columns 1 - 5 (with additional columns included for the variants \href{https://terpconnect.umd.edu/~toh/spectrum/measurepeaks.m}{measurepeaks.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksnr.m}{findpeaksnr.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksGSS.m}{findpeaksGSS.m}, and \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksLSS.m}{findpeaksLSS.m}). You can assign this matrix to a variable (e.g., P, in the examples above) and then use Matlab/Octave notation and built-in functions to extract specific information from that matrix. \textit{The powerful combination of functions and matrix/vector "colon" notation allows you to construct compact expressions that extract the very specific information that you need}. Here are several examples:

\texttt{[P(:,2) P(:,3)]} is the time series of peak heights (peak position in the first column and peak height in the second column).

\texttt{mean(P(:,3))} returns the average peak height of all peaks (because peak height is in column 3). Also works with ``\texttt{median''}. 

\texttt{max(P(:,3))} returns the maximum peak height of all the peaks. Also works with \texttt{min}. 

\texttt{hist(P(:,3))} displays the histogram of peak heights (using built-in ``hist'' function).

\texttt{std(P(:,4))./mean(P(:,4))} returns the relative standard deviation of the peak widths (column 4).

\texttt{P(:,3)./max(P(:,3))} returns the ratio of each peak height (column 3) to the height of the highest peak detected. 

\texttt{100.*P(:,5)./sum(P(:,5))} returns the percentage of each peak area (column 5) of the total area of all peaks detected. 

\texttt{sortrows(P,2)} sorts P by peak position; \texttt{sort rows(P,3)} sorts P by peak height (small to large).

To create "d" as the vector of x-axis (position) differences between adjacent peaks (because peak position is in column 2):

\texttt{for n=1:length(P)-1;d(n)=max(P(n+1,2)-P(n,2));end}

(In Matlab/Octave, multiple statements can be placed on one line, separated by semicolons.)

\textbf{The val2ind function}. My downloadable function \href{https://terpconnect.umd.edu/~toh/spectrum/val2ind.m}{val2ind.m} (syntax [\texttt{index,closestval] =}\href{https://terpconnect.umd.edu/~toh/spectrum/val2ind.m}{ \texttt{val2ind}}\texttt{(v,val)}) returns the index and the value of the element of vector 'v' that is closest to 'val' (download this function and place in the Matlab path). This simple function is very useful in working with peak tables:

\texttt{val2ind(P(:,3),7.5)} returns the peak number whose height (column 3) is closest to 7.5.

\texttt{P(val2ind(P(:,2),7.5),3)} returns the peak height (column 3) of the peak whose position (column 2) is closest to 7.5. \texttt{P(val2ind(P(:,3),max(P(:,3))),:)} returns the row vector of peak parameters of the highest peak in peak table P.

The three statements \texttt{j=P(:,4){\textless}5.8;k=val2ind(j,1);P(k,:)} return the matrix of peak parameters of all peaks in P whose widths (column 4) are less than 5.8. Whew!

\section{Demo scripts \label{ref-0316}}

\InsImageInline{0.5}{l}{DemoFindPeakSmall.GIF.png}\href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindPeak.m}{DemoFindPeak.m} is a simple demonstration script using the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaks}{findpeaksG} function on noisy synthetic data. The function numbers the peaks and prints out the peak table in the Matlab command window: 

Peak \# Position Height Width Area\index{Area}

Measuredpeaks =

1 799.95 6.9708 51.222 380.12

2 1199.4 3.9168 50.44 210.32

3 1600.6 2.9955 49.683 158.44

4 1800.4 2.0039 50.779 108.33

 \textbf{......etc.}

\InsImageInline{0.5}{l}{DemoFindPeaksb.gif.png}\href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindPeakSNR.m}{DemoFindPeakSNR} is a variant of DemoFindPeak.m that uses \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#PeakSNR}{findpeaksnr.m} to compute the signal-to-noise ratio (SNR) of each peak and returns it in the 5th column (\href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindPeakSNR.png}{click for a graphic}). 

\href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindPeaksb.m}{DemoFindPeaksb.m} is a similar demonstration script that uses the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaksb}{findpeaksb} function on noisy synthetic data consisting of variable numbers of Gaussian peaks \textit{superimposed on a variable curved background}. (The findpeaksG function would not give accurate measurements of peak height, width, and area for this signal, because it does not correct for the background). \href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindPeaksb.gif}{Click for animation}.

 \texttt{Relative Percent Errors}

   \texttt{Position Height Width Area}\index{Area}

 \texttt{-0.002246 0.54487 1.4057 1.9429}

 \texttt{-0.02727 5.0091 8.9204 13.483}

 \texttt{0.008429 -1.1224 -1.4923 -2.6315 {\ldots}etc.}

\texttt{\% Root mean square errors}

\texttt{ans =}

\texttt{0.044428 2.2571 3.8253 5.850}

\section{Peak Identification \label{ref-0317}\label{ref-0318}}

The command line function \href{https://terpconnect.umd.edu/~toh/spectrum/idpeaks.m}{idpeaks.m} is used for identifying peaks \textit{according to their x-axis maximum positions}, which is very useful in spectroscopy and in chromatography. The syntax is 

\texttt{[IdentifiedPeaks, AllPeaks]=idpeaks(DataMatrix, AmpT, SlopeT, SmoothWidth, FitWidth, maxerror, Positions, Names)}

It finds peaks in the signal "DataMatrix" (x-values in column 1 and y-values in column 2), according to the peak detection parameters "AmpT", "SlopeT", "SmoothWidth", "FitWidth" (see the "findpeaksG" function above), then compares the found peak positions (x-values) to a database of known peaks, in the form of an array of known peak maximum positions ('Positions') and matching cell array of names ('Names'). If the position of a peak found in the signal is closer to one of the known peaks by less than the specified maximum error ('maxerror'), that peak is considered a match and its peak position, name, error, and peak amplitude (height) are entered into the output cell array "IdentifiedPeaks". The full list of detected peaks, identified or not, is returned in "AllPeaks". Use "cell2mat" to access numeric elements of IdentifiedPeaks, e.g., \texttt{cell2mat(IdentifiedPeaks(2,1))} returns the position of the first identified peak, \texttt{cell2mat(IdentifiedPeaks(2,2))} returns its name, etc. Obviously, for your own applications, it is up to you to provide your own array of known peak maximum positions ('Positions') and matching cell array of names ('Names') for your particular types of signals. The related function \href{https://terpconnect.umd.edu/~toh/spectrum/idpeaktable.m}{idpeaktable.m} does the same thing for a peak table P returned by any of my peak finder or peak fitting functions, having one row for each peak and columns for peak number, position, and height as the first three columns. The syntax is \texttt{[IdentifiedPeaks] = idpeaktable(P, maxerror, Positions, Names)}. The interactive \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#ipeak}{\textit{iPeak} function} described in the next section has \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#iPeakID}{this function built-in} as one of the keystroke commands (page \pageref{ref-0242}).

\textbf{Example:} Download \href{https://terpconnect.umd.edu/~toh/spectrum/idpeaks.zip}{idpeaks.zip}, extract it, and place the extracted files in the Matlab or Octave path. This contains a high-resolution atomic emission spectrum of copper ('spectrum', x = wavelength in nanometers; y = amplitude) and a data table of known Cu I and Cu II atomic lines ('DataTable') containing the positions and names of many copper lines. The idpeaks function detects and measures the peak locations of all the peaks in "spectrum", then looks in 'DataTable' to see if any of those peaks are within .01 nm of any entry in the table and prints out the peaks that match.

{\textgreater}{\textgreater} load DataTable

{\textgreater}{\textgreater} load spectrum

{\textgreater}{\textgreater} idpeaks(Cu,0.01,.001,5,5,.01,Positions,Names)

ans=

  'Position' 'Name' 'Error' 'Amplitude'

  [ 221.02] 'Cu II 221.027' [ -0.0025773] [ 0.019536]

  [ 221.46] 'Cu I 221.458' [ -0.0014301] [ 0.4615]

  [ 221.56] 'Cu I 221.565' [-0.00093125] [ 0.13191] {\ldots}\label{ref-0319}\label{ref-0320}

\section{\textit{iPeak}, for Matlab\label{ref-0321}\label{ref-0322}}

\textit{iPeak} (ipeak.m) is a keyboard-operated interactive peak finder for time series data, based on the "findpeaksG.m" and "findpeaks\textbf{L}.m" functions, for Matlab only. The interactive keypress operation works on your computer, even if you run \href{https://www.mathworks.com/products/matlab-online.html}{Matlab in a web browser}, but not on \href{https://itunes.apple.com/us/app/matlab-mobile/id370976661?mt=8}{Matlab Mobile} or in Octave. Its basic operation is similar to \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{\textbf{i}}\href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{\textbf{Signal}} and \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{\textbf{ipf.m}}\textbf{.} It accepts data in a single vector, a pair of vectors, or a matrix with the independent variable in the first column and the dependent variable in the second column. If you call \textit{iPeak} with \textit{only} those one or two input arguments, it estimates a default initial value for the peak detection parameters (AmpThreshold, SlopeThreshold, SmoothWidth, and FitWidth) based on the formulas below and displays those values at the bottom of the screen.

   \texttt{WidthPoints=length(y)/20;} 

  \texttt{SlopeThreshold=WidthPoints\textasciicircum{}-2;} 

  \texttt{AmpThreshold=abs(min(y)+0.1*(max(y)-min(y)));} 

  \texttt{SmoothWidth=round(WidthPoints/3);} 

  \texttt{FitWidth=round(WidthPoints/3);}

 You can then fine-tune the peak detection up/down by using these pairs of adjacent keys:

Amplitude threshold: \textbf{A/Z}

Slope threshold: \textbf{S/X}

Smooth width: \textbf{D/C}

Fit width: \textbf{F/V}.

\textbf{Example 1:} One input argument; data in single vector:

\texttt{{\textgreater}{\textgreater} y=cos(.1:.1:100);}

\texttt{{\textgreater}{\textgreater} ipeak(y}\texttt{)} 


\begin{center}
\textbf{\InsImageInline{0.5}{l}{iPeakExample1.png}}
\end{center}


\textbf{Example 2:}  One input argument; data in two columns of a matrix:

\texttt{{\textgreater}{\textgreater} x=[0:.01:5]';}

\texttt{{\textgreater}{\textgreater} y=x.*sin(x.\textasciicircum{}2).\textasciicircum{}2;M=[x y];}

\texttt{{\textgreater}{\textgreater} ipeak(M}\texttt{)}


\begin{center}
\textbf{\InsImageInline{0.5}{l}{example2.png}}
\end{center}


\textbf{Example 3:}  Two input arguments; data in separate x and y vectors:

\texttt{{\textgreater}{\textgreater} x=[0:.1:100];}

\texttt{{\textgreater}{\textgreater} y=(x.*sin(x)).\textasciicircum{}2;}

\texttt{{\textgreater}{\textgreater} ipeak(x,y);}

\InsImageInline{0.5}{l}{ipeak4.gif.png}\textbf{Example 4}: When you start \textit{iPeak} using the simple syntax above, the initial values of the peak detection parameters are calculated as described above, but if it starts off by picking up far \textit{too many} or \textit{too few} peaks, you can add an additional input argument (after the data) to control peak sensitivity.

\texttt{{\textgreater}{\textgreater} x=[0:.1:100];y=5+5.*cos(x)+randn(size(x));ipeak(x,y,}\texttt{\textbf{10}}\texttt{);}

  \texttt{or {\textgreater}{\textgreater} ipeak([x;y],}\texttt{\textbf{10}}\texttt{);}

  \texttt{or {\textgreater}{\textgreater} ipeak(humps(0:.01:2),}\texttt{\textbf{3}}\texttt{)}

  \texttt{or {\textgreater}{\textgreater} x=[0:.1:10];y=exp(-(x-5).\textasciicircum{}2);ipeak([x' y'],}\texttt{\textbf{1}}\texttt{)}

The additional numeric argument is an estimate of \textit{maximum peak density} (PeakD), the ratio of the typical peak width to the length of the entire data record. Small values detect fewer peaks; larger values detect more peaks. It effects only the \textit{starting} values for the peak detection parameters. (It is just a quick way to set reasonable initial values of the peak detection parameters, so you will not have so much adjusting to do). 

\texttt{{\textgreater}{\textgreater} load sunspots}

\texttt{{\textgreater}{\textgreater} ipeak(year,number,20)}


\begin{center}
\textbf{\InsImageInline{0.5}{l}{sunspotsipeak.png}}
\end{center}



\begin{center}
\textit{Peaks in annual sunspot numbers from 1700 to 2009 (download the} \href{https://terpconnect.umd.edu/~toh/spectrum/sunspots.txt}{\textit{datafile}}\textit{).} 
\end{center}



\begin{center}
\textit{Sunspot data downloaded from} \href{http://www.ngdc.noaa.gov/stp/solar/ssndata.html}{\textit{NOAA}}
\end{center}


\textit{iPeak} displays the entire signal in the lower half of the Figure window and an adjustable zoomed-in section in the upper window. Pan and zoom the portion in the upper window using the cursor arrow keys. The peak closest to the center of the upper window is labeled in the upper left of the top window, and its peak position, height, and width are listed. The \textbf{Spacebar/Tab} keys jump to the next/previous detected peak and displays it in the upper window at the current zoom setting (use the up and down cursor arrow keys to adjust the zoom range). Or you can press the \textbf{J} key to jump to a specified peak number. 


\begin{center}
\textbf{\InsImageInline{0.5}{l}{iPeak73.png}}
\end{center}


Adjust the peak detection parameters AmpThreshold (\textbf{A/Z} keys), SlopeThreshold (\textbf{S/X}), SmoothWidth (\textbf{D/C}), FitWidth (\textbf{F/V}) so that it detects the desired peaks and ignores those that are too small, too broad, or too narrow to be of interest. You can also type in a specific value of AmpThreshold by pressing \textbf{Shift-A} or a specific value of SlopeThreshold by pressing \textbf{Shift-S}. Detected peaks are numbered from left to right. 

Press \textbf{P} to display the peak table of all the detected peaks (Peak \#, Position, Height, Width, Area\index{Area}, and percent fitting error)\textbf{:}

  \texttt{Gaussian shape mode (press Shift-G to change)}

  \texttt{Window span: 169 units}

  \texttt{Linear baseline subtraction}

  \texttt{Peak\# Position Height Width Area}\index{Area}  \texttt{Error}

  \texttt{1 500.93 6.0585 34.446 222.17 9.5731}

  \texttt{2 767.75 1.8841 105.58 211.77 25.979}

  \texttt{3 1012.8 0.20158 35.914 7.7 269.21}

      \texttt{.............}

Press \textbf{Shift-G} to cycle between Gaussian\textbf{,} Lorentzian, and flat-top shape modes. Press \textbf{Shift-P} to save peak table as disc file. Press \textbf{U} to switch between peak and valley mode. Do not forget that only valleys \textit{above} (that is, more positive or less negative than) the AmpThreshold are detected; if you wish to detect valleys that have negative minima, then AmpThreshold must be set more negative than that. Note: to speed up the operation for signals over 100,000 points in length, the lower window is refreshed only when the number of detected peaks changes or if the \textbf{Enter} key is pressed. Press \textbf{K} to see all the keystroke commands. 


\begin{center}
\textbf{\InsImageInline{0.5}{l}{ValleyMode.png}}
\end{center}



\begin{center}
\textit{Press U key to switch between peak and valley mode.}
\end{center}


If the density of data points on the peaks is \textit{too low} - less than about 4 points - the peaks may not be reliably detected; you can improve reliability by using the interpolation command (\textbf{Shift-I}) to re-sample the data by linear interpolation to a \textit{larger} number of points. Conversely, if the density of data points on the peaks of interest is very high - say, more than 100 points per peak - then you can speed up the operation of \textit{iPeak} by re-sampling to a \textit{smaller} number of points. 



\textbf{Peak Summary Statistics.} The \textbf{E} key prints a table of summary statistics of the peak intervals (the x-axis interval between adjacent detected peaks), heights, widths, and areas, listing the maximum, minimum, average, and percent standard deviation, and displaying the \textit{histograms} of the peak intervals, heights, widths, and areas in \href{https://terpconnect.umd.edu/~toh/spectrum/histograms.png}{figure window 2}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/ipeak63.png}{\texttt{Peak Summary Statistics}}

\texttt{149 peaks detected}

\texttt{No baseline correction}

  \texttt{Interval Height Width Area}\index{Area}

\texttt{Maximum 1.3204 232.7724 0.33408 80.7861}

\texttt{Minimum 1.1225 208.0581 0.27146 61.6991}

\texttt{Mean 1.2111 223.3685 0.31313 74.4764}

\texttt{\% STD 2.8931 1.9115 3.0915 4.0858}

\textbf{Example 5:} Six input arguments. As above, but input arguments 3 to 6 directly specifies initial values  of AmpThreshold (AmpT), SlopeThreshold (SlopeT), SmoothWidth (SmoothW), FitWidth (FitW). PeakD is ignored in this case, so just type a '0' as the second argument after the data matrix).

\texttt{{\textgreater}{\textgreater} ipeak(datamatrix,0,.5,.0001,20,20);}


\begin{center}
\textbf{\InsImageInline{0.5}{l}{SpectrumMedium.png}}
\end{center}



\begin{center}
\textit{Pressing 'L' toggles ON and OFF the peak labels in the upper window.}
\end{center}


Keystrokes allow you to pan and zoom the upper window, to inspect each peak in detail if desired. You can set the initial values of pan and zoom in optional input arguments 7 ('xcenter') and 8 ('xrange'). See example 6 below. 

The \textbf{Y} key toggles between linear and log y-axis scale in the lower window (a log axis is good for inspecting signals with high dynamic range). It effects only the lower window display and has no effect on the data itself or on the peak detection and measurements. 


\begin{center}
\textbf{\InsImageInline{0.5}{l}{logYmode.png}}
\end{center}



\begin{center}
\textit{Log scale (Y key) makes the smaller peaks easier to see in the lower window.} 
\end{center}


\textbf{Example 6:} Eight input arguments. As above, but input arguments 7 and 8 specify the initial pan and zoom settings, 'xcenter' and 'xrange', respectively. In this example, the x-axis data are wavelengths in nanometers (nm), and the upper window zooms in on a very small 0.4 nm region centered on 249.7 nm. (These data, provided in the \href{https://terpconnect.umd.edu/~toh/spectrum/ipeak7.zip}{ZIP file}, are from a high-resolution atomic spectrum).

 \texttt{{\textgreater}{\textgreater} load ipeakdata.mat}

   \texttt{{\textgreater}{\textgreater} ipeak(Sample1,0,100,0.05,3,4,249.7,0.4);}

\textbf{Baseline correction modes.} The \textbf{T} key cycles the \textit{baseline correction mode} from \textit{off, linear, quadratic}, \textit{flat}, \textit{linear} \textit{mode(y), flat mode(y)}, and then back to \textit{off}. The current mode is displayed above the upper panel. When the baseline correction mode is OFF, peak heights are measured relative to zero. (Use this mode when the baseline is zero or if you have previously subtracted the baseline from the entire signal using the \textbf{B} key). In the \textit{linear} or \textit{quadratic} modes, peak heights are automatically measured relative to the local baseline interpolated from the points at the ends of the segment displayed in the upper panel; use the zoom controls to isolate a group of peaks so that the signal returns to the local baseline at the beginning and end of the segment displayed in the upper window. The peak heights, widths, and areas in the peak table (\textbf{R} or \textbf{P} keys) will be automatically corrected for the baseline. The \textit{linear} or \textit{quadratic} modes will work best if the peaks are well separated so that the signal returns to the local baseline between the peaks. (If the peaks are highly overlapped, or if they are not Gaussian in shape, the best results will be obtained by using the curve fitting function - the \textbf{N} or \textbf{M} keys. The \textit{flat mode} is used only for curve fitting function, to account for a flat baseline offset \textit{without} reference to the edges of the signal segment being fit).  The \textit{mode(y)} method subtracts the \textit{most common y value} from all the points in the selected region. For peak-type signals where the peaks usually return to the baseline between peaks, this is usually the baseline even if the signal does not return to the baseline at the ends like modes 2 and 3 (\href{https://terpconnect.umd.edu/~toh/spectrum/Mode\%28y\%29example.png}{graphic example}).

\textbf{Example 7:}  Nine input arguments. As example 6, but the 9\textsuperscript{th} input argument sets the background correction mode (equivalent to pressing the \textbf{T} key)' 0=OFF; 1=linear; 2=quadratic, 3=flat, 4=mode(y). If not specified, it is initially OFF.

   \texttt{{\textgreater}{\textgreater} ipeak(Sample1,0,100,0.00,3,4,249.7,0.4,1);}

\textbf{Converting to command-line functions.} To aid in writing your own scripts and functions to automate processing performed with iPeak, the '\textbf{Q}' key prints out the findpeaksG, findpeaksb, and findpeaksfit commands for the segment of the signal in the upper window and for the entire signal, with the input arguments in place, so you can Copy and Paste into your own scripts. The '\textbf{W}' key similarly prints out the peakfit.m and ipf.m commands. 

\textbf{Shift-Ctrl-S} transfers the current signal to i\textbf{S}ignal.m (page \pageref{ref-0434}) and \textbf{Shift-Ctrl-P} transfers the current signal to Interactive \textbf{P}eak Detector (iPeak.m), if those functions are installed in your Matlab path. 



\textbf{Ensemble averaging.} For signals that contain repetitive waveform patterns occurring in one continuous signal, with nominally the same shape except for noise, the ensemble averaging function (\textbf{Shift-E}) can compute the average of all the repeating waveforms. It works by detecting a single peak in each repeat waveform to synchronize the repeats (and therefore does not require that the repeats be equally spaced or synchronized to an external reference signal). To use this function, first adjust the peak detection controls to detect \textit{only one peak in each repeat pattern}, and then zoom in to isolate any one of those repeat patterns, and then press \textbf{Shift-E}. The average waveform is displayed in Figure 2 and saved as ``EnsembleAverage.mat'' in the current directory. See the script \href{https://terpconnect.umd.edu/~toh/spectrum/iPeakEnsembleAverageDemo.m}{iPeakEnsembleAverageDemo.m}.

\textbf{De-tailing peaks with exponential tails}. If your signal has peaks that tail to the right or left because they have been exponentially broadened, you can remove the tails by the first-derivative addition technique (page \pageref{ref-0108}): press \textbf{Shift-Y,} enter an estimate of the exponential time constant and then use the \textbf{1} and \textbf{2} keys to adjust the de-tailing factor by 10\% (or \textbf{Shift-1} and \textbf{Shift-2} to adjust by 1\%). \href{https://terpconnect.umd.edu/~toh/spectrum/iPeakShiftYDeTail.gif}{Click for animation}. Increase the factor until the baseline after the peak goes negative, then increase it slightly so that it is \textit{as low as possible but not negative.} This results in narrower, taller peaks, but has no effect on the peak areas. The effect is like deconvoluting the exponential function from the broadened peak, but it is faster and simpler. (It the peaks tail to the left rather than the right, use a negative factor).

\textbf{Normal and Multiple Peak fitting:} The \textbf{N} key applies \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{iterative curve fitting} to the \textit{detected peaks that are displayed in the upper window} (referred to here as "\textbf{N}ormal" curve fitting). The use of the iterative least-squares function can result in more accurate peak parameter measurements than the normal peak table (\textbf{R} or \textbf{P} keys), especially if the peaks are non-Gaussian in shape or are highly overlapped. (If the peaks are superimposed on a background, select the \textbf{baseline correction} mode using the \textbf{T} key, then use the pan and zoom keys to select a peak or a group of overlapping peaks in the upper window, with the signal returning all the way to the local baseline at the ends of the upper window if you are using the linear or quadratic baseline modes; see page \pageref{ref-0276}). Make sure that the AmpThreshold, Slope-Threshold, SmoothWidth are adjusted so that each peak is numbered once. Only numbered peaks are fit. Then press the \textbf{N} key, which will display this menu of peak shapes (graphic on page \pageref{ref-0469}):

\texttt{Gaussians: y=exp(-((x-pos)./(0.6005615.*width)) .\textasciicircum{}2)}

  \texttt{Gaussians with independent positions and widths...................1 (default)}

  \texttt{Exponentially--broadened Gaussian (equal time constants)..........5} 

  \texttt{Exponentially--broadened equal-width Gaussian.....................8} 

  \texttt{Fixed-width exponentially-broadened Gaussian...................36} 

  \texttt{Exponentially--broadened Gaussian (independent time constants)...31} 

  \texttt{Gaussians with the same widths....................................6} 

  \texttt{Gaussians with preset fixed widths...............................11} 

  \texttt{Fixed-position Gaussians.........................................16} 

  \texttt{Asymmetrical Gaussians with unequal half-widths on both sides....14} 

\texttt{Lorentzians: y=ones(size(x))./(1+((x-pos)./(0.5.*width)).\textasciicircum{}2)}

  \texttt{Lorentzians with independent positions and widths.................2} 

  \texttt{Exponentially--broadened Lorentzian..............................18} 

  \texttt{Equal-width Lorentzians...........................................7}

  \texttt{Fixed-width Lorentzian...........................................12}

  \texttt{Fixed-position Lorentzian........................................17}

\texttt{Gaussian/Lorentzian blend (equal blends)...........................13}

  \texttt{Fixed-width Gaussian/Lorentzian blend............................35}

  \texttt{Gaussian/Lorentzian blend with independent blends)...............33}

\texttt{Voigt profile with equal alphas....................................20}

  \texttt{Fixed-width Voigt profile with equal alphas......................34}

  \texttt{Voigt profile with independent alphas............................30}

\texttt{Logistic: n=exp(-((x-pos)/(.477.*wid)).\textasciicircum{}2); y=(2.*n)./(1+n).........3} 

\texttt{Pearson: y=ones(size(x))./(1+((x-pos)./((0.5.\textasciicircum{}(2/m)).*wid)).\textasciicircum{}2).\textasciicircum{}m..4}

  \texttt{Fixed-width Pearson..............................................37}

  \texttt{Pearson with independent shape factors, m........................32}

\texttt{Breit-Wigner-Fano..................................................15}

\texttt{Exponential pulse: y=(x-tau2)./tau1.*exp(1-(x-tau2)./tau1)..........9}

\texttt{Alpha function: y=(x-spoint)./pos.*exp(1-(x-spoint)./pos);.........19}

\texttt{Up Sigmoid (logistic function): y=.5+.5*erf((x-tau1)/sqrt(2*tau2)).10}

\texttt{Down Sigmoid y=.5-.5*erf((x-tau1)/sqrt(2*tau2))....................23}

\texttt{Triangular.........................................................21}

Type the number for the desired peak shape from this table and press \textbf{Enter}, then type in a number of repeat trial fits and press \textbf{Enter} (the default is 1; start with that and then increase if necessary). If you have selected a variable-shape peak (e.g., numbers 4, 5, 8 ,13, 14, 15, 18, 20, 30-33), the program will ask you to type in a number that fine-tunes the shape. The program will then perform the fit, display the results graphically in Figure window 2, and print out a table of results in the command window, e.g.:

\texttt{Peak shape (1-8): 2}

\texttt{Number of trials: 1}

\texttt{Least-squares fit to Lorentzian peak model}

\texttt{Fitting Error 1.1581e-006\%}

  \texttt{Peak\# Position Height Width Area}\index{Area} 

  \texttt{1 100 1 50 71.652}

  \texttt{2 350 1 100 146.13}

  \texttt{3 700 1 200 267.77}


\begin{center}
\textbf{\InsImageInline{0.5}{l}{iPeakPeakfitBestOf10.png}}
\end{center}



\begin{center}
\textit{Normal Peak Fit (N key) applied to a group of three overlapping Gaussians peaks}
\end{center}


There is also a "Multiple" peak fit function (\textbf{M} key) that will attempt to apply iterative curve fitting to \textit{all} \textit{the detected peaks in the signal simultaneously}. Before using this function, it is best to turn \textbf{\textit{off}} the automatic baseline correction (\textbf{T} key) and use the \textit{multi-segment baseline correction function} (\textbf{B} key) to remove the background (because the baseline correction function will probably not be able to subtract the baseline from the entire signal). Then press \textbf{M} and proceed as for the normal curve fit. A multiple curve fit may take a minute or so to complete if the number of peaks is large, possibly longer than the Normal curve fitting function on each group of peaks separately. 

The \textbf{N} and \textbf{M} key fitting functions perform \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{non-linear iterative curve fitting} using the \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit.m} function. The number of peaks and the starting values of peak positions and widths for the curve fit function are automatically supplied by the findpeaksG function, so it is essential that the \textbf{\InsImageInline{0.5}{l}{iPeakExample8.png}}peak detection variables in \textit{iPeak} be adjust so that all the peaks in the selected region are detected and numbered once. (For more flexible curve fitting, use \href{http://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{ipf.m}, page \pageref{ref-0461}, which allows manual optimization of peak groupings and start positions). 

\textbf{Example 8.}  This example generates four Gaussian peaks, all with the exact same peak height (1.00) and area (1.773). The first peak (at x=4) is isolated, the second peak (x=9) is slightly overlapped with the third one, and the last two peaks (at x= 13 and 15) are strongly overlapped.

\texttt{x=[0:.01:20];}

\texttt{y=exp(-(x-4).\textasciicircum{}2)+exp(-(x-9).\textasciicircum{}2)+exp(-(x-13).\textasciicircum{}2)+exp(-(x-15).\textasciicircum{}2);}

\texttt{ipeak(x,y)}

\InsImageInline{0.5}{l}{peakfitExample8.png}By itself, \textit{iPeak} does a fairly good just of measuring peaks positions and heights by fitting just the top part of the peaks, because the peaks are Gaussian, but the areas and widths of the last two peaks (which should be 1.665 like the others) are quite a bit too large because of the overlap: 

\texttt{Peak\# Position Height Width Area}\index{Area}

  \texttt{1 4 1 1.6651 1.7727}

  \texttt{2 9 1 1.6651 1.7727}

  \texttt{3 13.049 1.02 1.8381 1.9956}

  \texttt{4 14.951 1.02 1.8381 1.9956}

In this case, curve fitting (using the \textbf{N} or \textbf{M} keys) does a much better job, even if the overlap is even greater, but \textit{only if the peak shape is known}:  

\texttt{Peak\# Position Height Width Area}\index{Area} 

  \texttt{1 4 1 1.6651 1.7724}

  \texttt{2 9 1 1.6651 1.7725}

  \texttt{3 13 1 1.6651 1.7725}

  \texttt{4 15 0.99999 1.6651 1.7724}

\textbf{Note 1}: If the peaks are too overlapped to be detected and numbered separately, try pressing the \textbf{H} key to activate the even-derivative sharpen function before pressing \textbf{M} (version 4.0 and above only). This effects only the peak detection, not the signal itself.

\textbf{Note 2}: If you plan to use a variable-shape peak (numbers 4, 5, 8 ,13, 14, 15, 18, or 20) for the \textbf{M}ultiple peak fit, it is a good idea to obtain a reasonable value for the requested "extra" shape parameter by performing a \textbf{N}ormal peak fit on an isolated single peak (or small group of partly-overlapping peaks) of the same shape, then use that value for the \textbf{M}ultiple curve fit of the entire signal. 

\textbf{Note 3}: If the peak shape varies across the signal, you can either use the \textbf{N}ormal peak fit to fit each section with a different shape rather than the \textbf{M}ultiple peak fit, or you can use the \textit{unconstrained} shapes that fit the shape individually for each peak: Voigt (30), ExpGaussian (31), Pearson (32), or Gaussian/Lorentzian blend (33).

\textbf{Peak identification.} There is an optional "peak identification" operation if the optional input arguments 9 ('MaxError'), 10 ('Positions'), and 11 ('Names') are included. The "\textbf{I}" key toggles this function ON and OFF. This function compares the found peak positions (maximum x-values) to a reference database of known peaks, in the form of an array of known peak maximum positions ('Positions') and matching cell array of names ('Names'). If the position of a found peak in the signal is closer to one of the known peaks by less than the specified maximum error ('MaxError'), then that peak is considered a match and its name is displayed next to the peak in the upper window. When the '\textbf{O}' key is pressed (the letter 'O'), the peak positions, names, errors, and amplitudes are printed out in a table in the command window. 

\textbf{Example 9}: Eleven input arguments. As above, but also specifies 'MaxError', 'Positions', and 'Names' in optional input arguments 9, 10, and 11, for peak identification function. Pressing the '\textbf{I}' key toggles off and on the peak identification labels in the upper window. These data (provided in this \href{https://terpconnect.umd.edu/~toh/spectrum/ipeak7.zip}{ZIP file}) are from a high-resolution atomic spectrum (x-axis in nanometers).

\texttt{{\textgreater}{\textgreater} load ipeakdata.mat}

\texttt{{\textgreater}{\textgreater} ipeak(Sample1,0,100,0.05,3,6,296,5,0.1,Positions,Names);}


\begin{center}
\textbf{\InsImageInline{0.5}{l}{ipeakIDmedium.png}}
\end{center}



\begin{center}
\textit{The peak identification function applied to a high-resolution atomic emission spectrum of copper.} 
\end{center}



\begin{center}
\textbf{\InsImageInline{0.5}{l}{ipeak2.png}}
\end{center}



\begin{center}
\textit{An atomic emission spectrum of a sample containing trace amounts of several elements. iPeak is used to zoom in to three small peaks near 296 nm. iPeak identified and labeled three peaks based on the atomic line data in} \texttt{ipeakdata.mat}\textit{. Press the I key to display the peak ID names.}
\end{center}




Pressing "O" prints the peak positions, names, errors, and amplitudes in a table in the command window.  

 \texttt{\textbf{Name Position Error Amplitude}}

 \texttt{'Mg I 295.2' [295.2] [0.058545] [129.27]}

  \texttt{'Cu 296.1 nm' [296.1] [0.045368] [124.6]}

  \texttt{'Hg 297.6 nm' [297.6] [0.023142] [143.95]}

Here is another example, from a large atomic emission spectrum with over 10,000 data points and many hundreds of peaks. The reference table of known peaks in this case is taken from Table 1 of \href{https://www.astm.org/DATABASE.CART/HISTORICAL/C1301-95R09E1.htm}{ASTM} C1301 - 95(2009)e1. With the settings I was using, 10 peaks were identified, shown in the table below. You can see that some of these elements have more than one line identified. Obviously, the lower the settings of the AmpThreshold, SlopeThreshold, and SmoothWidth, the more peaks will be detected; and the higher the setting of "MaxError", the more peaks will be close enough to be considered identified. In this example, the element names in the table below are hot-linked to the screen image of the corresponding peak detected as identified by \textit{iPeak}. Some of these lines, especially Nickel 231.66nm, Silicon 288.18nm, and Iron 260.1nm, are rather weak and have poor signal-to-noise ratios, so their identification might be in doubt (especially Iron because its wavelength error is greater than the rest). It is up to the experimenter to decide which peaks are strong enough to be significant. In this example, I used an \textit{independently published table of element wavelengths}, rather than data acquired on that \textit{same} instrument, so these results really depend on the accurate wavelength calibration of the instrument. The evidence suggests that the wavelength calibration is excellent, based on the very small error for the \textit{two well-known and relatively strong Sodium lines} at 589 and 589.59 nm. (Even so, I set MaxError to 0.2 nm in this example to loosen up the wavelength matching requirements). 


\begin{center}
\texttt{\textbf{'Name' 'Position' 'Error' 'Amplitude'}}
\end{center}



\begin{center}
\texttt{'}\href{https://terpconnect.umd.edu/~toh/spectrum/iPeakCadmium.png}{\texttt{Cadmium}}\texttt{' [ 226.46] [-0.039471] [ 44.603]}
\end{center}



\begin{center}
\texttt{'}\href{https://terpconnect.umd.edu/~toh/spectrum/iPeakNickel.png}{\texttt{Nickel}}\texttt{' [ 231.66] [ 0.055051] [ 26.381]}
\end{center}



\begin{center}
\texttt{'}\href{https://terpconnect.umd.edu/~toh/spectrum/iPeakSilicon.png}{\texttt{Silicon}}\texttt{' [ 251.65] [ 0.041616] [ 45.275]}
\end{center}



\begin{center}
\texttt{'}\href{https://terpconnect.umd.edu/~toh/spectrum/iPeakIron.png}{\texttt{Iron}}\texttt{' [ 260.1] [ 0.156] [ 38.04]}
\end{center}



\begin{center}
\texttt{'Silicon' [ 288.18] [ 0.022458] [ 27.214]}
\end{center}



\begin{center}
\texttt{'}\href{https://terpconnect.umd.edu/~toh/spectrum/iPeakStrontiuml.png}{\texttt{Strontium}}\texttt{' [ 421.48] [-0.068412] [ 41.119]}
\end{center}



\begin{center}
\texttt{'}\href{https://terpconnect.umd.edu/~toh/spectrum/iPeakBarium.png}{\texttt{Barium}}\texttt{' [ 493.35] [-0.057923] [ 72.466]}
\end{center}



\begin{center}
\texttt{'}\href{https://terpconnect.umd.edu/~toh/spectrum/iPeakSodium.png}{\texttt{Sodium}}\texttt{' [ 589] [0.0057964] [ 405.23]}
\end{center}



\begin{center}
\texttt{'}\href{https://terpconnect.umd.edu/~toh/spectrum/iPeakSodium.png}{\texttt{Sodium}}\texttt{' [ 589.57] [-0.015091] [ 315.2]}
\end{center}



\begin{center}
\texttt{'}\href{https://terpconnect.umd.edu/~toh/spectrum/iPeakPotassium.png}{\texttt{Potassium}}\texttt{' [ 766.54] [ 0.051585] [ 61.987]}
\end{center}


Note: The \href{https://terpconnect.umd.edu/~toh/spectrum/ipeak7.zip}{ZIP file} contains the latest version of the \textit{iPeak} function as well as some sample data to demonstrate peak identification (Example 8). Obviously for your own applications, it is up to you to provide your own array of known peak maximum positions ('Positions') and matching cell array of names ('Names') for your particular types of signals. 

\subsection{\textit{iPeak} keyboard Controls (version 8.1): \label{ref-0323}}

\texttt{Pan signal left and right...Coarse pan:} \texttt{\textbf{{\textless}}} \texttt{or} \texttt{\textbf{{\textgreater}}} 

                             \texttt{Fine pan: left or right cursor arrow keys}

  \texttt{Nudge one point left or right:} \texttt{\textbf{[}} \texttt{and} \texttt{\textbf{]}}

 \texttt{Zoom in and out.............Coarse zoom:} \texttt{\textbf{/}} \texttt{or} \texttt{\textbf{'}} 

  \texttt{Fine zoom: up or down cursor arrow keys}

 \texttt{Resets pan and zoom.........}\texttt{\textbf{ESC}}

 \texttt{Select entire signal}\texttt{\textbf{........Ctrl-A}}

 \texttt{Refresh entire plot.........}\texttt{\textbf{Enter}} \texttt{(Updates cursor position in} 

 \texttt{lower plot)}

 \texttt{Change plot color...........}\texttt{\textbf{Shift-C}} \texttt{(cycles through standard colors)}

 \texttt{Adjust AmpThreshold.........}\texttt{\textbf{A,Z}}  \texttt{(Larger values ignore short peaks)}

 \texttt{Type in AmpThreshold........}\texttt{\textbf{Shift-A}}  \texttt{(Type value and press} \texttt{\textbf{Enter}}\texttt{)}

 \texttt{Adjust SlopeThreshold.......}\texttt{\textbf{S,X}}  \texttt{(Larger values ignore broad peaks)}

 \texttt{Type in SlopeThreshold......}\texttt{\textbf{Shift-S}} \texttt{(Type value and press} \texttt{\textbf{Enter}}\texttt{)}

 \texttt{Adjust SmoothWidth..........}\texttt{\textbf{D,C}}  \texttt{(Larger values ignore sharp peaks\}}

 \texttt{Adjust FitWidth.............}\texttt{\textbf{F,V}} \texttt{(Adjust to cover top part of peaks)}

 \texttt{Toggle sharpen mode ........}\texttt{\textbf{H}}  \texttt{Helps detect overlapped peaks.}

 \texttt{Enter de-tailing factor.....}\texttt{\textbf{Shift-Y}}\texttt{. Removes exponential peak tails}

 \texttt{Adjust de-tailing 10\%.......}\texttt{\textbf{1}}\texttt{,}\texttt{\textbf{2}} \texttt{Adjust peak de-tailing factor 10\%.}

 \texttt{Adjust de-tailing 1\%........}\texttt{\textbf{Shift-1}}\texttt{,}\texttt{\textbf{Shift-2}} \texttt{Adjust de-tailing 1\%.}

 \texttt{Baseline correction.........}\texttt{\textbf{B}}\texttt{, then click baseline at multiple points} 

 \texttt{Restore original signal.....}\texttt{\textbf{G}}  \texttt{to cancel previous background subtraction}

 \texttt{Invert signal...............}\texttt{\textbf{-}}  \texttt{Invert (negate) the signal (flip + and -)}

 \texttt{Set minimum to zero.........}\texttt{\textbf{0}} \texttt{(Zero) Sets minimum signal to zero}

 \texttt{Interpolate signal..........}\texttt{\textbf{Shift-I}}  \texttt{Interpolate (re-sample) to N points}

 \texttt{Toggle log y mode OFF/ON....}\texttt{\textbf{Y}} \texttt{Plot log Y axis on lower graph}

 \texttt{Cycles baseline modes.......}\texttt{\textbf{T}} \texttt{0=none; 1=linear; 2=quad; 3=flat;}

  \texttt{4=linear mode(y); 5=flat mode(y)}

 \texttt{Toggle valley mode OFF/ON...}\texttt{\textbf{U}}  \texttt{Switch to valley mode}

 \texttt{Gaussian/Lorentzian mode....}\texttt{\textbf{Shift-G}} \texttt{Cycle} \texttt{between Gaussian,}

 \texttt{Lorentzian, and flat-top modes}

 \texttt{Print peak table............}\texttt{\textbf{P}}  \texttt{Prints Peak \#, Position, Height, Width}

 \texttt{Save peak table.............}\texttt{\textbf{Shift-P}}  \texttt{Saves peak table as disc file}

 \texttt{Step through peaks..........}\texttt{\textbf{Space/Tab}}  \texttt{Jumps to next/previous peak}

 \texttt{Jump to peak number}\texttt{\textbf{.........J}} \texttt{Type peak number and press Enter}

 \texttt{Normal peak fit.............}\texttt{\textbf{N}}  \texttt{Fit peaks in upper window with peakfit.m}

 \texttt{Multiple peak fit...........}\texttt{\textbf{M}}  \texttt{Fit all peaks in signal with peakfit.m}

 \texttt{Ensemble average all peaks..}\texttt{\textbf{Shift-E}}  \texttt{(}\href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#EnsembleAveraging}{\texttt{Read instructions first}}\texttt{)}

 \texttt{Print keyboard commands.....}\texttt{\textbf{K}}  \texttt{Prints this list}

 \texttt{MeasUre peak areas .........}\texttt{\textbf{Shift-U}}  \texttt{Areas by perp. drop and tan. skim.}

 \texttt{Print findpeaks arguments...}\texttt{\textbf{Q}}  \texttt{Prints findpeaks function with arguments.}

 \texttt{Print ipeak arguments.......}\texttt{\textbf{W}}  \texttt{Prints ipeak function with all arguments.}

 \texttt{Print report................}\texttt{\textbf{R}}  \texttt{Prints Peak table and parameters}

 \texttt{Print peak statistics.......}\texttt{\textbf{E}}  \texttt{prints mean, std of peak intervals,} 

 \texttt{heights, etc.}

 \texttt{Peak labels ON/OFF.........} \texttt{\textbf{L}}  \texttt{Label all peaks detected in upper window.}

 \texttt{Peak ID ON/OFF..............}\texttt{\textbf{I}}  \texttt{Identifies closest peaks in} 

 \texttt{'Names' database.}

 \texttt{Print peak IDs..............}\texttt{\textbf{O}}  \texttt{Prints table of peaks IDs}

 \texttt{Switch to ipf.m.............}\texttt{\textbf{Shift-Ctrl-F}} \texttt{Transfer current signa}\texttt{l to}

      \texttt{Interactive Peak} \texttt{\textbf{F}}\texttt{itter}

 \texttt{Switch to iSignal...........}\texttt{\textbf{Shift-Ctrl-S}}  \texttt{Transfer current signal} 

   \texttt{to iSignal.m}

\href{https://terpconnect.umd.edu/~toh/spectrum/ipeak.html}{Click for Animated step-by-step instructions}

\subsection{\textit{iPeak} Demo functions\label{ref-0324}\label{ref-0325}}

\href{https://terpconnect.umd.edu/~toh/spectrum/demoipeak.m}{demoipeak.m} is a demonstration function that generates a noisy signal with peaks, calls \textit{iPeak}, and then prints out a table of the actual peak parameters and a list of the peaks detected and measured by \textit{iPeak} for comparison. Before running this demo, \href{https://terpconnect.umd.edu/~toh/spectrum/ipeak.m}{ipeak.m} must be downloaded and placed in the Matlab path. The ZIP file at \url{http://terpconnect.umd.edu/~toh/spectrum/ipeak8.zip} contains several demo functions (ipeakdemo.m, ipeakdemo1.m, etc.) that illustrate various aspects of the \textit{iPeak} function and how it can be used effectively. Download the zip file, right-click and select "Extract all", then put the resulting files in the Matlab path and run them by typing their names at the Matlab command window prompt. To test for the proper installation and operation of \textit{iPeak}, run ``\href{https://terpconnect.umd.edu/~toh/spectrum/testipeak.m}{testipeak}\textcolor{color-7}{.m}''.

\href{https://terpconnect.umd.edu/~toh/spectrum/PeakAreaMethods.m}{PeakAreaMethods.m} is a script that compares the three methods are available in iPeak.m for peak area measurements, using a computer-generated signal of 5 overlapping peaks with unequal heights and widths but equal areas. The peak areas are measured by Gaussian estimation (GE), perpendicular drop (PD), and iterative curve fitting (Fa), which are taken as the correct values. You can control the peak shape and widths in lines 18 and 19.

\InsImageInline{0.5}{l}{ipeakdemo.png}\href{https://terpconnect.umd.edu/~toh/spectrum/ipeakdemo.m}{\textbf{ipeakdemo}}\textbf{: effect of the peak detection parameters}

 Four Gaussian peaks with the same heights but different widths (10, 30, 50 and 70 units). This demonstrates the effect of SlopeThreshold and SmoothWidth on peak detection. Increasing SlopeThreshold (S key) will discriminate against the broader peaks. Increasing SmoothWidth (D key) will discriminate against the narrower peaks and noise. FitWidth (adjusted by the F and V keys) controls the number of points around the "top part" of the (unsmoothed) peak that are taken to estimate the peak heights, positions, and widths. A reasonable value is ordinarily about equal to 1/2 of the number of data points in the half-width of the peaks. In this case, where the peak widths are quite different, set it to about 1/2 of the number of data points in the narrowest peak. 

\InsImageInline{0.5}{l}{image235.png}\href{https://terpconnect.umd.edu/~toh/spectrum/ipeakdemo1.m}{\textbf{ipeakdemo1}}\textbf{: the baseline correction mode.} Demonstration of background correction, with separated, narrow peaks on a large baseline. Each time you run this demo, you will get a different set of peaks and noise. A table of the actual peak positions, heights, widths, and areas is printed out in the command window. Jump to the next/ previous peaks using the \textbf{Spacebar/Tab} keys. Hint: Select the linear baseline correction mode (\textbf{T} key), adjust the zoom setting so that the peaks are shown one at a time in the upper window, then press the \textbf{P} key to display the peak table. For peak signals like this, the mode(y) baseline correction modes 4 and 5 are often useful, subtracting the \textit{most common y value} (the statistical \textit{mode)} from the points in the selected region, which usually removes the baseline and does not need the signal to return to the baseline to the edges of the selected region like modes 2 and 3.

\textbf{ipeakdemo2: peak overlap and the curve fitting functions.} 

\InsImageInline{0.5}{l}{ipeakdemo2.png}Demonstration of error caused by overlapping peaks on a large offset baseline. Each time you run this demo, you will get a different set of peaks and noise. A table of the actual peak positions, heights, widths, and areas is printed out in the command window. (Jump to the next/ previous peaks using the Spacebar/ Tab keys). Hint: Use the \textbf{B} key and click on the baseline points, then press the \textbf{P} key to display the peak table. Or use background correction modes 2-4 and use the Normal curve fit (\textbf{N} key) with peak shape 1 (Gaussian).

\href{https://terpconnect.umd.edu/~toh/spectrum/ipeakdemo3.m}{\textbf{ipeakdemo3}}:  \textbf{Baseline shift caused by overlapping peaks}

\InsImageInline{0.5}{l}{ipeakdemo3.png}Demonstration of overlapping Lorentzian peaks, without an added background. A table of the actual peak positions, heights, widths, and areas is printed out in the command window; in this example, the true peak heights are 1,2 3,...10. Overlap of peaks can cause significant errors in measuring peak parameters, especially for Lorentzian peaks, because they have gently sloping sides that contribute to the baseline of any peaks in the region.

Hint: turn OFF the background correction mode (\textbf{T} key) and use the Normal curve fit (\textbf{N} key) to fit small groups of 2-5 peaks numbered in the upper window, with peak shape 2 (Lorentzian). For the greatest accuracy in measuring a particular peak, include one or two additional peaks on either side, to help account for the baseline shift caused by the sides of those neighboring peaks. Alternatively, if the total number of peaks is not too great, you can use the Multiple curve fit (\textbf{M} key) to fit the entire signal in the lower window. 

\href{https://terpconnect.umd.edu/~toh/spectrum/ipeakdemo4.m}{\textbf{ipeakdemo4}}\textbf{: dealing with very noisy signals}

\InsImageInline{0.5}{l}{ipeakdemo4.png}Detection and measurement of four peaks in a very noisy signal. The signal-to-noise ratio of the first peak is 2. Each time you run this demo, you will get a different set of noise. A table of the actual peak positions, heights, widths, and areas is printed out in the command window. Jump to the next/previous peaks using the \textbf{Spacebar/ Tab} keys. The peak at x=100 is usually detected, but the accuracy of peak parameter measurement is poor because of the low signal-to-noise ratio. \textbf{ipeakdemo6} is similar but has the option of different kinds of noise (white, pink, proportional, etc.)

Hint: With very noisy signals it is usually best to increase \textit{SmoothWidth} and \textit{FitWidth} to help reduce the effect of the noise.

\href{https://terpconnect.umd.edu/~toh/spectrum/ipeakdemo5.m}{\textbf{ipeakdemo5}}\textbf{: dealing with highly overlapped peaks}

\InsImageInline{0.5}{l}{demo5.gif.png}In this demo, the peaks are so highly overlapped that only one or two of the highest peaks yield distinct maxima that are detected by iPeak. The height, width, and area estimates are highly inaccurate because of this overlap. The normal peak fit function ('\textbf{N}' key) would be useful in this case, but it depends on iPeak for the number of peaks and for the initial guesses, and so it would fit only the peaks that were found and numbered.

To help in this case, pressing the '\textbf{H}' key will activate the peak sharpen function that decreases peak width and increases the peak height of all the peaks, making it easier to detect and number all the peaks for use by the peakfit function (\textbf{N} key). Note: peakfit fits the original unmodified peaks; the sharpening is used only to help locate the peaks to provide peakfit with suitable starting values.

\section{ Spreadsheet Peak Finder Templates \label{ref-0326}\label{ref-0327}\label{ref-0328}}

\InsImageInline{0.5}{l}{PeakAndValleyDetectionExample.jpg}\textbf{Simple peak and valley detection.} The spreadsheet pictured above, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakAndValleyDetectionTemplate.xlsx}{PeakAndValleyDetectionTemplate.xlsx} (or \href{https://terpconnect.umd.edu/~toh/spectrum/PeakAndValleyDetectionExample.xlsx}{PeakAndValleyDetectionExample.xlsx} with sample data), is a peak and valley detector that defines a \textit{peak} simply as any point with \textit{lower} points on both sides and a \textit{valley} as any point with \textit{higher} points on both sides. Peaks and valleys are marked by colored cells in columns F through L and tabulated in columns T through Y with their x and y measured values based on the use of the INDIRECT, ADDRESS, and MATCH functions as described on page \pageref{ref-0420}. The raw data can be optionally smoothed by entering a smooth width (a positive odd integer) in cell E6 to suppress false detection caused by random noise. Directions for expanding the template are included.

\textbf{Selective peak detection.} The spreadsheet\href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionTemplate.xls}{ PeakDetectionTemplate.xls} (or \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionExample.xls}{PeakDetectionExample.xls} with sample data) implements the first-derivative zero-crossing peak detection method with amplitude and slope thresholds as described on page \pageref{ref-0298}. The input x, y data are contained in Sheet1, column \textbf{A} and \textbf{B}, rows 9 to 1200. (You can type or paste your own data there). The amplitude threshold and slope threshold are set in cells \textbf{B4} and \textbf{E4}, respectively. Smoothing and differentiation are performed by the convolution technique used by the spreadsheets \href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeSmoothing.xls}{DerivativeSmoothing.xls} described previously on page \pageref{ref-0094}. The Smooth Width and the Fit Width are both controlled by the number of non-zero convolution coefficients in row 6, columns \textbf{J} through \textbf{Z}. (In order to compute a symmetrical first derivative, the coefficients in columns \textbf{J} to \textbf{Q} must be the negatives of  the positive coefficients in columns \textbf{S} to \textbf{Z}). The original data and the smoothed derivative are shown in the two charts. To detect peaks in the data, a series of three conditions are tested for each data point in columns \textbf{F}, \textbf{G}, and \textbf{H}, corresponding to the three nested loops in \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaks.m}{findpeaksG.m}:  \begin{enumerate}[{1.}]


\item Is the signal greater than Amplitude Threshold? (line 45 of findpeaksG.m; column \textbf{F} in the spreadsheet)

\item Is there a downward directed zero crossing (page \pageref{ref-0086}) in the smoothed first derivative? (line 43 of findpeaksG.m; column \textbf{G} in the spreadsheet)

\item Is the slope of the derivative at that point greater than the Slope Threshold? (line 44 of findpeaksG.m; column \textbf{H} in the spreadsheet)

\end{enumerate}
If the answer to \textit{all three} questions is \textit{yes} (highlighted by blue cell coloring), a peak is registered at that point (column \textbf{I}), counted in column \textbf{J}, and assigned an index number in column \textbf{K}. The peak index numbers, X-axis positions, and peak heights are listed in columns \textbf{AC} to \textbf{AF}. Peak heights are computed \textit{two} ways: "Height" is based on slightly smoothed Y values (more accurate if the peaks are broad and noisy, as in \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionDemo2b.xls}{PeakDetectionDemo2b.xls}) and "Max" is the highest individual Y value near the peak (more accurate if the data are smooth or if the peaks are very narrow, as in \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionDemo2a.xls}{PeakDetectionDemo2a.xls}). \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionDemo2.xls}{PeakDetectionDemo2.xls}/\href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionDemo2.xlsx}{xlsx} is a demonstration with a user-controlled computer-generated series of four noisy Gaussian peaks with known peak parameters. \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionSineExample.xls}{PeakDetectionSineExample.xls} is a demo that generates a sinusoidal signal with an adjustable number of peaks. 

You can extend these spreadsheets to \textit{longer columns of data} by dragging the last row of columns \textbf{A} through \textbf{K} as needed, then select and edit the data in the graphs to include all the points in the data (Right-click, Select data, Edit). You can extend the spreadsheet to a \textit{greater number of peaks} by dragging the last row of columns \textbf{AC} and \textbf{AD} as needed. Edit \textbf{R7} and the data range in the equations of cells in row 9, columns \textbf{U}, \textbf{V}, \textbf{W}, \textbf{X}, \textbf{AE}, and \textbf{AF} to include all the rows containing data, then copy-drag them down to cover all expected peaks. 

\textbf{Peak detection with least-squares measurement}. An extension of that method is made in the spreadsheet template \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionAndMeasurement.xlsx}{PeakDetectionAndMeasurement.xlsx} (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingAndMeasurement.png}{screen image}), which makes the assumption that the peaks are Gaussian and measures their height, position, and width more precisely using a \textit{least-squares technique}, just like "\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaks.m}{findpeaksG.m}". For the first 10 peaks found, the x,y original unsmoothed data are \textbf{copied} to Sheets 2 through 11, respectively, where that segment of data is subjected to a Gaussian least-squares fit, using the same technique as \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#GaussFit}{GaussianLeastSquares.xls}. The best-fit Gaussian parameter results are copied back to Sheet1, in the table in columns \textbf{AH-AK}. (In its present form, the spreadsheet is limited to measuring 10 peaks, although it can detect any number of peaks. Also, it is limited in Smooth Width and Fit Width by the 17-point convolution coefficients).

The spreadsheet is available in OpenOffice (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionAndMeasurement.ods}{ods}) and in Excel (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionAndMeasurement.xls}{xls} and \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionAndMeasurement.xlsx}{xlsx}) formats. They are functionally equivalent and differ only in minor cosmetic aspects. There are two example spreadsheets (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionExample2.xlsx}{A}, and \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionExample3.xlsx}{B}) with calculated noisy waveforms that you can modify. \textit{Note: To enter data into the .xls and .xlsx versions, click the "Enable Editing" button in the yellow bar at the top.} 

If the peaks in the data are too much overlapped, they may not make sufficiently distinct maxima to be detected reliably. If the noise level is low, the peaks can be sharpened artificially by the \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html}{technique described previously.} This is implemented by \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionAndMeasurementPS.xlsx}{PeakDetectionAndMeasurementPS.xlsx} and its demo version with example data already entered \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionAndMeasurementDemoPS.xlsx}{PeakDetectionAndMeasurementDemoPS.xlsx}. 

Expanding the PeakDetectionAndMeasurement.xlsx spreadsheet to \textit{larger numbers of measured peaks} is more difficult. You will have to drag down row 17, columns \textbf{AC} through \textbf{AK}, and adjust the formulas in those rows for the required number of additional peaks, then copy all Sheet11 and paste it into a series of new sheets (Sheet12, Sheet13, etc.), one for each additional peak, and finally adjust the formulas in columns \textbf{B} and \textbf{C} in each of these additional sheets to refer to the appropriate row in Sheet1. Modify these additional equations in the same pattern as those for peaks 1-10. (In contrast to these spreadsheets, the Matlab/Octave findpeaks functions adapt automatically to \textit{any} length of data and \textit{any} number of peaks). 

\textbf{Spreadsheet vs Matlab/Octave}. A comparison of this spreadsheet to its Matlab/Octave equivalent \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksplot.m}{findpeaksplot.m} is instructive. On the positive side, the spreadsheet literally "spreads out" the data and the calculations spatially over many cells and sheets, breaking down the discrete steps in a very graphic way. In particular, the use of \href{http://office.microsoft.com/en-us/excel-help/quick-start-apply-conditional-formatting-HA010370614.aspx}{conditional formatting} in columns \textbf{F} through \textbf{K} makes the peak detection decision process more evident for each peak, and the least-squares sheets 2 through 11 lay out every detail of those calculations. Spreadsheet programs have many flexible and easy-to-use formatting options to make displays more attractive. On the downside, a spreadsheet as complicated as PeakDetectionAndMeasurement.xlsx is far more difficult to construct than its Matlab/Octave equivalent. Much more serious, the spreadsheet is \textit{less flexible} and harder to expand to larger signals and to larger number of peaks. In contrast, the Matlab/Octave equivalent, while requiring some understanding of programming to create initially, is easy to use, faster in execution, much more flexible, and \textit{can easily handle signals and smooth/fit widths of any size}. Spreadsheets become cumbersome with very large data sets. Moreover, a Matlab/ Octave \textit{function} can readily be employed as an element in your own custom Matlab/Octave programs to perform even larger tasks. It is harder to do that in a spreadsheet. 

To compare the computation speed of this spreadsheet peak finder to the Matlab/Octave equivalent, we can take as an example the spreadsheet \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionExample2.xls}{PeakDetectionExample2.xls}, or \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionExample2.ods}{PeakDetectionExample2.ods}, which computes and plots a test signal consisting of a noisy sine-squared waveform with 300 data points and then detects and measures 10 peaks in that waveform and displays a table of peak parameters. This is equivalent to the Matlab/Octave script:

 \texttt{tic}

 \texttt{x=1:300;}

 \texttt{y(1:99)=randn(size(1:99));}

 \texttt{y(100:300)=10.*sin(.16.*x(100:300)).\textasciicircum{}2. + randn(size(x(100:300)));}

 \texttt{P=findpeaksplot(x,y,0.005,5,7,7,3);}

 \texttt{disp(P)}

 \texttt{drawnow}

 \texttt{toc}

The table below compares the elapsed times measured for Matlab 2020 and for Octave 6.1.0 running on a Dell XPS i7 3.5Ghz desktop.  The speed advantage of Matlab is clear. 


\begin{table}
\begin{tabularx}{\textwidth}{|
p{\dimexpr 0.561\linewidth-2\tabcolsep-2\arrayrulewidth}|
p{\dimexpr 0.439\linewidth-2\tabcolsep-\arrayrulewidth}|} \hline 
\centering\arraybackslash{}\textbf{Method} & \centering\arraybackslash{}\textbf{Elapsed time} \\\hline 
\centering\arraybackslash{}Excel spreadsheet & \centering\arraybackslash{}\textasciitilde{} 1 sec \\\hline 
\centering\arraybackslash{}Calc spreadsheet & \centering\arraybackslash{}\textasciitilde{} 1 sec \\\hline 
\centering\arraybackslash{}Matlab script & \centering\arraybackslash{}0.035 sec \\\hline 
\centering\arraybackslash{}Octave script & \centering\arraybackslash{}0.5 sec \\\hline 
\end{tabularx}
\end{table}
This is a rather small test; many real-world applications have many more data points and many more peaks, in which the speed advantage of Matlab would be more significant. Moreover, Matlab would be the method of choice if you have many separate data sets to which you need to apply a peak detection/measurement algorithm completely automatically (page \pageref{ref-0414}). See also \href{https://www.google.com/search?q=Excel+VS+Matlab+Notes.&oq=Excel+VS+Matlab+Notes.&aqs=chrome..69i57.1064j0j9&sourceid=chrome&es\_sm=93&ie=UTF-8\#q=Excel+VS+Matlab}{Excel VS Matlab.} 

\label{ref-0329}\label{ref-0330}\label{ref-0331}\label{ref-0332}\label{ref-0333}\label{ref-0334}\label{ref-0335}

\chapter{Hyperlinear Quantitative Absorption Spectrophotometry\label{ref-0336}}

Specific knowledge of the instrumentation design is often essential in designing an effective signal-processing regimen. This example shows how a custom signal processing procedure for an optical measurement system is constructed by combining several of the methods and concepts introduced in this book, \textit{expanding the classical limits of measurement} in optical spectroscopy. This is a computational method for quantitative analysis by \href{http://oceanoptics.com/measurementtechnique/absorbance/}{multiwavelength absorption spectroscopy}, called the transmission-fitting or ``TFit'' method, which is based on \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{fitting a model} of the instrumentally-broadened transmission spectrum to the observed transmission data, as an alternative method of calculating the absorbance, rather than the simple "classical" definition of log10(Io/I). The method is described in T. C. O'Haver and J. Kindervater, \textit{J. of Analytical Atomic Spectroscopy} \textbf{1}, 89 (1986); T. C. O'Haver and Jing-Chyi Chang, \textit{Spectrochim. Acta} \textbf{44B}, 795-809 (1989); T. C. O'Haver, \textit{Anal. Chem.} \textbf{68}, 164-169 (1991). It is included as an example of combining techniques because it uses many of the important concepts that have been covered in this book: signal-to-noise ratio (page \pageref{ref-0020}), Fourier convolution (page \pageref{ref-0136}), multicomponent spectroscopy (page \pageref{ref-0241}), iterative least-squares fitting (page \pageref{ref-0258}), and calibration (page \pageref{ref-0401}). 

Advantages of the TFit method compared to conventional methods are: 

(a) much wider \textit{dynamic range} (i.e., the concentration range over which one calibration curve can be expected to give good results),

 (b) greatly improved \href{https://terpconnect.umd.edu/~toh/models/BeersLaw.html}{calibration linearity} ("hyperlinearily"), which reduces the labor and cost of preparing and running large numbers of standard solutions \textit{and safely disposing of them afterward}, and

 (c) the ability to operate under conditions that are optimized for \href{https://terpconnect.umd.edu/~toh/models/AbsSlitWidth.html}{signal-to-noise ratio} rather than for Beer's Law ideality, that is, using small spectrometers with shorter focal length, lower dispersion and larger slit widths to increase light throughput and reduce the effect of photon and detector noise (assuming, of course, that the detector is not saturated or overloaded).

Just like the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html}{multilinear regression (classical least-squares)} methods conventionally used in absorption spectroscopy, the Tfit method 

(a) requires an accurate reference spectrum of each analyte,

(b) utilizes multiwavelength data such as would be acquired on diode-array, Fourier transform, or automated scanning spectrometers, 

(c) applies both to single-component and \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html}{multi-component mixture} analysis, and

(d) requires that the absorbances of the components \textit{vary with wavelength}, and, for multi-component analysis, that the absorption spectra of the components be sufficiently different. Black or grey absorbers do not work with this method. 

The disadvantages of the Tfit method are: 

(a) It makes the \textit{computer} work harder than the multilinear regression methods (but, on a typical personal computer, calculations take only a fraction of a second, even for the analysis of a mixture of several components).

(b) It requires knowledge of the instrument function, i.e., the slit function or the resolution function of an optical spectrometer (however, this is a \textit{fixed characteristic} of the instrument and can be measured beforehand by scanning the spectrum of a narrow atomic line source such as a hollow cathode lamp). It changes only if you change the slit width of the spectrometer.

 (c) It is an iterative method that can under unfavorable circumstances converge on an incorrect local optimum, but this is handled by proper selection of the starting values, which in this case can be calculated by the traditional log (Io/I) method), and

 (d) It will not work for gray-colored substances whose absorption spectra do not vary over the spectral region measured.

You can perform the required calculations in a spreadsheet or in Matlab or Octave, using the software described below.

The following sections give the \href{https://terpconnect.umd.edu/~toh/spectrum/TFit.html\#backgrouond}{background of the method }and a description of the \href{https://terpconnect.umd.edu/~toh/spectrum/TFit.html\#TheFITMfunction}{main function} and \href{https://terpconnect.umd.edu/~toh/spectrum/TFit.html\#tfit.m}{demonstration programs }and \href{https://terpconnect.umd.edu/~toh/spectrum/TFit.html\#spreadsheet}{spreadsheet templates}:

\section{\textbf{Background}\label{ref-0337}}

\InsImageInline{0.5}{l}{724px-Spetrophotometer-en.svg.png}
In \href{http://en.wikipedia.org/wiki/Absorption\_spectroscopy}{optical absorption spectroscopy}, the intensity I of monochromatic light passing through an absorbing sample is given (in Matlab notation) by the \href{http://en.wikipedia.org/wiki/Beer\_lambert\_law}{Beer-Lambert Law}:

         I = Io.*10\textasciicircum{}-(alpha*L*c) 

where ``I\textsubscript{o}'' (pronounced "eye-zero") is the intensity of the light incident on the sample, ``alpha'' is the absorption coefficient of the absorber, ``L'' is the distance that the light travels through the material (the path length), and ``c'' is the concentration of absorber in the sample. The variables I, Io, and alpha are all functions of wavelength; L and c are scalar. 

Traditionally, measured values of I and Io are used to compute a quantity called \href{http://en.wikipedia.org/wiki/Absorbance}{"absorbance"}, defined as

 A = log10(Io/I)

Absorbance is defined in this way so that, when you combine this definition with the Beer-Lambert law, you get: 

 A = alpha*L*c  

In this way, absorbance is proportional to concentration, ideally, which simplifies analytical calibration. This works for monochromatic light beams. However, any real spectrometer has a \textit{finite spectral resolution}, meaning that the light beam passing through the sample is not truly monochromatic, with the result that an intensity reading at one wavelength setting is an average over a small spectral interval. More exactly, what is measured is a \href{https://terpconnect.umd.edu/~toh/spectrum/Convolution.html}{convolution} of the true spectrum of the absorber and the \textit{instrument function}, the instrument's response as a function of the wavelength of the light. Ideally, the instrument function is infinitely narrow (a "delta" function), but practical spectrometers have a \textit{non-zero} \href{https://terpconnect.umd.edu/~toh/models/BeersLaw.html}{\textit{slit width}}, the width of the adjustable aperture in the diagram above, which passes a small spectral interval of wavelengths of light from the dispersing element (prism) onto the sample and detector. If the absorption coefficient "alpha" varies over that spectral interval, then the traditionally calculated absorbance will no longer be linearly proportional to the concentration (this is called the \href{https://terpconnect.umd.edu/~toh/models/BeersLaw.html}{``polychromicity'' error}). The effect is most noticeable at high absorbances. In practice, many instruments will become non-linear starting at an absorbance of 2 (\textasciitilde{}1\% Transmission). As the absorbance increases, the effect of unabsorbed stray light and instrument noise becomes more significant.

The theoretical best signal-to-noise ratio and absorbance precision for a photon-noise limited optical absorption instrument can be shown to be close to an absorbance close to 1.0 (see ``\href{http://terpconnect.umd.edu/~toh/models/AbsSlitWidth.html\#BestAbsorbance}{\textit{Is there an optimum} \textbf{\textit{absorbance}} \textit{for best signal-to-noise ratio?}}\textit{''}). The range of absorbances \textit{below} 1.0 is easily accessible down to at least 0.001, but the range \textit{above} 1.0 is limited. Even an absorbance of 10 is unreachable on most instruments and the direct measurement of an absorbance of 100 is unthinkable, as it implies the measurement of light attenuation of 100 powers of ten - no real measurement system has a dynamic range even close to that. In practice, it is difficult to achieve a dynamic range even as high as 5 or 6 absorbance units, so that much of the theoretically optimum absorbance range is unusable. (c.f. \url{http://en.wikipedia.org/wiki/Absorbance}). So, in conventional practice, greater sample dilutions and shorter path lengths are required to force the absorbance range to lower values, even if this means poorer signal-to-noise ratio and measurement precision at the low end. 

It is true that the non-linearity caused by polychromicity can be reduced by operating the instrument at the highest resolution setting (reducing the instrumental slit width). However, this has a serious undesired side effect: in \href{http://en.wikipedia.org/wiki/Monochromator}{dispersive instruments}, reducing the slit width to increase the spectral resolution degrades the signal-to-noise substantially. It also reduces the number of atoms or molecules that are measured. Here is why: UV/visible absorption spectroscopy is based on the absorption of photons of light by molecules or atoms resulting from transitions between electronic energy states. It is well known that the absorption peaks of molecules are more-or-less wide bands, not monochromatic lines, because the molecules are undergoing vibrational and rotational transitions as well as electronic ones and are also under the perturbing influence of their environment. This is the case also in \href{https://terpconnect.umd.edu/~toh/models/AAMeasurement.html}{\textit{atomic} absorption spectroscopy}: the absorption "lines" of gas-phase free atoms, although much narrower that molecular bands, have a finite non-zero width, mainly due to their velocity (temperature or Doppler broadening) and collisions with the matrix gas (pressure broadening). A macroscopic collection of molecules or atoms, therefore, presents to the incident light beam a \textit{distribution} of energy states and absorption wavelengths. Absorption results from the collective interaction of many \textit{individual} atoms or molecules with \textit{individual} photons. A purely \textit{monochromatic} incident light beam would have photons all the \textit{same} energy, ideally corresponding to the average energy distribution of the collection of atoms or molecules being measured. But most of the atoms or molecules in the light path would have an energy \textit{greater or less than} the average and \textit{would thus not be measured}. If the bandwidth of the incident beam is increased, more of those non-average atoms or molecules would be available to be measured, but then the simple calculation of absorbance as log10(Io/I) no longer results in a linear response to concentration. 

\href{https://terpconnect.umd.edu/~toh/models/AbsSlitWidth.html}{Numerical simulations} show that the optimum signal-to-noise ratio is typically achieved when \textit{the spectral resolution of the instrument matches the width of the analyte absorption}, but under those conditions using the conventional log10(Io/I) absorbance would result in very substantial non-linearity over the higher range of absorbance values because of the \href{https://terpconnect.umd.edu/~toh/models/BeersLaw.html}{``polychromicity'' error}. This non-linearity has its origin in the \textit{spectral domain} (intensity vs wavelength), not in the \textit{calibration domain} (absorbance vs concentration). Therefore, it should be no surprise that curve fitting in the calibration domain, for example fitting the calibration data with a quadratic or cubic fit, would not be the best solution, because there is no theory that says that the deviations from linearity would be expected to be exactly quadratic or cubic. A \InsImageInline{0.5}{l}{10-Diode-array-spectrophotometer.jpg}more rigorous theory-based approach would be to perform the curve fitting in the \textit{spectral domain,} where the source of the non-linearity arises. \href{https://terpconnect.umd.edu/~toh/spectrum/10-Diode-array-spectrophotometer.jpg}{}This is possible with \textit{modern} absorption spectrophotometers that use \textit{array detectors,} which have many tiny detector elements that slice up the spectrum of the transmitted beam into many small wavelength segments, rather than detecting the sum of all those segments with one big photo-tube detector, as older instruments do. An instrument with an array detector typically uses a slightly different optical arrangement, as shown by the simplified diagram above. The spectral resolution is determined by both the entrance slit width and by the range of optical detector elements that are summed to determine the transmitted intensity.

The TFit method sidesteps the above problems by calculating the absorbance in a completely different way. It starts with the reference spectra (an accurate absorption spectrum\index{absorption spectrum} for each analyte, also required by the multilinear regression methods). It normalizes the reference spectra to unit height, multiplies each by an adjustable coefficient \textendash{} usually starting with the conventional log10(Io/I) absorbance as a first guess - adds them up, computes the antilog, and convolutes it with the previously-measured slit function. The result, representing the instrumentally broadened transmission spectrum, is compared to the observed transmission spectrum. The program adjusts the coefficients (one for each unknown component in the mixture) until the computed transmission model is a least-squares best fit to the observed transmission spectrum. The best-fit coefficients are then equal to the absorbances determined under ideal optical conditions. The program also compensates for unabsorbed stray light and changes in background intensity (background absorption). These calculations are performed by the function \href{https://terpconnect.umd.edu/~toh/spectrum/fitM.m}{fitM}, which is used as a fitting function for Matlab's iterative non-linear fitting function \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{fminsearch}. It sounds complicated but, in fact, takes only a fraction of a second to compute. The TFit method gives measurements of absorbance that are much closer to the "true" peak absorbance that would have been measured in the absence of stray light and polychromatic light errors. More important, it allows linear and wide dynamic range measurements to be made even if the slit width of the instrument is increased to optimize the signal-to-noise ratio. 

\textit{\InsImageInline{0.5}{l}{TFitCalCurveAbs.png}}From a historical perspective, by the time Pierre Bouguer formulated what became to be known as the \textit{Beer-Lambert} \textit{law} in 1729, the \textit{logarithm} was already well known, having been introduced by \href{https://en.wikipedia.org/wiki/Logarithm\#History}{John Napier in 1614}. So the additional mathematical work needed to compute the \textit{absorbance}, log(Io/I), rather than the simpler relative \textit{absorption}, (Io-I)/Io, was justified because of the better linearity of absorbance with respect to concentration and path length, and the log calculation could easily be performed simply by a \href{https://terpconnect.umd.edu/~toh/spectrum/Spectronic20Meter.jpeg}{slide-rule type graduated scale}. Certainly, by today's standards, the calculation of a logarithm is considered completely routine. In contrast, the TFit method presented here is more mathematically complex than a logarithm and cannot be done without the aid of software (at least a \href{https://terpconnect.umd.edu/~toh/spectrum/TFit.html\#spreadsheet}{spreadsheet}) and \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#RaspberryPi}{some sort of computational hardware}, but it offers a further improvement in linearity beyond that achieved by the logarithmic calculation of absorbance, and it additionally allows the small slit width limitation to be loosened. The figure on the right above compares the analytical curve linearity of simple relative absorption (blue x), single-wavelength logarithmic absorbance (red dots), multilinear regression or CLS method (cyan +) based on absorbance, and the TFit method (green o). This plot was created by the Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/TFitCalCurveAbs.m}{TFitCalCurveAbs.m}.

\textbf{Bottom line:} \textit{The TFit method is based on the Beer-Lambert Law}; it simply \textit{calculates the absorbance in a different way} that does not require the assumption that stray light and polychromatic radiation effects are zero. Because it allows larger slit widths and shorter focal lengths to be used, it yields greater signal-to-noise ratios while still achieving a much wider linear dynamic range than previous methods, thus requiring fewer standards to properly define the calibration curve and avoiding the need for non-linear calibration models. Keep in mind that the log(Io/I) absorbance calculation is a \href{https://www.aps.org/publications/apsnews/201108/physicshistory.cfm}{\textit{165-year-old simplification} }that was driven by the need for \textit{mathematical convenience}, and limited also by the mathematical skills of the college students to whom this subject is typically first presented, \textit{not} by the desire to optimize linearity and signal-to-noise ratio. It dates from the time before electronics and computers, when the only computational tools were pen and paper and slide rules, and when a method such as described here would have been unthinkable. That was then; this is now. \textit{Tfit} \textit{is the 21}\textsuperscript{\textit{st}} \textit{century way to do quantitative absorption spectrophotometry.}

\textbf{Note 1}: The TFit method compensates for the non-linearity caused by unabsorbed stray light and the polychromatic light effect, but other potential sources of non-linearity remain, specifically, \textit{chemical effects}, such as photolysis, equilibrium shifts, temperature and pH effects, binding, dimerization, polymerization, molecular phototropism, fluorescence, etc. A well-designed quantitative analytical method is designed to minimize those effects.

\textbf{Note 2}: In practical applications, the information required by the Tfit method may not be known exactly, but a reasonable estimate is better than nothing. For example, even an imperfect estimate of the instrumental bandwidth and/or stray light is better that assuming that they are zero. 

\section{Spreadsheet implementation\label{ref-0338}\label{ref-0339}}

 The Tfit method can also be implemented in an \textit{Excel} or \textit{Calc} spreadsheet; it is a bit more cumbersome that the \href{https://terpconnect.umd.edu/~toh/spectrum/TFit.html\#TheFITMfunction}{Matlab/Octave implementation}, but it works. The shift-and-multiply method is used for the convolution of the reference spectrum with the slit function, and the "Solver" add-in for Excel and Calc is used for the iterative fitting of the model to the observed transmission spectrum. It is very handy, but not essential, to have a "macro" capability to automate the process (See \url{http://peltiertech.com/Excel/SolverVBA.html\#Solver2} for more info about setting up macros and solver on your version of Excel). 

 \href{https://terpconnect.umd.edu/~toh/spectrum/TransmissionFittingTemplate.xls}{TransmissionFittingTemplate.xls} (\href{https://terpconnect.umd.edu/~toh/spectrum/TFitSpreadsheetTemplate.png}{screen image}) is an empty template; all you have to do is to enter the data in the cells marked by a gray background: wavelength (Column \textbf{A}), observed absorbance of the sample (Column \textbf{B}), the high-resolution reference absorbance spectrum (Column \textbf{D}), the stray light (\textbf{A6}) and the slit function used for the observed absorbance of the sample (\textbf{M6-AC6}). (\href{https://terpconnect.umd.edu/~toh/spectrum/TransmissionFittingTemplateExample.xls}{TransmissionFittingTemplateExample.xls} (\href{https://terpconnect.umd.edu/~toh/spectrum/TFitSpreadsheetTemplateExample.png}{screen image}) is the same template with example data entered. 

 \href{https://terpconnect.umd.edu/~toh/spectrum/TransmissionFittingDemoGaussian.xls}{TransmissionFittingDemoGaussian.xls} (\href{https://terpconnect.umd.edu/~toh/spectrum/TFitSpreadsheetDemoGaussian.png}{screen image}) is a demonstration with a simulated Gaussian absorption peak with a variable peak position, width, and height, plus added stray light, photon noise, and detector noise, as viewed by a spectrometer with a triangular slit function. You can vary all the parameters and compare the best-fit absorbance to the true peak height and to the conventional log(1/T) absorbance. 

All of these spreadsheets include a \href{https://terpconnect.umd.edu/~toh/spectrum/macro1.txt}{macro}, activated by pressing \textbf{Ctrl-f,} that uses the Solver function to perform the iterative least-squares calculation (see page \pageref{ref-0375}). However, if you prefer not to use macros, you can do it manually by clicking the \textbf{Data} tab, \textbf{Solver}, \textbf{Solve}, and then \textbf{OK}.

\href{https://terpconnect.umd.edu/~toh/spectrum/TransmissionFittingCalibrationCurve.xls}{TransmissionFittingCalibrationCurve.xls} (\href{https://terpconnect.umd.edu/~toh/spectrum/TransmissionFittingCalibrationCurve.png}{screen image}) is a demonstration spreadsheet that includes \InsImageInline{0.5}{l}{TFitCalibrationCurve.png}another Excel \href{https://terpconnect.umd.edu/~toh/spectrum/macro2.txt}{macro} that constructs calibration curves comparing the TFit and conventional log(1/T) methods for a series of 9 standard concentrations that you can specify. To create a calibration curve, enter the standard concentrations in AF10 - AF18 (or just use the ones already there, which cover a 10,000-fold concentration range from 0.01 to 100), then press \textbf{Ctrl-f} to run the macro. In this spreadsheet the macro does a lot more than in the previous example: it automatically goes through the first row of the little table in AF10 - AH18, extracts each concentration value in turn, places it in the concentration cell A6, recalculates the spreadsheet, takes the resulting conventional absorbance (cell J6) and places it as the first guess in cell I6, brings up the Solver to compute the best-fit absorbance for that peak height, places both the conventional absorbance and the best-fit absorbance in the table in AF10 - AH18, then goes to the next concentration and repeats for each concentration value. Then it constructs and plots the log-log calibration curve (shown on the right) for both the TFit method (blue dots) and the conventional (red dots) and computes the trend-line equation and the R\textsuperscript{2 }value for the TFit method, in the upper right corner of the graph. Each time you press \textbf{Ctrl-f} it repeats the whole calibration curve with another set of random noise samples. (Note: you can also use this spreadsheet to compare the precision and reproducibility of the two methods by entering the \textit{same} concentration 9 times in AF10 - AF18. The result should ideally be a straight flat line with zero slope).

\section{Matlab/Octave implementation: The \href{https://terpconnect.umd.edu/~toh/spectrum/fitM.m}{fitM.m} function\label{ref-0340}}

\texttt{\textbf{function err = fitM(lam, yobsd, Spectra, InstFun, StrayLight}}\texttt{\textbf{)}}

\href{https://terpconnect.umd.edu/~toh/spectrum/fitM.m}{fitM} is a fitting function for the Tfit method, for use with Matlab's or \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{Octave}'s \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{fminsearch} function. The input arguments of fitM are:

\textbf{absorbance}= vector of absorbances that are calculated to give the best fit to the transmission spectrum.

\textbf{yobsd} = observed transmission spectrum of the mixture sample over the spectral range (column vector)

\textbf{Spectra} = reference spectra for each component, over the same spectral range, one column/component, normalized to 1.00. 

\textbf{InstFun} = Zero-centered instrument function or slit function (column vector)

\textbf{StrayLigh}t = fractional stray light (scalar or column vector, if it varies with wavelength)

Note: \textbf{yobsd}, \textbf{Spectra}, and \textbf{InstFun} must have the same number of rows (wavelengths). \textbf{Spectra} must have one column for each absorbing component.

Typical use:

\texttt{\textbf{absorbance = fminsearch(@(lambda)(fitM(lambda, yobsd, TrueSpectrum, InstFun, StrayLight)), start);}} 

where ``\texttt{start}'' is the first guess (or guesses) of the absorbance(s) of the analyte(s); it is convenient to use the conventional log10(Io/I) estimate of absorbance for \texttt{start}. The other arguments (described above) are passed on to FitM. In this example, the fminsearch function returns the value of absorbance that would have been measured in the absence of stray light and polychromatic light errors (which is either a single value or a vector of absorbances, if it is a multi-component analysis). The absorbance can then be converted into concentration by any of the \href{https://terpconnect.umd.edu/~toh/models/Bracket.html}{usual calibration procedures} (Beer's Law, \href{https://terpconnect.umd.edu/~toh/models/CalibrationCurve.html}{external standards}, \href{https://terpconnect.umd.edu/~toh/models/Bracket.html\#Multiple\_Addition}{standard addition}, etc.). 

Here is a very simple numerical example, for a \textbf{single-component measurement} where the true absorbance is 1.00, using only \textit{4-point spectra} for illustrative simplicity (Naturally, array-detector systems would acquire \textit{many} more wavelengths than that, but the principle is the same). In this case the instrument width (``InstFun'') is \textit{twice} the absorption width, the stray light is constant at 0.01 (1\% relative). The conventional single-wavelength estimate of absorbance is far too low: 

\texttt{log10(1/.38696)=0.4123}. In contrast, the TFit method using fitM is set up like this:

\texttt{\textbf{yobsd=[0.56529 0.38696 0.56529 0.73496]';}}

\texttt{\textbf{TrueSpectrum=[0.2 1 0.2 0.058824]';}}

\texttt{\textbf{InstFun=[1 0.5 0.0625 0.5]';}}

\texttt{\textbf{straylight=.01;}}

\texttt{\textbf{start=.4;}}

\texttt{\textbf{absorbance=fminsearch(@(lambda)(fitM(lambda,yobsd,TrueSpectrum,InstFun,straylight)),start)}}

This returns the correct absorbance value of 1.000. The "start" value is not critical in this case and can be just about any value you like, but I like to use the conventional log10(Io/I) absorbance, which is easily calculated, and which is a \textit{rough but reasonable estimate} of the correct value. 

For a \textbf{\textit{multiple-component}} \textbf{measurement}, the only difference is that the variable "\texttt{TrueSpectrum}" would be a \textit{matrix} rather than a \textit{vector}, with one column for each absorbing component. The resulting "\texttt{absorbance}" would then be a \textit{vector} rather than a \textit{single number}, with one absorbance value for each component. (See \href{https://terpconnect.umd.edu/~toh/spectrum/TFit3.m}{TFit3.m} below for an example of a 3-component mixture).

\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{Iterative least-squares methods} are ordinarily considered to be more difficult and less reliable than \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html}{multilinear regression methods}. This can be true if there is more than one nonlinear variable that must be iterated, especially if those variables are correlated. However, in the TFit method, there is only \textit{one} iterated variable (absorbance) per measured component, and reasonable first guesses are readily available from the conventional single-wavelength absorbance calculation or standard multiwavelength regression methods. As a result, the iterative least-squares method works very well in this case. The expression for absorbance given above for the TFit method can be compared to that for the \textit{weighted regression method}:

\texttt{\textbf{absorbance}}\textbf{=([weight weight].*[Background} \texttt{\textbf{RefSpec}}\textbf{])\textbackslash (-log10(yobsd).*weight)}

where RefSpec is the matrix of reference spectra of all the pure components. You can see that, in addition to the RefSpec and observed transmission spectrum (yobsd), the TFit method also requires a measurement of the Instrument function (spectral bandpass) and the stray light (which the linear regression methods assume to be zero), but these are characteristics of the \textit{spectrometer} and need be done only once for a given spectrometer. Finally, although the TFit method does make the \textit{computer} work harder, the computation time on a typical laboratory personal computer is only a fraction of a second (roughly 25 ${\mathrm{\mu}}$sec per spectral data point per component analyzed), using Matlab as the computational environment. The cost of the computational hardware need not be burdensome; the method can even be performed, with some loss in speed, in \textit{Octave}, a freely downloadable Matlab clone, or on a \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#RaspberryPi}{\$35 single-board computer} (see page \pageref{ref-0408}).

\subsection{Demo function for \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html}{Octave} or Matlab\label{ref-0341}}

The \href{https://terpconnect.umd.edu/~toh/spectrum/tfit.m}{tfit.m} function is a self-contained command-line demonstration function that compares the TFit method to the single-wavelength (SingleW), simple regression (SimpleR), and weighted regression (WeightR) methods. The syntax is \textbf{tfit(absorbance)}, where 'absorbance' is the \textit{true underlying peak absorbance} (True A) of an absorber with a Lorentzian spectral profile of width 'width' (line 29), measured with a spectrometer with a Gaussian spectral bandpass of width 'InstWidth' (line 30), fractional unabsorbed stray light level of 'straylight' (line 32), photon noise level of 'noise' (line 31) and a random Io shift of 'IzeroShift' (line 33). Plots the spectral profiles and prints the measured absorbances of each method in the command window. Examples:

\texttt{\textbf{{\textgreater}{\textgreater} tfit(1)}}

\texttt{\textbf{width = 10}}

\texttt{\textbf{InstWidth = 20}}

\texttt{\textbf{noise = 0.01}}

\texttt{\textbf{straylight = 0.01}}

\texttt{\textbf{IzeroShift = 0.01}}

  \texttt{\textbf{True A SingleW SimpleR WeightR TFit}}

  \texttt{\textbf{1 0.38292 0.54536 0.86839 1.0002}}

\texttt{\textbf{{\textgreater}{\textgreater} tfit(10)}}

            \texttt{\textbf{10 1.4858 2.2244 9.5123 9.9979}}

\texttt{\textbf{{\textgreater}{\textgreater} tfit(100)}}

    \texttt{\textbf{100 2.0011 3.6962 57.123 99.951}}

\texttt{\textbf{{\textgreater}{\textgreater} tfit(200)}}

    \texttt{\textbf{200 2.0049 3.7836 56.137 200.01}}

\texttt{\textbf{{\textgreater}{\textgreater} tfit(.001)}}

    \texttt{\textbf{0.001 0.00327 0.00633 0.000520 0.000976}}

\subsection{TFitDemo.m: Interactive demo for the Tfit method for Matlab, version 2.1\label{ref-0342}}


\begin{center}
\InsImageInline{0.5}{l}{TFitAnimated.gif.png}
\end{center}


\href{https://terpconnect.umd.edu/~toh/spectrum/TFitDemo.m}{TFitDemo.m} is a keypress-operated interactive explorer for the Tfit method (for Matlab only), applied to the measurement of a single component with a Lorentzian (or Gaussian) absorption peak, with controls that allow you to adjust the true absorbance (Peak A), spectral width of the absorption peak (AbsWidth), spectral width of the instrument function (InstWidth), stray light, and the photon noise level (Noise) continuously while observing the effects graphically and numerically. (If the animation is not visible, click \href{https://terpconnect.umd.edu/~toh/spectrum/TFitAnimated.gif}{this} link). The demo simulates the effect of photon noise, unabsorbed stray light, and random background intensity shifts (light source flicker). Compares observed absorbances by the single-wavelength, weighted multilinear regression (sometimes called Classical Least-squares in the chemometrics literature), and the TFit methods. To run this file, right-click \href{https://terpconnect.umd.edu/~toh/spectrum/TFitDemo.m}{TFitDemo.m} click "Save link as...", save it in a folder in the Matlab path, then type "TFitDemo" at the Matlab command prompt. With the figure window topmost on the screen, press \textbf{K} to get a list of the keypress functions. Version 2.1, November 2011, adds signal-to-noise ratio calculation and uses the \textbf{W} key to switch between Transmission and Absorbance display. 

In the example shown above, the true peak absorbance is shown varying from 0.0027 to 57 absorbance units, the absorption widths and instrument function widths are equal (\href{https://terpconnect.umd.edu/~toh/models/AbsSlitWidth.html}{which results in the optimum signal-to-noise ratio}), the unabsorbed stray light is 0.5\%, and the photon noise is 5\%. (For demon-stration purposes, the lowest 6 absorption peak shapes are \textit{Gaussian} and the highest 3 are \textit{Lorentzian}). The results below the graphs show that, at every absorbance and for either a Gaussian or a Lorentzian peak shape, the TFit method gives much more accurate measurements than either the single-wavelength method or weighted multilinear regression method.

\texttt{\textbf{KEYBOARD COMMANDS}}

\texttt{Peak shape....}\texttt{\textbf{Q}}  \texttt{Toggles between Gaussian and Lorentzian} 

  \texttt{absorption peak shape}

\texttt{True peak A...}\texttt{\textbf{A}}\texttt{/}\texttt{\textbf{Z}}  \texttt{True absorbance of analyte at peak center, without}

  \texttt{instrumental broadening, stray light, or noise.}

\texttt{AbsWidth......}\texttt{\textbf{S}}\texttt{/}\texttt{\textbf{X}}  \texttt{Width of the absorption peak}

\texttt{SlitWidth.....}\texttt{\textbf{D}}\texttt{/C Width of instrument function (spectral bandpass)}

\texttt{Straylight....}\texttt{\textbf{F}}\texttt{/}\texttt{\textbf{V}}  \texttt{Fractional unabsorbed stray light.}

\texttt{Noise.........}\texttt{\textbf{G}}\texttt{/}\texttt{\textbf{B}}  \texttt{Random noise level}

\texttt{Re-measure....}\texttt{\textbf{Spacebar}}  \texttt{Re-measure signal with independent random noise}

\texttt{Switch mode...}\texttt{\textbf{W}}  \texttt{Switch between Transmission and Absorbance display}

\texttt{Statistics....}\texttt{\textbf{Tab}}  \texttt{Prints table of statistics of 50 repeats}

\texttt{Cal. Curve....}\texttt{\textbf{M}}   \texttt{Displays analytical calibration curve in Fig. window 2}

\texttt{Keys..........}\texttt{\textbf{K}}  \texttt{Print this list of keyboard commands}

\textbf{Why does the noise on the graph change if I change the instrument function (slit width or} \textbf{InstWidth)?} In the most common type of absorption spectrometer, using a continuum light source and a dispersive spectrometer, there are two adjustable apertures or slits, one before the dispersing element, which controls the physical width of the light beam, and one after, which controls the wavelength range of the light measured (and which, in an array detector, is controlled by the software reading the array elements). A spectrometer's spectral bandwidth ("\textit{InstWidth}") is changed by changing both of those, which also affects the light intensity measured by the detector and thus the \href{https://terpconnect.umd.edu/~toh/models/AbsSlitWidth.html}{signal-to-noise ratio}. Therefore, in all these programs, when you change \textit{InstWidth}, the photon noise is automatically changed accordingly just as it would in a real spectrophotometer. The detector noise, in contrast, remains the same. I am also assuming that the detector does not become saturated or overloaded if the slit width is increased. 

\subsection{Statistics of methods compared (\href{https://terpconnect.umd.edu/~toh/spectrum/TFitStats.m}{TFitStats.m}, for Matlab or \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html}{Octave})\label{ref-0343}}

This is a simple script that computes the statistics of the TFit method compared to single- wavelength (SingleW), simple regression (SimpleR), and weighted regression (WeightR) methods. Simulates photon noise, unabsorbed stray light and random background intensity shifts. Estimates the precision and accuracy of the four methods by repeating the calculations 50 times with different random noise samples. Computes the mean, relative percent standard deviation, and relative percent deviation from true absorbance. You can easily change the parameters in lines 19 - 26. The program displays its results in the MATLAB command window. 

In the sample output shown below, the program has computed results for true absorbances of 0.001 and 100, demonstrating that the accuracy and the precision of the TFit method are superior to the other methods over a 10,000-fold range. 

This statistics function is included as a keypress command (\textbf{Tab} key) in \href{https://terpconnect.umd.edu/~toh/spectrum/TFitDemo.m}{TFitDemo.m}. 

 Results for true absorbances of 0.001

 \textbf{True A SingleW SimpleR WeightR TFit} 

\textbf{MeanResult =}

    \textbf{0.0010 0.0004 0.0005 0.0006 0.0010} 

\textbf{PercentRelativeStandardDeviation =}

  \textbf{1.0e+003 *}

    \textbf{0.0000 1.0318 1.4230 0.0152 0.0140} 

\textbf{PercentAccuracy =}

    \textbf{0.0000 -60.1090 -45.1035 -38.6300 0.4898}

Results for true absorbances of 100

\textbf{MeanResult =}

  \textbf{100.0000 2.0038 3.7013 57.1530 99.9967}

\textbf{PercentRelativeStandardDeviation =}

         \textbf{0 0.2252 0.2318 0.0784 0.0682}

\textbf{PercentAccuracy =}

         \textbf{0 -97.9962 -96.2987 -42.8470 -0.0033}



As you can see, the Tfit method offers improved accuracy \textit{and} precision.

\subsection{Comparison of analytical curves (\href{https://terpconnect.umd.edu/~toh/spectrum/TFitCalDemo.m}{TFitCalDemo.m}, for Matlab or \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html}{Octave}) \label{ref-0344}}

TFitDemo.m is a demonstration function that compares the analytical curves for single-wavelength, simple regression, weighted regression, and the TFit method over any specified absorbance range (specified by the vector ``absorbancelist'' in line 20). Simulates photon noise, unabsorbed stray light and random background intensity shifts. Plots a log-log scatter plot with each repeat measurement plotted as a separate point (so you can see the scatter of points at low absorbances). The parameters can be changed in lines 20 - 27. 

In the sample result shown below, analytical curves for the four methods are computed over a 10,000-fold range, up to a peak absorbance of 100, demonstrating that the TFit method (shown by the green circles) is much more nearly linear over the whole range than the single-wavelength, simple regression, or weighted regression methods. The linearity of Tfit is especially important in a regulated lab where \href{https://terpconnect.umd.edu/~toh/spectrum/RegulatedLabRules.txt}{quadratic least-squares fits are discouraged}.

This calibration curve function is included as a keypress command (\textbf{M} key) in \href{https://terpconnect.umd.edu/~toh/spectrum/TFitDemo.m}{TFitDemo.m}.

\InsImageInline{0.5}{l}{TFitCalCurve.png}
\begin{center}
\textit{Comparison of the simulated analytical curves for single-wavelength, simple regression, weighted regression, and TFit methods over a 10,000-fold absorbance range, created by} \href{https://terpconnect.umd.edu/~toh/spectrum/TFitCalDemo.m}{\textit{TFitCalDemo.m}}.
\end{center}




\subsection{Application to a three-component mixture \label{ref-0345}}

for Matlab/\href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{Octave} (and \href{https://terpconnect.umd.edu/~toh/spectrum/TFit3Demo.m}{TFit3Demo.m}, for Matlab only)


\begin{table}
\begin{tabularx}{\textwidth}{|
p{\dimexpr 0.474\linewidth-2\tabcolsep-2\arrayrulewidth}|
p{\dimexpr 0.526\linewidth-2\tabcolsep-\arrayrulewidth}|} \hline 
\href{https://terpconnect.umd.edu/~toh/spectrum/TFit3a.GIF}{\InsImageInline{0.5}{l}{TFit3a.GIF.png}} \par \href{https://terpconnect.umd.edu/~toh/spectrum/TFit3a.GIF}{} \par \href{https://terpconnect.umd.edu/~toh/spectrum/TFit3a.GIF}{Figure No. 1 window: Click to enlarge} \par \href{https://terpconnect.umd.edu/~toh/spectrum/TFit3a.GIF}{} & \href{https://terpconnect.umd.edu/~toh/spectrum/TFit3b.GIF}{\InsImageInline{0.5}{l}{TFit3bSmall.GIF.png}} \par \href{https://terpconnect.umd.edu/~toh/spectrum/TFit3b.GIF}{} \par \href{https://terpconnect.umd.edu/~toh/spectrum/TFit3b.GIF}{Figure No. 2 window: Click to enlarge} \par \href{https://terpconnect.umd.edu/~toh/spectrum/TFit3b.GIF}{} \\\hline 
\end{tabularx}
\end{table}
The application of absorption spectroscopy to \textit{mixtures} of absorbing components requires the adoption of one additional assumption: that of additivity of absorbances, meaning that the measured absorbance of a mixture is equal to the sum of the absorbances of the separate components. In practice, this requires that the absorbers do not interact chemically; that is, that they do not react with themselves or with the other components or modify any property of the solution (e.g., pH, ionic strength, density, etc.) that might affect the spectra of the other components. These requirements apply equally to the conventional multi-component methods as well as to the T-Fit method. 

\href{https://terpconnect.umd.edu/~toh/spectrum/TFit3.m}{TFit3.m} is a demonstration of the T-Fit method applied to the \href{https://terpconnect.umd.edu/~toh/models/CLS.html}{multi-component absorption spectroscopy} of a mixture of three absorbers. The adjustable parameters are the absorbances of the three components (A1, A2, and A3), the spectral overlap between the component spectra ("Sepn"), the width of the instrument function ("InstWidth"), and the noise level ("Noise"). Compares quantitative measurement by weighted regression and TFit methods. Simulates photon noise, unabsorbed stray light, and random background intensity shifts. Note: After executing this m-file, slide the "Figure No. 1" and "Figure No.2" windows side-by-side so that they do not overlap. Figure window 1 shows a log-log scatter plot of the true vs. measured absorbances, with the three absorbers plotted in different colors and symbols. Figure window 2 shows the transmission spectra of the three absorbers plotted in the corresponding colors. As you use the keyboard commands (below) in Figure No. 1, both graphs change accordingly. 

In the sample calculation shown above, component 2 (shown in blue) is almost completely buried by the stronger absorption bands of components 1 and 3 on either side, giving a much weaker absorbance (0.1) than the other two components (3 and 5, respectively). Even in this case, the TFit method gives a result (T2=0.101) within 1\% of the correct value (A2=0.1). In fact, over \textit{most} combinations of the three concentrations, the TFit method works better (although of course, \textit{nothing} works if the spectral difference between the components is too small). Note: in this program, as in all the above, when you change InstWidth, the photon noise automatically changes accordingly just as it would in a real variable-slit dispersive spectrophotometer.

You can also download the \href{https://terpconnect.umd.edu/~toh/spectrum/TFit3Demo.m}{newer self-contained keyboard-operated version} that works in recent versions of Matlab:

\texttt{KEYBOARD COMMANDS}

\texttt{A1} \texttt{\textbf{A}}\texttt{/}\texttt{\textbf{Z}}  \texttt{Increase/decrease true absorbance of component 1}

\texttt{A2} \texttt{\textbf{S}}\texttt{/}\texttt{\textbf{X}} \texttt{Increase/decrease true absorbance of component 2}

\texttt{A3} \texttt{\textbf{D}}\texttt{/}\texttt{\textbf{C}} \texttt{Increase/decrease true absorbance of component 3}

\texttt{Sepn} \texttt{\textbf{F}}\texttt{/}\texttt{\textbf{V}} \texttt{Increase/decrease spectral separation of the} 

  \texttt{components}

\texttt{InstWidth} \texttt{\textbf{G}}\texttt{/}\texttt{\textbf{B}} \texttt{Increase/decrease width of instrument function} 

  \texttt{(spectral bandpass)}

\texttt{Noise} \texttt{\textbf{H}}\texttt{/}\texttt{\textbf{N}} \texttt{Increase/decrease random noise level when} 

  \texttt{InstWidth = 1}

\texttt{Peak shape} \texttt{\textbf{Q}}  \texttt{Toggles between Gaussian and Lorentzian} 

  \texttt{absorption peak shape}

\texttt{Table} \texttt{\textbf{Tab}}  \texttt{Print table of results}

 \texttt{\textbf{K}}  \texttt{Print this list of keyboard commands}

Sample table of results (by pressing the Tab key):

\texttt{---------------------------------------------------------------}

  \texttt{True Weighted TFit}

  \texttt{Absorbance Regression method}

\texttt{Component 1 3 2.06 3.001}

\texttt{Component 2 0.1 0.4316 0.09829}

\texttt{Component 3 5 2.464 4.998}

Note for \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{Octave} users: the current versions (October 2012 or later) of \href{https://terpconnect.umd.edu/~toh/spectrum/fitM.m}{fitM.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/TFit3.m}{TFit3.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/TFitStats.m}{TFitStats.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/TFitCalDemo.m}{TFitCalDemo.m} work in \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{Octave} as well as in Matlab. However, the interactive features of TfitDemo.m and Tfit3Demo.m work only in Matlab; Octave users can use the command-line function \href{https://terpconnect.umd.edu/~toh/spectrum/tfit.m}{tfit.m} (single absorbing component) or \href{https://terpconnect.umd.edu/~toh/spectrum/TFit3.m}{TFit3.m} (a mixture of 3 absorbing components) instead. See page \pageref{ref-0498} or \href{http://tinyurl.com/cey8rwh\%20}{http://tinyurl.com/cey8rwh} for a list of and links to these and other Matlab and Octave functions.

\chapter{Tutorials, Case Studies and Simulations.\label{ref-0346}}

\section{Can smoothed noise may be mistaken for an actual signal?\label{ref-0347}\label{ref-0348}}

\InsImageInline{0.5}{l}{SmoothedNoise.png} Here are two examples that show that the answer to this question is \textit{yes}. The first example is shown on the left. This shows iSignal (page \pageref{ref-0434}) displaying a computer-generated 4000-point signal consisting of pure random noise that has been smoothed with a 19-point P-spline smooth. The upper window shows a tiny slice of this signal that looks like a Gaussian peak with a calculated SNR over 1000. Only by looking at the entire signal (bottom window) do you see the true picture; that ``peak'' is just part of the noise, smoothed to look nice. Do not fool yourself.

The second example is a simple series of three Matlab commands that use the 'randn' function to generate a 10000-point data set containing only normally distributed white noise. Then it uses 'fastsmooth.m' to smooth that noise, resulting in a 'signal' with a standard deviation of about 0.3 and a maximum value around 1.0. That signal is then submitted to \href{https://terpconnect.umd.edu/~toh/spectrum/ipeak.html}{\textit{iPeak}} (page \pageref{ref-0319}). If the peak detection criteria (e.g., AmpThreshold and SmoothWidth) are set too low, many peaks will be found. But setting the AmpThreshold to 3 times the standard deviation (3 x 0.3 = 0.9) will greatly reduce the incidence of these false peaks.

\texttt{{\textgreater}{\textgreater} noise=randn(1,10000);}

\texttt{{\textgreater}{\textgreater} signal=fastsmooth(noise,13);}

\texttt{{\textgreater}{\textgreater} ipeak([1:10000;signal],0,0.6,1e-006,17,17)}

 

The \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#idpeaks}{peak }\href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#idpeaks}{\textit{identification} function}, which identifies peaks based on their exact x-axis peak position and a stored table of previously identified peak positions, is even \textit{less} likely to be fooled by random noise, because in addition to the peak detection criteria of the findpeaks algorithm, any detected peak must also match closely to a peak position in the table of known peaks.

\section{Signal or Noise?\label{ref-0349}}

The client’s experimental signal in this case study was unusual because it did not look like a typical signal when plotted; in fact, it looked a lot like noise at first glance. The figure below compares the raw signal (bottom) with the same number of points of normally-distributed white noise (top) with a mean of zero and a standard deviation of 1.0 (obtained from the Matlab/Octave 'randn' function). 


\begin{center}
\InsImageInline{0.5}{l}{JComparisonSmall.png}
\end{center}


As you can see, the main difference is that the signal has more large 'spikes', especially in a positive direction. This difference is evident when you look at the \href{https://www.socialresearchmethods.net/kb/statdesc.php}{descriptive statistics} of the signal and the randn function:


\begin{table}
\begin{tabularx}{\textwidth}{|
p{\dimexpr 0.419\linewidth-2\tabcolsep-2\arrayrulewidth}|
p{\dimexpr 0.163\linewidth-2\tabcolsep-\arrayrulewidth}|
p{\dimexpr 0.418\linewidth-2\tabcolsep-\arrayrulewidth}|} \hline 
DESCRIPTIVE STATISTICS & Raw signal & random noise (randn function) \\\hline 
Mean & 0.4 & 0 \\\hline 
Maximum & 38 & about 5 - 6 \\\hline 
Standard Deviation (STD) & 1.05 & 1.0 \\\hline 
Inter-Quartile Range (IQR) & 1.04 & 1.3489 \\\hline 
Kurtosis & 38 & 3 \\\hline 
Skewness & 1.64 & 0 \\\hline 
\end{tabularx}
\end{table}
You can see that the \textit{standard deviations of these two are nearly the same}, but the other statistics (especially the \href{http://www.une.edu.au/WebStat/unit\_materials/c4\_descriptive\_statistics/determine\_skew\_kurt.html}{kurtosis and skewness}) indicate that the \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#PDF}{probability distribution} of the signal is \textit{far from} \href{http://en.wikipedia.org/wiki/Normal\_distribution}{\textit{normal}}; there are far more positive spikes in the signal than expected for pure noise. Most of these turned out to be the peaks of interest for this signal; they look like spikes only because the length of the signal \InsImageInline{0.5}{l}{J10.png}(over 1,000,000 points) causes the peaks to be compressed into one screen pixel or less when the entire signal is plotted on the screen. In the figures on the left, \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{iSignal} (page \pageref{ref-0434}) is used to "\textit{zoom in}" on some of the larger of these peaks (using the cursor arrow keys). The peaks are very sparsely separated (by an average of 1000 half-widths between peaks) and are well above the level of background noise (which has a standard deviation of roughly 0.9 throughout the signal). 

\InsImageInline{0.5}{l}{J.png}The researcher who obtained this signal said that a 'good' peak was 'bell-shaped', with an amplitude above 5 and a width of 500-1000 x-axis units. So that means that we can expect the signal-to-background-noise ratio to be at least 5/0.9 = 5.5. You can see in the three example peaks on the left that the peak widths do indeed meet those expectations. The interval between adjacent x-axis points is 25, so that means that we can expect the peaks to have about 20 to 40 points in their widths. Based on that, we can expect that the positions, heights and widths of the peaks should be able to be measured fairly accurately using least-squares methods (which reduce the uncertainty of measured parameters by about the square root of the number of points used - about a factor of 5 in this case). However, \textit{the noise appears to be signal-dependent}; the noise on the top of the peaks is distinctly greater than the noise on the baseline. The result is that the actual signal-to-noise (S/N) ratio of peak parameter measurement for the larger peaks will not be as good as might be expected based on the ratio of the peak height to the noise on the background. Most likely, the total noise in this signal is the sum of two major components, one with a fixed standard deviation of 0.9 and the other roughly equal to 10\% of the peak height.

\InsImageInline{0.5}{l}{J5.png}To automate the detection of large numbers of peaks, we can use the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaks}{findpeaksG} or \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#ipeak}{\textit{iPeak}} (page \pageref{ref-0461}) functions. Reasonable values of the input arguments \textit{AmplitudeThreshold}, \textit{SlopeThreshold}, \textit{SmoothWidth}, and \textit{FitWidth} for those functions can be estimated based on the expected peak height (5) and width (20 to 40 data points) of the "good" peaks. For example, using \textit{AmplitudeThreshold}=5, \textit{SlopeThreshold}=.001, \textit{SmoothWidth}=25, and \textit{FitWidth}=25, these functions detect and measure 76 peaks above an amplitude of 5 and with an average peak width of 523. The interactive \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#ipeak}{\textit{iPeak}} function (page \pageref{ref-0461}) is especially convenient for exploring the effect of these peak detection parameters and for graphically inspecting the peaks that it finds. Ideally, the objective is to find a set of peak detection arguments that detect and accurately measure all the peaks that you would consider 'good' and skip all the 'bad' ones. But, the criteria for good and bad peaks is at least partly subjective, so it is usually best to err on the side of caution and avoid skipping 'good' peaks at the risk of including a few 'bad' peaks in the mix, which can be weeded out manually based on unusual position, height, width, or appearance. 

Of course, it must be expected that the values of the peak position, height, and width given by the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaks}{findpeaksG} or \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#ipeak}{\textit{iPeak}} functions will only be approximate and will vary depending on the exact setting of the peak detection arguments; the noisier the data, the greater the uncertainty in the peak parameters. In this regard, the peak-fitting functions \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#command}{peakfit.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{ipf.m} usually give more accurate results, because they make use of \textit{all} the data across the peak, not just the top of the peak as do findpeaksG and \textit{iPeak}. For example, compare the results of the peak near x=3035200 measured with \href{https://terpconnect.umd.edu/~toh/spectrum/peak30353.png}{\textit{iPeak}} (\href{https://terpconnect.umd.edu/~toh/spectrum/peak30353.png}{click to view}) and with \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit30353.png}{peakfit} (\href{https://terpconnect.umd.edu/~toh/spectrum/peakfit30353.png}{click to view}). Also, the peak fitting functions are better for dealing with overlapping peaks and for estimating the uncertainty of the measured peak parameters, using the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Bootstrap}{bootstrap} options of those functions. For example, the largest peak in this signal has an x-axis position of 2.8683e+007, a height of 32, and a width of 500. The bootstrap method determines that the standard deviations are 4, 0.92, and 9.3, respectively. 

Because the signal in the case study was so large (over 1,000,000 points), the interactive programs such as \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#ipeak}{\textit{iPeak}}, \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{iSignal}, and \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{ipf} may be sluggish in operation, especially if your computer is not fast computationally or graphically. If this is a serious problem, it may be best to break the signal up into two or more segments and deal with each segment separately, then combine the results. Alternatively, you can use the \href{https://terpconnect.umd.edu/~toh/spectrum/condense.m}{condense} function to average the entire signal into a smaller number of points by a factor of 2 or 3 (at the risk of slightly reducing peak heights and increasing peak widths), but then you should reduce \textit{SmoothWidth} and \textit{FitWidth} by the same factor to compensate for the reduced number of data points across the peaks. Run \href{https://terpconnect.umd.edu/~toh/spectrum/testcondense.m}{testcondense.m} for a demonstration of the condense function.

\section{Buried treasure\label{ref-0350}}

The experimental signal in this case study had several narrow spikes rising above a \textit{seemingly flat baseline}.

 \InsImageInline{0.5}{l}{Case2.png}

Using \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{iSignal} (page \pageref{ref-0434}) to investigate the signal, I found that the visible positive spikes were \textit{single points} of very large amplitude (up to 10\textsuperscript{6}), whereas the regions \textit{between} the spikes were not really flat but contained bell-shaped peaks that were so much smaller (below 10\textsuperscript{3}) that they were not even visible on this scale. For example, using \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{iSignal }to zoom in to the region around x=26300, you can see one of those bell-shaped peaks with a small single-point negative-going spike artifact near its peak.


\begin{center}
\InsImageInline{0.5}{l}{Case2Figure2.png}
\end{center}


Very narrow spikes like this are common artifacts in some experimental signals; they are easy to eliminate by using a \href{http://en.wikipedia.org/wiki/Median\_filter}{\textit{median filter}}. The \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal}{iSignal} function (page \pageref{ref-0434}) has such a filter, activated by the ``\textbf{M}'' key. The result (on the next page) shows that the single-point spike artifacts have been eliminated, with little effect on the character of the bell-shaped peak.


\begin{center}
\InsImageInline{0.5}{l}{Case2Figure3.png}
\end{center}


Other filter types, like most forms of \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothing} (page \pageref{ref-0048}), would be far less effective than a median filter for this type of artifact and would distort the peaks. 

The negative spikes in this signal turned out to be steep \textit{steps}, which can either be reduced by using iSignal's \href{http://en.wikipedia.org/wiki/Slew\_rate}{slew-rate limit} function (the ` key) or manually eliminated by using the semicolon key (;) to set the selected region between the dotted red cursor lines to zero. Using the latter approach, the entire cleaned-up signal is shown below. The remaining peaks are all positive, bell-shaped and have amplitudes from about 6 to about 750.


\begin{center}
\InsImageInline{0.5}{l}{Case2y2.png}
\end{center}


\textit{iPeak} (page \pageref{ref-0461}) can automate measurements of peak positions and heights for the entire signal.


\begin{center}
\InsImageInline{0.5}{l}{Case2Figure6.png}
\end{center}


If required, individual peaks can be measured more accurately by fitting the whole peak with \textit{iPeak}'s ``\textbf{N}'' key (page \pageref{ref-0461}) or with peakfit.m or ipf.m (page \pageref{ref-0461}). The peaks are all slightly asymmetrical; they fit an \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Exponential\_broadening}{exponentially-broadened Gaussian model} (page \pageref{ref-0282}) to a fitting error less than about 0.5\%, as shown on the left. The smooth residual plots suggests that the signal was smoothed \textit{before} the spikes were introduced. 


\begin{center}
\InsImageInline{0.5}{l}{Case2Figure7.png}
\end{center}


Note that fitting with an exponentially-broadened Gaussian model gives the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Exponential\_broadening}{peak parameters of the Gaussian }\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Exponential\_broadening}{\textit{before} broadening}. iSignal (page \pageref{ref-0432}) and \textit{iPeak} (page \pageref{ref-0461}) estimate the peak parameters of the broadened peak. As before, the effect of the broadening is to shift the peak position to larger values, reduce the peak height, and increase the peak width. 

\texttt{\textbf{Position Height Width Area}}\index{\textbf{Area}}  \texttt{\textbf{error}} 

\texttt{\textbf{isignal 16871 788.88 32.881 27612 S/N Ratio = 172}}

\texttt{\textbf{ipeak 16871 785.34 33.525 28029}}

\texttt{\textbf{peakfit 16871 777.9 33.488 27729 1.68\%}} Gaussian model     

\texttt{\textbf{peakfit 16863 973.72 27.312 28308 0.47\%}} Exponentially-broadened Gaussian  \label{ref-0351}

\section{The Battle Rounds: a comparison of methods\label{ref-0352}\label{ref-0353}\label{ref-0354}}

This case study demonstrates the application of several techniques described in this paper to the quantitative measurement of a peak that is buried in an unstable background, a situation that can occur in the quantitative analysis applications of various forms of spectroscopy, process monitoring, and remote sensing. The objective is to derive a measure of peak amplitude that varies linearly with the actual peak amplitude but that is not affected by the changes in the background and the random noise. In this example, the peak to be measured is located at a fixed location in the center of the recorded signal, at x=100 and has a fixed shape (Gaussian) and width (30). The background, on the other hand, is highly variable, both in amplitude and in shape. The simulation shows six superimposed recordings of the signal with six different peak amplitudes and with randomly varying background amplitudes and shapes (top row left in the following figures). The methods that are compared here include \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothing} (page \pageref{ref-0048}), \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html}{differentiation} (page \pageref{ref-0081}), \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html}{classical least-squares multicomponent method} (page \pageref{ref-0248}), and \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{iterative non-linear curve fitting} (page \pageref{ref-0258}). 

\href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudyC.m}{CaseStudyC.m} is a self-contained Matlab/Octave demo function that demonstrates this case. To run it, download it, place it in the path, and type ``CaseStudyC'' at the command prompt. Each time you run it, you will get the same series of true peak amplitudes (set by the vector ``SignalAmplitudes'', in line 12) but a different set of background shapes and amplitudes. The background is modeled as a Gaussian peak of randomly varying amplitude, position, and width; you can control the average \textit{amplitude} of the background by changing the variable ``BackgroundAmplitude'' and the \textit{average} \textit{change} in the background by changing the variable ``BackgroundChange''. 

The five methods compared in the figures below are:

1: Top row center. A simple zero-to-peak measurement of the \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothed signal}, which assumes that the background is \textit{zero}.

2: Top row right. The difference between the peak signal and the average background on both sides of the peak (both smoothed), which assumes that the background is \textit{flat}.

3: Bottom row left. A \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html}{derivative-based method}, which assumes that the background is \textit{very broad} compared to the measured peak.

4: Bottom row center. \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html}{Classical least-squares} (CLS), which assumes that the background is a peak of \textit{known shape, width, and position} (the only unknown being the \textit{height}).

5: Bottom row right. \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{iterative non-linear curve fitting} (INLS), which assumes that the background is a peak of \textit{known shape} but unknown width and position. This method can track changes in the background peak position and width (within limits), if the measured peak and the background \textit{shapes} are independent of the concentration of the unknown.

These five methods are listed roughly in the order of increasing mathematical and geometrical complexity. They are compared below by plotting the actual peak heights (set by the vector ``SignalAmplitudes'') against the measure derived from that method, fitting those data to a straight line, and computing the \textit{coefficient of determination,} R\textsuperscript{2.}, which is 1.0000 for a perfectly linear plot.


\begin{center}
\InsImageInline{0.5}{l}{CaseStudy0.png}
\end{center}


For the first test (shown in the figure above), both ``BackgroundAmplitude'' and ``BackgroundChange'' are set to zero, so that only the random noise is present. In that case, all the methods work well, with R\textsuperscript{2 }values all very close to 0.9999. With a 10x higher noise level (\href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudy0noisy.png}{click to view}), all methods still work about equally well, but with a lower coefficient of determination R\textsuperscript{2}, as might be expected.


\begin{center}
\InsImageInline{0.5}{l}{CaseStudy1.png}
\end{center}


For the second test (shown in the figure immediately above), ``BackgroundAmplitude''=1 and ``BackgroundChange''=0, so the background has significant amplitude variation but a fixed shape, position, and width. In that case, the first two methods fail, but the derivative, CLS, and INLS methods work well.


\begin{center}
\InsImageInline{0.5}{l}{CaseStudy2.png}
\end{center}


For the third test, shown in the figure above, ``BackgroundAmplitude''=1 and ``BackgroundChange''=100, so the background varies in position, width, and amplitude (but remains broad compared to the signal). In that case, the CLS method fails as well, because it assumes that the background varies only in amplitude. However, if we go one step further \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudy3.png}{(click to view)} and set ``BackgroundChange''=1000, the background shape is now so unstable that even the INLS method fails, but the derivative method still remains effective as long as the background is broader than the measured peak, no matter what its shape. On the other hand, if the width and position of the \textit{measured} peak changes from sample to sample, the derivative method will fail and the INLS method is more effective \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudy4.png}{(click to view)}, as long as the fundamental shape of both measured peak and the background are both known (e.g. Gaussian, Lorentzian, etc.)

Not surprisingly, the more mathematically complex methods perform better, on average. Fortunately, software can "hide" that complexity, in the same way, for example, that a hand-held calculator hides the complexity of long division.

\section{Ensemble averaging patterns in a continuous signal\label{ref-0355}}

\href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#EnsembleAveraging}{Ensemble averaging} is a powerful method of reducing the effect of random noise in experimental signals when it can be applied. The idea is that the signal is repeated, preferably many times, and all the repeats are averaged. The signal builds up, and the noise gradually averages towards zero, as the number of repeats increases.

 An important requirement is that the repeats be aligned or synchronized so that in the absence of random noise, the repeated signals would line up exactly. There are two ways of managing this: 

(a) the signal repeats are triggered by some external event and the data acquisition can use that trigger to synchronize the signals, or 

(b) the signal itself has some feature that can be used to detect each repeat, whenever it occurs.

 The first method (a) has the advantage that the signal-to-noise (S/N) ratio can be arbitrarily low and the average signal will still gradually emerge from the noise if the number of repeats is large enough. However, not every experiment has a reliable external trigger.

The second method (b) can be used to average repeated patterns in one continuous signal without an external trigger that corresponds to each repeat, but the signal must then contain some feature (for example, a peak) with a signal-to-noise ratio large enough to detect reliably in each repeat. This method can be used even when the signal patterns occur at random intervals when the timing of the repetitions is not of interest. The interactive peak detector \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#ipeak}{\textit{iPeak 6}} (page \pageref{ref-0461}) has a built-in ensemble averaging function (Shift-E) that can compute the average of all the repeating waveforms. It works by detecting a single peak in each repeat to synchronize the repeats.

 \InsImageInline{0.5}{l}{iPeakEnsembleAverageDemo.png}\InsImageInline{0.5}{l}{EnsembleAverage.png}

The Matlab script \href{http://terpconnect.umd.edu/~toh/spectrum/iPeakEnsembleAverageDemo.m}{iPeakEnsembleAverageDemo.m} (on \url{http://tinyurl.com/cey8rwh}) demonstrates this idea, with a signal that contains a repeated underlying pattern of two overlapping Gaussian peaks, 12 points apart, with a 2:1 height ratio, both of width 12. These patterns occur at random intervals, and the noise level is about 10\% of the average peak height. Using \textit{iPeak} (page \pageref{ref-0461}) shown above left), you adjust the peak detection controls to detect only one peak in each repeat pattern, zoom in to isolate any one of those repeat patterns, and press Shift-E. In this case, there are about 60 repeats, so the expected signal-to-noise (S/N) ratio improvement is sqrt(60) = 7.7. You can save the averaged pattern (above right) into the Matlab workspace as ``\href{https://terpconnect.umd.edu/~toh/spectrum/FitEnsembleAverage.png}{}EA'' by typing

{\textgreater}{\textgreater} \texttt{load EnsembleAverage; EA=EnsembleAverage;}

\InsImageInline{0.5}{l}{FitEnsembleAverage.png}then curve-fit this averaged pattern to a 2-Gaussian model using the \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit.m function} (figure on the right):

\texttt{peakfit([1:length(EA);EA],40,60,2,1,0,10)}

 \texttt{Position Height Width Area}\index{Area}

 \texttt{32.54 13.255 12.003 169.36} 

 \texttt{44.72 6.7916 12.677 91.69}

You will see a big improvement in the accuracy of the peak separation, height ratio, and width, compared to fitting a \textit{single} pattern in the original x,y signal:

\texttt{{\textgreater}{\textgreater} peakfit([x;y],16352,60,2,1,0,10)}

\section{Harmonic Analysis of the Doppler Effect\label{ref-0356}\label{ref-0357}}

\InsImageInline{0.5}{l}{iSignalWithCarHorn.png}The wav file ``\href{http://terpconnect.umd.edu/~toh/spectrum/horngoby.wav}{horngoby.wav}'' (\textbf{Ctrl-click} to open) is a 2-second recording of the sound of a passing automobile horn, exhibiting the familiar \href{http://en.wikipedia.org/wiki/Doppler\_shift}{Doppler effect}. The sampling rate is 22000 Hz. Download\href{https://terpconnect.umd.edu/~toh/spectrum/iSignalWithCarHornLarge.png}{} this file and place it in your Matlab path. You can then load this into the Matlab workspace as the variable ``doppler'' and display it using \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{\textit{iSignal}} (page \pageref{ref-0434}):

\texttt{t=0:1/21920:2;}

\texttt{load} \texttt{horngoby.mat}

 \texttt{isignal(t,doppler);}

Within iSignal, you can switch to \href{http://terpconnect.umd.edu/~toh/spectrum/iSignal.html\#Spectrum}{frequency spectrum mode} by pressing Shift-S and zoom in on different portions of the waveform using the cursor keys, so you can observe the downward frequency shift and measure it quantitatively. (Actually, it is much easier to \textit{hear} the frequency shift - press Shift-P to play the sound - than to \textit{see} it graphically; the shift is rather small on a percentage basis, but \href{http://hyperphysics.phy-astr.gsu.edu/hbase/sound/earsens.html}{human hearing is very sensitive to small pitch (frequency) changes}). It helps to re-plot the data to stretch out the frequency region around the fundamental frequency or one of the harmonics. I used \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{iSignal} to zoom in on three slices of this waveform and then I plotted the frequency spectrum (Shift-S) near the \textit{beginning} (plotted in \textcolor{color-4}{blue}), \textit{middle} (\textcolor{color-14}{green}), and \textit{end} (\textcolor{color-19}{red}) of the sound. The frequency region between 150 Hz and 550 Hz are plotted in the figure below:

\InsImageInline{0.5}{l}{CarHornSpectrum3slices.png}

The group of peaks near 200 is the \href{http://en.wikipedia.org/wiki/Fundamental\_frequency}{fundamental frequency} of the lowest note of the horn and the group of peaks near 400 is the \href{http://www.physicsclassroom.com/class/sound/Lesson-4/Fundamental-Frequency-and-Harmonics}{second harmonic}. (Pitched sounds have a harmonic structure of 1, 2, 3... times a fundamental frequency). The group of peaks near 250 is the fundamental frequency of the next higher note of the horn and the group of peaks near 500 is its second harmonic. (Car and train horns often have two or three \href{http://en.wikipedia.org/wiki/Train\_horn}{harmonious notes} sounded together). In each of these groups of harmonics, you can clearly see that the blue peak (the spectrum measured at the \textit{beginning} of the sound) has a \textit{higher} frequency than the red peak (the spectrum measured at the \textit{end} of the sound). The green peak, taken in the middle, has an intermediate frequency. \textit{The peaks are ragged because the amplitude and frequency vary over the sampling interval}, but you can still get good quantitative measures of the frequency of each component by \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#FittingPeaks}{curve fitting to a Gaussian peak model} using \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit.m or ipf.m} (page \pageref{ref-0461}): 

\texttt{\textbf{Peak Position Height Width Area}}\index{\textbf{Area}}

\texttt{\textbf{Beginning 206.69 3.0191e+005 0.81866 2.4636e+005}}

\texttt{\textbf{Middle 202.65 1.5481e+005 2.911 4.797e+005}}

\texttt{\textbf{End 197.42 81906 1.3785 1.1994e+005}}

The estimated precision of the peak position (i.e. frequency) measurements is about 0.2\% relative, based on the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Bootstrap}{bootstrap method}, good enough to allow accurate calculation of the frequency shift (about 4.2\%) and of \href{http://www.school-for-champions.com/science/sound\_doppler\_effect\_equations.htm\#.VidvSfmrSUk}{the speed of the vehicle} and to demonstrate that the measured ratio of the second harmonic to the fundamental for these data is 2.0023, which is very close to the theoretical value of 2.

\section{Measuring spikes\label{ref-0358}\label{ref-0359}\label{ref-0360}}

\textit{Spikes}, narrow pulses with a width of only one or a few points, are sometimes encountered in signals as a result of an electronic ``glitch'' or stray pickup from nearby equipment, and they can easily be eliminated by the use of a \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#spikes}{``median'' filter}. But it is possible that in some experiments the spikes \textit{themselves} might be the important part of the signal and that it is required to count or measure them. This situation was encountered in a research application by one of my clients, and it brings up some interesting twists on the usual procedures.

 As a demonstration, the Matlab/Octave script \href{http://terpconnect.umd.edu/~toh/spectrum/SpikeDemo1.m/}{SpikeDemo1.m} creates a waveform (top panel of the figure below) in which a series of spikes are randomly distributed in time, contaminated by two types of noise: white noise and a large-amplitude oscillatory interference simulated by a swept-frequency sine wave. The objective is to count the spikes and locate their position on the x (time) axis. Direct application of findpeaks or \textit{iPeak} (page \pageref{ref-0461}) to the raw signal does not work well.

 \InsImageInline{0.5}{l}{SpikeDemo1.png} \InsImageInline{0.5}{l}{SpikeSpectrum1.png}

 A single-point spike, called a \textit{delta function} in mathematics, has a power spectrum that is \textit{flat}; that is, it has \textit{equal power at all frequencies}, just like white noise. But the oscillatory interference, in this case, is in a \textit{specific range of frequencies}, which opens some interesting possibilities. One approach would be to use a \href{https://terpconnect.umd.edu/~toh/spectrum/InteractiveFourierFilter.htm}{Fourier filter}, for example, a notch or band-reject filter, to \href{https://terpconnect.umd.edu/~toh/spectrum/iFIlterNotch1.png}{remove the troublesome oscillations} selectively. But if the objective of the measurement is only to count the spikes and measure their times, a simpler approach would be to (1) compute the \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html}{second derivative} (which greatly amplifies the spikes relative to the oscillations), (2) \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smooth} the result (to limit the \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html\#Smoothing}{white noise amplification caused by differentiation}), then (3) take the \href{http://en.wikipedia.org/wiki/Absolute\_value}{absolute value\index{absolute value}} (to yield positive-pointing peaks). This can be done in a \textit{single line} of nested Matlab/Octave code:

\texttt{y1=}\href{http://www.mathworks.com/help/matlab/ref/abs.html}{\texttt{abs}}\texttt{(}\href{https://terpconnect.umd.edu/~toh/spectrum/fastsmooth.m}{\texttt{fastsmooth}}\texttt{((}\href{https://terpconnect.umd.edu/~toh/spectrum/deriv2.m}{\texttt{deriv2}}\texttt{(y)).\textasciicircum{}2,3,2));}

The result, shown the lower panel of the figure on the left above, is an almost complete extraction of the spikes, which can then be counted with findpeaksG.m or peakstats.m or iPeak.m (page \pageref{ref-0461}):

\texttt{P=}\href{https://terpconnect.umd.edu/~toh/spectrum/ipeak.m}{\texttt{ipeak}}\texttt{([x;y1],0,0.1,2e-005,1,3,3,0.2,0);}

 

The second example, \href{http://terpconnect.umd.edu/~toh/spectrum/SpikeDemo2.m}{SpikeDemo2.m}, even more difficult. In this case the oscillatory interference is caused by \textit{two} fixed-frequency sine waves at a higher frequency, which \textit{completely obscures} the spikes in the raw signal (top panel of the left figure below). In the \href{https://terpconnect.umd.edu/~toh/spectrum/HarmonicAnalysis.html}{power spectrum} (bottom panel, in red), the oscillatory interference shows as two sharp peaks that dominate the spectrum and reach to y=10\textsuperscript{6}, whereas the spikes show as the much lower broad flat plateau at about y=10. In this case, use can be made of an interesting property of sliding-average smooths, such as the boxcar, triangular, and Gaussian \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smooths}; their frequency responses exhibit a series of deep \href{http://en.wikipedia.org/wiki/Cusp\_\%28singularity\%29}{cusps} at frequencies that are inversely proportional to their filter widths. So, this opens the possibility of suppressing specific frequencies of oscillatory interference by adjusting the filter widths until the cusps occur at or near the frequency of the oscillations. Since the signal, in this case, consists of spikes that have a flat power spectrum, they are simply smoothed by this operation, which will reduce their heights and increase their widths, but will have little or no effect on their number or x-axis positions. In this case, a 9-point P-spline smooth puts the first (lowest frequency) cusp right in between the two oscillatory frequencies.

       \InsImageInline{0.5}{l}{SpikeDemo2a.png} \InsImageInline{0.5}{l}{SpikeDemo2b.png}

In the figure on the right, you can see the effect of applying this filter; the spikes, which were \textit{not even visible} in the original signal, are now cleanly extracted (upper panel), and you can see in the power spectrum (right lower panel, in red) that the two sharp peaks of oscillatory interference are \textit{reduced by about a factor of 1,000,000!} (Note: the frequency spectra are plotted on a \textit{log-log scale}). This operation can be performed by a single command-line function, adjusting the smooth width (the second input argument, here a 9) by trial and error to minimize the oscillatory interference:

\texttt{y1=}\href{https://terpconnect.umd.edu/~toh/spectrum/fastsmooth.m}{\texttt{fastsmooth}}\texttt{(y,9,3);}

If the interference varies in frequency across the signal, you could use a \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#SegmentedSmooth}{segmented\index{segmented} smooth} rather than the standard fastsmooth. Alternatively, the \href{https://terpconnect.umd.edu/~toh/spectrum/PlotSegFreqSpect.m}{segmented Fourier Spectrum} (page \pageref{ref-0128}) could be used to \href{https://terpconnect.umd.edu/~toh/spectrum/SpikeDemo3.png}{visualize this signal}, and a \href{https://terpconnect.umd.edu/~toh/spectrum/ifilter.m}{Fourier filter} (page \pageref{ref-0167}) in ``notch'' mode could be employed to specifically \href{https://terpconnect.umd.edu/~toh/spectrum/SpikeDemo4.png}{eliminate the interfering frequencies}. The extracted peaks could then be counted using any of the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{peak finding functions}, such as:

\texttt{P=}\href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaks}{\texttt{findpeaksG}}\texttt{(x,y1,2e-005,0.01,2,5,3);}

or

\texttt{P=}\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksplot.m}{\texttt{findpeaksplot}}\texttt{(x,y1,2e-005,0.01,2,5,3);}

or

\texttt{PS=}\href{https://terpconnect.umd.edu/~toh/spectrum/peakstats.m}{\texttt{peakstats}}\texttt{(x,y1,2e-005,0.01,2,5,3,1);}

The simple script ``\href{https://terpconnect.umd.edu/~toh/spectrum/iSignalDeltaTest.m}{iSignalDeltaTest}'' demonstrates the power spectrum of the smoothing and differentiation functions of iSignal (page \pageref{ref-0434}) by applying them to a \href{https://en.wikipedia.org/wiki/Dirac\_delta\_function}{delta function}. Change the smooth type, smooth width, and derivative order and other functions to see how the power spectrum changes.

\section{Fourier deconvolution vs curve fitting (they are \textit{not} the same)\label{ref-0361}\label{ref-0362}\label{ref-0363}}

Some experiments produce peaks that are distorted by being convoluted by processes that make peaks less distinct and modify peak position, height, and width. \href{https://terpconnect.umd.edu/Tom/Dropbox/SPECTRUM/CurveFittingC.html\#Exponential\_broadening}{\textit{Exponential}}\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Exponential\_broadening}{ }\href{https://terpconnect.umd.edu/Tom/Dropbox/SPECTRUM/CurveFittingC.html\#Exponential\_broadening}{\textit{broadening}} is one of the most common of these processes. \href{https://terpconnect.umd.edu/~toh/spectrum/Deconvolution.html}{Fourier deconvolution} and \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{iterative curve fitting} are two methods that can help to measure \textit{the true underlying peak parameters}. Those two methods are conceptually distinct because, in Fourier deconvolution, the underlying peak shape is unknown but the nature and width of the broadening function (e.g., exponential) is assumed to be known; whereas in iterative least-squares curve fitting it is just the reverse: the underlying peak \textit{shape} (i.e., Gaussian, Lorentzian, etc.) must be \InsImageInline{0.5}{l}{DeconvDemo.png}known, but the width of the broadening process is initially unknown. 

In the script shown below and the resulting graphic shown above \href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo.m}{(Download this script}), the underlying signal (uyy) is a set of four Gaussians with peak heights of 1.2, 1.1, 1, 0.9 located at x=10, 20, 30, 40 and peak widths of 3, 4, 5, 6, but in the \textit{observed} signal (yy) these peaks are broadened exponentially by the exponential function cc, resulting in \href{https://terpconnect.umd.edu/~toh/spectrum/uyyvsyy.png}{shifted, shorter, and wider peaks}, and then a little constant white noise is added \textit{after} the broadening. The deconvolution of cc from yy successfully removes the broadening (yydc), but at the expense of a \href{https://terpconnect.umd.edu/~toh/spectrum/uyyvsyydc.png}{substantial}\href{https://terpconnect.umd.edu/~toh/spectrum/uyyvsyydc.png}{ noise increase}. However, the extra noise in the deconvoluted signal is high-frequency weighted ("\href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#Frequency}{blue}") and so is easily reduced by \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothing }and \textit{has less effect on least-square fits than does white noise}. (For a greater challenge, try adding more noise in line 6 or use a bad estimate of time constant in line 10). To plot the recovered signal overlaid with the underlying signal\texttt{:} \href{https://terpconnect.umd.edu/~toh/spectrum/uyyvsyydc.png}{\texttt{plot(xx, uyy, xx, yydc)}}. To plot the observed signal overlaid with the underlying signal: \href{https://terpconnect.umd.edu/~toh/spectrum/uyyvsyy.png}{\texttt{plot(xx,uyy,xx,yy)}}\texttt{. Excellent values for the original underlying peak positions, heights, and widths can be obtained by} \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{curve-fitting} the recovered signal to four Gaussians: \texttt{[}\href{https://terpconnect.umd.edu/~toh/spectrum/peakfitxxyydc.png}{\texttt{FitResults,FitError]= peakfit([xx;yydc],26,42,4,1,0,10)}}. With \textit{ten times} the previous noise level (Noise=.01), the values of peak parameters determined by curve fitting are \href{https://terpconnect.umd.edu/~toh/spectrum/10xMoreNoise.png}{still quite good}, and even with \textit{100x more noise} (Noise=0.1) the peak parameters are \href{https://terpconnect.umd.edu/~toh/spectrum/100xMoreNoise.png}{more accurate than you might expect} for that amount of noise (because that noise is \textit{blue}). Visually, the noise is so great that the situation looks \textit{hopeless}, but the curve fitting works well.\label{ref-0364}

\texttt{xx=5:.1:65;}

\texttt{\textcolor{color-9}{\% Underlying Gaussian peaks with unknown heights, positions, and widths.}}

\texttt{uyy=}\href{https://terpconnect.umd.edu/~toh/spectrum/modelpeaks2.m}{\texttt{modelpeaks2}}\texttt{(xx,[1 1 1 1],[1.2 1.1 1 .9],[10 20 30 40],[3 4 5 6],...}

\texttt{[0 0 0 0]);}

\texttt{\textcolor{color-9}{\% Observed signal yy, with noise added AFTER the}} \href{https://terpconnect.umd.edu/~toh/spectrum/ExpBroaden.m}{\texttt{broadening convolution}} 

\texttt{Noise=.001;} \texttt{\textcolor{color-9}{\% {\textless}---Try more noise to see how this method handles it.}}

\texttt{yy=modelpeaks2(xx,[5 5 5 5],[1.2 1.1 1 .9],[10 20 30 40],[3 4 5 6],...}

\texttt{[-40 -40 -40 -40])+Noise.*randn(size(xx));}

\texttt{\textcolor{color-9}{\% Compute transfer function, cc,}} 

\texttt{cc=exp(-(1:length(yy))./40); \% {\textless}---Change exponential factor here}

\texttt{\textcolor{color-9}{\% Attempt to recover original signal uyy by deconvoluting cc from yy}}

\texttt{\textcolor{color-9}{\% It is necessary to zero-pad the observed signal yy as shown here.}}

\texttt{yydc=deconv([yy zeros(1,length(yy)-1)],cc).*sum(cc);}

\texttt{\textcolor{color-9}{\% Plot and label everything}}

\texttt{subplot(2,2,1);plot(xx,uyy);title('Underlying signal, uyy');}

\texttt{subplot(2,2,2);plot(xx,cc);title('Exponential transfer function, cc')}

\texttt{subplot(2,2,3);plot(xx,yy);title('observed broadened, noisy signal, yy');}

\texttt{subplot(2,2,4);plot(xx,yydc);title('Recovered signal, yydc')}

An alternative to the above deconvolution approach is to curve fit the observed signal directly with an \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Exponential\_broadening}{exponentially broadened Gaussian} (shape number 5): \href{https://terpconnect.umd.edu/~toh/spectrum/yyExpGfit.png}{[FitResults,FitError]=peakfit([xx;yy],26,50,4,5,40,10)}. Both methods give good values of the peak parameters, but the deconvolution method is considerably faster because curve fitting with a simple Gaussian model is faster than fitting with an exponentially broadened peak model, especially if the number of peaks is large. Also, if the exponential factor is not known, it can be determined by curve fitting one or two of the peaks in the observed signal, using ipf.m (page \pageref{ref-0461}), adjusting the exponential factor interactively to get the best fit. Note that \textit{you must give peakfit a reasonably good value for the time constant} ('extra'), the input argument right after the peakshape number. If the value is too far off, the fit may fail completely, returning all zeros. A little trial and error suffice. Alternatively, you could try to use \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m version 7} with the \textit{independent variable} exponent broadened Gaussian shape number 31 or 39, to measure the time constant as an iterated variable (to understand the difference, see \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Examples}{example 39}). If the time constant is expected to be the \textit{same} for all peaks, better results will be obtained by using shape number 31 or 39 initially to measure the time constant of an isolated peak (preferably one with a good S/N ratio), then apply that fixed time constant in peak shape 5 to all the other groups of overlapping peaks.

\label{ref-0365}\label{ref-0366}\label{ref-0367}

\section{Digitization noise - can adding noise really help?\label{ref-0368}}

\textit{Digitization noise}, also called \href{https://en.wikipedia.org/wiki/Quantization\_\%28signal\_processing\%29}{quantization noise}, is an artifact caused by the rounding or truncation of numbers to a fixed number of figures. It can originate in the \href{https://en.wikipedia.org/wiki/Analog-to-digital\_converter}{analog-to-digital converter} that converts an analog signal to a digital one, or in the circuitry or software involved in transmitting the digital signal to a computer, or even in the process of transferring the data from one program to another, as in copying and pasting data to and from a spreadsheet. The result is a series of non-random steps of equal height. The frequency distribution is white, because of the sharpness of the steps, as you can see by observing the \href{https://terpconnect.umd.edu/~toh/spectrum/DigitizationNoiseSpectrum.png}{power spectrum}.

\InsImageInline{0.5}{l}{DigitizationNoise.png}  \InsImageInline{0.5}{l}{DigitizationNoise2.png}

The figure on the left, top panel, shows the effect of integer digitization on a sine wave with an amplitude of +/- 10. Ensemble averaging, which is usually the most effective of noise reduction techniques, does not reduce this type of noise (bottom panel) because it is non-random. 

\InsImageInline{0.5}{l}{RoundingError.gif.png}Interestingly, if additional random noise is present in the signal, then ensemble averaging becomes effective in reducing \textit{both} the random noise \textit{and} the digitization noise. In essence, the added noise \textit{randomizes the digitization}, allowing it to be reduced by ensemble averaging. Moreover, if there is insufficient random noise already in the signal, it can be beneficial to add additional noise artificially! Seriously. The script \href{https://terpconnect.umd.edu/~toh/spectrum/RoundingError.m}{RoundingError.m} illustrates this effect, as shown the \href{https://terpconnect.umd.edu/~toh/spectrum/RoundingError.gif}{animation} on the right, which shows the digitized sine wave with gradually increasing amounts of added random noise in line 8 (generated by the randn.m function) followed by ensemble averaging of 100 repeats (in lines 17-20). Look closely at the waveform in this animation as it changes in response to the random noise addition shown in the title. You can clearly see how the noise starts out mostly quantization noise but then quickly decreases as small but increasing amounts of random noise are added before the ensemble averaging step, then eventually increases as too much noise is added. The optimum standard deviation of random noise is about 0.36 times the quantization size, as you can demonstrate by adding lesser or greater amounts via the variable \textit{Noise} in line 6 of this script. This technique is called "\href{https://en.wikipedia.org/wiki/Dither}{dithering}" and it is also used in audio and in image processing. 

\InsImageInline{0.5}{l}{DigitizedSpeech.png} An \textit{audible} example of this idea is illustrated by the Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/DigitizedSpeech.m}{DigitizedSpeech.m}, which starts with an audio recording of the spoken phrase "Testing, one, two, three", previously recorded at 44000 Hz and saved in WAV format (\href{https://terpconnect.umd.edu/~toh/spectrum/TestingOneTwoThree.wav}{TestingOneTwoThree.wav}) and in .mat format (\href{https://terpconnect.umd.edu/~toh/spectrum/testing123.mat}{testing123.mat}), rounds off the amplitude data progressively to 8 bits (256 steps; \href{https://terpconnect.umd.edu/~toh/spectrum/s256.wav}{sound link}), shown on the left, 4 bits (16 steps; \href{https://terpconnect.umd.edu/~toh/spectrum/s8.wav}{sound link}), and 1 bit (2 steps; \href{https://terpconnect.umd.edu/~toh/spectrum/s2.wav}{sound link}), and then the 2-step case again \textit{with random white noise added} before the rounding (2 steps + noise; \href{https://terpconnect.umd.edu/~toh/spectrum/s2n.wav}{sound link}), plots the waveforms and plays the resulting sounds, demonstrating both the degrading effect of rounding and the remarkable improvement caused by adding noise. (Click on these sound links to hear the sounds on your computer). Although the computer program, in this case, does \textit{not} perform an \textit{explicit} ensemble averaging operation as does \href{https://terpconnect.umd.edu/Tom/Dropbox/SPECTRUM/RoundingError.m}{RoundingError.m}, it is likely that the neurons of the hearing center of your brain provide that function by virtue of their response time and memory effect.

\section{How low can you go? Performance with very low signal-to-noise ratios.\label{ref-0369}}

This is a simulation of several techniques described in this paper applied to the quantitative measurement of a peak that is \textit{buried in an excess of random noise}, where the signal-to-noise (S/N) ratio is \textit{below 2}. (Ordinarily, an S/N ratio of 3 is desired for reliable detection).

The Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/LowSNRdemo.m}{LowSNRdemo.m} performs the simulations and calculations and compares the results graphically, focusing on the behavior of each method as the S/N ratio approaches zero. Four methods are compared:

(1) \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothing}, followed by the peak-to-peak measure of the smoothed signal (page \pageref{ref-0048}); 

(2) a peak finding method based on \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{findpeakG}  (page \pageref{ref-0302});

(3) \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{unconstrained iterative least-squares fitting} (INLS) based on \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m} (page \pageref{ref-0258});

(4) \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html}{constrained classical least-squares fitting} (CLS) based on the \href{https://terpconnect.umd.edu/~toh/spectrum/cls2.m}{cls2.m} function (page \pageref{ref-0247}); 

The measurements are carried out over a range of peak heights for which the S/N ratio varies from 0 to 2. The \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html}{noise }is random, constant, and white. Each time you run the script, you get the same set of underlying signals but independent samples of the random noise.



 Results for the initial values in the script are shown in the plots and in the table printed below, both of \InsImageInline{0.5}{l}{LowSNRdemo.png}which are created by the script \href{https://terpconnect.umd.edu/~toh/spectrum/LowSNRdemo.m}{LowSNRdemo.m}. The graphs on the left show correlation plots of the measured peak height vs the real peak height, which should ideally be a straight line with a slope of 1, an intercept of zero, and an R-squared of 1. As you can see, the simplest smoothed-peak method (upper left) is completely inadequate, with a low slope (because smoothing reduces peak height) and a high intercept (because even smoothed noise has a non-zero peak-to-peak value). The findpeaks function (upper right) works OK for height for higher peak heights but fails completely below an S/N ratio of 0.5 because the peak height falls below the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaks}{amplitude threshold} setting. In comparison, the two least-squares techniques work much better, reporting much better values of slope, intercept of zero, and R-squared. But if you look closely at the low end of the peak height range, near x=zero, you can see that the values reported by the unconstrained fit (lower left) occasionally stray from the line, whereas the constrained fit (lower right) decrease gracefully all the way to zero every time you run the script. Essentially the reason why it is even possible to make measurements at such low S/N ratios is that \textit{the data density is very high}: that is, there are many data points in each signal (about 1000 points across the half-width of the peak with the initial script values). The results are summarized in the table below. 

\texttt{Number of points in half-width of peak: 1000}

\texttt{Method Height Error Position Error}

\texttt{Smoothed peak 21.2359\% 120.688\%}

\texttt{findpeaksG.m 32.3709\% 33.363\%}

\texttt{peakfit.m 2.7542\% 4.6466\%}

\texttt{cls2.m 1.6565\%} 

The height errors are reported as a percentage of the maximum height (initially 2). (For the first three methods, the peak position is also measured, and its relative accuracy is reported. The \href{https://terpconnect.umd.edu/Tom/Dropbox/SPECTRUM/CurveFittingB.html}{constrained classical least-squares fitting} does not measure peak position but rather assumes that it remains fixed at the initial value of 100). You can see that the CLS method has a slight advantage in the accuracy, but you must consider also that this method works well only if the peak shape, position, and width are known. The unconstrained iterative method can track changes in peak position and width.\href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#Top}{}

 You can change several of the factors in this simulation to test the robustness of these methods. Search for the word 'change' in the comments for values that can be changed. Reduce MaxPeakHeight (line 8) to make the problem harder. Change peak position and./or width (lines 9 and 10) to show how the CLS method fails. As usual, the more you know, the better your results. Change the increment (line 4) to change the data density; more data is always better. 

(Surprisingly, as we will see on page \pageref{ref-0402}\href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#Calibration}{, Measurement Calibration}, \textit{it is not even necessary to have an accurate peak shape model} in order to get a good correlation between measured and actual height).

LowSNRdemo.m also computes the \href{https://terpconnect.umd.edu/~toh/spectrum/HarmonicAnalysis.html}{power spectrum} of the signal and the amplitude (square root of the power) of the fundamental, where most of the power of a broad Gaussian peak falls, and plots it in \href{https://terpconnect.umd.edu/~toh/spectrum/LowSNRdemoFigure2.png}{Figure window(2)}. The correlation to peak height is like the CLS method, but \textit{the intercept is higher} because there is a non-zero quantity of noise even in that one frequency slice of the power spectrum. 

  We are now, in the 21\textsuperscript{st} century, into the era of "big data", where high-speed automated data systems can acquire, store, and process greater quantities of data than ever before. As this little example shows, greater quantities of data allow researchers to probe deeper and measure smaller effects than previously. 



\section{\textbf{Signal processing in the search for extraterrestrial intelligence}\label{ref-0370}}

\InsImageInline{0.5}{l}{image278.jpeg}The signal detection problems facing those who search the sky for evidence of extraterrestrial civilizations or interesting natural phenomena are enormous. Among those problems are the fact that we do not know much about what to expect. We do not know exactly where to look in the sky, or what frequencies might be used, or the possible forms of the transmissions. Moreover, astronomers do not want to confuse the many powerful sources of natural and \textit{terrestrial} sources of interfering signals for genuine \textit{extraterrestrial} ones. There is also the massive computer power required, which has driven the development of specialized hardware and software as well as distributed computation over thousands of Internet-connected personal computers across the world using the\href{http://setiathome.ssl.berkeley.edu/}{ SETI@home} computational screen-saver pictured here. Although some of the \href{http://astronomyonline.org/Astrobiology/HowSETIWorks.asp}{computational tecniques} used in this search are more sophisticated than those covered in this book, they begin with the basic concepts covered here.

One of the reoccurring themes of this book has been that the more you know about your data, the more likely you are to obtain a reliable measurement. In the case of possible extraterrestrial signals, we do not know much, but we do know a few things. 

We do know that electromagnetic radiation over a wide range of frequencies is used for long-distance transmission on earth and between earth and satellites and probes far from earth. Astronomers already use radio telescopes to receive natural radiations from vast distances. To look at different frequencies at once, \href{https://terpconnect.umd.edu/~toh/spectrum/HarmonicAnalysis.html}{\textit{Fourier transforms}} of the raw telescope signals can be computed over multiple time segments. On page \pageref{ref-0123}, I showed a \href{https://terpconnect.umd.edu/~toh/spectrum/HarmonicAnalysis.html\#SignalAndNoise}{simulation} that demonstrated how hard it is to see a periodic component in the presence of an equal amount of random noise and yet how easy it is to pick it out in the frequency spectrum. 

Also, transmissions from extraterrestrial civilizations might be in the form of groups of pulses, so their detection and verification are also part of SETI signal processing. Interestingly, triplets and other groups of equally spaced pulses appear in the Fourier transforms of high-frequency carrier waves that are \href{https://terpconnect.umd.edu/~toh/spectrum/iPowerAnimated.gif}{amplitude or frequency modulated} (like \href{https://en.wikipedia.org/wiki/Modulation}{AM or FM radio}). Of course, there is no reason to assume, nor to reject, that extraterrestrial civilizations might use the same methods of communication as ourselves. 

One thing that we know for sure is that the earth rotates around its axis once a day and that it revolves around the sun once a year. So, if we look at a fixed direction out from the earth, the distant stars will seem to move in a predictable pattern, whereas terrestrial sources will remain fixed on earth. The huge \href{https://en.wikipedia.org/wiki/Arecibo\_Observatory}{Arecibo Observatory} dish in Puerto Rico (sadly no longer operational) was fixed in position and was often used to look in one selected direction for extended periods of time. The field of view of this telescope is such that a point source at a distance takes 12 seconds to pass, as the earth rotates. \href{https://setiathome.berkeley.edu/sah\_glossary/gaussians.php}{As SETI says}: 

``Radio signals from a distant transmitter should get stronger and then weaker as the telescope's focal point moves across that area of the sky. Specifically, the power should increase and then decrease with a bell-shaped curve (\href{https://www.google.com/?ion=1&espv=2\#q=seti\%20gaussian}{a Gaussian curve}). \href{https://setiathome.berkeley.edu/sah\_glossary/gaussian\_graphs.php}{\textit{Gaussian curve-fitting} }is an excellent test to determine if a radio wave was generated 'out there' rather than a simple source of interference somewhere here on Earth since signals originating from Earth will typically show constant power patterns rather than curves''.

 Also, any observed 12-second peaks can be re-examined with another focal point shifted towards the west to see if it repeats with the expected time and duration. 

 We also know that there will be a \href{https://en.wikipedia.org/wiki/Doppler\_effect}{\textit{Doppler shift}} in the frequencies observed if the source is moving relative to the receiver; this is observed with \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#F}{sound waves} as well as with \href{http://coolcosmos.ipac.caltech.edu/cosmic\_classroom/cosmic\_reference/redshift.html}{electromagnetic waves like radio or light}. Because the earth is rotating and revolving at a known and constant speed, we can accurately predict and compensate for the Doppler shift caused by earth's motion (this is called ``\href{https://setiathome.berkeley.edu/sah\_glossary/chirping.php}{de-chirping}'' the data). 

For more on the details of SETI signal processing, see \href{http://setiathome.ssl.berkeley.edu/}{SETI@home}.

\section{\textbf{Why measure peak area rather than peak height? } \label{ref-0371}\label{ref-0372}\label{ref-0373}}

\InsImageInline{0.5}{l}{AppendixLfigure1.png}This simulation examines more closely the question of measuring peak area rather than peak height to reduce the effect of peak broadening, which commonly occurs in chromatography, for reasons that are discussed \href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html}{previously}, and also in some forms of spectroscopy. Under what conditions the measurement of peak area might be better than peak height?

 The Matlab/Octave script ``\href{https://terpconnect.umd.edu/~toh/spectrum/HeightVsArea.m}{HeightVsArea.m}'' simulates the measurement of a series of standard samples whose concentrations are given by the vector 'standards'. Each standard produces an isolated peak whose peak height is directly proportional to the corresponding value in 'standards' and whose \textit{underlying} shape is a Gaussian with a constant peak position ('pos') and width ('wid'). To simulate the measurement of these samples under typical conditions, the script changes the shape of the peaks (by exponential broadening) and adds a variable baseline and random noise. You can control, by means of the variable definitions in the first few lines of the script, the peak beginning and end, the sampling rate 'deltaX' (increment between x values), the peak position and width ('pos' and 'wid'), the sequence of peak heights ('standards'), the baseline amplitude ('baseline') and its degree of variability ('vba'), the extent of shape change ('vbr'), and the amount of random noise added to the final signal ('noise').

The resulting peaks are shown in the figure above. The script prepares a series of ``\href{https://terpconnect.umd.edu/~toh/models/CalibrationCurve.html}{calibration curves}'' plotting the values of 'standard' against the measured peak heights or areas for each measurement method. The measurement methods include peak height in Figure window 2, peak area in Figure window 3, and \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{curve fitting} height and area in \href{https://terpconnect.umd.edu/~toh/spectrum/AppendixLfigure4.png}{Figures 4} and \href{https://terpconnect.umd.edu/~toh/spectrum/AppendixLfigure5.png}{5}, respectively. These plots should ideally have an intercept of zero and an R\textsuperscript{2} of 1.000, but the \textit{slope} is greater for the peak area measurements because the area has different units and is numerically greater than peak height. All the measurement methods are baseline corrected; that is, they include code that attempts to compensate for changes in the baseline (controlled by the variable 'baseline').

 With the initial values of 'baseline', 'noise', 'vba', and 'vbr', you can clearly see the advantage of peak area measurements (figure 3) compared to peak height (figure 2). This is primarily because of the variability of peak shape broadening ('vbr') and to the averaging out of random noise in the computation of area.


\begin{center}
\InsImageInline{0.5}{l}{AppendixLfigure2.png}\InsImageInline{0.5}{l}{AppendixLfigure3.png}
\end{center}



\begin{center}
Figure window 2                                                           Figure window 3
\end{center}


If you set 'baseline', 'noise', 'vba', and 'vbr' all to zero, you've simulated a perfect world in which all methods work perfectly.

\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{\textit{Curve fitting}} can measure both peak height and area; it is not even absolutely necessary to use an accurate peak shape model. Using a simple Gaussian model in this example works much better for peak area (\href{https://terpconnect.umd.edu/~toh/spectrum/AppendixLfigure5.png}{Figure window 5}) than for peak height (\href{https://terpconnect.umd.edu/~toh/spectrum/AppendixLfigure4.png}{Figure window 4}) but is not significantly better than a simple peak area measurement (Figure window 3). The best results are obtained if an \textit{exponentially broadened} Gaussian model (shape 31 or 39) is used, using the code in line 30, but that computation takes longer. Moreover, if the measured peak \textit{overlaps} another peak significantly, curve fitting both of those peaks together can give much \href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html\#Matlab}{more accurate results }that other peak area measurement methods.



\section{Using macros to extend the capability of spreadsheets (updated)\label{ref-0374}\label{ref-0375}\label{ref-0376}\label{ref-0377}}

Both Microsoft \textit{Excel} and OpenOffice \textit{Calc} can automate repetitive tasks using ``macros'', saved sequences of commands or keystrokes that are stored for later use. Macros can be most easily created using the built-in ``Macro Recorder'', which will literally watch all your clicks, drags, and keystrokes and record them for later playback. Or you can write or edit your macros in the macro language of that spreadsheet (VBA in \textit{Excel}; Python or JavaScript in \textit{Calc}). Or you can do both: use the macro recorder first, then edit the resulting code manually to modify it. 

To enable macros in Excel, go to the \textbf{File} tab {\textgreater} \textbf{Options}. On the left-side pane, select \textbf{Trust Center}, and then click \textbf{Trust Center Settings}. In the Trust Center dialog box, click \textbf{Macro Settings} on the left, select \textbf{Enable all macros} and click OK. Then perform your spreadsheet operations, and when finished, click \textbf{Stop Recording} and save the spreadsheet. Thereafter, simply pressing your Ctrl-key shortcut will run the macro and perform all the spreadsheet operations that you recorded.

Here I will demonstrate two applications in Excel using macros with the Solver function. (See \url{http://peltiertech.com/Excel/SolverVBA.html\#Solver2} for information about setting up macros and solver on your version of Excel).

\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Spreadsheets}{A previous section} (page \pageref{ref-0260}) described the use of the Solver function applied to the iterative fitting of overlapping peaks in a spreadsheet. The steps listed there can easily be captured with the macro recorder and saved with the spreadsheet. However, a different macro will be needed for each different number of peaks, because the block of cells representing the ``Proposed Model'' will be different for each number of peaks. For example, the template ``\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitter2Gaussian.xlsm}{CurveFitter2Gaussian.xlsm}\textcolor{color-3}{''} includes a macro named 'fit' for a 2-peak fit, activated by pressing \textbf{Ctrl-f}. Here is the text of that macro:

\texttt{Sub fit()}

\texttt{'}

\texttt{' fit Macro}

\texttt{'}

\texttt{' Keyboard Shortcut: Ctrl+f}

\texttt{'}

\texttt{SolverOk} 

\texttt{SetCell:="\$C\$12", MaxMinVal:=2, ValueOf:=0, ByChange:="\$C\$8:\$D\$9",} 

 \texttt{Engine:=1, EngineDesc:="GRG Nonlinear"}

\texttt{SolverSolve}

\texttt{End Sub}

 You can see that the text of the macro uses only two macro instructions: "SolverOK" and "SolverSolve". SolverOK specifies all the information in the "Solver Parameters" dialog box in its input arguments: 'SetCell' sets the objective as the percent fitting error in cell C12, 'MaxMinVal' is set to the second choice (Minimum), and 'ByChange' specifies the table of cells representing the proposed model (C8:D9) whose values are to be changed to minimize the objective in cell C12. The last argument sets the solver engine to 'GRC Nonlinear', the best one for iterative peak fitting. Finally, "SolverSolve" starts the Solver engine. You could easily modify this macro for curve fitter templates with other numbers of peaks just by changing the cells referenced in the 'ByChange' argument, e.g., C8:E9 for a 3-peak fit. In this case, though, it is probably just as easy to use the macro recorder to record a macro for each curve fitter template.

A more elaborate example of a spreadsheet using a macro is \href{https://terpconnect.umd.edu/~toh/spectrum/TransmissionFittingCalibrationCurve.xls}{TransmissionFittingCalibrationCurve.xls }(\href{https://terpconnect.umd.edu/~toh/spectrum/TransmissionFittingCalibrationCurve.png}{screen image}) that creates a calibration curve for a series of standard concentrations in the TFit method, which was previously described on \textcolor{color-3}{page} \pageref{ref-0338}\textcolor{color-3}{.} Here's a portion of that macro:



  \texttt{Range("AF10").Select}

  \texttt{Application.CutCopyMode = False}

  \texttt{Selection.Copy}

  \texttt{Range("A6").Select}

  \texttt{Selection.PasteSpecial Paste:=xlPasteValues, Operation:=xlNone, SkipBlanks \_}

  \texttt{:=False, Transpose:=False}

  \texttt{Calculate}

  \texttt{Range("J6").Select}

  \texttt{Selection.Copy}

  \texttt{Range("I6").Select}

  \texttt{Selection.PasteSpecial Paste:=xlPasteValues, Operation:=xlNone, SkipBlanks \_}

  \texttt{:=False, Transpose:=False}

  \texttt{Calculate}

  \texttt{SolverOk SetCell:="\$H\$6", MaxMinVal:=2, ValueOf:=0, ByChange:="\$I\$6", Engine:=1 \_}

  \texttt{, EngineDesc:="GRG Nonlinear"}

  \texttt{SolverOk SetCell:="\$H\$6", MaxMinVal:=2, ValueOf:=0, ByChange:="\$I\$6", Engine:=1 \_}

  \texttt{, EngineDesc:="GRG Nonlinear"}

  \texttt{SolverSolve userFinish:=True}

  \texttt{SolverSolve userFinish:=True}

  \texttt{SolverSolve userFinish:=True}

  \texttt{Range("I6:J6").Select}

  \texttt{Selection.Copy}

  \texttt{Range("AG10").Select}

  \texttt{Selection.PasteSpecial Paste:=xlPasteValues, Operation:=xlNone, SkipBlanks \_}

  \texttt{:=False, Transpose:=False}

The macro in this spreadsheet repeats this chunk of code several times, once for each concentration in the calibration curve (changing only the "AF10" in the first line to pick up a different concentration from the "Results table" in column AF). This macro uses several additional instructions, to select ranges ("Range...Select"), copy ("Selection.Copy") and paste ("Selection.PasteSpecial Paste:=xlPasteValues") values from one place to another and re-calculate the spreadsheet ("Calculate"). Each separate click, menu selection, or keypress creates one or more lines of macro text. The syntax is wordy but quite explicit and clear; you can learn quite a bit just by recording various spreadsheet actions and looking at the resulting macro text.

\label{ref-0378}\label{ref-0379}\label{ref-0380}

\section{Random walks and baseline correction\label{ref-0381}}

The \textit{random walk} was mentioned in the section on \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#Frequency}{signals and noise} (page \pageref{ref-0021}) as a type of low-frequency ("pink") noise. \href{https://en.wikipedia.org/wiki/Random\_walk}{Wikipedia }says: "A random walk is a mathematical formalization of a path that consists of a succession of random steps. For example, the path traced by a molecule as it travels in a liquid or a gas, the search path of a foraging animal, superstring behavior, the price of a fluctuating stock, and the financial status of a gambler can all be modeled as random walks, although they may not be truly random in reality." 

\InsImageInline{0.5}{l}{RandomWalkVsWhiteNoise.png}

  Random walks describe and serve as a model for many kinds of unstable behavior. Whereas white, 1/f, and blue noises are tethered to a mean value to which they tend to return, random walks tend to be more aimless and often drift off on one or another direction, possibly never to return. Mathematically, a random walk can be modeled as the cumulative sum of some random process, for example the 'randn' function. The graph on the right compares a 200-point sample of white noise (computed as 'randn' and shown in \textcolor{color-4}{blue}) to a random walk (computed as a cumulative sum, 'cumsum', and shown in \textcolor{color-19}{red}). \textit{Both samples are scaled to have the same standard deviation}, but their behavior is vastly different. The random walk has much more low-frequency behavior, in this case wandering off beyond the amplitude range of the white noise. This type of random behavior is very disruptive to the measurement process, distorting the shapes of peaks and causing baselines to shift and making them hard to define, and it cannot be reduced significantly by smoothing (as shown by \href{https://terpconnect.umd.edu/~toh/spectrum/NoiseColorTest.m}{NoiseColorTest.m}). In this example, the random walk has an overall positive slope and a "bump" near the middle that could be confused for a real signal peak (it is not; it is just noise). But another sample might have very \textit{different} behavior. Unfortunately, it is common to observe this behavior in experimental signals.

  To demonstrate the measurement difficulties, the script \href{https://terpconnect.umd.edu/~toh/spectrum/RandomWalkBaseline.m}{RandomWalkBaseline.m} simulates a Gaussian peak with randomly variable position and width, on a random walk baseline, with an S/N ratio of 15. The peak is measured by least-squares curve fitting methods using \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m} with two different methods of baseline correction to handle the random walk:

(a) a single-component Gaussian model (shape 1) with BaselineMode set to 1 (meaning a linear baseline is first interpolated from the edges of the data segment and subtracted from the signal): \texttt{peakfit([x;y],0,0,1,1,0,10,1)}; 

(b) a 2-component model, the first being a Gaussian (shape 1) and the second a linear slope (shape 26), with BaselineMode set to 1\texttt{: peakfit([x;y],0,0,2,[1 26],[0 0],10,0)}. 

In this case, the fitting error is lower for the second method, especially if the peak falls near the edges of the data range.


\begin{center}
\InsImageInline{0.5}{l}{RandomWalkBaselineMethod1.png}\InsImageInline{0.5}{l}{RandomWalkBaselineMethod2.png}
\end{center}


But the relative percent errors of the peak parameters show that the \textit{first method} gives a lower error for position and width, at least in this case. On average, the peak parameters are about the same. 

   \texttt{Position Error Height Error Width Error}

\texttt{Method a: 0.2772 3.0306 0.0125}

\texttt{Method b: 0.4938 2.3085 1.5418}

You can compare this to \href{https://terpconnect.umd.edu/~toh/spectrum/WhiteNoiseBaseline.m}{WhiteNoiseBaseline.m} which has a similar signal and S/N ratio, except that the noise is \textit{white}. Interestingly, the \textit{fitting error} with white noise is \textit{greater}, but the \textit{parameter errors} (peak position, height, width, and area) are \textit{lower,} and the residuals are more random and less likely to produce false noise peaks. This is because the random walk noise is \href{https://terpconnect.umd.edu/~toh/spectrum/RandomWalkFrequencySpectrum.png}{very highly concentrated at }\href{https://terpconnect.umd.edu/~toh/spectrum/RandomWalkFrequencySpectrum.png}{\textit{low frequencies}} where the signal frequencies usually lie, whereas white noise also has considerable power at \textit{higher frequencies}, which \textit{increases the fitting error} but does comparatively little damage to signal measurement accuracy. This may be counter-intuitive, but it is important to realize that fitting error does not \textit{always} correlate with peak parameter error. Bottom line: the random walk is troublesome.

Depending on the type of experiment, an instrumental design based on \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#Frequency}{modulation techniques} may help, and \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#EnsembleAveraging}{ensemble averaging} multiple measurements can help with any type of unpredictable random noise, which is discussed in the very next section.\label{ref-0382}\label{ref-0383}

\section{\textbf{Modulation and synchronous detection.}\label{ref-0384}\label{ref-0385}}

\InsImageInline{0.5}{l}{OpticalChopper.png}\href{https://terpconnect.umd.edu/~toh/spectrum/OpticalChopper.png}{}In some experimental designs it may be beneficial to apply the technique of \href{https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8\#q=modulation}{modulation}, in which one of the controlled independent variables is oscillated in a periodic fashion, and then to \href{http://www.thinksrs.com/downloads/PDFs/ApplicationNotes/AboutLIAs.pdf}{detect the resulting oscillation} in the measured signal. The right instrumental design can reduce or eliminate some types of noise and drift.

\InsImageInline{0.5}{l}{AmplitudeModulation2.png}A simple example applies to optical measurement systems like the one pictured on the right above. A light source illuminates a test object (DUT = ``Device Under Test'') and the resulting light from the test object is measured by the photodetector. Depending on the objective of the experiment and the arrangement of the parts, the detector might measure the light \textit{transmitted by}, \textit{reflected by}, \textit{scattered by}, or \textit{excited by} the light beam. An \href{https://en.wikipedia.org/wiki/Optical\_chopper}{optical chopper} rapidly and repeatedly interrupts the light beam falling on the test object so that the photodetector sees an oscillating signal, and the following electronic system is designed to measure \textit{only} the oscillating component and to ignore the constant (direct current) component. The advantage of this arrangement is that any interfering signals introduced \textit{after} the chopper - such as constant light that comes from the test object itself, or ambient light that leaks in from the outside, or any constant background signal generated by the photodetector itself - \textit{are not oscillating} and are thus rejected. This works best if the electronics are \textit{synchronized} to the chopper frequency. That's actually the function of the \href{https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8\#q=lock-in+amplifier}{lock-in amplifier}, which receives a synchronizing reference signal directly from the chopper to guarantee synchronization even if the chopper frequency were to vary (c.f. the interactive simulation on \url{https://terpconnect.umd.edu/~toh/models/lockin.html} and T. C. O'Haver, "Lock-in Amplifiers," \textit{J. Chem. Ed}. 49, March and April (1972). The lock-in amplifier is sometimes viewed as a "black box" with almost magical abilities, but in fact, it is performing a rather simple (but very useful) operation, as shown in this simulation.

\href{https://terpconnect.umd.edu/~toh/spectrum/AmplitudeModulation.m}{AmplitudeModulation.m} \href{https://terpconnect.umd.edu/~toh/spectrum/AmplitudeModulation2.png}{} is a Matlab/Octave script simulation of modulation and synchronous detection, in which the signal created when the light beam scans the test sample is modeled as a Gaussian band ('y'), whose parameters are defined in the first few lines of the script. As the spectrum of the sample is scanned, the light beam is amplitude modulated by the chopper, represented as a square wave defined by the bipolar vector 'reference', which switches between +1 and -1, shown in the top panel on the left. The modulation frequency is many times faster than the rate at which the sample is scanned. The light emerging from the sample, therefore, shows a finely chopped Gaussian ('my'), shown in the second panel on the left. But the \textit{total signal} seen by the detector also includes an unstable background introduced \textit{after} the modulation ('omy'), such as lighted emitted by the sample itself or detector background, which in this simulation this is modeled as a random walk (page \pageref{ref-0378}), which seriously distorts the signal, shown in the third panel. The detector signal is then sent to a lock-in amplifier that is synchronized to the reference waveform. The essential action of the lock-in is to multiply the signal by the bipolar reference waveform, \textit{inverting the signal} when the light is \textit{off} and \textit{passing it unchanged} when the light is \textit{on}. This causes the unmodulated background signal to be converted into a bipolar square wave, whereas the modulated signal is not affected because it is "off" when the reference signal is negative. The result ('dy') is shown in the 4\textsuperscript{th} panel. Finally, this signal is \href{https://en.wikipedia.org/wiki/Low-pass\_filter}{low-passed filtered} by the last stage in the lock-in amplifier to remove the modulation frequency, resulting in the recovered signal peak 'sdy' shown in the bottom panel. In effect, the modulation transforms the signal to a higher frequency ('frequency' in line 44) where low-frequency weighted noise on the baseline (line 50) is less intense.

These various signals are compared in the figure on the right. The original Gaussian signal peak ('y') is \InsImageInline{0.5}{l}{AmplitudeModulation.png}shown as the blue line, and the contaminating background ('baseline') is shown in black, in this case, modeled as a random walk. The total signal ('oy') that would have been seen by the detector if modulation were not used is shown in green. The signal distortion is evident, and any attempt to measure the signal peak in that signal would be greatly in error. The signal 'sdy' recovered by the modulation and lock-in system is shown in red and overlaid with the original signal peak 'y' in blue for comparison. The fact that the blue and red line are so close to each other indicates the extent to which this method is successful. To make a more quantitative comparison, this script also uses the \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit.m} function, which employs a least-squares method to measure the peak parameters in the original unmodulated total signal (green line) and in the modulated recovered signal (blue) and to compute the relative percent error in peak position, width, and width by both methods:

SignalToNoiseRatio = 4

\textbf{Relative \% Error: Position Height Width} 

  Original:   8.07  23.1    13.7

  Modulated:     0.11            0.22           1.01

Each time you run it you will get the \textit{same signal peak} but a very \textit{different} random walk background. The S/N ratio will vary from about 4 to 9. It is not uncommon to see a \textit{100-fold improvement} in peak height accuracy with modulation, as in the example shown here. (If you wish, you can change the signal peak parameters and the noise level in the first few code lines of this simulation. For an even greater challenge, change line 47 to "\texttt{baseline=10.*noise + cumsum(noise);}" to make the noise a mixture of white and random walk drift, which results in a really \href{https://terpconnect.umd.edu/~toh/spectrum/UglySignal.png}{ugly raw signal}; you can see that the white noise makes it through the synchronous detector but is reduced by the smoothing low pass filter in the last stage). In effect, the low-pass filter determines the frequency bandwidth of the lock-in system, but it also increases the response time to step changes (as in the \href{https://terpconnect.umd.edu/~toh/spectrum/FourierFilter.html\#MorseCode}{Morse Code example}). 

This improvement in measurement accuracy works only because the dominant random error, in this case, is:

 (a) introduced \textit{after} the modulation, and 

 (b) a mostly \textit{low}-frequency noise. 

If the noise were \textit{white}, there would be \textit{no} improvement, because white noise is the same at all frequencies; in fact, there would be a slight reduction in precision because the chopper blocks half of the light on average. If the sample (device under test) generates an "absorption" peak that starts a some positive value and then dips down to a lower value before returning, the demodulated output will be a negative-going peak rather than a positive peak (see \href{https://terpconnect.umd.edu/~toh/spectrum/AmplitudeModulationAbsorption.m}{AmplitudeModulationAbsorption.m}).

In a computer-interfaced experimental system, you may not actually need a physical lock-in amplifier. It is possible to simulate the effect in software, as is done in this simulation. You need only digitize both the modulated sample signal and the modulation reference signal, and then invert the total signal whenever the reference signal is "off". 

In some spectroscopic applications another useful type of modulation is ``\href{https://terpconnect.umd.edu/~toh/models/modspec.html}{wavelength modulation}'', in which the \textit{wavelength} of the light source oscillates over the wavelength region of an emission or absorption peak in the spectrum (reference 32); this is often used in atomic emission and absorption spectroscopy (references 25, 26) and in tunable diode laser spectroscopy applied to the measurement of gases such as methane, water vapor, and \href{http://www.umu.se/digitalAssets/120/120419\_wms.pdf}{carbon dioxide}, especially in remote sensing, where the sample may be far from the detector. Less commonly modulation techniques are also applied in ``AC'' (alternating current) \href{https://books.google.com/books?id=Q6CjAgAAQBAJ&pg=PA319&lpg=PA319&dq=electrochemical+modulation+lock-in+amplifier&source=bl&ots=SjGwnVJU8X&sig=uOaGPyxG6VHWkNrYik\_X\_aNLfCM&hl=en&sa=X&ved=0ahUKEwi2oeSwhofOAhUHrD4KHaEwB\_UQ6AEIRjAI\#v=onepage&q=electrochemical\%20modulation\%20lock-in\%20amplifier&f=false}{electrochemistry} and in \href{http://pubs.acs.org/doi/abs/10.1021/ac00280a040}{spectroelectrochemistry}.

\section{Measuring a buried peak\label{ref-0386}\label{ref-0387}\label{ref-0388}\label{ref-0389}}

This simulation explores the problem of measuring the height of a small peak (a "child peak") that is buried in the tail of a much stronger overlapping peak (a "parent peak"), in the especially challenging case that the smaller peak \textit{is not even visible} to the unaided eye. Three different measurement tools will be explored: \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{iterative least-squares}, \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html}{classical least-squares regression}, and \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{peak detection}, using the Matlab/Octave tools \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/cls.m}{cls.m}, or \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksG.m}{findpeaksG.m,} respectively. (Alternatively, you could use the corresponding \href{https://terpconnect.umd.edu/~toh/spectrum/functions.html\#spreadsheets}{spreadsheet templates}). 

In this example the larger peak is located at x=4 and has a height of 1.0 and a width of 1.66; the smaller measured peak is located at x=5 and has a height of 0.1; both have a width of 1.66. Of course, for the purposes of this simulation, we pretend that we do not necessarily know all these facts and we will try to find methods that will extract such information as possible from the data, even if the signal is noisy. The measured peak is small enough and close enough to the stronger overlapping peak (separated by \InsImageInline{0.5}{l}{SingleGaussian.png}less than the width of the peaks) that it \textit{never forms a maximum} in the total signal. So it \textit{looks} like there is only \textit{one} peak, as shown on the figure on the right. For that reason, the findpeaks.m function (which automatically finds peak maxima) will not be useful by itself to locate the smaller peak. Simpler methods for detecting the second peak also fail to provide a way to measure the smaller second peak, such as inspecting the derivatives of the signal (the smoothed fourth derivative shows \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothedFourthDerivative.png}{some evidence of asymmetry,} but that could just be due to the shape of the larger peak), or \href{https://terpconnect.umd.edu/~toh/spectrum/Deconvolution.html}{Fourier self-deconvolution} to narrow the peaks so they are distinguishable, but that is unlikely to be successful with this much noise. Least-squares methods work better when the signal-to-noise ratio is poor, and they can be fine-tuned to make use of available information or constraints, as will be demonstrated below.

The selection of the best method will depend on what is known about the signal and the constraints that can be imposed; this will depend on your knowledge of your experimental signal. In this simulation (performed by the Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/SmallPeak.m}{SmallPeak.m}), the signal is composed of two Gaussian peaks (although that can be changed if desired in line 26). The first question is: is there more than one peak there? If we perform an unconstrained iterative fit of a \textit{single} Gaussian to the data, as shown in the figure on the right, it shows little or no evidence of a second peak - the residuals look random. (If you could reduce the noise, or if you \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#EnsembleAveraging}{ensemble-average}\textcolor{color-3}{d} even as few as 10 repeat signals, then the noise would be low enough to see \href{https://terpconnect.umd.edu/~toh/spectrum/SingleGaussianLowerNoise.png}{evidence of a second peak}). However, as it is, there is nothing that pops out at you suggesting a second peak.

\InsImageInline{0.5}{l}{UnconstrainedFit.gif.png}But suppose we suspect that there \textit{should} be another peak of the same Gaussian shape just on the right side of the larger peak. We can try fitting a \textit{pair} of Gaussians to the data (figure on the right), but in this case, \textit{there is so much random noise that the fit is not stable.} When you run \href{https://terpconnect.umd.edu/~toh/spectrum/SmallPeak.m}{SmallPeak.m}, the script performs 20 repeat fits (``NumSignals'' in line 20) with the same underlying peaks but with 20 different random noise samples, revealing the stability (or instability) of each measurement method. The fitted peaks in Figure window 1 bounce around all over the place as the script runs. (If the animation is not visible, click \href{https://terpconnect.umd.edu/~toh/spectrum/UnconstrainedFit.gif}{this} link). The fitting error is on average \textit{lower} than the single-Gaussian fit, but that by itself does not mean that the peak parameters so measured will be reliable; it could just be "fitting the noise". \textit{If it were isolated all by itself}, the small peak would have an \href{https://terpconnect.umd.edu/~toh/spectrum/SmallPeakByItself.png}{S/N ratio of about 5} and it could be measured to a peak height precision of about 3\%, but the presence of the larger interfering peak makes the measurement much more difficult. (Hint: After running SmallPeak.m the first time, spread out all the figure windows so they can all be seen separately and do not overlap. That way you can compared the stability of the different methods more easily.)



But suppose that we have reason to expect that the \textit{two peaks will have the same width}, but we do not know what that width might be. We could try an \textit{equal width} Gaussian fit (peak shape \#6, shown in Matlab/Octave Figure window 2); the resulting fit is much more stable and shows that a small peak is located at about x=5 on the right of the bigger peak, shown below on the left. On the other hand, if we know the peak \textit{positions} beforehand, but not the widths, we can use a \textit{fixed-position} Gaussian fit (shape \#16) shown on the right (Figure window 3). In the very common situation where the objective is to measure an \textit{unknown concentration} of a \textit{known} component, then it is possible to prepare standard samples where the concentration of the sought component is high enough for its position or width to be determined with certainty. 

\InsImageInline{0.5}{l}{EqualWidth.png}\InsImageInline{0.5}{l}{FixedPositions.png}So far all these examples have used iterative peak fitting with at least one peak parameter (position \InsImageInline{0.5}{l}{cls.png}and/or width) unknown and determined by measurement. If, on the other hand, \textit{all} the peak parameters are known \textit{except} the peak height, then the faster and more direct \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html}{classical least-squares regression} (CLS) can be employed (Figure window 4). In this case, you need to know the peak position and width of both the measured and the larger interfering peaks (the computer will calculate their heights). If the positions and the heights really are constant and known, then this method gives the best stability and precision of measurement. It is also computationally faster, which might be important if you have lots of data to process automatically. 

The problem with CLS is that it fails to give accurate measurements if the peak position and/or width changes without warning, whereas two of the iterative methods (unconstrained Gaussian and equal-width Gaussian fits) can adapt to such changes. It some experiments it quite common to have small, unexpected shifts in the peak position, especially in chromatography or other flow-based measurements, caused by unexpected changes in temperature, pressure, flow rate or other instrumental factors. In SmallPeaks.m, such x-axis shifts can be simulated using the variable "xshift" in line 18. It is initially zero, but if you set it to something greater (e.g., 0.2) you will find that the equal-width Gaussian fit (Figure window 2) works better because it can keep up with the changes in x-axis shifts.

But with a greater x-axis shift (xshift=1.0) even the equal-width fit has trouble. Still, if we know the \textit{separation} between the two peaks, it is possible to use the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{findpeaksG }function to search for and locate the larger peak and to use that to calculate the position of the smaller peak. Then the CLS method, with the peak positions so determined for each separate signal, shown in Figure window 5 and labeled "findpeaksP" in the table below, works better. Alternatively, another way to use the findpeaks results is a variation of the equal-width iterative fitting method in which the first guess peak positions (line 82) are derived from the findpeaks results, shown in Figure window 6 and labeled "findpeaksP2" in the table below; that method does not depend on accurate knowledge of the peak widths, only their equality.

Each time you run SmallPeaks.m, \textit{all these methods are computed} ``NumSignals'' times (set in line 20) and compared in a table giving the average peak height accuracy of all the repeat runs: 

\texttt{xshift=0}

\texttt{Unconstr. EqualW FixedP FixedP\&W findpeaksP findpeaksP2}

\texttt{35.607 16.849 5.1375 4.4437 13.384 16.849}

\texttt{xshift=1}

\texttt{Unconstr. EqualW FixedP FixedP\&W findpeaksP findpeaksP2}

\texttt{31.263 44.107 22.794 46.18 10.607 10.808}

The bottom line is this: the more you know about your signals, the better you can measure them. A stable signal with \textit{known} peak positions and widths is the most precisely measurable in the presence of random noise ("FixedP\&W"), but if the positions or widths vary from measurement to measurement, different methods must be used, and precision is degraded because more of the available information is used to account for changes \textit{other} than the ones you want to measure.

\section{Signal and Noise in the Stock Market\label{ref-0390}\label{ref-0391}\label{ref-0392}}

From a signal-to-noise perspective, the stock market is an interesting example. A national or global stock market is an aggregation of large numbers of buyers and sellers of shares in publicly traded companies. They are described by stock market \textit{indexes}, which are computed as the weighted average of many selected stocks. For example, the \href{http://us.spindices.com/indices/equity/sp-500}{S\&P 500 index} is computed from the stock valuations of 500 large US companies. Millions of individuals and organizations participate in the buying and selling of stocks daily, so the S\&P 500 index is a prototypical "big data" conglomerate, reflecting the overall value of 500 of the largest companies in the largest stock market on earth. Other stock indices, such as the Russel 2000, include an even larger number of smaller companies. Individual stocks can fail or fall drastically in value, but the market indexes average out the performance of hundreds of companies.

A plot of the daily value, V, of the S\&P 500 index vs time, T, from 1950 through September of 2016 is shown in the following graphs.

\InsImageInline{0.5}{l}{SnPvs7percent1.png}  \InsImageInline{0.5}{l}{LogSnP_index_since1950logy.png}

Each plot contains 16608 data points, one for each business day, shown in red. The graph on the left plots V and the graph on the right plots the \textit{natural logarithm} of V, ln(V). There are considerable up-and-down fluctuations over time that can be related to historical events: the oil crisis of the 1970s, the tech boom and bust of 2000, the subprime mortgage crisis of 2008. (On page ~\ref{ref-0394}~\ref{ref-0394}\pageref{ref-0394}, I update these data to include the trade wars of 2019). Still, the \textit{long-term} trend of the value is upwards - the current value is over 100 times greater than its value in 1950. This is basically why people invest in the stock market, because on average, \textit{over the long run}, stock values eventually go up. The most common way to model this overall long-term increase with time is based on the \href{http://www.investopedia.com/terms/e/exponential-growth.asp}{equation for compound interest} that predicts the growth of investments that have a constant rate of return, such as savings accounts or certificates of deposit:

 V = S*(1 + R)\textsuperscript{T}

where V is the value, S is the starting value, R is the annual rate of return, and T is time. By itself, this expression would yield a smooth upward curving exponential curve, without all the peaks and dips. The values of S and R that result in the \textit{best fit} to the stock market data (shown by the blue lines in the graphs) can be determined in two ways:



(1) directly, using the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{iterative curve fitting method,} shown on the left above, or

(2) by taking the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Transforming}{logarithm of the values} and fitting those to a straight line, shown on the right above.

\href{https://terpconnect.umd.edu/~toh/spectrum/FitSandP.m}{FitSandP.m} is a Matlab/Octave script that performs both calculations using the data in \href{https://terpconnect.umd.edu/~toh/spectrum/SandPfrom1950.mat}{SandPfrom1950.mat}. When applied to the S\&P 500 index data, the rate of return R is about 0.07 (or 7\%), but interestingly these two methods give slightly \textit{different results}, even though the \textit{exact same data} are used for both, and even though \textit{both} methods yield the \textit{same} 7\% rate if they are applied to noiseless \textit{synthetic data} calculated from this expression. This difference between methods is caused by the irregularities in the stock data that deviate from a smooth line - in other words, the \textit{noise} - and it is exacerbated by the large range of the value data V over time and by the fact that the average return from 1950 to 1983 is slightly lower than that from 1983 to 2016. 

\InsImageInline{0.5}{l}{LogSnP_index_spectrum.png}

From the point of view of curve fitting, the deviation from a smooth curve described by the compound interest expression is just \textit{noise}. But from the point of view of the stock market investor, those deviations can be an opportunity and a warning. Naturally, most investors would like to know how the stock market will behave in the future, but that requires extrapolation beyond the range of the available data, which is always uncertain and dangerous. But still, it is \textit{most likely} (but not certain) that the \textit{long-term} behavior of the market (say, over a period of 10 years or more) will be like the past - that is, growing exponentially at about the same rate as before but with unpredictable fluctuations similar to what has occurred in the past. 

We can take a closer look at those fluctuations by inspecting the \textit{residuals} \textendash{} that is, subtracting the fitted curve from the raw data, as shown in \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{iSignal }(page \pageref{ref-0434}) on the left\href{https://terpconnect.umd.edu/~toh/spectrum/LogSnP\_index\_spectrum.png}{}. There are several notable features of this "noise". First, the \textit{deviations are roughly proportional to V} and thus relatively equal when plotted on a log scale. Second, the noise has a distinctly \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#Frequency}{\textit{low-frequency} }\href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#Frequency}{character} (page \pageref{ref-0032}); the \href{https://terpconnect.umd.edu/~toh/spectrum/HarmonicAnalysis.html}{periodogram} (lower panel, in red) shows peaks at 33, 16, 8, and 4 years. There are also, notably, numerous instances over the years when there is a sharp dip followed by a slower recovery close to the previous value. And conversely, every peak is eventually followed by a dip. The conventional advice in investing is to "buy low" (on the dips) and "sell high" (on the peaks). But of course, the problem is that you cannot reliably determine \textit{in advance} exactly where the peaks and dips will fall; you have only the past to guide you. Still, if the current market value is much \textit{higher} than the long-term trend, it will likely fall, and if the market value is much \textit{lower} than the long-term trend, it will likely rise, eventually. The only thing you can be sure of is that, in the long run, the market will eventually rise. This is the reason saving for retirement by investing in the stock market, and \textit{starting as soon as possible}, is so important: over a 30-year working life, the market is almost guaranteed to rise substantially. The most painless way to do this is with your employer's 401k or 403b automatic payroll withdrawal plan. You cannot invest in the stock market as a whole, but you can invest in \textit{index mutual funds} or \textit{exchange-traded funds} (ETFs), which are collections of stocks that are constructed to match or track the components of a market index. Such funds typically have \textit{very low management fees}, an important factor in selecting an investment. Other mutual funds attempt to ``beat the market'' by carefully buying and selling stocks to create a return that is greater than the overall market indexes; some are \textit{temporarily} successful in doing that, but they charge higher management fees. Mutual funds and ETFs are much less risky investments than individual stocks.

 Some companies periodically distribute payouts to investors called ``dividends''. Those dividends are independent of the day-to-day variations in stock price, so even if the stock value drops temporarily, you still get the same dividend. For that reason, it is important that you set your investment account to ``automatically reinvest dividends'', so when the share price drops, the dividends are buying shares at the \textit{lower price}. The S\&P 500 index values used above, called \textit{price returns}, did \textit{not} include dividend reinvestment; the \textit{total returns} with dividends reinvested (\url{https://en.wikipedia.org/wiki/S\%26P_500_Index\#Versions}) would have been \href{https://dqydj.com/sp-500-return-calculator/}{substantially higher}, closer to 11\%. With an average total annual return of 11\%, and starting with an investment of \$170 the first month - that's less than \$6 a day - and in\InsImageInline{0.5}{l}{StockMarketInvestmentSpreadsheet.png}creasing it 5\% each year, you could accumulate over \$600,000 over a 30-year working life, or \$1,000,000 if you continued investing an additional 5 years or if you began 5 years earlier, as shown by the \href{https://terpconnect.umd.edu/~toh/spectrum/InvestmentExample.xls}{spreadsheet }graphic above. And that is starting at just \textit{\$6 per day}, about the cost of a fancy coffee at Starbucks. Think about that the next time you see a line of young people waiting to order their daily coffee. The hard part is not so much giving up the coffee as it is finding a keeping a steady job that allows you to make routine automatic contributions to your retirement account.

To illustrate how much influence stock market volatility fluctuation (``noise'') has on the market gains, the Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/SnPsimulation.m}{SnPsimulation.m} adds \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#Spectroscopy}{proportional noise} (page \pageref{ref-0032}) to the compound interest calculation to mimic the S\&P data, performs the two curve fitting methods described above, repeats the allocations over and over with independent samples of proportional noise, and then calculates the mean and the relative standard deviation (RSD) of the rates of return. A typical result is:

\texttt{TrueRateOfReturn = 0.07} 

  \texttt{Measured Rate RSD}

\texttt{Coordinate transformation: 0.07112 8.9\%}

\texttt{Iterative curve fitting: 0.07972 19.9\%}

As you can see, the two methods do not agree. In this example, the return calculated by the iterative method is higher, but it \textit{could just have easily been the other way}. The fact is that the standard deviations are large, and the iterative method always has a higher standard deviation\textit{,} because it weights the higher values more heavily, where deviations from the line are higher, whereas the log transformation method weights the data more evenly. Even with this uncertainty, investing in a stock market index fund almost always performs better \textit{in the long run} than more predictable investments such as savings accounts or CDs, which have much lower rates of return.

In investing in the stock market, it is important to focus on the long-term trends and not to be frightened by the short-term up and down fluctuations, even though most of the news coverage understandably emphasizes the short-term. It is similar to the difference between \href{https://www.nasa.gov/mission\_pages/noaa-n/climate/climate\_weather.html}{weather} and \href{https://www.nasa.gov/mission\_pages/noaa-n/climate/climate\_weather.html}{climate}; the large and dramatic short-term \textit{weather} variations tend to disguise the much smaller long term \textit{climate} warming that is slowly melting the icecaps and \href{https://oceanservice.noaa.gov/facts/sealevel.html}{raising the sea levels} (whether it is caused by human activity or by natural causes alone or by a combination of both). Everyone talks about changes in the weather, but the climate changes so slowly that it is easy to conclude that it stays the same. The hour hand on the clock is never seen to move.

For a spreadsheet template that allows you to calculate the possible returns on long-term investments in stock market mutual funds, see \url{https://terpconnect.umd.edu/~toh/simulations/Investment.html}.\label{ref-0393}

\textbf{Note added in June 2020}. You might be wondering what effect more recent events have had on the overall stock market performance, in particular the trade wars of 2019 and the Coronavirus pandemic of 2020. Extending the data plotted above to include the S\&P results for the dates up to June 2020 has remarkably little effect, as seen in the log plot on the next page. The added data are just at the top right-hand corner and the fluctuations are small compared to the previous historical events. The overall average return is still about 7\%.\label{ref-0394}


\begin{center}
\InsImageInline{0.5}{l}{SandPto2020log.png}
\end{center}


The recent changes are more evident if you take a much closer look at the period from 2016 to 2020, below, for which the return over that short period was indeed greater, about 9.5\%. The 2019 and the 2020 dips, although they were quite sharp and caused a lot of anxiety at the time, recovered quickly and as a result had little effect on the overall long-term performance. When stocks drop, even for well-known and valid reasons, some investors buy shares at the reduced prices, and when stocks rise, especially when they hit all-time highs, some investors sell shares, to "lock in their gains". This behavior has been consistent throughout the years and acts as a natural brake on the fluctuations of the market. 


\begin{center}
\InsImageInline{0.5}{l}{SandP2016to2020log.png}
\end{center}


\section{Measuring signal-to-noise ratio in complex signals\label{ref-0395}}

In the section \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html}{Signals and Noise} on page \pageref{ref-0021}, I said: ``The quality of a signal is often expressed as the signal-to-noise (S/N) ratio, which is the ratio of the true signal amplitude ... to the standard deviation of the noise.'' That is a simple enough statement but automating the measurement of signal and the noise in real signals is not always straightforward. Sometimes it is difficult to separate or distinguish between the signal and the noise, because it depends not only on the numerical nature of the data, but also on the objectives of the measurement.

For a simple DC (direct current) signal, for example, measuring a fluctuating voltage, the signal is just the average voltage value and the noise is its standard deviation. This is easily calculated in a spreadsheet or in Matlab/Octave:

\texttt{{\textgreater}{\textgreater} signal=mean(NoisyVoltage);}

\texttt{{\textgreater}{\textgreater} noise=std(NoisyVoltage);}

\texttt{{\textgreater}{\textgreater} SignalToNoiseRatio=signal/noise}

\textbf{\InsImageInline{0.5}{l}{RectPulseExample.png}}But usually, things are more complicated. For example, if the signal is a rectangular pulse (as in the figure on the right) with constant random noise, then the simple formulation above will not give accurate results. If the signal is stable enough that you can get two successive signal recordings \textit{m1} and \textit{m2} that are \textit{identical except for the noise}, then you can \textit{simply subtract the signal out}: the standard deviation of the noise is then given by sqrt((std(\textit{m1}-\textit{m2})\textsuperscript{2})/2), where ``std'' is the \href{https://en.wikipedia.org/wiki/Standard\_deviation}{standard deviation} function (because \href{http://www.eso.org/~ohainaut/ccd/sn.html}{random noise adds quadratically}). But not every signal source is stable and repeatable enough for that to work perfectly. Alternatively, you can try to measure the average signal just over the top of the pulse and the noise only over the baseline interval before and/or after the pulse. That is not so hard to do by hand, but it is harder to automate with a computer, especially \textbf{\InsImageInline{0.5}{l}{GaussPulseExample.png}}if the position or width of the pulse changes. It is basically the same for smooth peak shapes like the commonly encountered Gaussian peak (as in the figure on the right)\href{https://terpconnect.umd.edu/~toh/spectrum/GaussPulseExample.png}{}. You can estimate the height of the peak by \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothing }it and then taking the maximum of the smoothed peak as the signal: \texttt{max(fastsmooth(y,10,3))}, but the accuracy would degrade if you choose too high or too low a smooth width. And clearly, all this depends on having a well-defined baseline in the data where there is only noise. It does not work if the noise varies with the amplitude of the peak.

In many cases, \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html}{curve fitting} can be helpful. For example, you could use \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peak fitting} or a \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{peak detector} to locate multiple peaks and measure their peak heights and their S/N ratios on a peak-to-peak basis, by computing the noise as the standard deviation of the difference between the raw data and the best-fit line over the top part of the peak. That is how \textcolor{color-3}{iSignal} (page \pageref{ref-0434}) measures S/N ratios of peaks. Also, iSignal has baseline correction capabilities that allow the peak to be measured relative to the nearby baseline.

\InsImageInline{0.5}{l}{image301.png}Curve fitting also works for complex signals of indeterminate shape that can be approximated by a \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Matlab}{high-order polynomial} or as the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{sum of a number of basic functions such as Gaussians}, as in the example shown on the right. In this example, five Gaussians are used to fit the data to the point where the residuals are random and unstructured. The residuals (shown in blue below the graph) are then just the noise remaining in the signal, whose standard deviation is easily computed using the built-in standard deviation function in a spreadsheet ("STDEV") or in Matlab/Octave ("std"). In this example, the standard deviation of the residuals is 111 and the maximum signal is 40748, so the percent relative standard deviation of the noise is 0.27\% and the S/N ratio is 367. (The positions, heights, and widths of the Gaussian components, usually the main results of the curve fitting, are not used in this case; curve fitting is used only to obtain a measure the noise via the residuals). The advantage of this approach over simply subtracting two successive measurements of the signal is that it adjusts for slight changes in the signal from measurement to measurement. The only assumption is that the signal is a smooth, low-frequency waveform that can be fitted with a polynomial or a collection of basic peak shapes and that the noise is random and mostly high-frequency compared the signal. But do not use too high a polynomial order' otherwise you are just "fitting the noise".

With periodic signal waveforms, the situation is a bit more complicated. As an example, consider the \InsImageInline{0.5}{l}{TestingOneTwoThreeEnvelope.png}audio recording of the spoken phrase "Testing, one, two, three" (click to download in \href{https://terpconnect.umd.edu/~toh/spectrum/testing123.mat}{.mat format} or in \href{https://terpconnect.umd.edu/~toh/spectrum/TestingOneTwoThree.wav}{WAV format}). The Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/PeriodicSignalSNR.m}{PeriodicSignalSNR.m} loads the audio file into a variable ``waveform'', then computes the average amplitude of the waveform (the ``envelope'') by smoothing (page \pageref{ref-0050}) the absolute value\index{\textcolor{color-3}{absolute value}} of the waveform:

\texttt{envelope = fastsmooth(abs(waveform), SmoothWidth, SmoothType);}

The result is plotted on the left, where the waveform is in blue and the envelope is in red. The signal is easy to measure as the maximum or perhaps the average of the waveform, but the noise is not so evident. The human voice is not reproducible enough to get a second identical recording to subtract out the signal as above. Still, there will often be gaps in the sound, during which the background noise will be dominant. In an audio (voice or music) recording, there will typically be such gaps at the beginning, then the recording process has already started but the sound has not yet begun, and possibly at other short periods when there are pauses in the sound. The idea is that, by monitoring the envelope \InsImageInline{0.5}{l}{TestingOneTwoThreeBackgroundRed.png}of the sound and noting when it falls below some adjustable threshold value, we can automatically record the noise that occurs in those gaps, whenever they may occur in a recording. 

In \href{https://terpconnect.umd.edu/~toh/spectrum/PeriodicSignalSNR.m}{PeriodicSignalSNR.m}, this operation is done in lines 26-32, and the threshold is set in line 12. The threshold value must be optimized for each recording. When the threshold value is set to 0.015 in the "Testing, one, two, three" recording, the resulting noise segments are located and marked in red in the plot on the right\href{https://terpconnect.umd.edu/~toh/spectrum/TestingOneTwoThreeBackgroundRed.png}{}. The program determines the average noise level in this recording simply by computing the standard deviation of those segments (line 46), then computes and prints out the peak-to-peak S/N ratio and the RMS (root mean square) S/N ratio.

\texttt{PeakToPeak\_SignalToNoiseRatio = 143.7296}

\texttt{RMS\_SignalToNoiseRatio = 12.7966}

\InsImageInline{0.5}{l}{FrequencySpectrum.png}The frequency distribution of the noise is also determined (lines 60-61) and shown in the figure on the left, using the PlotFrequencySpectrum function, or you could have used iSignal (page \pageref{ref-0434}) in the frequency spectrum mode (Shift-S). The spectrum of the noise shows a strong component very near 60 Hz, which is almost certainly due to \textit{power line pickup} (the recording was made in the USA, where AC power is 60 Hz); this suggests that better shielding and grounding of the electronics might help to clean up future recordings. The peak at 20 Hz is harder to attribute: perhaps it is the low hum of a fan or an air conditioner. The lack of strong components at 100 Hz and above suggests that the vocal sounds have been effectively suppressed at this threshold setting. The script can be applied to other sound recordings in WAV format simply by changing the file name and time axis in lines 8 and 9.\label{ref-0396}\label{ref-0397}\label{ref-0398}

\section{Dealing with wide-ranging signals: segmented\index{\textcolor{color-3}{segmented}} processing \label{ref-0399}\label{ref-0400}}

To facilitate the inspection of very large and complex signals, it is useful to be able to ``zoom in'' to different parts of the x-axis range, which can be done with the interactive Matlab tools \textit{iSignal} (page \pageref{ref-0435}), \textit{iPeak} (page \pageref{ref-0321}), and \textit{ipf.m} (page \pageref{ref-0463}) or by the Matlab/Octave function segplot.m (page \pageref{ref-0504}). Sometimes an experimental signal will vary so much across its x-axis range that it is impossible to find a single setting for operations like smoothing or peak detection that is optimized for all regions of the signal. It is always possible to break up the signal into pieces and treat each separately, for example using segplot, but in some cases, it is easier to use a single segmented application over the entire signal. That's the idea behind the Matlab/Octave functions and the Excel spreadsheet templates in this section.

\href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSmooth.m}{SegmentedSmooth.m}, illustrated on the right, is a segmented\index{\textcolor{color-3}{segmented}} variant of \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#Matlab}{fastsmooth.m},\href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSmoothDemo.png}{} which can be useful if the widths of the peaks or the noise level varies substantially across the signal. The syntax is that same as fastsmooth.m, except that the second input argument "smoothwidths" can be a \textit{vector}: \texttt{[SmoothedSignal,SmoothMatrix] = SegmentedSmooth (Y, smoothwidths,} \textbf{\InsImageInline{0.5}{l}{image305.png}}\texttt{type, ends)}. The function divides Y into several equal-length regions defined by the length of the vector 'smoothwidths', then smooths each region with a smooth of type 'type' and width defined by the elements of vector 'smoothwidths'. In the simple example in the figure on the right, \texttt{smoothwidths = [31 52 91]}, which divides up the signal into three regions and smooths the first region with smoothwidth 31, the second with 51, and the last with 91. \textit{Any number and sequence of smooth widths can be used.} Optionally returns ‘SmoothMatrix’ consisting of all segments assembled into a matrix. Type "help SegmentedSmooth" for other examples. 

\href{https://terpconnect.umd.edu/~toh/spectrum/DemoSegmentedSmooth.m}{DemoSegmentedSmooth.m} demonstrates the operation with different signals consisting of noisy variable-width peaks that get progressively wider, like the figure on the right. FindpeaksSL.m is the same thing for Lorentzian peaks.

\href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSmoothTemplate.xlsx}{SegmentedSmoothTemplate.xlsx} is a segmented multiple-width data smoothing \textit{spreadsheet} template, which is functionally like SegmentedSmooth.m. In this version, there are 20 segments. \href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSmoothExample.xlsx}{SegmentedSmoothExample.xlsx} is an example spreadsheet with data (\href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSmoothExample.png}{graphic}). A related spreadsheet \href{https://terpconnect.umd.edu/~toh/spectrum/GradientSmoothTemplate.xlsx}{GradientSmoothTemplate.xlsx} (\href{https://terpconnect.umd.edu/~toh/spectrum/GradientSmoothExample.png}{graphic}) performs a linearly increasing (or decreasing) smooth width across the entire signal, given only the start and end values, automatically generating as many segments are necessary. Of course, as is usual with spreadsheets, you will have to modify these templates for your number of data points, usually by inserting rows somewhere in the middle and then drag-copying down from above the insert, plus you may have to change the x-axis range of the graph. (In contrast, the Matlab/Octave functions do that automatically).

\href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedFouFilter.m}{SegmentedFouFilter.m} is a segmented version of \href{https://terpconnect.umd.edu/~toh/spectrum/FouFilter.m}{FouFilter.m} that applies different center frequencies and widths to different segments of the signal. The syntax, \texttt{[ffSignal,ffMatrix] = SegmentedFouFilter(y,samplingtime,centerFrequency,filterWidth,shape,mode),} is like FouFilter.m except that the two input arguments centerFrequency and filterWidth must be vectors with the values of centerFrequency of filterWidth for each segment. The signal is divided equally into several segments determined by the length of centerFrequency and filterWidth, which must be equal in length. Optionally returns ffMatrix of all segments assembled into a matrix. Type help SegmentedFouFilter for help and examples; the figure on the left shows Example 2. It may help to visualize the signal by using a related function, \href{https://terpconnect.umd.edu/~toh/spectrum/PlotSegFreqSpect.m}{PlotSegFreqSpect.m}, which creates and displays a \textit{time-segmented} Fourier power spectrum (see page \pageref{ref-0127} and following).

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSG.m}{findpeaksSG.m} is a variant of the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaks}{findpeaksG function}, with the same syntax, except that the four peak detection parameters can be \textit{vectors}, dividing up the signal into regions that are optimized for peaks of \textbf{\InsImageInline{0.5}{l}{TestPrecisionFindpeaskSG.png}}different widths. Any\href{https://terpconnect.umd.edu/~toh/spectrum/TestPrecisionFindpeaskSG.png}{} number of segments can be declared, based on the length of the SlopeThreshold input argument. (Note: you need only enter vectors for those parameters that you want to vary between segments; to allow any of the other peak detection parameters to remain unchanged across all segments, simply enter a single scalar value for that parameter; only the SlopeThreshold must be a vector). In the example shown on the left, the script \href{https://terpconnect.umd.edu/~toh/spectrum/TestPrecisionFindpeaskSG.m}{TestPrecisionFindpeaksSG.m} creates a noisy signal with three peaks of widely different widths, measures the peak positions, heights and widths of each peak using findpeaksSG, and prints out the percent relative standard deviations of parameters of the three peaks in 100 measurements with independent random noise. With 3-segment peak detection parameters, findpeaksSG reliably detects and accurately measures all three peaks. In contrast, findpeaksG, tuned to the middle peak (using line 26 instead of line 25), measures the first and last peaks poorly. You can also see that the precision of peak parameter measurements gets progressively better (smaller relative standard deviation) the larger the peak widths, simply because there are more data points in those peaks. (You can change any of the variables in lines 10-18). 

A related function is \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSGw.m}{\textbf{findpeaksSGw.m}} which is like the above except that is uses \textit{wavelet denoising} (page \pageref{ref-0177}) instead of smoothing. It takes the wavelet level rather than the smooth width as an input argument. The script \href{https://terpconnect.umd.edu/~toh/spectrum/TestPrecisionFindpeaksSGvsW.m}{TestPrecisionFindpeaksSGvsW.m} compares the precision and accuracy for peak position and height measurement for both the \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSGw.m}{findpeaksSG.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSGw.m}{findpeaksSGw.m} functions.

\textbf{\InsImageInline{0.5}{l}{DemoFindPeaksSb.png}}\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSG.m}{findpeaksSb.m} is a segmented variant of the \textcolor{color-3}{findpeaksb.m} function. \href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindPeaksSb.png}{}It has the same syntax as findpeaksSb, except that the arguments "SlopeThreshold", "AmpThreshold", "smoothwidth", "peakgroup", "window", "PeakShape", "extra", "NumTrials", "BaselineMode", and "fixedparameters" can all be optionally scalars or \textit{vectors with one entry for each segment}. This allows the function to handle widely varying signals with peaks of very different shapes and widths and backgrounds. In the example on the right, the Matlab/Octave script DemoFindPeaksSb.m creates a series of Gaussian peaks whose widths increase \textit{by a factor of 25} and that are superimposed in a curved baseline with random white noise that increases gradually across the signal. In this example, \textit{four segments} are used, changing the peak detection and curve fitting values so that all the peaks are measured accurately.

\texttt{SlopeThreshold = [.01 .005 .002 .001];}

\texttt{AmpThreshold = 0.7;}

\texttt{SmoothWidth = [5 15 30 35];}

\texttt{FitWidth = [10 12 15 20];}

\texttt{windowspan = [100 125 150 200];}

\texttt{peakshape = 1;}

\texttt{BaselineMode = 3;}

The script also computes the relative percent error of the measurement of peak position, height, width, and area for each peak.



\href{https://terpconnect.umd.edu/~toh/spectrum/measurepeaks.m}{measurepeaks.m} is an automatic peak detector peaks of arbitrary shape. It is based on findpeaksSG, with which it shares the first 6 input arguments; the four peak detection arguments can be \textit{vectors} to accommodate signals with peaks of widely varying widths. It returns a \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest.txt}{table} containing the peak number, peak position, absolute peak height, peak-valley difference, perpendicular drop area, and tangent skim area of each peak it detects. If the last input argument ('plots') is set to 1, it will \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest.png}{plot the \InsImageInline{0.5}{l}{HeightAndAreaTest2.png}entire signal }with numbered peaks and will also \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest2.png}{plot the} \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest2.png}{\textit{individual} peaks} with the peak maximum, valley points, and tangent lines marked (as shown on the right). \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest2.png}{}Type ``help measurepeaks'' and try the examples there or run \href{https://terpconnect.umd.edu/~toh/spectrum/testmeasurepeaks.m}{testmeasurepeaks.m} (\href{https://terpconnect.umd.edu/~toh/spectrum/testmeasurepeaks.gif}{graphic animation}). The related functions \href{https://terpconnect.umd.edu/~toh/spectrum/autopeaks.m}{autopeaks.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/autopeaksplot.m}{autopeaksplot.m} are similar, except that the four peak detection parameters \textit{can be} \textit{omitted} and the function will calculate estimated initial values.

The script \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndArea.m}{HeightAndArea.m} tests the accuracy of peak height and area measurement with signals that have multiple peaks with variable width, noise, background, and peak overlap. Generally, the values for absolute peak height and perpendicular drop area (page \pageref{ref-0192}) are best for peaks that have no background, even if they are slightly overlapped, whereas the values for peak-valley difference and for tangential skim area are better for isolated peaks on a straight or slightly curved background. Note: this function uses smoothing (specified by the SmoothWidth input argument) only for peak detection; it performs its measurements on the raw unsmoothed y data. If the raw data are noisy, it may be best to smooth the y data yourself before calling measurepeaks.m, using any smooth function of your choice.

Other segmented\index{\textcolor{color-3}{segmented}} functions. The same segmentation code used in \href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSmooth.m}{SegmentedSmooth.m} (lines 53-65) can be applied to other functions simply by editing the first line in the first for/end loop (line 59) to refer to the function that you want to apply in a segmented fashion. For example, segmented \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html\#segmented}{peak sharpening} can be useful when a signal has multiple peaks that vary in width, and segmented \href{https://terpconnect.umd.edu/~toh/spectrum/Deconvolution.html}{deconvolution }can be useful when a signal has multiple peaks that vary in width or tailing vary substantially across the signal: \href{https://terpconnect.umd.edu/~toh/spectrum/SegExpDeconv.m}{SegExpDeconv(x,y,tc)} deconvolutes y with a vector of exponential functions whose time constants are specified by the vector tc. \href{https://terpconnect.umd.edu/~toh/spectrum/SegExpDeconvPlot.m}{SegExpDeconvPlot.m} is the same except that it plots the original and deconvoluted signals and \textit{shows the divisions between the segments by vertical magenta lines}.

\section{Measurement Calibration\label{ref-0401}\label{ref-0402}\label{ref-0403}\label{ref-0404}}

Most scientific measurements involve the use of an instrument that measures something else and converts it to the desired measure. Examples are simple weight scales (which first measure the compression of a spring), thermometers (which measure thermal expansion), pH meters (which measure a voltage), and devices for measuring you heart rate, hemoglobin in blood, CO\textsubscript{2} in air, or sugar in wine grape juice (which measure a light beam). These instruments are \textit{single purpose}, designed to measure one quantity, and automatically convert what they first measure into the desired quantity and display it directly. But to ensure accuracy, such instruments must be \href{https://en.wikipedia.org/wiki/Calibration}{\textit{calibrated}}, that is, used to measure one or more calibration standards of known accuracy, such as a standard weight or a sample that is carefully prepared to a known temperature, pH, or sugar content. Most are pre-calibrated at the factory for the measurement of a specific substance in a specific type of sample.

\textbf{Analytical calibration}. In contrast to single-purpose measurements, \textit{general purpose} instrumental methods are used to measure the quantity of many different chemical components in various types of samples. These methods include various kinds of spectroscopy, chromatography, and electrochemistry, or combination techniques like ``\href{https://en.wikipedia.org/wiki/Gas\_chromatography\%96mass\_spectrometry}{GC-mass spec}''. These must also be calibrated, but because those instruments can be used to measure a wide range of compounds or elements, they must be calibrated \textit{by the user} for each substance and for each type of sample. Usually, this is accomplished by carefully preparing (or purchasing) one or more ``standard samples'' of known concentration, such as solution samples in a suitable solvent. Each standard is inserted or injected into the instrument, and the resulting instrument readings are plotted against the known concentrations of the standards, using \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#MathDetails}{least-squares calculations} to compute the \textit{slope} and \textit{intercept}, as well as the standard deviation of the slope (\textit{sds}) and intercept (\textit{sdi}). Then the "unknowns" (that is, the samples whose concentrations are to be determined) are measured by the instrument and their signals are converted into concentrations with the aid of the calibration curve. If the calibration is linear, the sample concentration C of any unknown is given by (A - \textit{intercept}) / \textit{slope}, where A is the measured signal (height or area) of that unknown. The predicted standard deviation in the sample concentration is C*SQRT((\textit{sdi}/(A-\textit{intercept}))\textasciicircum{}2+(\textit{sds}/\textit{slope})\textasciicircum{}2) by the \href{https://terpconnect.umd.edu/~toh/spectrum/ErrorPropagation.pdf}{rules for propagation of error}. All these calculations are done in the spreadsheet template \href{https://terpconnect.umd.edu/~toh/models/CalibrationLinear.xls}{CalibrationLinear.xls}. In some cases the thing measured cannot be detected directly but must undergo a chemical reaction that makes it measurable; in that case, the exact same reaction must be carried out on all the standard solutions and unknown sample solutions, as \href{http://academics.wellesley.edu/Biology/Concepts/Html/standardcurve.html}{demonstrated in the nice animation} \textcolor{color-3}{at this link,} thanks to Cecilia Yu of Wellesley College.

Various calibration methods are used to compensate for problems such as random errors in standard preparation or instrument readings, \href{https://en.wikipedia.org/wiki/Matrix\_\%28chemical\_analysis\%29}{interferences}, \href{https://terpconnect.umd.edu/~toh/models/CalibrationCurve.html\#Drift}{drift}, and \href{https://terpconnect.umd.edu/~toh/models/Bracket.html\#Background}{non-linearity} in the relationship between concentration and instrument reading. For example, the \href{https://terpconnect.umd.edu/~toh/models/Bracket.html\#Multiple\_Addition}{standard addition calibration technique} can be used to compensate for \href{https://terpconnect.umd.edu/~toh/models/Bracket.html\#Background}{multiplicative interferences}. I have prepared a series of ``fill-in-the-blanks'' templates for various calibrations methods, with \href{https://terpconnect.umd.edu/~toh/models/CalibrationCurve.html\#Instructions}{instructions}, as well as a series of s\href{https://terpconnect.umd.edu/~toh/models/Bracket.html}{preadsheet-based simulations} of the \href{https://terpconnect.umd.edu/~toh/spectrum/ErrorPropagation.pdf}{error propagation} in widely-used analytical calibration methods, including a \href{https://terpconnect.umd.edu/~toh/models/Bracket.html\#assignment}{step-by-step exercise}.

\InsImageInline{0.5}{l}{PeakCalibrationCurve.png}\textbf{Calibration and signal processing}. Signal processing often intersects with calibration. For example, if you use \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{\textit{smoothing} or \textit{filtering}} to reduce noise, or \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html}{\textit{differentiation} }to reduce the effect of background, or measure \href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html}{\textit{peak area}} to reduce the effect of peak broadening, or use \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#Modulation}{\textit{modulation} }to reduce the effect \href{https://terpconnect.umd.edu/~toh/spectrum/PeakCalibrationCurve.png}{}of low-frequency drift, then you \textit{must} use the exact same signal processing for both the standard samples and the unknowns, because the choice of signal processing technique can have a big impact on the magnitude and even on the \textit{units} of the resulting processed signal (as for example in the \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html\#Spectroscopy}{derivative technique} and in choosing between \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#HeightWidth}{peak height and peak area}).

\href{https://terpconnect.umd.edu/~toh/spectrum/PeakCalibrationCurve.m}{PeakCalibrationCurve.m} is a Matlab/Octave example of this. This script simulates the calibration of a \href{https://en.wikipedia.org/wiki/Flow\_injection\_analysis}{flow injection} system that produces signal peaks that are related to an underlying concentration or amplitude ('amp'). In this example, six known standards are measured sequentially, resulting in six separate peaks in the observed signal. (We assume that the detector signal is linearly proportional to the concentration at any instant). To simulate a more realistic measurement, the script adds four sources of "disturbance" to the observed signal:

a. \textit{random} \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html}{white noise} added to all the signal data points, controlled by the variable "Noise"; 

b. \textit{background} - broad curved background of \textit{random amplitude}, tilt, and curvature, controlled by "background"; 

c. \textit{broadening} - \href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html}{exponential peak broadening} that \textit{varies randomly} from peak to peak, controlled by "broadening"; 

d. a final \textit{smoothing} before the peaks are measured, controlled by "FinalSmooth".

The script uses \href{https://terpconnect.umd.edu/~toh/spectrum/measurepeaks.m}{measurepeaks.m} as an internal function to determine the absolute peak height, peak-valley difference, perpendicular drop area, and tangent skim area (page \pageref{ref-0184}). It plots separate calibration curves for each of these measures in Matlab Figure windows 2-5 against the true underlying amplitudes (in the vector "amp"), fitting the data to a straight line and computing the slope, intercept, and R2. (If the detector response were non-linear, a quadratic or cubic least squares would work better). The slope and intercept of the best-fit line are different for the different methods, but if the R\textsuperscript{2 }is close to 1.000, a successful measurement can be made. (If all the random disturbances are set to zero in lines 33-36, the R\textsuperscript{2 }values will all be 1.000. Otherwise, the measurements will not be perfect, and some methods will result in better measurements - R\textsuperscript{2 }closer to 1.000 - than others). Here is a typical result:

\InsImageInline{0.5}{l}{PeakCalibrationCurve2.png}\texttt{Peak Position PeakMax Peak-val. Perp drop Tan skim}

 \texttt{1 101.56 1.7151 0.72679 55.827 11.336}

 \texttt{2 202.08 2.1775 1.2555 66.521 21.425}

 \texttt{3 300.7 2.9248 2.0999 58.455 29.792}

 \texttt{4 400.2 3.5912 2.949 66.291 41.264}

 \texttt{5 499.98 4.2366 3.7884 68.925 52.459}

 \texttt{6 601.07 4.415 4.0797 75.255 61.762}

 \texttt{\textbf{R2 values:}}         \texttt{\textbf{0.9809 0.98615 0.7156 0.99824}}

In this case, the tangent skim method works best, giving a linear calibration curve (shown on the right) with the highest R2.

In this type of application, the peak heights and/or area measurements do not actually have to be \textit{accurate}, but they must be \textit{precise}. That is because the objective of an analytical method such as flow injection or chromatography is \textit{not} to measure the peak \textit{heights} and \textit{areas}, but rather to measure \textit{concentrations}, which is why calibration curves are used. \href{https://terpconnect.umd.edu/~toh/spectrum/TrueVsMeasuredAreas.png}{Figure window 6} shows the correlation between the measured tangent skim areas and the actual true areas under the peaks in the signal shown above, right; the slope of this plot shows that the tangent skim areas are about 6\% lower than the true areas, but that does not make a difference in this case because the standards and the unknown samples are measured the same way. In some \textit{other} applications, you may need to measure the peak heights and/or areas accurately, in which case curve fitting is generally the best way to go.

If the peaks partly overlap, the measured peak heights and areas may be affected. To reduce the problem, it may be possible to reduce the overlap by using \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html}{peak sharpening methods}, for example the \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html}{derivative method}, \href{https://terpconnect.umd.edu/~toh/spectrum/Deconvolution.html}{deconvolution }or the \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html\#power}{power transform method}, as demonstrated by the self-contained Matlab/Octave function \href{https://terpconnect.umd.edu/~toh/spectrum/PowerTransformCalibrationCurve.m}{PowerTransformCalibrationCurve.m}.

\textbf{Curve fitting the signal data.} Ordinarily in curve fitting methods, such as the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html}{classical least-squares} (CLS) method and in \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{iterative nonlinear least-squares}, the selection of a model shape is very important. However, in the \textit{quantitative analysis} applications of curve fitting, where the peak height or area measured by curve fitting is used only to determine the concentration of the substance that created the peak by constructing a \href{https://terpconnect.umd.edu/~toh/models/CalibrationCurve.html}{calibration curve}, having the exact model shape is \textit{surprisingly uncritical}. The Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/PeakShapeAnalyticalCurve.m}{PeakShapeAnalyticalCurve.m} shows that, for a single isolated peak whose shape is constant and independent of concentration, if the wrong model shape is used, the peak \textit{heights} measured by curve fitting will be inaccurate, but that error will be \textit{exactly the same} for the unknown samples and the \InsImageInline{0.5}{l}{WrongShape.png}known calibration standards, so the error will ``cancel out'' and the measured concentrations will still be accurate, provided you use the \textit{same} inaccurate model for both the known standards and the unknown samples. \href{https://terpconnect.umd.edu/~toh/spectrum/WrongShape.png}{}In the example shown on the right, the peak shape of the actual peak is Gaussian (blue dots), but the model used to fit the data is Lorentzian (red line). That is an intentionally bad fit to the signal data; the R\textsuperscript{2 }value for the fit to the signal data is only 0.962 (a \InsImageInline{0.5}{l}{PeakShapeAnalyticalCurve2.png}poor fit by the standards of measurement science). The result of this is that the \textit{slope} of the calibration curve (shown below on the left) is \textit{greater than expected}; it should have been 10 (because that's the value of the ``sensitivity'' in line 18), but it is actually 10.867 in the figure on the left\href{https://terpconnect.umd.edu/~toh/spectrum/PeakShapeAnalyticalCurve2.png}{}, but nevertheless, the \textit{calibration curve is still linear} and its R\textsuperscript{2 }value is 1.000, meaning that the analysis should be accurate. (Note that curve fitting is actually applied \textit{twice} in this type of application, once using iterative curve fitting to fit the \textit{signal data}, and then again using polynomial curve fitting to fit the \textit{calibration data}).

Despite all this, it is still better to use as accurate a model peak shape as possible for the signal data, because the percent fitting error or the R\textsuperscript{2 }\textit{of the signal fit} can be used as a warning that something unexpected is wrong, such as an increase in noise or the appearance of an interfering peak from a foreign substance.

\section{Numerical precision of computer software\label{ref-0405}\label{ref-0406}\label{ref-0407}}

Computations carried out by computer software with non-integer numbers have a natural limit to the precision with which they can be represented; for example, the number 1/3 is represented as 0.3333333..., using a large but finite number of ``3''s, whereas theoretically there is an \textit{infinite} string of ``3''s in the decimal representation of 1/3. It is the same with irrational numbers such as "pi" and the square root of 2; they can never have an \textit{exact} decimal representation. In principle, these tiny errors could accumulate in a very complex multiple-step calculation and could conceivably become a significant source of error. In most applications to scientific computation, however, these limits will be minuscule compared to the errors and random noise that is already present in most real-world measurements. But it is best to know what those numerical limits are, under what circumstances they might occur, and how to minimize them.

\textbf{Multicomponent spectroscopy.} Probably the most common calculation where numerical precision is an issue is in the matrix methods that are used in \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html}{multicomponent spectroscopy}. In the derivation of the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html\#cls}{Classical Least-squares (CLS)} method, the \href{https://www.mathsisfun.com/algebra/matrix-inverse.html}{\textit{matrix inverse}} is used to solve large systems of linear equations. The matrix inverse is a standard function in programming languages such as \textit{Matlab}, \textit{Octave}, Wolfram's \textit{Mathematica}, and in spreadsheets. But if you use that function in Matlab, the function name (``inv'') is \textit{automatically flagged by the editor} with the following warning:

``\texttt{For solving a system of linear equations, the inverse of a matrix is primarily of theoretical value. Never use the inverse of a matrix to solve a linear system Ax=b with x=inv(A)*b, because it is slow and inaccurate.... Instead of multiplying by the inverse, use matrix right division (/) or matrix left division (\textbackslash ). That is: Replace inv(A)*b with A\textbackslash b...[and]...replace b*inv(A) with b/A}''

\InsImageInline{0.5}{l}{RegressionNumericalPrecisionTest.png}"Slow and inaccurate"? Scary words. But how serious a problem is this really in actual applications? To answer that question, the Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/RegressionNumericalPrecisionTest.m}{RegressionNumericalPrecisionTest.m} applies the CLS method to a mixture of two \textit{very closely-spaced} \textit{noiseless} overlapping Gaussian peaks (blue and green lines in the figure on the left) using three different mathematical formulations of the least-squares calculation that give different results. The difficulty of such a measurement depends on the ratio of the peak separation to the peak half-width; small ratios mean very highly overlapped peaks which are hard to measure accurately. In this example the separation-to-width ratio is 0.0033, which is very small (i.e., difficult); this is equivalent to trying to measure a mixture of two absorption spectroscopy peaks that are 300 nm wide and \textit{separated by only 1 nm}, a tiny difference that you would not even notice with the naked eye. The results of this script show that the matrix inverse ("inv") method does indeed have an \textit{error thousands of times larger} than the method using matrix division, but even the matrix division error is still very small. Practically, the difference between these methods is unlikely to be significant when applied to real experimental data, because even the tiniest bit of signal instability (like that caused by small changes in the temperature of the sample or random noise in the signal, which you can simulate in line 15) produces a far greater error. So basically, that warning message is the voice of a mathematician or computer programmer, \textit{not} an experimental scientist. 

\InsImageInline{0.5}{l}{DemoADCNumericalNoise.png}\textbf{Analog-to-digital resolution}. Potentially more significant than the computer's numerical resolution is the resolution of the \textit{analog-to-digital converter} (ADC) that is used to convert analog signals (e.g., voltage) to a number. The Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/RegressionADCbitsTest.m}{RegressionADCbitsTest.m} demonstrates this, with two slightly overlapping Gaussian bands with a \textit{large} (\textit{50-fold) difference in peak height} (blue and green lines in the figure on the right); in this case the separation to width ratio is 0.25, much larger (i.e. easier) than the previous example. For this example, the simulation shows that the relative percent error of peak height measurement is 0.19\% for the larger peak and 6.6\% for the smaller peak. You can change the resolution of the simulated analog-to-digital converter in the \textit{number of bits} (line 9). The amplitude resolution of an analog-to-digital converter is 2 raised to the power of the number of bits. Common ADC resolutions are 10, 12, and 14 bits, corresponding to resolutions of one part in 1024, 4096 and 16384, respectively. Of \InsImageInline{0.5}{l}{DerivativeNumericalPrecisionFigure1.png}course, the effective resolution for the \textit{smaller} peak, in this case, is 50 times less, and you cannot simply turn up the amplification on the smaller peak without overloading the ADC for the larger one. Surprisingly, if \textit{most} of the noise in the signal is this kind of digitization noise, it may help to \textit{add some additional random noise} (specified in line 10 in this script), as was seen on page \pageref{ref-0365}.

\textbf{Differentiation}. \href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeNumericalPrecisionFigure1.png}{}Another application where you can see numerical precision noise is in \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html}{\textit{differentiation}}, which involves the subtraction of very nearly equal adjacent numbers in a data series. The self-contained Matlab/Octave function \href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeNumericalPrecisionDemo.m}{DerivativeNumericalPrecisionDemo.m} shows how the numerical precision limits of the computer affect the first through fourth derivatives of a Gaussian band that is very finely sampled (over 16,000 points in the half-width in this case) and that has no added noise. The plot on the left shows the four waveforms on the right, and their frequency spectra are shown in the figure just below. The numerical precision limit of the computer creates random noise at very high frequencies, which are emphasized by differentiation. In the frequency spectra below, the big low-frequency bump near a frequency of 10\textsuperscript{-2} is the \textit{signal} and everything above that is numerical \textit{noise}. The lower-order derivatives are seldom a problem, but by the time you reach the fourth derivative, those noise frequencies approach the strength of the signal frequencies, as you can see in the frequency spectrum of the fourth derivative in the lower right. But this noise is only a very high-frequency noise, so \InsImageInline{0.5}{l}{DerivativeNumericalPrecisionFigure2.png}smoothing with as little as a 3-point sliding average smooth removes most of it \href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeNumericalPrecisionFigure3.png}{(click to view)}. 

An alternative derivative method based on the Fourier Transform (page 84) has slightly lower numerical errors but is seldom used in practice (reference 88). The fundamental difference between the two is that the finite difference method works locally, on one small segment of the data at a time, whereas the FT method works globally because each frequency in the Fourier representation extends throughout the entire time domain. The Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/FDvsFTderivative.m}{FDvsFTderivative.m} compares the  numerical errors of finite difference (FD) and Fourier transform (FT) methods of differentation. Creates a very broad, finely sampled Gaussian peak and then computes its fourth derivative both ways. (The noise is casued only by software numerical resolutiion limitations). The result (\href{https://terpconnect.umd.edu/~toh/spectrum/FFDvsFTderivative.png}{graphic}) is that the numerical errors are lower for the FT method near the peak but are greater far from the peak center.

\textbf{Smoothing}. Finally, there might potentially be a numerical problem with the \href{http://terpconnect.umd.edu/\%7Etoh/spectrum/fastsmooth.m}{fastsmooth }algorithm, covered in the section on \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothing}, because it is a \textit{recursive} algorithm that uses the results of a previous step in the calculation to calculate the next step. The numerical precision of fastsmooth.m is shown by the Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/FastsmoothNumericalPrecisionTest.m}{FastsmoothNumericalPrecisionTest.m}. Even for 4000-point P-spline smooth applied to a 100,000-point signal, the numerical noise relative standard deviation is only 0.00027\%, and most of that occurs in the edges of the signal (first 4\% and last 4\% of the points); the error over 90\% of the signal is \textit{orders of magnitude less}, a negligible problem in most cases.

\section{Miniaturized signal processing: The Raspberry Pi\label{ref-0408}\label{ref-0409}\label{ref-0410}}

Signal processing does not necessarily require expensive computer systems. The Raspberry Pi is a remarkably tiny and inexpensive computer board that is about the \textit{size of a deck of cards} and \href{https://www.amazon.com/gp/product/B01CD5VC92/ref=s9\_dcacsd\_dcoop\_bw\_c\_x\_1\_w}{\textit{costs \$38}}\textit{!} \href{https://www.raspberrypi.org/blog/raspberry-pi-3-model-bplus-sale-now-35/}{Version 3 B+} has a \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#Top}{}1.4GHz 64-bit quad-core ARMv8 CPU with 1GB RAM, 4 USB ports, 40 general-purpose input-output pins, HDMI port, 300 mbps Ethernet port, audio jack and composite video, video camera and display interfaces, micro SD card slot for mass storage, VideoCore IV 3D \InsImageInline{0.5}{l}{image315.png}graphics core, 802.11ac Wireless LAN, and Bluetooth 4.2. You can get it with a bunch of installed software, including a version of the Linux operating system, a simple but effective graphical desktop modeled on Windows, a Web browser, the complete \href{https://en.wikipedia.org/wiki/LibreOffice}{LibreOffice suite}, Wolfram's \href{http://www.wolfram.com/raspberry-pi/?source=footer}{\textit{Mathematica}} (\href{https://terpconnect.umd.edu/~toh/spectrum/MathematicaRaspberryPi.png}{screenshot}), several programming languages, a bunch of games (including \href{https://www.raspberrypi.org/learning/getting-started-with-minecraft-pi/}{\textit{Minecraft}}), and various utilities. \href{https://www.raspberrypi.org/help/faqs/\#softwareRun}{All of these are installed by default} on the Raspberry Pi's \href{https://www.raspberrypi.org/downloads/noobs/}{operating system installer}. (There are even smaller and cheaper models called the \href{https://www.raspberrypi.org/blog/raspberry-pi-zero-w-joins-family/}{Zero }and the \href{https://www.raspberrypi.org/products/raspberry-pi-pico/}{Pico} microcontroller, that cost \$4  -  \$5. Because of their low cost and small size, these models are ideal in situations where they might be damaged or lost, as in rocket or balloon-borne experiments). The new \href{https://www.zdnet.com/article/raspberry-pi-400-is-out-70-for-a-complete-pc-with-a-faster-pi-4-in-a-keyboard/}{Raspberry Pi 4}, a \$70 version that is built into a full keyboard and runs Windows 10, needs only a monitor and a mouse.

To build a complete computer from version 3, you need a 5 volt, 2 amp power supply, a USB keyboard and mouse (which you can probably pick up at a junk shop), a TV/monitor with an HDMI input, and a mini SD card (8 to 16 Gbytes) for mass storage (you can buy this card with all the software already installed or a blank one to which you can download the software yourself). In fact, if you already have a Wi-Fi network and an Internet-connected computer, tablet, or smartphone, you do not need a separate monitor, keyboard and mouse: once it is set up, you can log onto the Raspberry Pi via your Wi-Fi network or over the Internet, using \href{http://www.putty.org/}{Putty} (for command-line UNIX-style access) or a graphical desktop sharing system such as \href{https://www.realvnc.com/}{RealVNC} (free for Windows, Mac, IOS, and Android), which reproduces the entire graphical desktop on your local device, complete with a pop-up virtual keyboard. It can also \href{http://raspberrypihq.com/how-to-share-a-folder-with-a-windows-computer-from-a-raspberry-pi/}{share files with Windows}. The Pi has been used as a low-cost alternative for school computer labs, using its included software for both Office-type applications (\textit{Writer} word processor, \textit{Calc} spreadsheet, etc.), and for programming instruction (Python, C, C++, Java, Scratch, and Ruby). It is also ideal for ``\href{https://www.raspberrypi.org/forums/viewtopic.php?t=74176}{headless}'' applications where it is \textit{only accessed remotely} via WiFi or Bluetooth, for example as a \href{https://www.howtogeek.com/139433/how-to-turn-a-raspberry-pi-into-a-low-power-network-storage-device/}{network file server}, \href{https://www.raspberryweather.com/}{weather station}, \href{http://lifehacker.com/5929913/build-a-xbmc-media-center-with-a-35-raspberry-pi}{media center} or as a networked \href{https://pimylifeup.com/raspberry-pi-security-camera/}{security camera}. 

For scientific data acquisition and \href{https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8\#q=raspberry+pi+laboratory+measurement+science&*}{signal processing applications}, the Pi version of Linux has all the \href{http://practical-data-science.blogspot.com/2012/09/basic-unix-shell-commands-for-data.html}{"usual" UNIX terminal commands} for data gathering, searching, cleaning and summarizing. In addition, there are many add-on libraries for \href{https://www.embeddedrelated.com/showarticle/197.php}{Python}, including \href{http://www.scipy.org/}{SciPi}, \href{http://www.numpy.org/}{NumPy}, and \href{http://matplotlib.org/}{Matplotlib}, all of which are free downloads. Allen B. Downey's 153-page PDF book "\href{http://greenteapress.com/thinkdsp/thinkdsp.pdf}{Think DSP}" has many examples of Python code in traditional engineering applications. Add-on hardware devices available at low cost, include \href{https://www.amazon.com/Raspberry-Pi-Camera-Module-Megapixel/dp/B01ER2SKFS}{video cameras} and a piggyback \href{https://www.amazon.com/Raspberry-Pi-Sense-HAT-AstroPi/dp/B014HDG74S}{\textit{sensor board}} that \href{https://pythonhosted.org/sense-hat/api/}{reads and displays sensor data} from several built-in sensors: gyroscope, accelerometer, magnetometer, barometer, temperature, relative humidity. (It is based on the \href{http://issabove.com/iss-above-and-the-raspberry-pi/}{same hardware that is currently in orbit on the International Space Station}). 

My \href{https://terpconnect.umd.edu/~toh/spectrum/functions.html\#spreadsheets}{signal processing spreadsheets} (page \pageref{ref-0522}) run just fine on the version of \textit{Calc} that comes with the Pi, as do the \href{https://terpconnect.umd.edu/~toh/models/CalibrationCurve.html}{Calibration worksheets} (page \pageref{ref-0490}) and my \href{https://terpconnect.umd.edu/~toh/models/index.html}{analytical instrument models} (page \pageref{ref-0422}). 

 For school applications, Element14 markets a \href{https://www.element14.com/community/docs/DOC-77882/l/learn-to-program-pack}{Learn to Program Pack Starter Kit} (\$177) that includes a \InsImageInline{0.5}{l}{OctaveRaspberryPi.png}license for \href{https://www.mathworks.com/campaigns/academia/ppc/google/buy-matlab-student.html?s\_eid=ppc\_6162840202&q=matlab\%20student\%20version}{student version of Matlab} for Windows or Macintosh and a Raspberry Pi 3 with MicroSD card, power supply, and enclosure (Matlab does not currently run directly on the Pi but can communicate with it). Even cheaper, \href{http://wiki.octave.org/Rasperry\_Pi}{Octave 3.6 \textit{can} run directly on a Raspberry Pi}; the screen above shows Octave 3.6 running within the Pi's built-in graphical user interface (showing off the 3D graphic functions "mesh" and "surf"). 

There are many \href{https://www.raspberrypi.org/blog/the-raspberry-pi-in-scientific-research/}{laboratory and field applications}, especially in combination with an \href{https://create.arduino.cc/projecthub/sankarCheppali/interfacing-arduino-with-raspberry-pi-6d9870}{Arduino micro-controller}. However, the \href{https://terpconnect.umd.edu/~toh/spectrum/TimeTrial.txt}{slowness of Octave (compared to Matlab)}, combined with the modest speed of the Raspberry Pi 3 may be limiting in some applications (Altogether it is about \href{https://terpconnect.umd.edu/~toh/spectrum/RaspberryPiOctaveSpeed.txt}{30 times slower} than Matlab on a big contemporary desktop computer). But it is also possible to communicate with Raspberry Pi hardware remotely from a faster computer running MATLAB using the \href{https://www.mathworks.com/help/supportpkg/raspberrypiio/examples/getting-started-with-matlab-support-package-for-raspberry-pi-hardware.html}{MATLAB Support Package for Raspberry Pi Hardware} for Matlab R2016b, using one or more remotely accessed Raspberry Pi's for experiment control and data acquisition and local storage and doing the heavy-duty number crunching on the main computer. Or you could simply have the Pi save data or results in a \href{http://raspberrypihq.com/how-to-share-a-folder-with-a-windows-computer-from-a-raspberry-pi/}{shared folder} that is accessed via WiFi from another computer.

\href{https://en.wikipedia.org/wiki/Python\_(programming\_language)}{Python} is the primary programming language that comes with the Raspberry Pi. This language is quite different than the older languages traditionally used by scientists, such as Fortran or Pascal, and it can be confusing for people without a computer science background. For example,, here is a simple \href{https://terpconnect.umd.edu/~toh/spectrum/temptime.html}{real-time example of data acquisition and plotting on a Raspberry Pi}, using the commercially available add-on \href{https://www.amazon.com/Raspberry-Pi-Sense-HAT-AstroPi/dp/B014HDG74S}{Sense Hat} board with a \href{https://terpconnect.umd.edu/~toh/spectrum/temptime.py}{program written in Python}, measuring temperature as a function of time (click for \href{https://terpconnect.umd.edu/~toh/spectrum/temptime.gif}{real-time animation}). If you do not have a Sense Hat, here's a \href{https://terpconnect.umd.edu/\%7Etoh/spectrum/RandRunningAverage.py}{modification of the same Python program} that plots the running average of a random number, using the same autoscaling graphic technique, graphing a result that gradually settles down closer and closer to the average the longer you let it run. This \href{https://terpconnect.umd.edu/~toh/spectrum/RunningAverage.m}{Matlab/Octave script} does the \textit{same thing at the same speed}, but the Matlab/Octave script is substantially shorter. These two programs constitute a nice comparison of Matlab vs Python, illustrating the advantages and disadvantages of each. You decide which is ``better''.

Other competing systems include the \href{https://beagleboard.org/}{BeagleBoard} and the \href{https://www.newark.com/df-robot/dfr0418/lattepanda-windows-10-mini-pc/dp/79AC5536}{LattePanda}, a tiny \$130 Windows-10 computer board with 2 Gbytes RAM and 32 Gbytes flash storage. \href{https://www.google.com/search?q=enbedded+single+board+computers&oq=enbedded+single+board+computers&aqs=chrome..69i57j0l5.6127j0j4&sourceid=chrome&ie=UTF-8}{Many similar products are available}.



\section{Batch processing\label{ref-0411}\label{ref-0412}\label{ref-0413}\label{ref-0414}\label{ref-0415}}

In situations where you have a large volume of similar types of data to process, it is useful to automate the process. Let us assume that you have already acquired data in the form of multiple text files or numerical data files of some standardized format that are stored in a known directory (folder) somewhere on your computer. For example, they might be ASCII .txt or .csv (``comma separated value'') files with the independent variable ('x') in the first column and one or more dependent variables ('y') in the other columns. There may be a variable number of data files, and their file names and length may be variable, but, crucially, the data \textit{format} is consistent from file to file. You could write a Matlab script or function that will process those files \textit{one-by-one}, but suppose you want the computer to go through \textit{all} the data files in that directory \textit{automatically}, determine their file names, load each into the variable workspace, apply the desired processing operations (peak detection, deconvolution, curve fitting, wavelets, whatever), collect all the resulting terminal window output, each labeled with the file name, add the results to a growing "diary" file, and then go on to the next data file. Ideally, the program \textit{should not stop} if it encounters any kind of fatal error with a corrupted or incomplete data file; rather, it should just \textit{skip that one and go on to the next}. It sounds complicated, but it is easier than it seems.

\href{https://terpconnect.umd.edu/~toh/spectrum/BatchProcess.m}{BatchProcess.m} is a Matlab/Octave example of just such an automated process that you can use as a framework for your applications. The main things you need to change here are:

 (a) the directory name where the data are stored on your computer - (``DataDirectory'') in line 11; 

 (b) the directory name where the Matlab signal processing functions are stored on your computer - (``FunctionsDirectory'') in line 12; and 

 (c) the actual processing functions that you wish to apply to each file (which in this example

      perform peak fitting using the ``\href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m}'' function in lines 34 \textendash{} 41, but could be anything).

When it starts, the routine creates and opens a ``diary'' file in line 21, located in the FunctionsDirectory, with the file name ``BatchProcess{\textless}date{\textgreater}.txt'' (where {\textless}date{\textgreater} is the current date, e.g. 12-Jun-2017). This file captures all the terminal window output during processing - in this example, I am using the peakfit.m function that generates a FitResults matrix (with Peak\#, Position, Height, Width, and Area\index{Area} of the best-fit model), and a Goodness of Fit (GOF) matrix containing the percent fitting error and R\textsuperscript{2 }value, for each data file in that directory. (Subsequent runs of the program on the same date are appended to this dairy file. On each subsequent day, a new file is begun for that day). You can also optionally save some of the variables in the workspace to data files; add a ``save'' function after the processing and before the ``catch me'' statement (type ``help save'' at the command prompt for options).

This program uses a couple of coding techniques that are especially useful in automated file processing. It uses the ``function forms'' of the ``list'' command ``ls'' (line 13), ``diary'' (line 21), and ``load'' (line 29) to allow then to accept variables computed within the program. It also uses the ``\href{https://terpconnect.umd.edu/~toh/spectrum/try.txt}{try/catch/end}'' structure (lines 28, 47, 49), which prevents the program from stopping if it encounters an error on one of the data files. If an error occurs, it adds a line to the diary that reports the error for that file and skips to the next one.

After running this script, the ``BatchProcess...'' diary file will contain all the terminal output. Here is an excerpt from a typical diary file. In this example, \textit{the first two data files in the directory yielded errors}, but the third one ("\texttt{2016-08-05-RSCT-2144.txt"}) and all the following ones worked normally and reported the results of the peak fitting operations:

\texttt{Error with file number 1.}

\texttt{Error with file number 2.}

\texttt{3: 2016-08-05-RSCT-2144.txt}

  \texttt{Peak\# Position Height Width Area}\index{Area}

  \texttt{1 6594.2 0.1711 0.74403 0.13551}

  \texttt{2 6595.1 0.16178 0.60463 0.1041}

  \texttt{\% fitting error R2}

  \texttt{2.5735 0.99483}

 \texttt{4: 2016-09-05-RSCT-2146.txt}

  \texttt{Peak\# Position Height Width Area}

  \texttt{1 6594.7 0.11078 1.4432 0.17017}

  \texttt{2 6595.6 0.04243 0.38252 0.01727}

  \texttt{\% fitting error R2}

  \texttt{4.5342 0.98182}

\texttt{5: 2016-09-09-RSCT-2146.txt}

  \texttt{Peak\# Position Height Width Area}

  \texttt{2 6594 0.05366 0.5515 0.0315}

  \texttt{1 6594.9 0.1068 1.2622 0.1435}

  \texttt{\% fitting error R2}

  \texttt{3.709 0.98743}

6: .... Etc....

Note: You could optionally import the dairy file into Excel by opening an Excel worksheet, click on a cell, click \textbf{Data {\textgreater} From Text}, select the diary file, click to \textit{specify that spaces are to be used as column separators}, and click \textbf{Import}. This will put all the collected terminal output into that spreadsheet. Additionally, you might want to save the workspace variables (e.g., as a .mat file).

\section{Real-time signal processing\label{ref-0416}\label{ref-0417}\label{ref-0418}\label{ref-0419}}

All the signal processing techniques covered so far assume that you have acquired and have stored the data in computer memory before beginning processing. In some cases, however, it is necessary to do the signal processing in "real-time", that is, point-by-point as the data are acquired from the sensor or instrument. That requires some modification of the software, but the main conceptual ideas still apply. In this section we will look at ways to perform real-time data plotting, smoothing, differentiation, peak detection, harmonic analysis (frequency spectra), and Fourier filtering. Because the data acquisition details vary with each individual experimenter and instrumental setup, these demonstration scripts will \textit{simulate} real-time data so that you can run them immediately on your computer to see how they work, without additional hardware. I will do this in either of two ways: 

(a) by using mouse-clicks to generate each data point, using Matlab's "ginput" function, or 

(b) by pre-calculating some simulated data and then accessing it point-by-point in a loop. 

The first method is illustrated by the simple script \href{https://terpconnect.umd.edu/~toh/spectrum/realtime.m}{realtime.m.} When you run this script, it displays a graphical coordinate system. Position your mouse pointer along the \textit{y} (vertical) axis and click to enter data points as you move the mouse pointer up and down. The "ginput" function waits for each click of the mouse button, then the program records the \textit{y} coordinate position and counts the number of clicks. Data points are assigned to the vector \textit{y} (line 17), plotted on the graph as black points (line 18), and print out in the command window (line 19). The script \href{https://terpconnect.umd.edu/~toh/spectrum/realtimeplotautoscale.m}{realtimeplotautoscale.m} is an expanded version that changes the graph scale as the data come in. If the number of data points exceeds 20 ('maxdisplay'), the \textit{x}-axis maximum is re-scaled to twice that (line 32). If the data amplitude equals or exceeds ('maxy'), the \textit{y}-axis is re-scaled to 1.1 times the data amplitude (line 36). 

\InsImageInline{0.5}{l}{realtimeplotautoscale2.gif.png} The script \href{https://terpconnect.umd.edu/~toh/spectrum/realtimeplotautoscale2.zip}{realtimeplotautoscale2.m} uses the second method to simulate real-time data, using \href{https://terpconnect.umd.edu/~toh/spectrum/DataMatrix2.mat}{pre-calculated data} (loaded from your hard drive in line 13) that is accessed point-by-point in lines 25 and 26 (If the animation is not visible, click on the figure to open in a web browser). The script \href{https://terpconnect.umd.edu/~toh/spectrum/realtimeplotdatedtime.m}{realtimeplotdatedtime.m} demonstrates one way to use Matlab's 'clock' function to record the date and time of each data point that is acquired by clicking. You could also have the computer control the time of data acquisition by reading the clock in a loop until the desired time and date arrives, then take a data point. Of course, a Windows machine is not ideal for high-speed, precisely timed data acquisition, because there are typically so many interrupts and other processes going on in the background, but it is adequate for low-speed applications. For higher speeds, \href{https://www.mathworks.com/products/connections/product\_detail/data-translation-daq-hardware.html}{specialized hardware} and \href{https://www.mathworks.com/products/daq.html}{software }are available. 

\InsImageInline{0.5}{l}{RealTimeSmooth.gif.png} \textbf{Smoothing}. The script \href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeSmoothTest.zip}{RealTimeSmoothTest.m} demonstrates real-time \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothing} (page \pageref{ref-0048}), plotting the raw unsmoothed data as a black line and the smoothed data in red. In this case, the script pre-calculates simulated data in line 28 and then accesses the data point-by-point in the processing 'for' loop (lines 30-51). The total number of data points is controlled by 'maxx' in line 17 (initially set to 1000) and the smooth width (in points) is controlled by 'SmoothWidth' in line 20. (To do this with real-time data from your sensor, comment out line 29 and replace line 32 with the code that acquires one data point from your sensor). 

 As you can see in the screen image on the left (\href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeSmooth.gif}{link to animation}), the smoothed data (in red) is \textit{delayed} compared to the raw data, because a smoothed data point cannot be computed until a number of data points equal to the smooth width have been acquired - 21 points in this example. (However, knowing the smooth width, you can correct the recorded y-axis positions of signal features, such as maxima, minima, peaks, or inflection points). This particular example implements a \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#algorithms}{sliding average smooth}, but other smooth shapes can be implemented simply by uncommenting line 24 (rectangular), 25 (triangular), or 26 (Gaussian), which requires that the functions '\href{https://terpconnect.umd.edu/~toh/spectrum/triangle.m}{triangle}' and '\href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian}' be in the Matlab/Octave path. 

\InsImageInline{0.5}{l}{RealTimeSmoothTestNoisy2.png}A practical application of a sliding average smooth like this is in a control system where a noisy signal turns on a valve, switch, or alarm signal whenever the signal exceeds a certain value. In the example shown in the figure on the right, the threshold value is 0.5 and the signal is so noisy that smoothing is required to prevent the signal from prematurely triggering the control. Too much smoothing, however, will cause an unacceptable delay in operation. In this case, the alarm occurs at about t=160 and stops at 350.

On a standard desktop PC (Intel Core i5 3 Ghz) running Windows 10 home, the smooth operation adds about 2 microseconds per data point to the data acquisition time (without plotting, PlottingOn=0 in line 20) and 20 milliseconds per point (50 Hz max) with point-by-point plotting (PlottingOn=1). With plotting off, the script acquires, smooths, and stores the smoothed data in the variable "sy" in real-time, then plots the data only after data acquisition is complete, which is much faster than plotting in real-time.  

\InsImageInline{0.5}{l}{RealTimeSmoothFirstDerivative.png}\textbf{Differentiation}. The script \href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeSmoothFirstDerivative.zip}{RealTimeSmoothFirstDerivative.m} demonstrates real-time smoothed \href{https://terpconnect.umd.edu/~toh/spectrum/Differentiation.html}{differentiation} (page \pageref{ref-0081}), using a simple adjacent-difference algorithm (line 47) and plotting the raw data as a black line and the first derivative data in red. The demonstration script \href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeSmoothSecondDerivative.zip}{RealTimeSmoothSecondDerivative.m} computes the smoothed \textit{second} derivative by using a central difference algorithm (line 47). Both scripts pre-calculate the simulated data in line 28 and then accesses the data point-by-point in the processing loop (lines 31-52). In both cases, the maximum number of points is set in line 17 and the smooth width is set in line 20. Again, the derivatives are delayed compared to the original signal. Any derivative order can be calculated this way using the derivative coefficients in the Matlab/Octave derivative functions listed on \textcolor{color-3}{page} \pageref{ref-0098}\textcolor{color-3}{.}

\textbf{Peak detection}. The little script \href{https://terpconnect.umd.edu/~toh/spectrum/realtimepeak.m}{realtimepeak.m} demonstrates simple real-time \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{peak de}\href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{tection} based on derivative zero-crossing (page \pageref{ref-0296}), using mouse clicks to simulate data. Each time your mouse clicks form a peak (that is, go up and then down again), the program will register and label the peak on the graph and print out its \textit{x} and \textit{y} values. 

\texttt{Peak detected at x=13 and y=7.836}

\texttt{Peak detected at x=26 and y=1.707}

\InsImageInline{0.5}{l}{RealTimeSmoothedPeakDetectionGauss.gif.png} In this case, a peak is defined as any data point that has lower amplitude points adjacent to it on both sides, which is determined by the nested 'for' loops in lines 31-36. Of course, a peak cannot be registered until the point following the peak is recorded, because there is no way to predict ahead of time whether that point will be lower or higher than the previous point. If the data are noisy, it is better to \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{\textit{smooth} }the data stream before detecting the peaks, which is exactly what the Matlab/ Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeSmoothedPeakDetection.zip}{RealTimeSmoothedPeakDetection.m} does, which reduces the chance of false peaks due to random noise but has the disadvantage of delaying the peak detection further. Even better, the Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeSmoothedPeakDetectionGauss.gif}{}\href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeSmoothedPeakDetectionGauss.zip}{RealTimeSmoothedPeakDetectionGauss.m} uses the technique described on page \pageref{ref-0301}; it locates the positive peaks in a noisy data set that rise above a set amplitude threshold ("AmpThreshold" in line 55), performs a \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#FittingPeaks}{least-squares curve-fit of a Gaussian function} to the top part of the raw data peak (in line 58), identifies each peak (line 59), computes the position, height, and width (FWHM) of each peak from that least-squares fit, and prints out each peak found in the command window. The peak parameters are measured on the raw data, so they are not distorted by smoothing. The "peak" label pops up next to each detected peak just a fraction of a second after the top of the peak, but the actual peak times listed are based on the raw data and are not delayed. In this example, the actual peak times are x=500, 1000, 1100, 1200, 1400. (Also note that the first visible peak, at x=300, is not detected because it falls below the amplitude threshold, which is 0.1 in this case, but if that peak is important, you could simply set the threshold lower). Link to \href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeSmoothedPeakDetectionGauss.gif}{animation}. 

\texttt{Peak detected at x=500.1705, y=0.42004, width= 61.7559}

\texttt{Peak detected at x=1000.0749, y=0.18477, width= 61.8195}

\texttt{Peak detected at x=1100.033, y=1.2817, width= 60.1692}

\texttt{Peak detected at x=1199.8493, y=0.36407, width= 63.8316}

\texttt{Peak detected at x=1400.1473, y=0.26134, width= 58.9345}

The script additionally \href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeSmoothedPeakDetectionGauss.png}{numbers the peaks} and saves the peak parameters of all the peaks in a matrix called PeakTable, which you can interrogate as each peak is encountered if you are looking for particular peak patterns. See page \pageref{ref-0313} for some ideas on using Matlab/Octave notation and functions to do this.

\InsImageInline{0.5}{l}{RealTimePeakSharpening.png}\textbf{Peak sharpening}. The Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/RealTimePeakSharpening.zip}{RealTimePeakSharpening.m} demonstrates real-time \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html}{peak sharpening} (page \pageref{ref-0099}) using the second derivative technique. It uses pre-calculated simulated data in line 30 and then accesses the data point-by-point in the processing loop (lines 33-55). In both cases the maximum number of points is set in line 17, the smooth width is set in line 20, and the weighting factor (K\textsubscript{1}) is set in line 21. In this example on the left, the smooth width is 101 points, which accounts for the delay in the sharpened peak compared to the original.

\textbf{Real-Time} \textbf{Frequency Spectrum}. The script \href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeFrequencySpectrumWindow.zip}{RealTimeFrequencySpectrumWindow.m} computes and plots the Fourier frequency spectrum of a signal (page \pageref{ref-0120}). Like the scripts above, it loads the simulated real-time data from a ``.mat file'' and then accesses that data point-by-point in the processing loop. A critical variable in this case is ``WindowWidth'', the number of data points taken to compute each frequency spectrum. The larger this number, the fewer the number of spectra that will be generated, but the higher will be the frequency resolution. On an average desktop PC (Intel Core i5 3 Ghz running Windows 10 home), this script generates about 50 spectra per second with an average data rate (points per seconds) of about 50,000 Hz. Smaller spectra (i.e. lower values of WindowWidth) generate proportionally lower average data rates (because the signal stream is interrupted more often to calculate and graph a spectrum).\InsImageInline{0.5}{l}{RealTimeFrequencySpectrum.png} \href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeFrequencySpectrum.png}{}If the data stream is an audio signal, it is also possible to play the sound through the computer's sound system synchronized with the display of the frequency spectra; to do this, set the variable \texttt{PlaySound} to \texttt{1}. Each segment of the signal is played, just before the spectrum of that segment is displayed, as shown on the right. The sound reproduction will not be perfect, because of the slight delay while the computer computes and displays the spectrum before going on to the next segment. In this demonstration script, the data file is in fact an audio recording of an 8-second excerpt of the 'Hallelujah Chorus' from Handel's \textit{Messiah} with a sampling rate of 8192 Hz, which is included as demonstration data in the Matlab distribution \texttt{(}handel.mat). The figure on the right shows one of the 70 spectra generated with a WindowWidth of 1024. You can adjust the argument of the 'pause' function for your computer to minimize this problem and to make the sound play smoothly at the correct pitch. 

\textbf{Real-Time Fourier Filter.} The script \href{https://terpconnect.umd.edu/~toh/spectrum/real-time\%20Fourier\%20bandpass\%20filter.zip}{RealTimeFourierFilter.m} is a demonstration of a real-time \InsImageInline{0.5}{l}{RealTimeFourierFilter.png}Fourier filter. It pre-computes a simulated signal starting in line 38, then access the data point-by-point (lines 56, 57), and divides up the data stream into segments to \href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeFourierFilter.png}{}compute each filtered section. ``WindowWidth'' (line 55) is the number of data points in each segment. The larger this number, the fewer the number of segments that will be generated, but the higher will be the frequency resolution within each segment. On an average desktop PC (Intel Core i5 3 Ghz running Windows 10 home), with a window width of 1000 points, this script generates about 35 filtered segments per second with an average data rate (points per second) of about 34,000 Hz. Smaller segments (i.e., lower values of WindowWidth) generate proportionally lower average data rate (because the signal stream is interrupted more often to calculate and graph the filtered spectrum). The result of applying the filter to each segment is displayed in real-time during the data acquisition, and then at the end the script compares the entire raw signal input to the reassembled filtered output, shown the figure on the left.

In this demonstration, a \href{https://en.wikipedia.org/wiki/Band-pass\_filter}{bandpass} filter is used to detect a 500 Hz ('f' in line 28) sine wave that occurs in the middle third of a very noisy signal (line 32), from about 0.7 sec to 1.3 sec; the 500 Hz sine wave is so weak it cannot be seen at all in the raw signal (upper panel of the figure on the left), but it really stands out in the filtered output (lower panel). The filter center frequency (CenterFrequency) and width (FilterWidth) are set in lines 46 and 47. 

\textbf{To apply any of these examples} to real-time data from your sensor or instrument, you only need the main processing 'for' loop, replacing the first lines after the 'for' statement with a call to a function that acquires a single point of raw data and assigns it to y(n). If you do not need the data plotted out point-by-point in real-time, you can speed things up greatly by removing the ``drawnow'' statement at the end of the 'for' loop or by removing all the plotting code. 

In the examples here, the \textit{output} of the processing operation is used to plot or to print out the processed data point-by-point, but of course it could also be used as the input to another processing function or to a digital-to-analog converter or simply to trigger an alarm if certain specified results are obtained (e.g., if the signal exceeds a certain value for a specified length of time, or if a peak is detected at a specified position or height, etc.). 

\label{ref-0420}

\section{Dealing with variable data arrays in spreadsheets\label{ref-0421}}

When applying spreadsheet templates of the type described in this book to your own data, you often need to modify the templates to accommodate different numbers of data points or components. This can be tedious to do, especially because you need to remember the syntax of each of the spread-sheet functions that you want to modify. This section describes ways to construct spreadsheets that \textit{automatically} adapt to different data sets, without your taking the time and effort to modify the spreadsheet formulas for each case. This involves employing some less commonly used built-in functions in Excel or OpenOffice Calc, such as MATCH, INDIRECT, COUNT, IF, and AND.

\textbf{\InsImageInline{0.5}{l}{image325.png}The MATCH function}. In signal processing using spreadsheets, it is common to have x-y arrays of data of variable length, such as spectra (x=wavelength, y=absorbance or intensity) or chromatograms (x=time, y=detector response). For example, consider this small array of x and y values pictured in the spreadsheet fragment on the left. Spreadsheet formulas normally refer to cells by their row and column address, but for an x-y data set like this, it is more natural to refer to a data point by its independent variable x, rather than its row and column address. For example, suppose you want to select the data point where x=2, irrespective of what cells they inhabit. You can do that with the MATCH function. For example, if you set cell B2 to the desired x value (e.g., 2), then MATCH(B2,A5:A11)+ROW(A5) will return the row number of that point, which is 6 in this case. Later, if you were to move or expand this table, by dragging it or by inserting or deleting rows or columns, the \textit{spreadsheet will automatically adjust} the MATCH function to compensate, returning the new row number of the requested point. 

\textbf{The INDIRECT function}. The usual way to reference the value in a cell is to specify its row and column address. For example, in the array of x and y values pictured above, to refer to the contents of column B, row 6, you could write ``=B6'', which in this case will evaluate to 5.9. This is referred to as ``direct'' addressing. In contrast, to use ``indirect'' addressing you can write ``=INDIRECT("B"\&A1)'', then put the number ``6'' in cell A1. The ``\&'' character is simply ``glue'' that joins ``B'' to the contents of A1, so in that case "B"\&A1 evaluates to ``B6'' and the result is the same as before: the contents of cell B6, which is 5.9. \textit{However,} if you change cell A1 to 9, then "B"\&A1 would evaluate to ``B9'', and the result would be the contents of cell B9, which is 9.1. In other words, the indirect function allows the addresses of cells to be \textit{calculated within the spreadsheet} rather than being typed in as a fixed number. This makes it possible for spreadsheets to adjust their own addresses based on a calculated result, for example to adjust their calculations to fit the number of data points in that data set. 

These examples were done in what is called the ``A1'' reference style, where the columns are referred to by letters; it is also possible to use the ``R1C1'' reference style, where \textit{both} the rows and the columns are referred to by \textit{numbers}. For example, ``=INDIRECT("R"\&A2\&"C"\&A1,FALSE)'', with the row number in A2 and the columns number in A1. (The ``FALSE'' just means that the ``R1C1'' reference style is used).

 You can use the same technique to compute \textit{ranges} of cell addresses. For example, suppose you wanted to compute the sum of all the numbers in column B between a specified first row and specified last row. If you put the first row-number in A1 and the last row-number in row A2, the address of the \textit{first} cell would be "B"\&A1 and the address of the \textit{last} cell would be "B"\&A2. So, you would form the range of cell addresses by using ``\&'' to glue together those two addresses, with a ``:'' character in-between ("B"\&A1\&":B"\&A2). The sum would be SUM(INDIRECT("B"\&A1\&":B"\&A2)), which is 56. Yes, it is longer, but the advantage over direct addressing is that you can adjust the range by changing just two cells rather that retyping the formula. It is the same for other functions that need a range of cells, such as AVERAGE, MAX, MIN, STDEV, etc. For examples of its use, see page \pageref{ref-0071}.

 For functions that require \textit{two} ranges, separated by a comma, you can use the same technique. Suppose you want to compute the \href{https://en.wikipedia.org/wiki/Slope}{\textit{slope}} of the linear regression line between the x values in column A and the y values in column B in the spreadsheet excerpt on the previous page, using the built-in SLOPE function. SLOPE requires two ranges, first the dependent (y) values and them the independent x values. By \textit{direct} addressing, the slope is SLOPE(B5:B11,A5:A11). By \textit{indirect} addressing, you need two separate ``indirect'' functions, one for each range, separated by a comma. Here is what it looks like all together: SLOPE(INDIRECT("B"\&A1\&":B"\&A2),INDIRECT("A"\&A1\&":A"\&A2)), where the x values are in column A, the y values in column B, and the first and last row numbers are in cells A1 and A2 respectively. It works the same for the two related functions that calculate the INTERCEPT and RSQ (the R\textsuperscript{2 }value) of the regression line. I agree that it is confusing to read at first, but it works.

\InsImageInline{0.5}{l}{image326.png}\textbf{A working example.} An example of the use of the MATCH and INDIRECT functions working together is demonstrated in the spreadsheet ``\href{https://terpconnect.umd.edu/~toh/spectrum/SpecialFunctions.xlsx}{SpecialFunctions.xlsx}'' (\href{https://terpconnect.umd.edu/~toh/spectrum/SpecialFunctions.png}{Graphic}), which has a larger table of x-y data stored in columns A and B, starting in row 7. The idea here is that you can select a limited range of x values to work with by typing in the \textit{lowest} \textit{x} and the \textit{highest} \textit{x} value in cells B2 and B3, the two cells with a yellow background. The spreadsheet uses the MATCH functions in cells F2 and F3 to compute the corresponding row numbers, which are then used in the INDIRECT functions in the ``Properties of selected data range'' section to compute the maximum, average, and average of x and of y, and also the slope, intercept, and R\textsuperscript{2 }values of the y \textit{vs} x linear regression line (page \pageref{ref-0198}) over that selected x interval. The regression line, fitting \textit{only} the data from x=20 to 29, is shown on red in the \InsImageInline{0.5}{l}{image327.png}graph on the right, superimposed on the complete data set (blue dots). By simply changing the x-axis limits in cells B2 and B3, \textit{the spreadsheet and the graph re-calculates, without your having to edit any of the cell formulas}. Try it yourself. (By the way, you can float your mouse pointer over any cell with a red mark in upper right corner to reveal its cell formula or an explanation).

Columns J and K of this sheet also show how to use the ``IF'' and ``AND'' functions to copy data from columns A and B into columns J and K only those data points that fall between the two specific x limits. 

If desired, you can add more data to the end of columns A and B, limited only by the range of the match functions in cells F2 and F3 (which are initially set to 1000, but that could be as large as you need). The total number of numerical values in the data set is computed in cell I15, using the ``COUNT'' function (which, as the name suggests, counts the number of cells in a range that contains numbers). 

\textbf{Measuring peak location}. A common signal processing operation is finding the x-axis value where the y-axis value is maximum. This can be broken down into three steps: (1) determine the maximum y value in the selected range with the MAX function; (2) determine the row number in which that number appears with the MATCH function, and (3) determine the value of x in that row with the INDIRECT function. These steps are illustrated in the same ``\href{https://terpconnect.umd.edu/~toh/spectrum/SpecialFunctions.xlsx}{SpecialFunctions.xlsx}'' spreadsheet in column H, rows 20-23. The result is that the maximum y (21.5) occurs at x=28. The three steps can even be combined into one long formula (cell H23), although this is harder to read than the formulas for the separate steps. The \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#Spreadsheet}{peak finder spreadsheet} discussed on page \pageref{ref-0327} uses this technique.

\textbf{The LINEST function}. Indirect addressing is particularly useful when using \textit{array functions} such as LINEST (page \pageref{ref-0235}) or the matrix algebra functions (page \pageref{ref-0254}). The demonstration spreadsheet ``\href{https://terpconnect.umd.edu/~toh/spectrum/IndirectLINEST.xls}{IndirectLINEST.xls}'' (\href{https://terpconnect.umd.edu/~toh/spectrum/IndirectLINEST.png}{Graphic} link) shows how this works for the multiwavelength spectroscopy analysis of a mixture of three overlapping components by the CLS method (page \pageref{ref-0242}). The measured mixture spectrum is in column C, rows 29-99 and the spectra of the three pure components are in columns D, E, and F. Cell C12 ``=COUNT(C29:C1032)'' counts the number of rows of data (i.e., number of \textit{wavelengths}) in column C starting at row 29, and cell G3 counts the number of \textit{components} (in this case 3). These are used to determine the first and last row and column for the indirect addresses in LINEST in cell C17. The measured peaks heights calculated by LINEST for the three peaks are given in row 17, columns C, D, and E, and the predicted standard deviations are in the row below. In this spreadsheet the data are simulated (in columns O \textendash{} U), so the true peaks heights are known and therefore the \textit{absolute accuracy can be calculated} (row 26, C, D, and E) and compared to the predicted standard deviations. Press the \textbf{F9} key to recalculate with an independent noise sample, which is equivalent to taking another measurement of the same sample. Because of the use of INDIRECT addressing, you can add or subtract data points at the end of columns C \textendash{} E and the calculations work with no other changes. For examples of its use in signal processing, see page \pageref{ref-0256}.

\section{Computer simulation of signals and instruments.\label{ref-0422}\label{ref-0423}}

Throughout this book, I have often used computer simulations to test, demonstrate, and determine the range of applicability and the accuracy of various signal processing techniques. The aim is to generate realistic computer-simulated signal by adding together 

(a) known \textit{signal} component, such as one or more peaks, pulses, or sigmoidal steps, 

(b) a \textit{baseline}, which may be flat, sloped, curved, or stepped, and 

(c) random \textit{noise}, (page \pageref{ref-0023}), which may be various colors (page \pageref{ref-0032}) and amplitude dependences (page \pageref{ref-0034}). 

This can be done either in Matlab/Octave, using the built-in and downloadable functions (page \pageref{ref-0501}) for various peak shapes and types of random noise, or in spreadsheets, which can also be used to create attractive and intuitive user interfaces\InsImageInline{0.5}{l}{image328.gif.png}. Matlab ``apps'' and common spreadsheets have a built-in way to create a ``GUI'' (Graphic User Interface) with buttons, sliders, drop-down menus, etc. Some spreadsheet examples that I have created include  \href{https://terpconnect.umd.edu/~toh/spectrum/SimulatedSignal6Gaussian.xlsx}{SimulatedSignal6Gaussian.xlsx}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningDemo.xlsx}{PeakSharpeningDemo.xlsx}\textcolor{color-3}{,} \href{https://terpconnect.umd.edu/~toh/spectrum/CLSvsINLS.mhttps:/terpconnect.umd.edu/~toh/spectrum/PeakDetectionDemo2.xls}{PeakDetectionDemo2.xls}, \href{https://terpconnect.umd.edu/~toh/spectrum/TransmissionFittingDemoGaussian.xls}{TransmissionFittingDemoGaussian.xls}\textcolor{color-3}{,} \href{https://terpconnect.umd.edu/~toh/models/BeersLawCurveFit2.xls}{BeersLawCurveFit2.xls}\textcolor{color-3}{, and} \href{https://terpconnect.umd.edu/~toh/models/RegressionDemo.xls}{RegressionDemo.xls} (on the left).

It is possible to make any aspect of a computer-generated signal randomly variable from measurement to measurement, with the aim of making the simulation as close as possible to the real signal behavior that you may have to measure. For example, in the section ``The Battle Rounds: a comparison of methods'' on page \pageref{ref-0353}, the signal to be measured is a Gaussian peak located near the center of the recorded signal, with a fixed shape and width. The baseline, on the other hand, is highly variable, both in amplitude and in shape, and there is also added white noise. In another simulation, ``Why measure peak area rather than peak height?'', page \pageref{ref-0372}, the signal peak itself is subject to a variable broadening process that causes the measured peak to be shorter and wider, but which has no effect of the total area. In the section ``Measuring a buried peak'', page \pageref{ref-0388}, the signal is a small ``child'' peak that is buried under the tail of a much stronger ``parent'' peak. In all these cases, the \textit{true underlying signal} is known to the software, so that, after the software measures the simulated \textit{observed signal} with all its baseline and noise variability, it can calculate the error of measurement, allowing you to compare different methods or to optimize the method’s variables to obtain the best accuracy.

\InsImageInline{0.5}{l}{image329.png}\InsImageInline{0.5}{l}{image330.gif.png}In some cases, it may be possible to simulate important aspects of an entire measurement instrument system. Several examples are shown in \url{https://terpconnect.umd.edu/~toh/models/}. This is most useful if both the signal magnitude and the noise can be predicted from first principles. For example, in optical spectroscopy, the principles of physics and of geometrical optics can be used to predict the intensity of an \href{https://terpconnect.umd.edu/~toh/models/Blackbody.html}{incandescent light source}, the \href{https://terpconnect.umd.edu/~toh/models/Monochromator.html}{transmission of a monochromator}, and the signal generated by a \InsImageInline{0.5}{l}{image331.gif.png}\href{https://terpconnect.umd.edu/~toh/models/Photomultiplier.html}{photomultiplier}, including the \href{https://camera.hamamatsu.com/jp/en/technical\_guides/photon\_shot\_noise/index.html}{photon noise}. When these are combined, it is possible to simulate the fundamental aspects of such instruments as a \href{https://terpconnect.umd.edu/~toh/models/Fluorescence.html}{scanning fluorescence spectrometer} (above) or an \href{https://terpconnect.umd.edu/~toh/models/AAMeasurement.html}{atomic absorption instrument} (below), to predict the analytical \href{https://terpconnect.umd.edu/~toh/models/BeersLaw.html}{calibration curves of absorption spectroscopy}, to compare the theoretical signal-to-noise ratios of \href{https://terpconnect.umd.edu/~toh/models/UVVisSNR.html}{absorption} and \href{https://terpconnect.umd.edu/~toh/models/FluorescenceSNR.html}{fluorescence} measurement, and to predict the detection limits of \href{https://terpconnect.umd.edu/~toh/models/AES.html}{atomic emission measurement of various elements}, and the effect of \href{https://terpconnect.umd.edu/~toh/models/AbsSlitWidth.html}{slit width on signal-to-noise ratio} in absorption spectroscopy (above). You can also simulate the operation of a \href{https://terpconnect.umd.edu/~toh/models/lockin.html}{lock-in amplifier} (page \pageref{ref-0383}), a \href{https://terpconnect.umd.edu/~toh/models/modspec.html}{wavelength modulation} spectroscopy system, and even \href{https://terpconnect.umd.edu/~toh/ElectroSim/}{basic analog electronic and operational amplifier circuits}. Note that \textit{these are not simulations of commercial instrument}s that might be used to train instrument operators. Rather, they are interactively manipulated mathematical models that describe various parts of or aspects of each system, for the purpose of \textit{illuminating hidden aspects} of the instrument’s internal operation. 

\section{Who uses this book and its associated web site, documents and software?\label{ref-0424}}

In the last few years, this book and the associated web site (\url{http://terpconnect.umd.edu/~toh/spectrum/}) has been accessed from Internet Service Providers in over \textbf{162 countries} and 6 non-region-specific categories (e.g. satellite providers), including many countries in the developing world, some very small countries (e.g. Liechtenstein, the Faroe Islands), relatively isolated countries (Cuba, North Korea, Myanmar/Burma), and even some war-torn regions (Afghanistan, Syria, Iraq). Availability of Internet access is often an issue. For example, I've got fewer views from Cuba that from other Spanish-speaking countries with \textit{smaller} populations, such as Bolivia, Dominican Republic, Costa Rica, Puerto Rico, Panama, and Uruguay, even though Cuba has many active scientists, especially in the medical and pharmaceutical fields.

The first Web version went up in 1996, but I didn't start \href{http://statcounter.com/}{keeping track} of page views until 2008; since then there have been over \textit{2 million page views}. The distribution of page view counts among countries is very long-tailed, with one-third of the views coming from the USA (\href{https://terpconnect.umd.edu/~toh/spectrum/DayAfterThanksgiving.png}{except during major US holidays}), half of the views coming from only 5 countries (USA, India, Germany, United Kingdom, and China) and 99\% of the views coming from only 39 countries. Among the countries that have a relatively large number of page views \textit{relative to their populations} are the USA, Germany, UK, Canada, Australia, Netherlands, Switzerland, Singapore, Israel, Belgium, Taiwan, South Korea, and Scandinavia. Another web site of mine on a related subject, \href{https://terpconnect.umd.edu/~toh/models/}{Interactive Computer Models for Analytical Chemistry Instruction}, had got an additional 820,000 views.

The Internet Service Providers with the largest number of views are Comcast, Verizon FIOS, Time Warner, Cloudflare, At\&t U-verse, Deutsche Telekom (Germany), BSNL (India), and Cox Communication. Most views worldwide come from Windows machines, about 20\% from Linux and Macintosh, and 10\% from mobile devices. I have made efforts to make my pages more usable from mobile devices like smartphones.

About one-quarter of the views come \textit{directly from educational institution ISPs} that have "School", "Ecole", "College", "Hochschule", "Univ...", "Academic", or "Institute of Technology" in their names. (The number of educational users is certainly larger than that because some users are no doubt accessing from other ISPs in homes or businesses). An analysis of 200,000 views in 2015 showed that the biggest educational users have been the University of California System (UCLA, Berkley, etc.), Indian Institute Of Technology system, the University of Texas system, Massachusetts Institute Of Technology, the University of Michigan, the University of Maryland (my home institution), Delft University of Technology (Netherlands), Stanford University, China Education And Research Network Center, the University Of Wisconsin System, and the University of Illinois.

Many of the large national laboratories are users, including Bell Canada, Oak Ridge, Pacific Northwest, Lawrence Livermore, Sandia, Brookhaven, National Renewable Energy Laboratory, SLAC, Fermilab, Lawrence Berkeley, NRC Canada, CERN, NIST, NASA, JPL, and NIH.

The most popular pages on the site recently have been \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{Peak Finding and Measurement}, \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{Smoothing}, \href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html}{Integration}, \href{https://terpconnect.umd.edu/~toh/spectrum/Deconvolution.html}{Deconvolution}, \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{InteractivePeakFitter}, and \href{http://www.google.com/search?sa=t&rct=j&q=remove\%20spikes\%20matlab\%20signal&source=web&cd=9&ved=0CGcQFjAI&url=http\%3A\%2F\%2Fterpconnect.umd.edu\%2F\%7Etoh\%2Fspectrum\%2FSignalProcessingTools.html&ei=rZX7U6W0Dsi1iwLklIC4Bw&usg=AFQjCNFW1HUmFU01YPaIg3NKmrG\_j38Edg&sig2=RDcF9fWYL4nZPDfpHcdgKQ&bvm=bv.73612305,d.cGE}{Signal Processing Tools}. About 50\% of the page views originate from search engines (80\% of those using Google). The most common search keywords used are: "peak area", "convolution", "deconvolution", "peak detection", "signal processing pdf", "findpeaks matlab", "Fourier filter", and "smoothing". About 40\% of the traffic comes from direct links (bookmarks or typed URLs) and about 10\% comes from referring websites, usually from \href{https://en.wikipedia.org/w/index.php?search=\%22O\%27Haver\%2C+T.\%22&title=Special\%3ASearch&fulltext=1}{Wikipedia} or from \href{http://www.mathworks.com/matlabcentral/profile/authors/870532-tom-o-haver}{MathWorks}. Unfortunately, page loads and search terms have become almost completely encrypted in recent years, so I can no longer tell which pages are being viewed and what is being download. (Interestingly, that is not the case with \href{https://terpconnect.umd.edu/~toh/models/}{Interactive Computer Models for Analytical Chemistry Instruction}, which has only 75\% encryption).

There have been over 100,000 downloads of my software and documentation files, currently averaging about 500 file downloads per month, from both \href{https://terpconnect.umd.edu/~toh/spectrum/functions.html}{my web site} and from my files on the \href{https://www.mathworks.com/matlabcentral/fileexchange/?term=authorid\%3A24576&sort=downloads\_desc}{Matlab File Exchange}. The most commonly downloaded files are \href{http://firasaboulatif.free.fr/index\_files/gaidaa\%20book/Noise\%20signal\%20processing/IntroToSignalProcessing.pdf}{IntroToSignalProcessing.pdf}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFinder.zip}{PeakFinder.zip}, ipf12, \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitterStart4x100.xlsx}{CurveFitter....xlsx}, iSignal, ipeak, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetection.xlsx}{PeakDetection.xlsx}, and the complete site archive \href{https://terpconnect.umd.edu/~toh/spectrum/SPECTRUM.zip}{SPECTRUM.zip}. 

\textbf{What factors influence the number of page views from different countries?}  The tools of data analysis, specifically regression (for example, using LINEST), can help answer this question. Obviously, one would expect that a country's population would be a factor, but it turns out that \textit{the correlation between log(page views) and log(population) is very poor}, with a coefficient of determination (log-log correlation coefficient or R\textsuperscript{2} value) of only 0.36 (n=163 countries; over 160,000 total page loads over the period from 2008 to 2017; \href{https://terpconnect.umd.edu/~toh/spectrum/LogHitsVSLogPopulation.png}{graphic on} next page). Note that because of the very large range of population sizes, I did a \textit{log-log} correlation (page \pageref{ref-0492}) to prevent the results from being totally dominated by the top few countries.

I also investigated the effect of other factors that might be more specific to the language and subject matter of my particular site, including 
\begin{itemize}
\item the number of \textit{English speakers} in each country, 

\item the number of \textit{Internet users} in each country,

\item the number of \textit{universities} in each country, and 

\item the total \textit{research and development budget} of each country.


\end{itemize}
All that information is freely available on the internet for \textit{most} (but not all) of the countries (\href{https://terpconnect.umd.edu/~toh/spectrum/SeparateFactorRegressions.png}{graphic link}). By a good margin, \textit{the most influential factor was the} \textbf{\textit{research and development budget}}\textit{,} for which the R\textsuperscript{2} value was 0.76. This is perhaps not surprising given that my site concerns a very narrow and specialized topic: the technical aspects of computerized scientific data processing.

A log-log \textit{multilinear regression} on all 5 of these factors together yielded an R\textsuperscript{2} value of 0.84 (n=53 countries for which \textit{all 5} factors were reported), which is a modest improvement over the research and development budget alone. (Since these calculations were made in 2017, page views from China have risen substantially and are now typically second to those from the USA.).\InsImageInline{0.5}{l}{RegressionSummaryTable.png}\InsImageInline{0.5}{l}{image333.png}

For an Excel spreadsheet with all these data and calculations (between 2008 and 2015), see \href{https://terpconnect.umd.edu/~toh/spectrum/FinalCountriesSummary.xlsx}{FinalCountriesSummary.xlsx}

\textbf{What fields of study are represented?} The users of my site include students, instructors, workers, and researchers in industry, environmental, medical, engineering, earth science, space, military, financial, agriculture, communications, and even music and linguistics. This conclusion is based on emails I have received, on the \href{https://terpconnect.umd.edu/~toh/spectrum/papers.pdf}{titles of journal articles that have cited my work}, and on the ISPs of major web visitors. Judging from the ratio of downloads to emails, most people who have downloaded my software do not write to me about what they are doing, which of course is completely understandable. Also, of the people who do write to me, most do not tell me specifically what their applications are, which is their prerogative. As a result, those sources give me incomplete information about the application areas where my programs are being applied.  A much better indication of the width of applications can be got by looking at the titles of the over \href{https://terpconnect.umd.edu/~toh/spectrum/papers.pdf}{\textit{500} \textit{published papers and patents}} that have cited my web pages and book (page \pageref{ref-0536}).

\section{The Law of Large Numbers\label{ref-0425}\label{ref-0426}}

\href{https://en.wikipedia.org/wiki/Law\_of\_large\_numbers}{The Law of Large Numbers} is a theorem that describes large collections of numbers or observations that are subject to independent and identically distributed random variation, such as the result of performing the same experiment many times. The average of the results obtained from many trials should be close to the long-term value and will tend to become closer as more trials are performed. It is an important idea because it guarantees stable long-term results for the averages of some random events. This is the reason gambling casinos can make so much money; their games are designed to give the casino a small advantage in the long run but highly variable results in the short term, guaranteeing plenty of (noisy) winners that tend to encourage the gamblers, as well as enough (quiet) losers so they can make money. And that’s why investors in the stock market make money in the long run, despite the unpredictable day-to-day variation, up one day and down the next. It’s also why it is so hard to see \textit{climate} change in the much wilder short-term hot and cold day-to-day and year-to-year swings in the \textit{weather}.

\InsImageInline{0.5}{l}{image334.png}\InsImageInline{0.5}{l}{image335.png}But the idea that ``The average {\ldots} will tend to become closer as more trials are performed'' does \textit{not} mean that the average becomes \textit{steadily and irreversibly} closer. In fact, the average can wander around quite a bit as more data are included. Take the example on the left, which shows the running average of a set of normally distributed independent random numbers with a population mean of 1.000 and a standard deviation of 1.000, as more and more numbers from that population are averaged, up to 1000. (This is generated by the simple Matlab script \href{https://terpconnect.umd.edu/~toh/spectrum/RunningAverage.m}{RunningAverage.m}). Note that the average wanders around, reaching and crossing over the true population average (1.000) twice in this case before ending up near 1.0 after 1000 points are accumulated. But if you ran this script again, the final average may \textit{not} be so close to 1.0. In fact, the predicted standard deviation of the average of all 1000 random numbers is reduced by a factor of 1/sqrt(1000), which is about 0.031, or 3\% relative, meaning that most results will only fall \href{https://en.wikipedia.org/wiki/Normal\_distribution\#Standard\_deviation\_and\_coverage}{within plus or minus 6\% of the true average}, that is, from 0.94 to 1.06.

\textbf{The uncertainty of uncertainty}. The situation is even worse if you wish to estimate the \textit{standard deviation} of a population from small samples. The graphic on the left shows he Matlab script \href{https://terpconnect.umd.edu/~toh/spectrum/RunningStandardDeviation.m}{RunningStandardDeviation.m}. which simulates this for the same population in the previous example. The sample standard deviation wanders around alarmingly for small samples and only settles down slowly. Even worse, the standard deviation for very small samples is \href{https://terpconnect.umd.edu/~toh/spectrum/RunningStandardDeviation30.png}{biased down}, often returning values far lower than usual. The calculated standard deviation of two data points drawn from a normal distribution is almost always lower that the true population standard deviation.

There is a well-documented tendency for people to \textit{overestimate} the quality of small numbers of measurements, sometimes referred to as \href{https://en.wikipedia.org/wiki/Faulty\_generalization\#Hasty\_generalization}{hasty generalization}, or \href{https://en.wikipedia.org/wiki/Insensitivity\_to\_sample\_size}{insensitivity to sample size}, or the \href{https://en.wikipedia.org/wiki/Gambler\%27s\_fallacy}{gambler’s fallac}y. This is related to the field of study of a famous pair of psychologists named Amos Tversky and Daniel Kahneman, who collaborated in a long-running study of human cognitive biases in the 1970s. They formulated a hypothesis that people erroneously tend to believe in a false ``\href{https://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=7B46DE594D5AE4C4F4D7927875FCB9AC?doi=10.1.1.592.3838&rep=rep1&type=pdf}{Law of Small Numbers}'', the name they coined for the mistaken belief that a small sample drawn from a large population is representative of that large population. We would like to believe that scientists are immune to these foibles and that they always think logically and correctly. But scientists are only human, so it pays to be aware of this tendency, \textit{particularly when a small sample of data supports your favorite hypothesis}. It is tempting to stop there, ``while you’re ahead''. This is called "\href{https://en.wikipedia.org/wiki/Confirmation\_bias}{confirmation bias}". Avoid it like the plague. 

Of course, in many practical experimental measurements, you may really be constrained to a small number of repeated measurements. There may be a fixed number of data points and no possibility of gathering more. Or the cost, in money or in time, of gathering more data may be excessive. For example, the process of calibrating an analytical instrument for quantitative measurement (page \pageref{ref-0401}) may involve the preparation and measurement of several standard samples or solutions of known composition. If the calibration curve (the relationship between instrument reading and sample composition) is non-linear, it takes several different standards to define the curve, the more the better. But you have to consider not only cost of preparing many standards but also the cost of cleaning up and safely storing or disposing of the (potentially hazardous) chemicals afterwards. In other words, you may have to accept a smaller number of standards that would be ideal. The bottom line is, if you are limited to a small number of data points, do not over-estimate the quality of your results. To use the \href{https://en.wikipedia.org/wiki/Normal\_distribution\#Standard\_deviation\_and\_coverage}{3-sigma rule} (page \pageref{ref-0038}) to determine uncertainty ranges for a set of data, the distribution must be normal (Gaussian) \textit{and} you need to know the standard deviation. For small sets of data, \textit{both} are uncertain.\label{ref-0427}

\section{Spectroscopy and chromatography combined: time-resolved Classical Least-squares\label{ref-0428}}

The introduction of high-speed \href{https://www.hitachi-hightech.com/global/products/science/tech/ana/lc/basic/course7.html}{UV-Visible array detectors} into \href{https://en.wikipedia.org/wiki/High-performance\_liquid\_chromatography}{high-performance liquid chromatography} (HPLC) instruments has significantly increased the power of that method. The speed of such detectors is such that they can acquire a complete spectrum multiple times per second over the entire chromatogram. An example of this is described in a technical report from Shimadazu Scientific Instruments (\url{https://solutions.shimadzu.co.jp/an/n/en/hplc/jpl217011.pdf}) which considers the separation of three positional isomers of methyl acetophenone: o-methyl (o-MAP), m-methyl (m-MAP), and p-methyl (p-MAP). The ultraviolet absorption spectra of each of these three isomers at a concentration of 400 ${\upmu}$g/mL is shown below on the left; they are distinct but highly overlapping. The chromatographic separation, using the column and conditions specified in their report, is shown in the middle; the peaks are only partly resolved. The Shimadazu report describes their commercial software, which uses a complex iterative approach to extract the spectra and chromatographic characteristics from the raw data.

 \InsImageInline{0.5}{l}{TimeResolvedCLS.png}

\texttt{CLSPercentErrors = 0.0021993 0.0020162 0.0015607}

\texttt{PerpDropPercentErrors = -1.6315 -0.78697 3.272}

Here I present a comparatively simple non-iterative technique based on the same chemical system, in which we consider each spectrum acquired by the detector as a separate sample mixture and apply the Classic Least-squares method \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html}{previous discussed}, in which the \textit{spectra of the components are known beforehand} and where adherence to the Beer-Lambert Law is expected. The spectra and chromatographic peaks are simulated digitally in the Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/TimeResolvedCLS.m}{TimeResolvedCLS.m}, shown in the figure above, by modeling the spectrum of each component as the sum of three Gaussian peaks and the chromatographic peaks as exponentially modified Gaussians. To make this simulation as realistic as possible, the parameters were carefully adjusted to match the graphics in the technical report closely, and the other parameters, such as the spectral resolution, sampling rate, and detector noise, were based on that report also. You can see that the chromatographic peaks (middle figure) are nowhere near baseline resolved. Therefore, it is to be expected that quantitative calibration based on the measurement of peak areas in this chromatogram (for example by the perpendicular drop method, page \pageref{ref-0192}, might be inaccurate, especially if the peak heights are very different. In fact, in this case, where the concentrations of the three components are equal (0.05 ${\upmu}$g/mL for each component), the peak areas measured by perpendicular drop are only about 2\% from the true values, mainly due to the slight asymmetry and unequal height of the three peaks. The spectra (left-hand figure) are even more highly overlapped than the chromatographic peaks, but they are distinct in shape, and that is the key. 

Basically, we treat this as a series of 3-component CLS calculations, one for each time slice of the detector. The actual calculations can be done in two ways, depending on whether the spectra are processed one by one or are collected for the entire chromatogram and then processed all at once, using either "Alternative calculation \#1", lines 113-146, or "Alternative calculation \#2", lines 150-170. The first method looks like chromatography as it executes; it computes the chromatographic peaks of the three components point by point as they evolve in time and plots them in the first three quadrants of figure window 3 (on the right). The second method calculates the entire chromatogram in one step at the end and makes the same final plots. (The second method is faster computationally, but that is not significant because it is the \textit{chromatography} that is the rate-determining step, not the \textit{calculations}). Either way, the result is the same; the chromatographic peaks of the three components are \textit{completely separated mathematically}, so their areas are easily measured, \textit{no matter how much they overlap}. Note that, although the three \textit{spectra} must be known, no knowledge of the chromatography peaks is required; they emerge separate and intact from the data, purely computationally.

In order to test the abilities and limitations of this method, I have prepared a series of increasingly challenging scenarios, starting with one pictured above and making it progressively more difficult in the following ways: (a) making the chromatographic peaks more closely spaced, (b) making the peak more asymmetrical, (c) making the spectra more similar, and (d) making the concentrations more unequal. These scenarios are listed in the table below, along with the typical percent errors in peak area measurement by the CLS method and with links to the corresponding graphic and to the Matlab/Octave m-files. Each is a more challenging variation on the first one; \#2 has much more chromatographic peak overlap; \#3 has much more asymmetrical peaks (higher \textit{tau}); \#4 has much more similar spectra - in fact, the peak wavelengths differ by only 0.1 nm, making them look identical; in \#5, component 2 (the middle peak) has a concentration 100 times lower; and \#6 is the same as \#5 except that the peaks are highly asymmetrical. In all these cases, the normal perpendicular drop area measurement technique is either impossible (because there are no distinct peaks for each component) or is very much in error, but the CLS technique works well, giving very low errors, except when the middle peak concentration is 0.0001, which approaches the random noise limit of the detector. (Another variation, in the script \href{https://terpconnect.umd.edu/~toh/spectrum/TimeResolvedCLSbackground.m}{TimeResolvedCLSbaseline.m}, includes continuous {\hyperref[ref-0250]{correction}} for {\hyperref[ref-0091]{baseline drift}} by adding a 4\textsuperscript{th} component to account for the possibility of bubbles or turbidity in the light path of optical detectors).


\begin{table}
\begin{tabularx}{\textwidth}{|
p{\dimexpr 0.153\linewidth-2\tabcolsep-2\arrayrulewidth}|
p{\dimexpr 0.118\linewidth-2\tabcolsep-\arrayrulewidth}|
p{\dimexpr 0.154\linewidth-2\tabcolsep-\arrayrulewidth}|
p{\dimexpr 0.162\linewidth-2\tabcolsep-\arrayrulewidth}|
p{\dimexpr 0.277\linewidth-2\tabcolsep-\arrayrulewidth}|
p{\dimexpr 0.136\linewidth-2\tabcolsep-\arrayrulewidth}|} \hline 
\centering\arraybackslash{}\textbf{Peak resolution} & \centering\arraybackslash{}\textbf{Spectral similarity} & \centering\arraybackslash{}\textbf{Peak asymmetry} & \centering\arraybackslash{}\textbf{Concentration ratios} & \centering\arraybackslash{}\textbf{Typical percent errors in area measurement} & \centering\arraybackslash{}\textbf{Links} \\\hline 
1. Normal & Normal & Slight: tau=10 &  .05   .05   .05 &  .002\%  .002\%   .0016\% & \href{https://terpconnect.umd.edu/~toh/spectrum/TimeResolvedCLS.png}{Graph} \href{https://terpconnect.umd.edu/~toh/spectrum/TimeResolvedCLS.m}{mfile} \\\hline 
2. Unresolved & Normal & Slight: tau=10 &  .01   .01   .01 &   -.06\%   -.053\%   -.041\% & \href{https://terpconnect.umd.edu/~toh/spectrum/TimeResolvedCLS1.png}{Graph}  \href{https://terpconnect.umd.edu/~toh/spectrum/TimeResolvedCLS1.m}{mfile} \\\hline 
3. Blended & Normal & Great: tau=40 & .05   .05   .05 &  -.0004\%  -.013\%   -.066\% & \href{https://terpconnect.umd.edu/~toh/spectrum/TimeResolvedCLS5.png}{Graph}  \href{https://terpconnect.umd.edu/~toh/spectrum/TimeResolvedCLS5.m}{mfile} \\\hline 
4. Unresolved & Very close & Slight: tau=10 &  .01   .01   .01 &  .054\%  .049\% .04\% & \href{https://terpconnect.umd.edu/~toh/spectrum/TimeResolvedCLS2.png}{Graph}  \href{https://terpconnect.umd.edu/~toh/spectrum/TimeResolvedCLS2.m}{mfile} \\\hline 
5. Unresolved & Very close & Slight: tau=10 &  .01  .0001  .01 &  0.026\%  2.4\% 0.019\% & \href{https://terpconnect.umd.edu/~toh/spectrum/TimeResolvedCLS4.png}{Graph}  \href{https://terpconnect.umd.edu/~toh/spectrum/TimeResolvedCLS4.m}{mfile} \\\hline 
6. Unresolved & Very close & Great: tau=40 &  .01  .0001  .01 &  -0.04\%  -3.8\%   -0.03\% & \href{https://terpconnect.umd.edu/~toh/spectrum/TimeResolvedCLS6.png}{Graph}  \href{https://terpconnect.umd.edu/~toh/spectrum/TimeResolvedCLS6.m}{mfile} \\\hline 
\end{tabularx}
\end{table}
Even when the peaks are resolved well enough for the perpendicular drop method to work, it can suffer from an interaction between adjacent peak heights; that is, a change in the peak height of one peak can affect the measurement of the area of adjacent overlapped peaks, because of shifts in the valley point between them. This is illustrated by \href{https://terpconnect.umd.edu/~toh/spectrum/TimeResolvedCLScalibration.m}{TimeResolvedCLScalibration.m}, which simulates the calibration curves (concentration vs peak area) for a three-component mixture similar to the above but modified so perpendicular drop measurement works, and then allows the three components to vary independently and randomly over a 2 x 10\textsuperscript{-4} to 9 x 10\textsuperscript{-4} ${\upmu}$g/mL range. (Each time you run this you will get a different mix of concentrations). A typical set of calibration curves are shown below. In this case, the average absolute percentage error in area measurement is about 5\% for the perpendicular drop method, with an R\textsuperscript{2 }of 0.995, but it is \textit{less than 1\% for the CLS measurement}, with an R\textsuperscript{2 }of 0.9995, a big improvement.

\InsImageInline{0.5}{l}{TimeResolvedCalibrationCurves.png} The CLS method is clearly very effective, but this really proves only that the \textit{mathematics} works well; the method still requires that the spectra of all the components be known accurately. This requirement can be met in some applications, but in liquid chromatography there is a potential pitfall. If gradient elution and/or temperature programming are used \textit{and} if the spectra of those chemical compounds are sensitive to the solvent and/or to temperature, possibly shifting their peaks slightly, then there will likely be additional errors in the CLS procedure. Obviously, this depends on the chemical system and will have to be evaluated on a case-by-case basis. 

But this suggests another interesting use for this method: speeding up a chromatographic method that normally would achieve complete baseline separation (from which accurate spectra of each component could be obtained in situ), and then \textit{adjusting the column and/or flow rate to achieve faster but incompletely resolved chromatograms} to which the CLS method could be applied quickly and accurately to multiple samples.

In other applications, some or all the components may simply be unknown, and you may want to obtain their spectra. This would be easy if the chromatography were able to separate each component cleanly, but what if the peaks are overlapped so that pure component spectra are never achieved? In that case, more sophisticated methods must be used, such as the one described in the Shimadzu technical report. This involves making initial estimates of spectral and chromatographic peaks, followed by an \href{https://en.wikipedia.org/wiki/Hill\_climbing}{iterative search} for the best fit to the experimental data, subject to the imposition of some important known prior constraints, such as non-negativity of spectra and of the chromatography peaks (those peaks are always positive, except for random noise on the baseline), and the \href{https://en.wikipedia.org/wiki/Unimodality}{unimodality} of the chromatography peaks (that is, each component gives one and only one chromatography peak). Methods of this type will be left to a future expansion of this book.

\section{The mystery peak challenge\label{ref-0429}\label{ref-0430}}

The objective of this exercise is to learn as much as we can about the underlying properties of a digitized signal using the signal processing tools in this book and, if possible, to obtain a mathematical description of the signal (\href{https://terpconnect.umd.edu/~toh/spectrum/AsymmetricalOverlappingPeaks.m}{script}). At first glance, the signal (\href{https://terpconnect.umd.edu/~toh/spectrum/MysteryPeak.mat}{MysteryPeak.mat}) appears to be a single, asymmetrical peak with a maximum at x=55.5. The signal-to-noise ratio seems to be very good - there’s little visible noise unless you look very closely - and the signal begins and ends near zero, so baseline correction is likely not an issue. The bad news is that we do not know anything else. The asymmetry might be due to some asymmetrical process s applied to an originally symmetrical peak shape, but it could be a group of closely spaced overlapping peaks, which is suggested by the faint bumps in the shape. Some quick preliminary curve fitting can be done using command line \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m} (page \pageref{ref-0448}): 

\texttt{[FitResults,GOF]=peakfit([x y],0,0,2,1)} for a two-Gaussian model (shape 1)

\texttt{[FitResults,GOF]=peakfit([x y],0,0,1,3)}  for a single Logistic model (shape 3)

\texttt{[FitResults,GOF]=peakfit([x y],0,0,4,39,1)} for a 4 exponentially broadened Gaussian model (shape 39) \InsImageInline{0.5}{l}{Figures1to3.png} 

Or you could use the interactive peak fitter \href{https://terpconnect.umd.edu/~toh/spectrum/ipf.m}{ipf.m} (page \pageref{ref-0463}) for this purpose; you can quickly change the model shape, number of peaks, starting guesses, data region to be fitted, etc., with single keystrokes and mouse clicks. Either way, the three initial fits in the figures show that the signal contains a small amount of random noise, which appears to be white (so the signal has probably not been smoothed, which is fortunate) and which has a relative standard deviation of about 0.2\%, based on 1/5\textsuperscript{th} of the visual peak-to-peak value (page \pageref{ref-0021}). But \textit{unfortunately, these fits are not successful because their fitting errors (0.5 to 0.8\%) are all significantly larger than the 0.2\% random noise}. Trying different shapes and greater numbers of peaks does not help either, resulting in either higher fitting errors, unstable fits, or zero peak heights; there is just too much overlap for curve fitting (page \pageref{ref-0280}).

Another approach to the problem of asymmetrical peaks is to use the technique of \textit{first-derivative symmetrization} described on page \pageref{ref-0109}. This applies specifically to \textit{exponential} broadening, a common peak broadening mechanism. The idea is that if you compute the first derivative of an ex\InsImageInline{0.5}{l}{image339.png}ponentially broadened peak, multiply it by a weighting factor equal to the time constant \textit{tau} of the exponential, and add it to the original broadened signal, the result will be the original peak before broadening, which \textit{makes the peak overlap less severe.} This works for \href{https://www.researchgate.net/publication/333237821\_Reconstruction\_of\_exponentially\_modified\_functions}{any original peak shape}. Even if you do not know \textit{tau} beforehand, you can try different values until the baseline after the peak is as low as possible but not negative, as shown in \href{https://terpconnect.umd.edu/~toh/spectrum/SymmetricalizationAnimation.gif}{this GIF animation}. This is easily done by using the \href{https://terpconnect.umd.edu/~toh/spectrum/symmetrize.m}{symmetrize.m} function, or interactively in \href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{iSignal}, which has smoothing (\textbf{S} key), derivatives (\textbf{D}), symmetrization (\textbf{Shift-Y}), and curve fitting (\textbf{Shift-F}). The derivative of y with respect to x, by the ``derivxy.m'' function, shown by the green line in the figure above, is quite noisy. As usual, we must smooth the derivatives of noisy signals \InsImageInline{0.5}{l}{image340.png}to make them useful, but we must not over-smooth and distort the signals. As a rule of thumb, a smooth width equal to 1/10\textsuperscript{th} of the number of data points in the halfwidth does not distort the signal visibly, as shown on page \pageref{ref-0075}. Our peak has about 530 points, measured by \texttt{halfwidth(1:length(y),y)}, so a smooth width near 53 will not distort the signal peak, but it does eliminate most of the noise from the derivative (above right). Also, we can see that the derivative, in y/x units, is comparable in numerical magnitude to the original signal, so the time constant \textit{tau} (in x units) is probably somewhere near 1.0. Next, we add the product of the first derivative and \textit{tau} to the original signal, looking at the trailing edge as we try six different \textit{tau} values near 1.0. The graph above left shows that the optimum value is \InsImageInline{0.5}{l}{image341.png}about 1.25. 

When this is applied to the entire signal, the result, shown on the right, has \textit{more distinct bumps}. When that modified signal is used for curve fitting, we find that a 3-Gaussian model works quite well, with a fitting error of only 0.25\%, close to the noise. This is evidence that the signal consists of three closely spaced exponentially modified Gaussians (EMG). Normally there is no independent way to check the accuracy of the peak parameters so measured, but - full disclosure - the signal in the case was not actually unknown but rather was generated by the file \href{https://terpconnect.umd.edu/~toh/spectrum/MysteryPeak.m}{MysteryPeaks.m}; it does in fact consist of three EMGs, with peak maxima at x=53, 55, and 57.5, each with a time constant of 1.3, and all with peak areas of 1.0. The curve-fit results \textit{after} symmetrization are within 0.1\% for the peak positions and within 2\% of the peak areas. In contrast, direct fitting of exponential Gaussians to the \textit{original} data, using the \textit{tau} we just determined, looks good (\href{https://terpconnect.umd.edu/~toh/spectrum/AsymmetricalOverlappingPeaksFigure7.png}{graphic}) but gives \textit{less accurate peak parameters} and takes three times longer to compute. 

\chapter{Signal processing software details\label{ref-0431}}

\section{Interactive smoothing, differentiation, and signal analysis (iSignal)\label{ref-0432}\label{ref-0433}\label{ref-0434}\label{ref-0435}\label{ref-0436}\label{ref-0437}\label{ref-0438}}

\InsImageInline{0.5}{l}{SmoothAnimation.gif.png}\href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{\textit{iSignal}} is a Matlab-only function, written as a single self-contained m-file, for performing smoothing, differentiation, peak sharpening, interpolation, baseline subtraction, Fourier frequency spectrum, least-squares peak fitting, and other useful functions on time-series data. Using simple keystrokes, you can adjust the signal processing parameters continuously while observing the effect on your signal dynamically. \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal8.zip}{Click here to download the ZIP file "iSignal8.zip"} that also includes some sample data for testing. You can also download iSignal from the \href{http://www.mathworks.com/matlabcentral/fileexchange/authors/24576}{Matlab File Exchange}. You can also run iSignal \href{https://www.mathworks.com/products/matlab-online.html}{in a web browser} (just click on the figure window), even on \href{https://itunes.apple.com/us/app/matlab-mobile/id370976661?mt=8}{Matlab Mobile} (but not in Octave). The demo script "\href{https://terpconnect.umd.edu/~toh/spectrum/demoisignal.m}{demoisignal.m}" is a self-running demonstration of several features of the program and will test for proper installation; the title of each figure describes what is happening. Its basic operation of iSignal is similar to \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#ipeak}{\textbf{i}}\href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#ipeak}{\textbf{Peak}} and \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{\textbf{ipf.m}}\textbf{.} The syntax is: \texttt{pY=isignal(Data);} \texttt{or}\texttt{,} to specify all the settings in advance\texttt{:}

\texttt{[pY,Spectrum,maxy,miny,area,stdev] = isignal(Data, xcenter, xrange, SmoothMode, SmoothWidth, ends, DerivativeMode, Sharpen, Sharp1, Sharp2, Symize, Symfactor, SlewRate, MedianWidth, SpectrumMode);}

"Data" may be a 2-column matrix with the independent variable (x-values) in the first column and dependent variable (y values) in the second column, or separate x and y vectors, or a single y-vector (in which case the data points are plotted against their index numbers on the x-axis). Only the first argument (Data) is required; all the others are optional. iSignal returns the processed dependent axis ('\textbf{pY}') vector (and, in the \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html\#Spectrum}{Spectrum Mode}, the frequency spectrum matrix, '\textbf{Spectrum}') as the output arguments. It plots the data in the Matlab Figure window, the lower half of the window showing the entire signal, and the upper half showing a selected portion controlled by the pan and zoom keys (the four cursor arrow keys), with the initial pan and zoom settings optionally controlled by input arguments '\textbf{xcenter}' and '\textbf{xrange}', respectively. Other keystrokes allow you to control the smooth type, width, and ends treatment, the derivative order (0\textsuperscript{th} through 5\textsuperscript{th}), and peak sharpening. (The initial values of all these parameters can be passed to the function via the optional input arguments \textbf{SmoothMode}, \textbf{SmoothWidth}, \textbf{ends}, \textbf{DerivativeMode}, \textbf{Sharpen}, \textbf{Sharp1}, \textbf{Sharp2}, \textbf{SlewRate}, and \textbf{MedianWidth}. See the examples below). Press \textbf{K} to see all the keyboard commands. \textbf{\textit{Note}}\textit{:} Make sure you do not click on the ``Show Plot Tools'' button in the toolbar above the figure; that will disable normal program functioning. If you do; close the Figure window and start again.

\textbf{Smoothing}

  The \textbf{S} key (or input argument "\textbf{SmoothMode}") cycles through five smoothing modes:

  If \textbf{SmoothMode}=0, the signal is not smoothed.

  If \textbf{SmoothMode}=1, rectangular (sliding-average or boxcar) 

  If \textbf{SmoothMode}=2, triangular (2 passes of sliding-average)

  If \textbf{SmoothMode}=3, p-spline (3 passes of sliding-average)

  If \textbf{SmoothMode}=4, \href{http://en.wikipedia.org/wiki/Savitzky\%E2\%80\%93Golay\_smoothing\_filter}{Savitzky-Golay} smooth (thanks to ~\href{http://www.mathworks.com/matlabcentral/fileexchange/authors/62607}{Diederick}).

The \textbf{A} and \textbf{Z} keys (or optional input argument \textbf{SmoothWidth}) control the \textbf{SmoothWidth}, \textit{w}. The \textbf{X} key toggles "\textbf{ends}" between 0 and 1. This determines how the "ends" of the signal (the first \textit{w}/2 points and the last \textit{w}/2 points) are handled when smoothing:

\InsImageInline{0.5}{l}{iSignalSegmentSmooth.png}  If \textbf{ends}=0, the ends are zero.

  If \textbf{ends}=1, the ends are smoothed with progressively smaller smooths the closer to the end. Generally, \textbf{ends}=1 is best, except in some cases using the derivative mode when \textbf{ends}=0 result in better vertical centering of the signal. 

To specify a \textit{segmented}\index{segmented} smooth (more on page \pageref{ref-0398}) press \textbf{Shift-Q.} You can specify the smooth width vector in \textit{two ways}: at the prompt you can either (a) enter the number of segments (then you'll be prompted to enter the smooth widths in the first and last segments, and the computer will calculate integer values of smooth widths that are evenly divided between the specified first and last values, or (b) type in the smooth width vector \textit{directly} including the square brackets, e.g. [1 3 3 9]. In either case, subsequently adjusting the smooth width with the \textbf{A} and \textbf{Z} keys will vary \textit{all} the segments by the same percentage factor. (To return to an ordinary single segment smooth, enter 1 as the number of segments). See the picture of a 4-segment smooth on the right, with smooth widths of 1, 2, 4, and 5. Note: when you are smoothing peaks, you can easily measure the effect of smoothing on peak height and width by turning on peak measure mode (press \textbf{P}) and then press \textbf{S} to cycle through the smooth modes.

There are two special functions for removing or reducing sharp spikes in signals: the \textbf{M} key, which implements a median filter (it asks you to enter the spike width, e.g. 1,2, 3... points) and the \textbf{\textasciitilde{}} key, which limits the maximum rate of change.

\textbf{Differentiation}

The \textbf{D} / \textbf{Shift-D} keys (or optional input argument ``\textbf{DerivativeMode}'') increase/ decrease the derivative order. The default is 0. Careful optimization of the smoothing of derivatives is critical for an acceptable signal-to-noise ratio. An example is shown in the figure on the right. In SmoothModes 1 through 3, the derivatives are computed with respect to the independent variable (x-values), corrected for non-uniform x-axis intervals. In SmoothMode 4 (Savitzky-Golay) the derivatives are computed by the Savitzky-Golay algorithm. \href{https://terpconnect.umd.edu/~toh/spectrum/DerivAnimation.gif}{Click for GIF animation}.

\textbf{Peak sharpening}

The \textbf{E} key (or optional input argument "\textbf{Sharpen}") turns off and on peak sharpening. The sharpening strength is controlled by the \textbf{F} and \textbf{V} keys (or optional input argument "\textbf{Sharp1}") and \textbf{B} and \textbf{G} keys (or optional argument "\textbf{Sharp2}"). The optimum values depend on the peak shape and width. For peaks of Gaussian shape, a reasonable value for \textbf{Sharp1} is PeakWidth\textsuperscript{2}/25 and for \textbf{Sharp2} is PeakWidth\textsuperscript{4}/800 (or PeakWidth\textsuperscript{2}/6 and PeakWidth\textsuperscript{4}/700 for Lorentzian peaks), where PeakWidth is the full width at half \InsImageInline{0.5}{l}{iSignalPeakSharpening.gif.png}maximum of the peaks \textit{expressed in the number of data points}. However, you do not need to do the math yourself; \textit{iSignal} can calculate sharpening and smoothing settings for Gaussian and for Lorentzian peak shapes using the \textbf{Y} and \textbf{U} keys, respectively. Just isolate a single typical peak in the upper window using the pan and zoom keys, then press \textbf{Y} for Gaussian or \textbf{U} for Lorentzian peaks. Fine-tune the sharpening with the F/V and G/B keys and the smoothing with the A/Z keys. You can see this animation if you read this within \textit{Microsoft Word 365}, otherwise click \href{https://terpconnect.umd.edu/~toh/spectrum/iSignalPeakSharpening.gif}{this link}. (The optimum settings depend on the width of the peak, so if your signal has peaks of widely different widths, o\href{https://terpconnect.umd.edu/~toh/spectrum/iSignalPeakSharpening.gif}{}ne setting will not be optimum for all the peaks and you might consider using segmented smoothing, page \pageref{ref-0399}). You can expect a decrease in peak width (and a corresponding increase in peak height) of about 20\% - 50\%, depending on the shape of the peak (the peak area is largely unaffected by sharpening). Excessive sharpening leads to baseline artifacts and increased noise. iSignal allows you to experimentally determine the values of these parameters that give the best trade-off between sharpening, noise, and baseline artifacts, for your purposes. You can easily measure the effect of sharpening quantitatively by turning on peak measure mode (press P) and then press E to toggle the sharpen mode off and on. Note: only the Savitzky-Golay smooth mode is used for peak sharpening.

\textbf{Interactive convolution and deconvolution}

In \textbf{iSignal 8.3} you can press \textbf{Shift-V} to display the menu of Fourier convolution and deconvolution operations (page \pageref{ref-0150}) that allow you to convolute a Gaussian, Lorentzian, or exponential function with the signal, or to deconvolute a Gaussian, Lorentzian, or exponential function from the signal. 

\texttt{Fourier convolution/deconvolution menu} 

   \texttt{1. Convolution} 

   \texttt{2. Deconvolution}

\texttt{Select mode 1 or 2: 2}

\texttt{Shape of convolution/deconvolution function:}

   \texttt{1. Gaussian}

   \texttt{2. Lorentzian}

   \texttt{3. Exponential}

\texttt{Select shape 1, 2, or 3: 2}

\texttt{Enter the width in x units:} 

Then you enter the time constant (in x units) and press \textbf{Enter}. Then use the \textbf{3} and \textbf{4} keys to adjust the width of the deconvolution function by 10\% (or \textbf{Shift-3} and \textbf{Shift-4} to adjust by 1\%). You may need to adjust the smoothing also, if the signal is too noisy, but too much smoothing will broaden the peaks. For a real-signal application, see page \pageref{ref-0162}.

\textbf{Interactive Symmetrization} (or ``de-tailing'') of exponentially broadened signals is performed by \textit{weighted first-derivative addition} (page \pageref{ref-0105}). Press the \textbf{Shift-Y} key, type in an estimated weighting factor (which is the time constant or tau of the exponential), and press Enter. To adjust, press the "\textbf{1}" and "\textbf{2}" keys to change weighting factor by 10\% and "\textbf{Shift-1}" and "\textbf{Shift-2}" keys to change by 1\%. Increase the factor until the baseline after the peak goes negative, then decrease it slightly so that it is as low as possible but not negative. Run the script \href{https://terpconnect.umd.edu/~toh/spectrum/iSignalSymmTest.m}{iSignalSymmTest.m} (\href{https://terpconnect.umd.edu/~toh/spectrum/iSignalSymmDemo.png}{graphic}) for an example signal with two overlapping exponentially broadened Gaussians.

\textbf{Signal measurement}

\InsImageInline{0.5}{l}{Noise.png}\InsImageInline{0.5}{l}{MaxSignal.png}

\textit{Signal-to-noise ratio (SNR) measurement of a signal with very high SNR. Left: The peak height of the largest signal peak is measured by placing the green center cursor on the largest peak; peak-to-peak signal=66,000. Right: The noise is measured on a flat portion of the baseline: standard deviation of noise=0.01, therefore the SNR=66,000/.01 = 6,600,000}

The cursor keys control the position of the green cursor (left and right arrow keys) and the distance between the dotted red cursors (up and down arrow keys) that mark the selected range displayed in the upper graph window. The label under the top graph window shows the value of the signal (y) at the green cursor, the peak-to-peak (min and max) signal range, the area under the signal, and the standard deviation within the selected range (the dotted cursors). Pressing the Q key prints out a table of the signal information in the command window. If the optional output arguments maxy, miny, area, stdev are specified, iSignal returns the maximum value of y, the minimum value of y, the total area under the curve, and the standard deviation of y, in the selected range displayed in the upper panel. 

\InsImageInline{0.5}{l}{iSignalSpectrumMode.gif.png}\textbf{Frequency Spectrum mode}

The \textbf{Frequency Spectrum mode}, toggled on and off by the \textbf{Shift-S} key, computes the \href{https://terpconnect.umd.edu/~toh/spectrum/HarmonicAnalysis.html}{Fourier frequency spectrum} (page \pageref{ref-0118}) of the segment of the signal displayed in the upper window and displays it in the lower window (temporarily replacing the full-signal display). Use the pan and zoom keys to adjust the region of the signal to be viewed. Press \textbf{Shift-A} to cycle through four plot modes (linear, semilog X, semilog Y, or log-log) and press \textbf{Shift-X} to toggle between a \textit{frequency} on the x-axis and \textit{time} on the x-axis. Importantly, \textit{all signal processing functions remain active in the frequency spectrum mode} (smooth, derivative, etc.) so you can immediately observe the effect of these functions on the frequency spectrum of the signal. (You can see this animation if you read this within Microsoft Word 365, otherwise click the figure to open in a Web browser). Press \textbf{Shift-T} to transfer the frequency spectrum to the signal in the upper panel, so you can pan and zoom and do other processing and measurements on the frequency spectrum. Press \textbf{Shift-S} again to return to the normal mode. Spectrum mode is a \textit{visible mode}, indicated by the label at the top of the figure. To \textit{start off} in the spectrum mode, set the 13th input argument, SpectrumMode, to 1\texttt{.} To \textit{save} the spectrum as a new variable, call iSignal with the output arguments \texttt{[pY,Spectrum]:}

\texttt{{\textgreater}{\textgreater} x=0:.1:60; y=sin(x)+sin(10.*x);}

\texttt{{\textgreater}{\textgreater} [pY,Spectrum]=isignal([x;y],30,30,4,3,1,0,0,1,0,0,0,1);}

\texttt{{\textgreater}{\textgreater} plot(Spectrum(:,1),Spectrum(:,2)) or plotit(Spectrum) or isignal(Spectrum); or ipf(Spectrum); or ipeak(Spectrum)}

\textbf{Shift-Z} toggles on and off the peak detection and labeling on the frequency/time spectrum; peaks are labeled with their frequencies. You can adjust the peak detection parameters in lines 2192-2195 in version \textbf{5}. The \textbf{Shift-W} command displays the \href{http://en.wikipedia.org/wiki/Waterfall\_plot}{3D waterfall spectrum}, by dividing up the signal into segments and computing the power spectrum of each segment. This is mostly a novelty, but it may be useful for signals whose frequency spectrum varies over the duration of the signal. You are asked to choose the number of segments into which to divide the signal (that is, the number of spectra) and the type of 3D display (mesh, contour, surface, etc.)

\textbf{Background subtraction}

There are two ways to subtract the background from the signal: automatic and manual. To select an automatic baseline correction mode, press the \textbf{T} key repeatedly; it cycles through four modes (page \pageref{ref-0276}): \textit{No} baseline correction, \textit{linear} baseline subtraction, \textit{quadratic} baseline subtraction, \textit{flat} baseline correction, then back to \textit{no} baseline correction. When baseline mode is \textit{linear}, a straight-line baseline connecting the two ends of the signal segment in the upper panel will be automatically subtracted. When the baseline mode is \textit{quadratic}, a parabolic baseline connecting the two ends of the signal segment in the upper panel will be automatically subtracted. The baseline is calculated by computing a linear (or quadratic) least-squares fit to the first 1/10th of the points together with the last 1/10th of the points. Try to adjust the pan and zoom to include enough of the baseline at the beginning and end of the segment in the upper window to ensure that the automatic baseline subtract gets a good reading of the baseline. The \textit{flat} baseline mode is used only for \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html\#CurveFitting}{\textit{peak fitting}} (\textbf{Shift-F} key). The calculation of the signal amplitude, peak-to-peak signal, and peak area are all recalculated based on the baseline-subtracted signal in the upper window. If you are measuring peaks superimposed on a background, the use of the BaselineMode will have a big effect on the measured peak height, width, and area, but very little effect on the peak x-axis position, as demonstrated by the two figures on the next page.


\begin{table}
\begin{tabularx}{\textwidth}{|
p{\dimexpr 0.499\linewidth-2\tabcolsep-2\arrayrulewidth}|
p{\dimexpr 0.501\linewidth-2\tabcolsep-\arrayrulewidth}|} \hline 
\InsImageInline{0.5}{l}{iSignalAutoZeroOFF.png} & \InsImageInline{0.5}{l}{iSignalAutoZeroON.png} \\\hline 
\end{tabularx}
\end{table}
In addition to the four BaselineMode baseline subtraction modes for peak measurement, a \textit{manually estimated} piecewise linear baseline can be subtracted from the \textit{entire} signal in one operation. The \textbf{Backspace} key starts background correction operation. In the command window, type in the number of background points to click and press the \textbf{Enter} key. The cursor changes to crosshairs; click along the presumed background in the figure window, starting to the left of the x axis and placing the last click to the right of the x axis. When you click the last point, the linearly-interpolated baseline between those points is subtracted from the signal. To restore the original background (i.e. to start over), press the '\textbf{\textbackslash }' key (just below the backspace key).

\textbf{Peak and valley measurement}

The \textbf{P} key toggles off and on the "peak parabola" mode, which attempts to measure the one peak (or valley) that is centered in the upper window under the green cursor by superimposing a \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#GaussFit}{least-squares best-fit parabola} (page \pageref{ref-0229}) in red, on the center portion of the signal displayed in the upper window. (Zoom in so that the red overlays just the top of the peak or the bottom of the valley as closely as possible). The peak position, height, and width are measured by least-squares curve fitting of a Gaussian (colored red in the upper panel) to the central part of the selected segment. (Change the pan and zoom to modify that region; the readings will change as the segment measured is changed). The "RSquared" value is the coefficient of determination; the closer to 1.000 the better. The peak parameters will most accurate if the peaks are Gaussian. Other peak shapes, or very noisy peaks of any shape, will give only approximate results. However, the position and height, and area values are pretty good for any peak shape if the "RSquared" value is at least 0.99. The "SNR" is the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#PeakSNR}{signal-to-noise-ratio of the peak} under the green cursor; it is the ratio of the peak height to the standard deviation of the residuals between the data and the best-fit line in red. 

\InsImageInline{0.5}{l}{iSignal55.png}An example is shown in the figure on the right. If the peaks are superimposed on a non-zero background, subtract the background before measuring peaks, either by using the BaselineMode (\textbf{T} key) or the multi-point background subtraction (backspace key). Press the \textbf{R} key to print out the peak parameters in the command window. 

Peak \textit{width} is measured \textit{two ways}: the "Gaussian Width" is the width measured by Gaussian curve fitting (over the region colored in red in the upper panel) and is strictly accurate only for Gaussian peaks. Version \textbf{5.8} (shown below on the left) adds \href{https://terpconnect.umd.edu/~toh/spectrum/halfwidth.m}{direct measurement} of the \textit{full width at half maximum} ('FWHM') of the \textit{central peak} in the upper panel (the peak marked by the green vertical line); this works for peaks of \textit{any shape}, but it is computed only for the central peak and only if the half-maximum points fall within the zoom region displayed in the upper panel (otherwise it will return NaN). It will not be highly accurate for very noisy peaks. The Gaussian width will be more accurate for noisy or sparsely sampled peaks, but only if the peaks are at least approximately Gaussian. In the example on the left, the peaks are Lorentzian, with a true with of 3.0, plus added noise. In this case the measured FWHM (3.002) is more accurate than the Gaussian width (2.82), especially if a little smoothing is used to reduce the noise.

\InsImageInline{0.5}{l}{TestSignalAsymmetryTest.gif.png}Peak \textit{area} is also measured \textit{two ways}: the "Gaussian area" and the "Total area". The "Gaussian area" is the area under the Gaussian that is a best fit to the center portion of the signal displayed in the upper window, marked in red. The "Total area" is the area by the trapezoidal method over the entire selected segment displayed in the upper window. (The percent of total area is also calculated). If the portion of the signal displayed in the upper window is a pure Gaussian with no noise and a zero baseline, then the two measures should agree almost exactly. If the peak is not Gaussian in shape, then the total area is likely to be more accurate, if the peak is well separated from other peaks. If the peaks are overlapped, but have a known shape, then peak fitting (\textbf{Shift-F}) will give more accurate peak areas. In the example above, the Lorentzian peak at x=10 has a true area of 4.488, so in this case the total area (4.59) is more accurate than the Gaussian area (3.14), but it is too high because of overlap with the peak at x=3. Curve fitting both Lorentzians peaks together would yield the most accurate areas. If the signal is panned slightly left and right, using the left and right cursor keys or the "[" and "]" keys, the peak parameters displayed will change slightly due to the noise in the data - the more noise, the greater the change, as in the example on the left. If the peak is asymmetrical, as in this example, the peak widths displayed on one side will be greater than the other side.

There is an automatic peak finder that \InsImageInline{0.5}{l}{iSignalJkey.png}is based on the \href{https://terpconnect.umd.edu/~toh/spectrum/autopeaks.m}{autopeaks.m} function (activated by the \textbf{J} key); it asks for the peak density (roughly the number of peaks that fit into the signal record), then detects, measures, and displays the peak position, height, and area of all the peaks it detects in the processed signal currently displayed in the lower panel, plots and number the peaks as shown on the right and also \href{https://terpconnect.umd.edu/~toh/spectrum/iSignalJkeyFigure2.png}{plots each peak separately in Figure window 2} with peak, tangent, and valley points marked (click for \href{https://terpconnect.umd.edu/~toh/spectrum/iSignalJkeyFigure2.png}{graphic}). (The requested peak density controls the peaks sensitivity - larger numbers cause the routine to detect larger numbers of narrower peaks, and smaller numbers ignore the fine structure and looks for broader peaks). It also prints out the peak detection parameters that it calculates for use by any of the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{findpeaks... functions} (page \pageref{ref-0295}). To return to the usual iSignal display, press any cursor arrow key. (\textbf{Shift-J} does the same thing for the segment displayed in the upper window).

\textbf{Peak fitting}

iSignal has an \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{iterative curve fitting} (page \pageref{ref-0258}) method performed by \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit.m}. This is the most accurate method for the measurements of the areas of highly overlapped peaks. First, center the signal you wish to fit using the pan and zoom keys (cursor arrow keys), select the baseline mode by pressing the '\textbf{T}' key to \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html\#background\_subtraction}{cycle through the 4 baseline modes}: none, linear, quadratic, and flat (see page \pageref{ref-0276}). Press the \textbf{Shift-F} key, then type the desired peak shape by number from the menu displayed in the Command window (next page), enter the number of peaks, enter the number of repeat trial fits (usually 1-10), and finally \textit{click the mouse pointer on the top graph where you think the peaks might be}. (For off-screen peaks, click \textit{outside} the axis limits but \textit{inside} the graph window). A graph of the fit is displayed in Figure window 2 and a table of results is printed out in the command window. iSignal can fit many different combinations of peak shapes and constraints:

\texttt{Gaussians: y=exp(-((x-pos)./(0.6005615.*width)).\textasciicircum{}2)}

  \texttt{Gaussians with independent positions and widths(default)}\texttt{.}\texttt{...........1} 

 \texttt{Exponentially-broadened Gaussian (equal time constants).............5} 

  \texttt{Exponentially-broadened equal-width Gaussian........................8} 

  \texttt{Fixed-width exponentially-broadened Gaussian.......................36} 

  \texttt{Exponentially-broadened Gaussian (independent time constants)......31} 

  \texttt{Gaussians with the same widths......................................6} 

  \texttt{Gaussians with preset fixed widths.................................11} 

  \texttt{Fixed-position Gaussians...........................................16} 

  \texttt{Asymmetrical Gaussians with unequal half-widths on both sides......14} 

\texttt{Lorentzians: y=ones(size(x))./(1+((x-pos)./(0.5.*width)).\textasciicircum{}2)}

  \texttt{Lorentzians with independent positions and widths...................2} 

  \texttt{Exponentially-broadened Lorentzian.................................18} 

  \texttt{\textit{Equal}}\texttt{-width Lorentzians.............................................7}

  \texttt{\textit{Fixed}}\texttt{-width Lorentzian.............................................12}

  \texttt{Fixed-}\texttt{\textit{position}} \texttt{Lorentzian..........................................17}

\texttt{Gaussian/Lorentzian blend (equal blends).............................13}

  \texttt{Fixed-width Gaussian/Lorentzian blend..............................35}

  \texttt{Gaussian/Lorentzian blend with independent blends).................33}

\texttt{Voigt profile with equal alphas).....................................20}

  \texttt{Fixed-width Voigt profile with equal alphas........................34}

  \texttt{Voigt profile with independent alphas..............................30}

\texttt{Logistic: n=exp(-((x-pos)/(.477.*wid)).\textasciicircum{}2); y=(2.*n)./(1+n)...........3} 

\texttt{Pearson: y=ones(size(x))./(1+((x-pos)./((0.5.\textasciicircum{}(2/m)).*wid)).\textasciicircum{}2).\textasciicircum{}m....4}

  \texttt{Fixed-width Pearson................................................37}

  \texttt{Pearson with independent shape factors, m..........................32}

\texttt{Breit-Wigner-Fano....................................................15}

\texttt{Exponential pulse: y=(x-tau2)./tau1.*exp(1-(x-tau2)./tau1)............9}

\texttt{Alpha function: y=(x-spoint)./pos.*exp(1-(x-spoint)./pos);...........19}

\texttt{Up Sigmoid (logistic function): y=.5+.5*erf((x-tau1)/sqrt(2*tau2))...10}

\texttt{Down Sigmoid y=.5-.5*erf((x-tau1)/sqrt(2*tau2))......................23}

\texttt{Triangular...........................................................21}

\textbf{Note}: if you have a peak that is an exponentially-broadened Gaussian or Lorentzian, you can measure both the "after-broadening" height, position, and width using the \textbf{P} key function, and the "before-broadening" height, position, and approximate width by fitting the peak to an exponentially-broadened Gaussian or Lorentzian model (shapes 5, 8,36, 31, or 18) using the \textbf{Shift-F} key function. The peak areas will be the same; broadening does not affect the total peak area.

\textbf{Polynomial fitting.} 

\textbf{Shift-o} fits a simple polynomial (linear, quadratic, cubic, etc.) to the upper panel segment and displays the coefficients (in descending powers) and the correlation coefficient R\textsuperscript{2}\textsubscript{.}

\InsImageInline{0.5}{l}{image352.png}\textbf{Saving the results}

To save the processed signal to the disc as an x,y matrix in mat format, press the 'o' key, then type in the desired file name into the ``File name'' field, then press Enter or click \textbf{Save.} To load into the workspace, type ``load'' followed by the file name you typed. The processed signal will be saved in a matrix called ``Output''; to plot the processed data, type ``plot(Output(:,1),Output(:,2))''. 

\textbf{Other keystroke controls}

The \textbf{Shift-G} key toggles on and off a temporary grid on the upper and lower panel plots. 

The \textbf{L} key toggles off and on the Overlay mode, which shows the original signal as a dotted line overlaid on the current processed signal, for the purposes of comparison. 

\textbf{Shift-B} opens Figure window 2 and plots the original signal in the upper panel and the processed signal in the lower panel \href{https://terpconnect.umd.edu/~toh/spectrum/iSignalSegmentSmooth2.png}{(as shown on the right)}. 

The \textbf{Tab} key restores the original signal and cursor settings. 

The \textbf{";"} key sets the selected region to zero (use to eliminate artifacts and spikes). 

The "-" (minus sign) key is used to negate the signal (flip + for -). 

Press \textbf{H} to toggle display of semilog y plot in the lower window, which is useful for signals with very wide dynamic range, as in the example in the figures below (zero and negative points are ignored in the log plot). 

The '\textbf{+}' key takes the absolute value\index{\textcolor{color-3}{absolute value}} of the entire signal.

\textbf{Shift-L} replaces the signal with the processed version of itself, for the purpose of applying more passes of different widths of smoothing or higher orders of differentiation.

The \textbf{\textasciicircum{} (Shift-6)} key raises the signal to the specified power. To reverse this, simply raise to the reciprocal power. See \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html\#power}{Power transform method} of peak sharpening on page \pageref{ref-0111}. 


\begin{table}
\begin{tabularx}{\textwidth}{|
p{\dimexpr 0.505\linewidth-2\tabcolsep-2\arrayrulewidth}|
p{\dimexpr 0.495\linewidth-2\tabcolsep-\arrayrulewidth}|} \hline 
\InsImageInline{0.5}{l}{image353.png} \par Linear y-axis mode & \InsImageInline{0.5}{l}{image354.png} \par Log y mode (H key) \\\hline 
\end{tabularx}
\end{table}
\InsImageInline{0.5}{l}{TestingOneTwoThree.png}The \textbf{C} key condenses the signal by the specified factor \textbf{\textit{n}}, replacing each group of \textbf{\textit{n}} points with their average (\textbf{\textit{n}} must be an integer, such as 2,3, 4, etc.). The \textbf{I} key replaces the signal with a linearly interpolated version containing \textbf{\textit{m}} data points. This can be used either to increase or decrease the x-axis interval of the signal or to convert unevenly spaced values to evenly spaced values. After pressing \textbf{C} or \textbf{I}, you must type in the value of \textbf{\textit{n}} or \textbf{\textit{m}} respectively. You can press \textbf{Shift-C}, then click on the graph to print out the x,y coordinates of that point. This works on both the upper and lower panels, and on the frequency spectrum as well.



\textbf{Playing data as audio.} 

Press \textbf{Spacebar} or \textbf{Shift-P} to play the segment of the signal displayed in the upper window as audio through the computer's sound output. Press \textbf{Shift-R} to set the sampling rate - the larger the number the shorter and higher-pitched will be the sound. The default rate is 44000Hz. Sounds or music files in WAV format can be loaded into Matlab using the built-in "wavread" function. The example on the right shows a 1.5825 sec duration audio recording of the spoken phrase "Testing, one, two, three" recorded at 44000 Hz, saved in WAV format (\href{https://terpconnect.umd.edu/~toh/spectrum/TestingOneTwoThree.wav}{link}), loaded into iSignal and zoomed in on the "oo" sound in the word "two". Press \textbf{Spacebar} to play the selected sound; press \textbf{Shift-S} to display the \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html\#Spectrum}{frequency spectrum} (page \pageref{ref-0119}) of the selected region.

\texttt{{\textgreater}{\textgreater} v=wavread('}\href{https://terpconnect.umd.edu/~toh/spectrum/TestingOneTwoThree.wav}{\texttt{TestingOneTwoThree.wav}}\texttt{');}

\texttt{{\textgreater}{\textgreater} t=0:1/44001:1.5825;}

\texttt{{\textgreater}{\textgreater} isignal(t,v(:,2));}

Press \textbf{Shift-Z} to label the peaks in the frequency spectrum with their frequencies (\href{https://terpconnect.umd.edu/~toh/spectrum/SpectrumOfSelectedRegion.png}{right}). Press \textbf{Shift-}\InsImageInline{0.5}{l}{image356.png}\textbf{R} and type 44000 to set the sampling rate. This recorded sound example allows you to experiment with the effect of smoothing, differentiation, and interpolation on the sound of recorded speech. Interestingly, different degrees of smoothing and differentiation will change the \href{https://en.wikipedia.org/wiki/Timbre}{timbre }of the voice but has \textit{surprisingly} \textit{little effect on the intelligibility.} This is because the sequence of frequency components in the signal is not shifted in pitch or in time but merely changed in amplitude by smoothing and differentiation. Even computing the \textit{absolute value}\index{\textcolor{color-3}{absolute value}} (+ key), which effectively doubles the fundamental frequency, does not make the sound unintelligible. 

\textbf{Shift-Ctrl-F} transfers the current signal to Interactive Peak \textbf{F}itter (ipf.m, page \pageref{ref-0461}) and \textbf{Shift-Ctrl-P} to transfer the current signal to Interactive \textbf{P}eak Detector (iPeak.m, page \pageref{ref-0321}), if those functions are installed in your Matlab path. 

Press \textbf{K} to see \textit{all} the keyboard commands.

\textbf{EXAMPLE 1}: Single input argument; data in two columns of a matrix [x;y] or in a single y vector

  \texttt{{\textgreater}{\textgreater} isignal(y);}

  \texttt{{\textgreater}{\textgreater} isignal([x;y]);}

\textbf{EXAMPLE 2}: Two input arguments. Data in separate x and y vectors.

  \texttt{{\textgreater}{\textgreater} isignal(x,y);} 

\textbf{EXAMPLE 3:} Three or four input arguments. The last two arguments specify the initial values of pan (xcenter) and zoom (xrange) in the last two input arguments. Using data in the ZIP file:  

  \texttt{{\textgreater}{\textgreater} load data.mat}

  \texttt{{\textgreater}{\textgreater} isignal(DataMatrix,180,40); or}

  \texttt{{\textgreater}{\textgreater} isignal(x,y,180,40);}

\textbf{EXAMPLE 4}: As above, but additionally specifies initial values of SmoothMode, SmoothWidth, ends, and DerivativeMode in the last four input arguments. 

  \texttt{{\textgreater}{\textgreater} isignal(DataMatrix,180,40,2,9,0,1);}

\textbf{EXAMPLE 5}: As above, but additionally specifies initial values of the peak sharpening parameters Sharpen, Sharp1, and Sharp2 in the last three input arguments. Press the \textbf{E} key to toggle sharpening on and off for comparison.

 \texttt{{\textgreater}{\textgreater} isignal(DataMatrix,180,40,4,19,0,0,1,51,6000);}

\InsImageInline{0.5}{l}{PeakSharpeningAnimated.gif.png} \textbf{EXAMPLE 6}:

Using the built-in "humps" function:

\texttt{{\textgreater}{\textgreater} x=[0:.005:2];y=humps(x);Data=[x;y];}

4th derivative of the peak at x=0.9:

\texttt{{\textgreater}{\textgreater} isignal(Data,0.9,0.5,1,3,1,4);} 

\InsImageInline{0.5}{l}{iSignalAreaAnimation.gif.png}(You can see these animations run if you read this within \textit{Microsoft Word 365}, otherwise click on the figures).

Peak sharpening applied to the peak at x=0.3:

\texttt{isignal(Data,0.3,0.5,1,3,1,0,1,220,5400);}

 (Press 'E' key to toggle sharpening ON/OFF to compare)

\textbf{EXAMPLE 7}: Measurement of peak area. This example generates four Gaussian peaks, all with the exact same peak height (1.00) and area (1.77). Click figure for animated GIF.

\texttt{{\textgreater}{\textgreater} x=[0:.01:20];}

\texttt{{\textgreater}{\textgreater} y=exp(-(x-4).\textasciicircum{}2)+exp(-(x-9).\textasciicircum{}2)+exp(-(x-13).\textasciicircum{}2)+exp(-(x-15).\textasciicircum{}2);}

\texttt{{\textgreater}{\textgreater}} \texttt{isignal(x,y);} 

\InsImageInline{0.5}{l}{AnimatedSpikes.gif.png}The first peak (at x=4) is isolated, the second peak (x=9) is slightly overlapped with the third one, and the last two peaks (at x= 13 and 15) are strongly overlapped. To measure the area under a peak using the perpendicular drop method (page \pageref{ref-0184}), position the dotted red marker lines at the minimum between the overlapped peaks. Greater accuracy in peak area measurement using iSignal can be obtained by using the \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html\#peak\_sharpening}{peak sharpening function} to reduce the overlap between the peaks. This reduces the peak widths, increases the peak heights, but has no effect on the peak areas.

\textbf{EXAMPLE 8:} Single peak with random spikes (shown in the figure on the right). Compare smoothing vs spike filter (\textbf{M} key) and \href{http://en.wikipedia.org/wiki/Slew\_rate}{slew rate limit} (\textbf{\textasciitilde{}} key) to remove spikes. 

x=-5:.01:5;

y=exp(-(x).\textasciicircum{}2);

for n=1:1000,

 if randn(){\textgreater}2,y(n)=rand()+y(n),

 end,

end;

isignal(x,y);

\textbf{\InsImageInline{0.5}{l}{isignaldemo2.png}}\textbf{EXAMPLE 9:} \textbf{Weak peaks on a strong baseline.}

The demo script \href{https://terpconnect.umd.edu/~toh/spectrum/isignaldemo2.m}{isignaldemo2} (shown on the left) creates a test signal containing four peaks with heights 4, 3, 2, 1, with equal widths, superimposed on a very strong curved baseline, plus added random white noise. The objective is to extract a measure that is proportional to the peak height but independent of the baseline strength. Suggested approaches: (a) Use automatic or manual baseline subtraction to remove the baseline, measure peaks with the P-P measure in the upper panel; or (b) use differentiation (with smoothing) to suppress the baseline; or (c) use curve fitting (\textbf{Shift-F}), with baseline correction (\textbf{T}), to measure peak height. After running the script, you can press \textbf{Enter} to have the script perform an automatic 3rd derivative calibration, performed by lines 56 to 74. As indicated in the script, you can change several of the constants; search for the word "change". (To use the derivative method, the \textit{width} of the peaks must all be equal and stable, but the peak \textit{positions} may vary within limits, set by the Xrange for each peak in lines 61-67). You must have isignal.m and plotit.m installed.

\textbf{EXAMPLE 10:} Direct entry into frequency spectrum mode, plotting returned frequency spectrum.

  \texttt{{\textgreater}{\textgreater} x=0:.1:60; y=sin(x)+sin(10.*x);}

  \texttt{{\textgreater}{\textgreater} [pY, SpectrumOut]=isignal([x;y],30,30,4,3,1,0,0,1,0,0,0,1);}

  \texttt{{\textgreater}{\textgreater} plot(SpectrumOut)}

\textbf{EXAMPLE 11:} The demo script \href{https://terpconnect.umd.edu/~toh/spectrum/demoisignal.m}{demoisignal.m} is a self-running demo that requires iSignal 4.2 or later and the latest version of \href{https://terpconnect.umd.edu/~toh/spectrum/plotit.m}{plotit.m} to be installed.

\textbf{EXAMPLE 12:} Here's a simple example of a very noisy signal with lots of high-frequency (blue) noise obscuring a perfectly good peak in the center at x=150, height=1e-4; SNR=90. First, download the data file \href{https://terpconnect.umd.edu/~toh/spectrum/NoisySignal.mat}{NoisySignal.mat} into the Matlab path, then execute these statements:

\texttt{{\textgreater}{\textgreater} load} \href{https://terpconnect.umd.edu/~toh/spectrum/NoisySignal.mat}{\texttt{NoisySignal}}

\texttt{{\textgreater}{\textgreater} isignal(x,y);}

Use the \textbf{A} and \textbf{Z} keys to increase and decrease the smooth width, and the \textbf{S} key to cycle through the available smooth type. Hint: use the P-spline smooth and keep increasing the smooth width. Zoom in on the peak in the center, press P to enter the peak mode, and it will display the characteristics of the peak in the upper left.

\label{ref-0439}\label{ref-0440}

\textbf{\textit{iSignal}} \textbf{keyboard controls (Version 8.3):}

\texttt{Pan signal left and right...Coarse pan:} \texttt{\textbf{{\textless}}} \texttt{and} \texttt{\textbf{{\textgreater}}}

                             \texttt{Fine pan: left and right cursor arrows}

                             \texttt{Nudge:} \texttt{\textbf{[}} \texttt{and} \texttt{\textbf{]}} 

 \texttt{Zoom in and out.............Coarse zoom:} \texttt{\textbf{/}} \texttt{and} \texttt{\textbf{"}} 

                             \texttt{Fine zoom: up and down cursor arrows}

 \texttt{Resets pan and zoom.........ESC}

 \texttt{Select entire signal........}\texttt{\textbf{Ctrl-A}}

 \texttt{Display grid................}\texttt{\textbf{Shift-G}}  \texttt{temporarily display grid on both panels}

 \texttt{Adjust smooth width.........}\texttt{\textbf{A}}\texttt{,}\texttt{\textbf{Z}} \texttt{(A={\textgreater}more, Z={\textgreater}less)} 

 \texttt{Set smooth width vector.....}\texttt{\textbf{Shift-Q}}  \texttt{for segmented smooth}

 \texttt{Cycle smooth types..........}\texttt{\textbf{S}} \texttt{(No, Rect., Triangle, Gaussian, Savitzky-Golay)}

 \texttt{Toggle smooth ends..........}\texttt{\textbf{X}} \texttt{(0=ends zeroed 1=ends smoothed (slower)}

 \texttt{Symmetrize (de-tailing)} \texttt{\textbf{Shift-Y}} \texttt{allows entry of symmetrize weighting factor}

 \texttt{Adjust Symmetrization.......}\texttt{\textbf{1}}\texttt{,}\texttt{\textbf{2}} \texttt{keys: decrease, increase by 10\%}

 \texttt{\textbf{Shift-1}}\texttt{,}\texttt{\textbf{Shift-2}} \texttt{decrease, increase by 1\%}

 \texttt{ConV/deconVolution mode.....}\texttt{\textbf{Shift-V}} \texttt{presents menu of conv/deconv choices}

 \texttt{Adjust width................}\texttt{\textbf{3}}\texttt{,}\texttt{\textbf{4}}\texttt{: decrease,increase by 10\%}

 \texttt{\textbf{Shift-3}}\texttt{,}\texttt{\textbf{Shift-4}}\texttt{: decrease,increase by 1\%}

 \texttt{Cycle derivative orders.....}\texttt{\textbf{D}}\texttt{/}\texttt{\textbf{Shift-D}} \texttt{Increase/Decrease derivative order}

 \texttt{Toggle peak sharpening......}\texttt{\textbf{E}} \texttt{(0=OFF 1=ON)}

 \texttt{Sharpening for Gaussian.....}\texttt{\textbf{Y}}  \texttt{Set sharpen settings for Gaussian}

 \texttt{Sharpening for Lorentzian...}\texttt{\textbf{U}}  \texttt{Set sharpen settings for Lorentzian}

 \texttt{Adjust sharp1...............}\texttt{\textbf{F}}\texttt{,}\texttt{\textbf{V}}  \texttt{F={\textgreater}sharper, V={\textgreater}less sharpening}

 \texttt{Adjust sharp2 ............}\texttt{\textbf{G}}\texttt{,}\texttt{\textbf{B}}  \texttt{G={\textgreater}sharper, B={\textgreater}less sharpening}

 \texttt{Slew rate limit (0=OFF).....}\texttt{\textbf{\textasciitilde{}}}  \texttt{Largest allowed y change between points}

 \texttt{Spike filter width (O=OFF)..}\texttt{\textbf{m}}  \texttt{spike filter eliminates sharp spikes}

 \texttt{Toggle peak parabola........}\texttt{\textbf{P}}  \texttt{fits parabola to center, labels vertex}

 \texttt{Fit polynomial to segment...}\texttt{\textbf{Shift-o}}  \texttt{Asks for polynomial order}

 \texttt{Fits peak in upper window...}\texttt{\textbf{Shift-}}\texttt{F (Asks for shape, number of peaks, etc.)}

 \texttt{Find peaks in lower panel...}\texttt{\textbf{J}} \texttt{(Asks for Peak Density)}

 \texttt{Find peaks in upper panel...}\texttt{\textbf{Shift-J}} \texttt{(Asks for Peak Density)}

 \texttt{Spectrum mode on/off........}\texttt{\textbf{Shift-S}} \texttt{(Shift-A and Shift-X to change axes)}

 \texttt{Peak labels on spectrum.....}\texttt{\textbf{Shift-Z}} \texttt{in spectrum mode} 

 \texttt{Click graph to print x,y....}\texttt{\textbf{Shift-C}}  \texttt{Click graph to print coordinates}

 \texttt{Display Waterfall spectrum..}\texttt{\textbf{Shift-W}}  \texttt{Allows choice of mesh, surf, contour, etc.}

 \texttt{Transfer power spectrum.....}\texttt{\textbf{Shift-T}} \texttt{Replaces signal with power spectrum}

 \texttt{Lock in current processing..}\texttt{\textbf{Shift-L}} \texttt{Replace signal with processed version}

 \texttt{ConVolution/DeconVolution...}\texttt{\textbf{Shift-V}} \texttt{Convolution/Deconvolution menu}

 \texttt{Power transform method.....} \texttt{\textbf{\textasciicircum{}}} \texttt{(}\texttt{\textbf{Shift-6}}\texttt{) Raises the signal to a specified power.}

 \texttt{Print peak report...........}\texttt{\textbf{R}}  \texttt{prints position, height, width, area}

 \texttt{Toggle log y mode...........}\texttt{\textbf{H}}  \texttt{semilog plot in lower window}

 \texttt{Cycles baseline mode........}\texttt{\textbf{T}}  \texttt{none, linear, quadratic, or flat baseline mode}

 \texttt{Restores original signal....}\texttt{\textbf{Tab}} \texttt{or} \texttt{\textbf{Ctrl-Z}} \texttt{key resets to original signal and modes}

 \texttt{Toggle overlay mode.........}\texttt{\textbf{L}}  \texttt{Overlays original signal as dotted line}

 \texttt{Display current signals.....}\texttt{\textbf{Shift-B}} \texttt{Original (top) vs Processed (bottom)}

 \texttt{Baseline subtraction........}\texttt{\textbf{Backspace}}\texttt{, then click baseline at multiple points}

 \texttt{Restore background..........}\texttt{\textbf{\textbackslash }}  \texttt{to cancel previous background subtraction}

 \texttt{Invert signal...............}\texttt{\textbf{Shift-N}} \texttt{Invert (negate) the signal (flip + and -)}

 \texttt{Remove offset...............}\texttt{\textbf{0}}  \texttt{(zero) set minimum signal to zero} 

 \texttt{Sets region to zero.........}\texttt{\textbf{;}}  \texttt{sets selected region to zero}

 \texttt{Absolute value..............}\texttt{\textbf{+}}  \texttt{Computes absolute value of entire signal}

 \texttt{Condense signal.............}\texttt{\textbf{C}}  \texttt{Condense oversampled signal by factor of N}

 \texttt{Interpolate signal..........}\texttt{\textbf{i}}  \texttt{Interpolate (resample) to N points}

 \texttt{Print report................}\texttt{\textbf{Q}}  \texttt{prints signal info and current settings}

 \texttt{Print keyboard commands.....}\texttt{\textbf{K}}  \texttt{prints this list of keyboard commands}

 \texttt{Print isignal arguments.....}\texttt{\textbf{W}}  \texttt{prints isignal function with all current arguments}

 \texttt{Save output to disk.........}\texttt{\textbf{O}}  \texttt{Save .mat file with processed signal matrix}

 \texttt{Play signal as sound........}\texttt{\textbf{Spacebar}} \texttt{or} \texttt{\textbf{Shift-P}}  \texttt{Play selection through speaker}

 \texttt{Play signal as sound........}\texttt{\textbf{Shift-R}} \texttt{Change sampling rate for playing sound}

 \texttt{Switch to ipf.m.............}\texttt{\textbf{Shift-Ctrl-F}} \texttt{transfers current signal to} 

 \texttt{Interactive Peak Fitter, ipf.m}

 \texttt{Switch to iPeak.............}\texttt{\textbf{Shift-Ctrl-P}}  \texttt{transfers current signal to} 

 \texttt{Interactive Peak Detector, ipeak.m}

\href{http://terpconnect.umd.edu/~toh/spectrum/ProcessSignal.m}{ProcessSignal}, a Matlab/Octave command-line function that performs smoothing and differentiation on the time-series data set x,y (column or row vectors). Type "help ProcessSignal". It returns the processed signal as a vector that has the same shape as x, regardless of the shape of y. The syntax is \texttt{Processed=ProcessSignal(x,y,DerivativeMode,w,type,ends,Sharpen,factor1,factor2,Symize,Symfactor,SlewRate,MedianWidth)}\label{ref-0441}

\section{\textit{iFilter}, keyboard-operated interactive Fourier filter\label{ref-0442}}

\textbf{\textit{iFilter}} is a keyboard-operated interactive Fourier filter function (page \pageref{ref-0164}) for time-series signal (x,y), with keyboard controls that allow you to adjust the filter parameters continuously while observing the effect on your signal dynamically. Optional input arguments set the initial values of center frequency, filter width, shape, plotmode (1=linear; 2=semilog frequency; 3=semilog amplitude; 4=log-log) and filter mode ('band-pass', 'low-pass', 'high-pass', 'band-reject (notch), 'comb pass', and 'comb notch'). In the comb modes, the filter has multiple bands located at frequencies 1, 2, 3, 4... multiplied by the center frequency, each with the same (controllable) width and shape. The interactive keypress operation works even if you run \href{https://www.mathworks.com/products/matlab-online.html}{Matlab in a web browser}, but not on \href{https://itunes.apple.com/us/app/matlab-mobile/id370976661?mt=8}{Matlab Mobile} or in Octave.

The filtered signal can be returned as the function value, saved as a ".mat" file on the disk, or played through the computer's sound system. Press \textbf{K} to list keyboard commands. This is a self-contained Matlab function that does not require any toolboxes or add-on functions. Click \href{https://terpconnect.umd.edu/~toh/spectrum/ifilter.m}{here} to view or download and place it in the Matlab path. At the Matlab command prompt, type: \texttt{FilteredSignal}=\texttt{ifilter(x,y)} or \texttt{ifilter(y)} or \texttt{ifilter(xymatrix)} or 

\InsImageInline{0.5}{l}{iFilterAnimation.gif.png}\texttt{FilteredSignal=ifilter(x,y,center,width,shape,plotmode,filtermode)}

\textbf{Example 1} You can see this animation if you read this within \textit{Microsoft Word 365}, otherwise click the figure. Periodic waveform with 2 frequency components at 60 and 440 Hz.

\texttt{x=[0:.001:2*pi];}

\texttt{y=sin(60.*x.*2.*pi)+2.*sin(440.*x.*2.*pi);}

\texttt{ifilter(x,y);}

 \textbf{Example 2:} uses optional input arguments to set initial values:

 \texttt{x=0:(1/8000):.3;}

 \texttt{y=(1+12/100.*sin(2*47*pi.*x)).*sin(880*pi.*x)+(1+12/100.*sin(2*20*pi.*x)).*sin(2000*pi.*x);}

 \texttt{ry=ifilter(x,y,440,31,18,3,'Band-pass');}

\textbf{Example 3:} Picking one frequency out of a noisy sine wave.

 \texttt{x=[0:.01:2*pi]';}

 \texttt{y=sin(20*x)+3.*randn(size(x));}

 \texttt{ifilter(x,y,3.1,0.85924,15,1,'Band-pass');}


\begin{center}
 \InsImageInline{0.5}{l}{iFilterExample3.png}
\end{center}


\textbf{Example 4:} Square wave with band-pass vs Comb pass filter

 \texttt{t = 0:.0001:.0625;}

 \texttt{y=square(2*pi*64*t);}

 \texttt{ifilter(t,y,64,32,12,1,'Band-pass');}

 \texttt{ifilter(t,y,48,32,2,1,'Comb pass');}

\textbf{Example 5:} \href{https://terpconnect.umd.edu/~toh/spectrum/MorseCode.m}{MorseCode.m} uses iFilter to demonstrate the abilities and limitations of Fourier filtering. It creates a pulsed \href{https://terpconnect.umd.edu/~toh/spectrum/SOSspectrum.png}{\textit{fixed frequency} sine wave} that \href{https://terpconnect.umd.edu/~toh/spectrum/SOS.png}{spells out ``SOS'' in Morse code} (dit-dit-dit/dah-dah-dah/dit-dit-dit), adds random white noise so that the \href{https://terpconnect.umd.edu/~toh/spectrum/NoisySOSWideband.png}{SNR is very poor} (about 0.1 in this example). The white noise has a frequency spectrum that is \href{https://terpconnect.umd.edu/~toh/spectrum/WhiteNoiseSpectrum.png}{spread out over the entire range of frequencies;} the signal itself is concentrated mostly at a fixed frequency (0.05) but \href{https://en.wikipedia.org/wiki/Amplitude\_modulation}{the modulation of the sine wave by the Morse Code pulses} spreads out its spectrum over a \href{https://terpconnect.umd.edu/~toh/spectrum/MorseCodePowerSpectrum.png}{narrow frequency range of about 0.0004}. This suggests that a Fourier bandpass filter tuned to the signal frequency might be able to isolate the signal from the noise. As the \href{https://terpconnect.umd.edu/~toh/spectrum/NoisySOSbandpassTooWide.png}{bandwidth is reduced}, the signal-to-noise ratio improves and the \href{https://terpconnect.umd.edu/~toh/spectrum/NoisySOSbandpass.png}{signal begins to emerges from the noise} until \href{https://terpconnect.umd.edu/~toh/spectrum/NoisySOSbandpassNarrower.png}{it becomes clear}, but if the \href{https://terpconnect.umd.edu/~toh/spectrum/NoisySOSbandpassTooNarrow.png}{bandwidth is \textit{too} narrow}, the \textit{step response time} is too slow to give distinct ``dits'' and ``dahs''. The step response time is inversely proportional to the bandwidth. (Use the ? and " keys to adjust the bandwidth. Press '\textbf{P}' or the Spacebar to hear the \textbf{\InsImageInline{0.5}{l}{NoisySOSbandpass.png}}sound). You can actually \textit{hear} that sine wave component better than you can \textit{see} it in the waveform plot (upper panel), because \href{http://www.britannica.com/topic/Fourier-analysis}{the ear works like a spectrum analyzer}, with separate nerve endings assigned to specific frequency ranges, whereas the eye analyzes the graph spatially, looking at the overall amplitude and not at individual frequencies. \href{https://terpconnect.umd.edu/~toh/spectrum/MorseCode.mp4}{Click for mp4 video of this script in operation, with sound.} This video is also on YouTube at \url{https://youtu.be/agjs1-mNkmY}.

\textbf{Example 6:} This example (graphic on next page) shows a 1.5825 sec duration audio recording of the spoken phrase "Testing, one, two, three", previously recorded at 44001 Hz and saved in both WAV format (\href{https://terpconnect.umd.edu/~toh/spectrum/TestingOneTwoThree.wav}{download link}) and in ".mat" format (\href{https://terpconnect.umd.edu/~toh/spectrum/testing123.mat}{download link}). This data file is loaded into \href{https://terpconnect.umd.edu/~toh/spectrum/ifilter.m}{iFilter}, which is initially set to bandpass mode and tuned to a narrow segment above 1000 Hz that is \textit{well above} the frequency range of most of the signal. That passband misses most of the frequency components in the signal, yet even in that case, the speech is still intelligible, demonstrating the remarkable ability of the ear-brain system to make do with a highly compromised signal. Press \textbf{P} or \textbf{space} to hear the filter's output on your computer’s sound system. Different filter settings will change the \href{https://en.wikipedia.org/wiki/Timbre}{timbre} of the sound, but you can still understand it. \label{ref-0443}

\InsImageInline{0.5}{l}{image362.png}\textbf{\textit{iFilter}} \textbf{4.3 keyboard controls} (if the figure window is not topmost, click on it first):

 \texttt{Adjust filter frequency........Coarse (10\% change):} \texttt{\textbf{{\textless}}} \texttt{and} \texttt{\textbf{{\textgreater}}}

   \texttt{Fine (1\% change): left and right cursor}

   \texttt{arrows}

 \texttt{Adjust filter width............Coarse (10\% change):} \texttt{\textbf{/}} \texttt{and} \texttt{\textbf{"}} 

   \texttt{Fine (1\% change): up and down cursor arrows}

 \texttt{Filter shape...................}\texttt{\textbf{A,Z}} \texttt{(}\texttt{\textbf{A}} \texttt{more rectangular,} \texttt{\textbf{Z}} \texttt{more Gaussian)}

 \texttt{Filter mode....................}\texttt{\textbf{B}}\texttt{=bandpass;} \texttt{\textbf{N}} \texttt{or R=notch (band reject);}

                               \texttt{\textbf{H}}\texttt{=High-pass;}

  \texttt{\textbf{L}}\texttt{=Low-pass;} \texttt{\textbf{C}}\texttt{=Comb pass;} \texttt{\textbf{V}}\texttt{=Comb notch.}

 \texttt{Select plot mode...............}\texttt{\textbf{1}}\texttt{=linear;} \texttt{\textbf{2}}\texttt{=semilog frequency}

  \texttt{\textbf{3}}\texttt{=semilog amplitude;} \texttt{\textbf{4}}\texttt{=log-log}

 \texttt{Print keyboard commands........}\texttt{\textbf{K}}  \texttt{Prints this list}

 \texttt{Print filter parameters........}\texttt{\textbf{Q}} \texttt{or} \texttt{\textbf{W}} \texttt{Prints ifilter with input}

                                \texttt{arguments: center, width, shape,}

                                 \texttt{plotmode, filtermode}

 \texttt{Print current settings.........}\texttt{\textbf{T}}  \texttt{Prints list of current settings}

 \texttt{Switch SPECTRUM X-axis scale...}\texttt{\textbf{X}} \texttt{switch between frequency and period} 

  \texttt{on the horizontal axis}

 \texttt{Switch OUTPUT Y-axis scale.....}\texttt{\textbf{Y}} \texttt{switch output plot between fixed or}

                                \texttt{variable vertical axis.} 

 \texttt{Play output as sound...........}\texttt{\textbf{P}} \texttt{or Enter}

 \texttt{Save output as .mat file.......}\texttt{\textbf{S}}

\section{Matlab/Octave Command-line Peak Fitters\label{ref-0444}\label{ref-0445}\label{ref-0446}\label{ref-0447}}

A Matlab peak fitting program for time-series signals, which uses an unconstrained \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{non-linear optimization algorithm} (page \pageref{ref-0258}) to decompose a complex, overlapping-peak signal into its component parts. The objective is to determine whether your signal can be represented as the sum of fundamental underlying peaks shapes. Accepts signals of any length, including those with non-integer and non-uniform x-values. There are \textbf{two different versions}, 

(1) a \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#command}{command line version} (\textbf{peakfit.m}) for Matlab or \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html\#Octave}{Octave}, Matlab File Exchange "\href{http://blogs.mathworks.com/pick/2016/09/09/most-activeinteractive-file-exchange-entry/}{Pick of the Week}". The current version number is \textbf{9.2.} 

(2) a \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{keypress operated interactive version} (\textbf{ipf.m,} page \pageref{ref-0461}) for Matlab only. The current version number is 13.

The difference between them is that peakfit.m is completely controlled by command-line input arguments and returns its information via command-line output arguments; ipf.m allows interactive control via keypress commands. For automating the fitting of large numbers of signals, \textbf{peakfit.m} is better (see page \pageref{ref-0411}); but \textbf{ipf.m} is best for exploring signals to determine the optimum fitting range, peak shapes, number of peaks, baseline correction mode, etc. Otherwise, they have similar curve-fitting capabilities. The basic built-in peak shape models available are illustrated on page ~\ref{ref-0469}~\ref{ref-0469}\pageref{ref-0469} \href{https://terpconnect.umd.edu/~toh/spectrum/ShapeDemo.png}{;} custom peak shapes \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#NewShape}{can be added} (see page \pageref{ref-0477}). See pages \pageref{ref-0473} and \pageref{ref-0479} for more information and useful suggestions.\href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Top}{}

\subsection{Matlab/Octave command-line function: peakfit.m\label{ref-0448}\label{ref-0449}}

\InsImageInline{0.5}{l}{peakfit7.png}\uline{\textbf{\textcolor{color-6}{P}}}\href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{\textbf{eakfit.m}} is a user-defined command window peak fitting function for Matlab or Octave, usable from a remote terminal. It is written as a self-contained function in a single m-file. (To view of download, click \uline{\textcolor{color-6}{p}}\href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{eakfit.m}). It takes data in the form of a 2 by n matrix that has the independent variables (X-values) in row 1 and the dependent variables (Y-values) in row 2, or as a single dependent variable vector. The syntax is \texttt{[FitResults, GOF, baseline, coeff, residuals, xi, yi, BootResults]=peakfit(signal, center, window, NumPeaks, peakshape, extra, NumTrials, start, BASELINEMODE, fixedparameters, plots, bipolar, minwidth, DELTA, clipheight))}. Only the first input argument, the data matrix, is absolutely required; there are default values for all other inputs. All the input and output arguments are explained below.

The screen display is shown on the right; the upper panel shows the data as \textbf{\textcolor{color-10}{blue}} \textbf{dots}, the combined model as a \textbf{\textcolor{color-19}{red}} \textbf{line} (ideally overlapping the \textbf{blue dots}), and the model components as \textbf{\textcolor{color-20}{green}} \textbf{lines}. The dotted \textbf{\textcolor{color-21}{magenta}} lines are the first-guess peak positions for the last fit. The lower panel shows the residuals (difference between the data and the model).

You can download a \href{https://terpconnect.umd.edu/~toh/spectrum/ipf13zip}{ZIP file} containing peakfit.m, DemoPeakFit.m, ipf.m, Demoipf.m, some sample data for testing, and a test script (\href{https://terpconnect.umd.edu/~toh/spectrum/testpeakfit.m}{t}\href{https://terpconnect.umd.edu/~toh/spectrum/testpeakfit.m}{estpeakfit.m} or \href{https://terpconnect.umd.edu/~toh/spectrum/autotestpeakfit.m}{autotestpeakfit.m}) that runs all the examples sequentially to test for proper operation. For a discussion of the accuracy and precision of peak parameter measurement using peakfit.m, click \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#accuracy}{here}. 

The peakfit.m functionality can also be accessed by the keypress-operated interactive functions \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#3.\_Interactive\_keypress-operated\_}{\textbf{\textit{ipf}}} (page \pageref{ref-0462}), \href{https://terpconnect.umd.edu/~toh/spectrum/ipeak.html}{\textbf{\textit{iPeak}}} (page \pageref{ref-0461})\textbf{,} and \href{https://terpconnect.umd.edu/~toh/spectrum/isignal.html}{\textbf{\textit{iSignal}}} (page \pageref{ref-0432}) for \textit{Matlab} or \textit{Matlab Online}.

\textbf{Version 9.3:} August 2019. Fixed a bug in the Voigt peak shape. Previous version m 9.1 (January 2018) added peak shape \textbf{50} which implements the multilinear regression ("classical least-squares") method for cases in which the peak shapes, position, and widths are \textit{all known} and \textit{only} the peak heights are to be determined. See \textbf{Example 40} below and the demonstration scripts \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit9demo.m}{peakfit9demo.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit9demoL.m}{peakfit9demoL.m} for a demonstration and comparison of this to unconstrained iterative fitting. \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Top}{}

Peakfit.m can be called with several optional additional command line arguments. \textit{All input arguments (except the signal itself) can be replaced by zeros to use their default values.}

\texttt{\textbf{peakfit(signal);}}

Performs an iterative least-squares fit of a single unconstrained Gaussian peak to the entire data matrix "signal", which has x values in row 1 and Y values in row 2 (e.g. [x y]) or which may be a single signal vector (in which case the data points are plotted against their index numbers on the x axis).

\texttt{\textbf{peakfit(signal, center, window);}}

Fits a single unconstrained Gaussian peak to a portion of the matrix "signal". The portion is centered on the x-value "center" and has width "window" (in x units). 

\textit{In this and in all following examples,} \textbf{\textit{set "center" and "window" both to 0}} \textbf{\textit{to fit the entire signal}}\textit{.} 

\texttt{\textbf{peakfit(signal, center, window, NumPeaks);}}

"NumPeaks" = number of peaks in the model (default is 1 if not specified). 

\texttt{\textbf{peakfit(signal, center, window, NumPeaks, peakshape);}}

Number or vector that specifies the peak shape(s) of the model: \textbf{1}=unconstrained Gaussian, \textbf{2}=unconstrained Lorentzian, \textbf{3}=logistic \textit{distribution}, \textbf{4}=Pearson, \textbf{5}=exponentially broadened Gaussian; \textbf{6}=equal-width Gaussians, \textbf{7}=equal-width Lorentzians, \textbf{8}=exponentially broadened equal-width Gaussians, \textbf{9}=exponential pulse, \textbf{10}= up-sigmoid (logistic \textit{function}), \textbf{11}=fixed-width Gaussians, \textbf{12}=fixed-width Lorentzians, \textbf{13}=Gaussian/Lorentzian blend; \textbf{14}=bifurcated Gaussian, \textbf{15}=Breit-Wigner-Fano resonance; \textbf{1}\textbf{6}=Fixed-position Gaussians; \textbf{17}=Fixed-position Lorentzians; \textbf{18}=exponentially broadened Lorentzian; \textbf{19}=alpha function; \textbf{20}=Voigt profile; \textbf{21}=triangular; \textbf{23}=down-sigmoid; \textbf{25}=lognormal distribution; \textbf{26}=linear baseline (see Example 28); \textbf{28}=polynomial (extra=polynomial order; Example 30); \textbf{29}=articulated linear segmented\index{\textcolor{color-3}{segmented}} (see Example 29); \textbf{30}=independently-variable alpha Voigt; \textbf{31}=independently-variable time constant ExpGaussian; \textbf{32}=independently-variable Pearson; \textbf{33}=independently-variable Gaussian/Lorentzian blend; \textbf{34}=fixed-width Voigt; \textbf{35}=fixed-width Gaussian/Lorentzian blend; \textbf{36}=fixed-width exponentially-broadened Gaussian; \textbf{37}=fixed-width Pearson;\textbf{38}= independently-variable time constant ExpLorentzian; \textbf{39=} alternative independently-variable time constant ExpGaussian (see Example 39 below); \textbf{40}=sine wave; \textbf{41}=rectangle; \textbf{42}=flattened Gaussian; \textbf{43}=Gompertz function (3 parameter logistic: \texttt{Bo*exp(-exp((Kh*exp(1)/Bo)*(L-t)+1))}); \textbf{44}=1-exp(-k*x); \textbf{45}: Four-parameter logistic y = maxy*(1+(miny-1)/(1+(x/ip)\textasciicircum{}slope)); \textbf{46}=quadratic baseline (see Example 38); \textbf{47}=blackbody emission; \textbf{48}=equal-width exponential pulse; \textbf{50}=multilinear regression (known peak positions and widths). The function \href{https://terpconnect.umd.edu/~toh/spectrum/ShapeDemo.m}{ShapeDemo }demonstrates most of the basic peak shapes (graphic on page \pageref{ref-0469}) showing the variable-shape peaks as multiple lines.

\textbf{Note 1}: "unconstrained" simply means that the position, height, and width of each peak in the model can vary independently of the other peaks, as opposed to the equal-width, fixed-width, or fixed position variants. Shapes 4, 5, 13, 14, 15, 18, 20, and 34-37 are are constrained to the same \textit{shape constant}; shapes 30-33 are completely unconstrained in position, width \textit{and} shape; their shape variables are determined by iteration. 

\textbf{Note 2}: The value of the shape constant "extra" defaults to 1 if not specified in input arguments. 

\textbf{Note 3}: The peakshape argument can be a \textit{vector of different shapes for each peak}, e.g. [1 2 1] for three peaks in a Gaussian, Lorentzian, Gaussian sequence. (The next input argument, 'extra', must be a vector of the same length as 'peakshape'. See \textbf{Examples 24}, \textbf{25}, \textbf{28} and \textbf{38}, below.

\texttt{\textbf{peakfit(signal, center, window, NumPeaks, peakshape, extra)}}

  Specifies the value of 'extra', used in the Pearson, exponentially-broadened Gaussian, Gaussian/Lorentzian blend, bifurcated Gaussian, and Breit-Wigner-Fano shapes to fine-tune the peak shape. The value of "extra" defaults to 1 if not specified in input arguments. In version \textbf{5}, 'extra' can be a vector of different extra values for each peak).

  

\texttt{\textbf{peakfit(signal, center, window, NumPeaks, peakshape, extra, NumTrials);}}

  Restarts the fitting process "NumTrials" times \textit{with slightly different start values} and selects the best one (with lowest fitting error). NumTrials can be any positive integer (default is 1). In many cases, NumTrials=1 will be sufficient, but if that does not give consistent results, increase NumTrials until the result are stable. 

\texttt{\textbf{peakfit(signal, center, window, NumPeaks, peakshape, extra, NumTrials, start)}}

  Specifies the first guesses vector "start" for the peak positions and widths, e.g., start=[position1 width1 position2 width2 ...]. Only necessary for difficult cases, especially when there are a lot of adjustable variables. The start values can usually be approximate average values based on your experience. If you leave this off, or set start=0, the program will generate its own start values (which is often good enough). See examples 22, 28, 40, and 42 for situation where specifying the start values is useful.

\texttt{\textbf{peakfit(signal, center, window, NumPeaks, peakshape, extra, NumTrials, start, BaselineMode)}}

As above, but "BaselineMode" sets the baseline correction mode in the last argument: \texttt{\textbf{BaselineMode}}=0 (default) does \textit{not} subtract baseline from data segment. \texttt{\textbf{BaselineMode}}=1 interpolates a \textit{linear} baseline from the edges of the data segment and subtracts it from the signal (assumes that the peak returns to the baseline at the edges of the signal); BaselineMode=2, like mode 1 except that it computes a \textit{quadratic} curved baseline; BaselineMode=3 compensates for a \textit{flat} baseline without reference to the signal itself (does not require that the signal return to the baseline at the edges of the signal, as does modes 1 and 2). Coefficients of the polynomial baselines are returned in the third output argument "baseline".

\texttt{\textbf{peakfit(signal,0,0,0,0,0,0,0,2)}}

  Use zeros as placeholders to use the default values of input arguments. In this case, \texttt{\textbf{BaselineMode}} is set to 2, but all others are the default values.

\texttt{\textbf{peakfit(signal, center, window, NumPeaks, peakshape, extra, NumTrials, start, BaselineMode, fixedparameters)}}

 'fixedparameters' (10th input argument) specifies fixed widths or positions in shapes 11, 12, 16, 17, 34-37, one entry for each peak. When using peak shape 50 (multlinear regression), 'fixedparameters' must be a matrix listing the peak shape number (column 1), position (column 2), and width (column 3) of each peak, one row per peak.

\texttt{\textbf{peakfit(signal, center, window, NumPeaks, peakshape, extra, NumTrials, start, BaselineMode, fixedparameters, plots)}}

 'plots' (11th input argument) controls graphic plotting: 0=no plot; 1=plots draw as usual (default)

\texttt{\textbf{peakfit(signal, center, window, NumPeaks, peakshape, extra, NumTrials, start, BaselineMode, fixedparameters, plots, bipolar)}}

 (12th input argument) 'bipolar' = 0 constrain peaks heights to be positive; 'bipolar' = 1 allows positive and negative peak heights.

 

\texttt{\textbf{peakfit(signal, center, window, NumPeaks, peakshape, extra, NumTrials, start, BaselineMode, fixedparameters, plots, bipolar,minwidth)}}

'minwidth' (13th input argument) sets the minimum allowed peak width. The default if not specified is equal to the x-axis interval. Must be a vector of minimum widths, one value for each peak, if the multiple peak shape is chosen.

\texttt{\textbf{peakfit(signal,center, window, NumPeaks, peakshape, extra, NumTrials, start, BaselineMode, fixedparameters, plots, bipolar, minwidth, DELTA)}}

 'DELTA' (14th input argument) controls the restart variance when NumTrials{\textgreater}1. Default value is 1.0. Larger values give more variance. Version 5.8 and later only. 

\texttt{\textbf{[FitResults,FitError]= peakfit(signal, center, window...);}}

Returns the FitResults vector in the order peak number, peak position, peak height, peak width, and peak area), and the FitError (the percent RMS difference between the data and the model in the selected segment of that data) of the best fit. 

\textbf{Labeling the FitResults table}: Using the "table" function, you can display FitResults in a neat table with column labels, using only a single line of code: 

\texttt{\textbf{disp(table(FitResults(:,2), FitResults(:,3), FitResults(:,4), FitResults(:,5), 'VariableNames', \{'Position' 'Height' 'FWHM' 'Area}}\index{\textbf{Area}}\texttt{\textbf{'\}))}}

  \texttt{\textbf{Position Height FWHM Area}} 

  \texttt{\textbf{\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_ \_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_}} 

  \texttt{\textbf{8.0763 3.8474 10.729 3.4038e-01}} 

  \texttt{\textbf{20 1 3 3.1934}} 

Additional columns of FitResults and VariableNames can be added for those peak shapes that display five or more results, such as the Voight shape: 

\texttt{\textbf{disp(table(FitResults(:,2), FitResults(:,3), FitResults(:,4), FitResults(:,5), FitResults(:,6), 'VariableNames', \{'Position' 'Height' 'GauWidth' 'Area}}\index{\textbf{Area}}\texttt{\textbf{' 'LorWidth'\}))}}

 \texttt{\textbf{Position Height GauWidth Area}}\index{Area}  \texttt{\textbf{LorWidth}}

  \texttt{\textbf{\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_}}

  \texttt{\textbf{0.80012 0.99987 0.30272 0.34744 0.39708}} 

  \texttt{\textbf{1.2003 0.79806 0.40279 0.27601 0.30012}} 

\textbf{Calculating the precision of the peak parameters:}

\texttt{[FitResults, GOF, baseline, coeff, residuals, xi, yi, BootstrapErrors] = peakfit([x;y],0,0,2,6,0,1,0,0,0);}

Displays parameter error estimates by the \textit{bootstrap method}. See page \pageref{ref-0216}.

\textbf{Optional output parameters:}\begin{enumerate}[{1.}]


\item  \textbf{FitResults}: a table of model peak parameters, one row for each peak, listing peak number, peak position, height, width, and area (or, for shape 28, the polynomial coefficients, and for shape 29, the x-axis breakpoints).

\item  \textbf{GOF ("Goodness of Fit")}, a 2-element vector containing the RMS fitting error of the best trial fit and the R-squared (coefficient of determination).

\item  \textbf{baseline}: returns the polynomial coefficients of the interpolated baseline in linear and quadratic baseline modes (1 and 2) or the value of the constant baseline in flat baseline mode.

\item  \textbf{coeff}: Coefficients for the polynomial fit (shape 28 only; for other shapes, coeff=0)

\item  \textbf{residual}: vector of differences between the data and the best fit model. Can be used to measure the characteristics of the \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html}{noise }in the signal.

\item  \textbf{xi}: vector containing 600 interpolated x-values for the model peaks.

\item  \textbf{yi}: matrix containing the y values of model peaks at each xi. Type \texttt{plot(xi,yi(1,:))} to plot peak 1 or \texttt{plot(xi,yi)} to plot all the peaks.

\item  \textbf{BootstrapErrors}: a matrix containing bootstrap standard deviations and interquartile ranges for each peak parameter of each peak in the fit (page \pageref{ref-0213}).

\end{enumerate}
\subsection{Examples\label{ref-0450}}

Note: test script \href{https://terpconnect.umd.edu/~toh/spectrum/testpeakfit.m}{testpeakfit.m} runs all the following examples automatically; just press Enter to continue to the next one. (You can copy and paste, or drag and drop, any of these single-line or multi-line code examples into the Matlab or Octave editor or into the command line and press Enter to execute it). 

\textbf{Example 1.} Fits computed x vs y data with a single unconstrained Gaussian peak model. 

  \texttt{{\textgreater} x=[0:.1:10];y=exp(-(x-5).\textasciicircum{}2); peakfit([x' y'])}

  \texttt{ans =}

 \texttt{\textcolor{color-4}{Peak number Position Height Width Peak area}}

  \texttt{1 5 1 1.665 1.7725}

\textbf{Example 2.} Fits small set of manually-entered y data to a single unconstrained Gaussian peak model. 

 \texttt{{\textgreater} y=[0 1 2 4 6 7 6 4 2 1 0 ]; x=1:length(y);}

  \texttt{{\textgreater} peakfit([x;y],length(y)/2,length(y),0,0,0,0,0,0)}

  \texttt{\textcolor{color-4}{Peak number Position Height Width Peak area}}

  \texttt{1 6.0001 6.9164 4.5213 32.98}

\textbf{Example 3.} Measurement of very noisy peak with signal-to-noise ratio = 1. (Try several times).

\texttt{{\textgreater} x=[0:.01:10];y=exp(-(x-5).\textasciicircum{}2) + randn(size(x)); peakfit([x;y])}

 \texttt{\textcolor{color-4}{Peak number Peak position Height Width Peak area}}

    \texttt{1 5.0951 1.0699 1.6668 1.8984}

\textbf{Example 4.} Fits a noisy two-peak signal with a double unconstrained Gaussian model (NumPeaks=2).

  \texttt{{\textgreater} x=[0:.1:10]; y=exp(-(x-5).\textasciicircum{}2)+.5*exp(-(x-3).\textasciicircum{}2) + .1*randn(1,length(x));}

 \texttt{{\textgreater} peakfit([x' y'],5,19,}\texttt{\textbf{2}}\texttt{,1,0,1)}

 \texttt{\textcolor{color-4}{Peak number Position Height Width Peak area}}

  \texttt{1 3.0001 0.49489 1.642 0.86504}

  \texttt{2 4.9927 1.0016 1.6597 1.7696}

\textbf{Example 5.} Fits a portion of the humps function, 0.7 units wide and centered on x=0.3, with a single (NumPeaks=1) Pearson function (peakshape=4) with extra=3 (controls shape of Pearson function).

  \texttt{{\textgreater} x=[0:.005:1];y=humps(x);peakfit([x' y'],.3,.7,1,}\texttt{\textbf{4}}\texttt{,}\texttt{\textbf{3}}\texttt{);}

\textbf{Example 6.} Creates a data matrix 'smatrix', fits a portion to a two-peak unconstrained Gaussian model, takes the best of 10 trials. Returns optional output arguments FitResults and FitError.

  \texttt{{\textgreater} x=[0:.005:1]; y=(humps(x)+humps(x-.13)).\textasciicircum{}3; smatrix=[x' y'];}

 \texttt{{\textgreater} [FitResults,FitError]=peakfit(smatrix,.4,.7,2,1,0,10)}

 \texttt{\textcolor{color-4}{Peak number Position Height Width Peak area}}

  \texttt{FitResults =1 0.4128 3.1114e+008 0.10448 3.4605e+007}

  \texttt{2 0.3161 2.8671e+008 0.098862 3.0174e+007}

  \texttt{FitError = 0.68048}

\textbf{Example 7.} As above, but specifies the first-guess position and width of the two peaks, in the order [position1 width1 position2 width2]

  \texttt{{\textgreater} peakfit([x' y'],.4,.7,2,1,0,10,}\texttt{\textbf{[.3 .1 .5 .1]}}\texttt{);}

Supplying a first guess position and width is also useful if you have one peak on top of another (like example 4, with both peaks at the same position x=5, but with different widths, in square brackets):

\texttt{{\textgreater}{\textgreater} x=[2:.01:8];}

\texttt{{\textgreater}{\textgreater} y=exp(-((x-5)/.2).\textasciicircum{}2)+.5.*exp(-(x-5).\textasciicircum{}2) + .1*randn(1,length(x));}

\texttt{{\textgreater}{\textgreater} peakfit([x' y'],0,0,2,1,0,1,}\texttt{\textbf{[5 2 5 1]}}\texttt{)}

 \texttt{\textcolor{color-4}{Peak number Position Height Width Peak area}}

  \texttt{1 4.9977 0.51229 1.639 0.89377}

  \texttt{2 4.9948 1.0017 0.32878 0.35059}

\href{https://terpconnect.umd.edu/~toh/spectrum/TestExtractModel.m}{\textbf{Example 8}}\textbf{.} As above, returns the vector xi containing 600 interpolated x-values for the model peaks and the matrix yi containing the y values of each model peak at each xi. Type \texttt{plot(xi,yi(1,:))} to plot peak 1 or \texttt{plot(xi,yi,xi,sum(yi))} to plot all the model components and the total model (sum of components).

\texttt{{\textgreater} [FitResults, GOF, baseline, coeff, residuals, xi, yi]= {\ldots}}

\texttt{peakfit(smatrix,.4,.7,2,1,0,10);}

\texttt{{\textgreater} figure(2); clf; plot(xi,yi,xi,sum(yi))}

\textbf{Example 9.} Fitting a single unconstrained Gaussian on a linear background, using the linear BaselineMode  (9th input argument = 1) 

   \texttt{{\textgreater}{\textgreater}x=[0:.1:10]';y=10-x+exp(-(x-5).\textasciicircum{}2);peakfit([x y],5,8,0,0,0,0,0,}\texttt{\textbf{1}}\texttt{)}

\textbf{Example 10.} Fits a group of three peaks near x=2400 in \texttt{DataMatrix3} with three equal-width exponentially-broadened Gaussians.

\texttt{{\textgreater}{\textgreater} load DataMatrix3}

\texttt{{\textgreater}{\textgreater} [FitResults,FitError]= peakfit(DataMatrix3,2400,440,3,8,31,1)}

 \texttt{\textcolor{color-4}{Peak number Position Height Width Peak area}}

  \texttt{1 2300.5 0.82546 60.535 53.188}

  \texttt{2 2400.4 0.48312 60.535 31.131}

  \texttt{3 2500.6 0.84799 60.535 54.635}

\texttt{FitError = 0.19975}

\textbf{Note}: if your peaks are trailing off to the \textit{left}, rather than to the right as in the above example, simply use a \textit{negative} value for the time constant (in ipf.m, press \textbf{Shift-X} and type a negative value).

\textbf{Example 11.} Example of an unstable fit to a signal consisting of two unconstrained Gaussian peaks of equal height (1.0). The peaks are too highly overlapped for a stable fit, even though the fit error is small and the residuals are unstructured. Each time you re-generate this signal, it gives a different fit, with the peak’s heights varying about 15\% from signal to signal. 

\texttt{{\textgreater}{\textgreater} x=[0:.1:10]';} 

\texttt{{\textgreater}{\textgreater} y=exp(-(x-5.5).\textasciicircum{}2) + exp(-(x-4.5).\textasciicircum{}2) + .01*randn(size(x));} 

\texttt{{\textgreater}{\textgreater} [FitResults,FitError]= peakfit([x y],5,19,2,1)}

 \texttt{\textcolor{color-4}{Peak number Position Height Width Peak area}}

  \texttt{1 4.4059 0.80119 1.6347 1.3941}

  \texttt{2 5.3931 1.1606 1.7697 2.1864}

\texttt{FitError = 0.598}

 Much more stable results can be obtained using the equal-width Gaussian model (\texttt{peakfit([x y],5,19,2,6)}), but that is justified only if the experiment is legitimately expected to yield peaks of equal width. See page \pageref{ref-0269} - \pageref{ref-0274}.

\textbf{Example 12.} \textbf{Baseline correction.} Demonstrations of the four ``BaselineModes'', for a single Gaussian on large baseline, with position=10, height=1, and width=1.66. The BaselineMode is specified by the 9th input argument (which can be 0,1,2, or 3).

\textbf{BaselineMode=0} means to ignore the baseline (default mode if not specified). In this case, this leads to large errors.

\texttt{{\textgreater}{\textgreater} x=8:.05:12;y=1+exp(-(x-10).\textasciicircum{}2);}

\texttt{{\textgreater}{\textgreater} [FitResults,FitError,baseline]=peakfit([x;y],0,0,1,1,0,1,0,}\texttt{\textbf{0}}\texttt{)}

  \texttt{FitResults =}

  \texttt{1 10 1.8561 3.612 5.7641}

 \texttt{FitError =5.387}

\textbf{BaselineMode=1} subtracts linear baseline from edge to edge. Does not work well in this case because the signal does not return completely to the baseline at the edges.

\texttt{{\textgreater}{\textgreater} [FitResults,FitError,baseline]=peakfit([x;y],0,0,1,1,0,1,0,1)}

\texttt{FitResults =}

  \texttt{1 9.9984 0.96161 1.5586 1.5914}

\texttt{FitError = 1.9801}

\texttt{baseline = 0.0012608 1.0376}

 \textbf{BaselineMode=2} subtracts quadratic baseline from edge to edge. Does not work well in this case because the signal does not return completely to the baseline at the edges.

\texttt{{\textgreater}{\textgreater} [FitResults,FitError,baseline]=peakfit([x;y],0,0,1,1,0,1,0,2)}

\texttt{FitResults =}

  \texttt{1 9.9996 0.81762 1.4379 1.2501}

\texttt{FitError = 1.8205}

\texttt{baseline = -0.046619 0.9327 -3.469}

\textbf{BaselineMode=3} subtracts a flat baseline automatically, \textit{without} requiring that the signal returns to baseline at the edges. This mode works best for this signal.

\texttt{{\textgreater}{\textgreater} [FitResults,FitError,baseline]=peakfit([x;y],0,0,1,1,0,1,0,3)}

\texttt{FitResults =}

  \texttt{1 10 1.0001 1.6653 1.7645}

\texttt{FitError = 0.0037056}

\texttt{baseline = 0.99985}

In some cases, you can regard the baseline as an additional ``peak''. In the following example, the baseline is strongly sloped, but straight. In that case the most accurate result is obtained by using a \textit{two-shape} fit, specifying the peak shape as a \textit{vector}, which fits the peak as a \textit{Gaussian} (shape 1) and the baseline as a \textit{variable-slope straight li}ne (\textbf{shape 26}).

\texttt{{\textgreater}{\textgreater} x=8:.05:12;y=x + exp(-(x-10).\textasciicircum{}2);}

\texttt{{\textgreater}{\textgreater} [FitResults,FitError]=peakfit([x;y],0,0,2,}\texttt{\textbf{[1}} \texttt{\textbf{26]}}\texttt{,[1 1],1,0)}

\texttt{FitResults =} 

  \texttt{1 10 1 1.6651 1.7642}

  \texttt{2 4.485 0.22297 0.05 40.045}

\texttt{FitError =0.093}

In the following example, the baseline is \textit{curved}, so you may be able to get good results with BaselineMode=2:

\texttt{{\textgreater}{\textgreater} x=[0:.1:10]';y=1./(1+x.\textasciicircum{}2)+exp(-(x-5).\textasciicircum{}2);}

\texttt{{\textgreater}{\textgreater} [FitResults,FitError,baseline]=peakfit([x y],5,5.5,0,0,0,0,0,}\texttt{\textbf{2}}\texttt{)}

\texttt{FitResults =}

  \texttt{1 5.0091 0.97108 1.603 1.6569}

\texttt{FitError = 0.97661}

\texttt{baseline = 0.0014928 -0.038196 0.22735}

\textbf{Example 13.} Same as example 4, but with \textit{fixed-width} Gaussian (shape 11), width=1.666. The 10th input argument is a vector of fixed peak widths (in square brackets), one entry for each peak, which may be the same or different for each peak.

\texttt{{\textgreater}{\textgreater} x=[0:.1:10];y=exp(-(x-5).\textasciicircum{}2)+.5*exp(-(x-3).\textasciicircum{}2)+.1*randn(size(x));}

\texttt{{\textgreater}{\textgreater} [FitResults,FitError]=peakfit([x' y'],0,0,2,}\texttt{\textbf{11}}\texttt{,0,0,0,0,[1.666 1.666])}

 \texttt{\textcolor{color-4}{Peak number Position Height Width Peak area}}

  \texttt{1 3.9943 0.49537 1.666 0.87849}

  \texttt{2 5.9924 0.98612 1.666 1.7488}

\textbf{Example 14.} Peak area measurements. Four Gaussians with a height of 1 and a width of 1.6651. All four peaks have the same theoretical peak area (1.772). The four peaks can be fit together in one fitting operation using a 4-peak Gaussian model, with only rough estimates of the first-guess positions and widths (in square brackets). The peak areas thus measured are much more accurate than the perpendicular drop method (page \pageref{ref-0184}):

\texttt{{\textgreater}{\textgreater} x=[0:.01:18];}

\texttt{{\textgreater}{\textgreater} y=exp(-(x-4).\textasciicircum{}2)+exp(-(x-9).\textasciicircum{}2)+exp(-(x-12).\textasciicircum{}2)+exp(-(x-13.7).\textasciicircum{}2);}

\texttt{{\textgreater}{\textgreater} peakfit([x;y],0,0,}\texttt{\textbf{4}}\texttt{,1,0,1,[4 2 9 2 12 2 14 2],0,0)}

 \texttt{\textcolor{color-4}{Peak number Position Height Width Peak area}}

  \texttt{1 4 1 1.6651 1.7725}

  \texttt{2 9 1 1.6651 1.7725 {\ldots}}

  \texttt{3 12 1 1.6651 1.7724}

  \texttt{4 13.7 1 1.6651 1.7725}

This works well even in the presence of substantial amounts of random noise:

\texttt{{\textgreater}{\textgreater} x=[0:.01:18]; y=exp(-(x-4).\textasciicircum{}2)+exp(-(x-9).\textasciicircum{}2)+exp(-(x-12).\textasciicircum{}2)+exp(-(x-13.7).\textasciicircum{}2)+.1.*randn(size(x));}

\texttt{{\textgreater}{\textgreater} peakfit([x;y],0,0,4,1,0,1,[4 2 9 2 12 2 14 2],0,0)}

 \texttt{\textcolor{color-4}{Peak number Position Height Width Peak area}}

  \texttt{1 4.0086 0.98555 1.6693 1.7513}

  \texttt{2 9.0223 1.0007 1.669 1.7779}

  \texttt{3 11.997 1.0035 1.6556 1.7685}

  \texttt{4 13.701 1.0002 1.6505 1.7573}

Sometimes experimental peaks are affected by exponential broadening, which does not by itself change the true peak areas, but does shift peak positions and increases peak width, overlap, and asymmetry, as shown when you try to fit the peaks with Gaussians\texttt{.} Using the same noise signal from above:

\texttt{{\textgreater}{\textgreater}} \texttt{y1=}\href{http://terpconnect.umd.edu/~toh/spectrum/ExpBroaden.m}{\texttt{ExpBroaden}}\texttt{(y',-50);}

\texttt{{\textgreater}{\textgreater} peakfit([x;y1'],0,0,4,1,50,1,0,0,0)}

  \texttt{\textcolor{color-4}{Peak number Position Height Width Peak area}}

  \texttt{1 4.4538 0.83851 1.9744 1.7623}

  \texttt{2 9.4291 0.8511 1.9084 1.7289}

  \texttt{3 12.089 ( 0.59632 1.542 0.97883}

  \texttt{4 13.787 1.0181 2.4016 2.6026}

Peakfit.m (and ipf.m) have an exponentially-broadened Gaussian peak shape (shape \#5) that works better in those cases, recovering the \textit{original} peak positions, heights, widths, and areas. (Adding a first-guess vector as the 8th argument may improve the reliability of the fit in some cases).

\texttt{{\textgreater}{\textgreater} y1=E}\href{https://terpconnect.umd.edu/~toh/spectrum/ExpBroaden.m}{\texttt{xpBroaden}}\texttt{(y',-50);}

\texttt{{\textgreater}{\textgreater} peakfit([x;y1'],0,0,4,}\texttt{\textbf{5}}\texttt{,}\texttt{\textbf{50}}\texttt{,1,[4 2 9 2 12 2 14 2],0,0)}

\texttt{ans= Peak\# Position Height Width Area}\index{Area}

          \texttt{1 4 1 1.6651 1.7725}

  \texttt{2 9 1 1.6651 1.7725}

  \texttt{3 12 1 1.6651 1.7725}

  \texttt{4 13.7 0.99999 1.6651 1.7716}

\textbf{Example 15.} Displays a table of parameter error estimates. See \href{https://terpconnect.umd.edu/~toh/spectrum/DemoPeakfitBootstrap.m}{DemoPeakfitBootstrap} for a self-contained demo of this function.

\texttt{{\textgreater}{\textgreater} x=0:.05:9; y=exp(-(x-5).\textasciicircum{}2)+.5*exp(-(x-3).\textasciicircum{}2)+.01*randn(1,length(x));}

\texttt{{\textgreater}{\textgreater} [FitResults,LowestError,baseline,residuals,xi,yi,}\texttt{\textbf{BootstrapErrors}}\texttt{]= peakfit([x;y],0,0,2,6,0,1,0,0,0);}

\texttt{Peak \#1 Position Height Width Area}\index{Area}

\texttt{Mean: 2.9987 0.49717 1.6657 0.88151}

\texttt{STD: 0.0039508 0.0018756 0.0026267 0.0032657}

\texttt{STD (IQR): 0.0054789 0.0027461 0.0032485 0.0044656}

\texttt{\% RSD: 0.13175 0.37726 0.15769 0.37047}

\texttt{\% RSD (IQR): 0.13271 0.35234 0.16502 0.35658}

\texttt{Peak \#2 Position Height Width Area}

\texttt{Mean: 4.9997 0.99466 1.6657 1.7636}

\texttt{STD: 0.001561 0.0014858 0.00262 0.0025372}

\texttt{STD (IQR): 0.002143 0.0023511 0.00324 0.0035296}

\texttt{\% RSD: 0.031241 0.14938 0.15769 0.14387}

\texttt{\% STD (IQR): 0.032875 0.13637 0.16502 0.15014}

\textbf{Example 16.} Fits both peaks of the Humps function with a Gaussian/Lorentzian blend (shape 13) that is 15\% Gaussian (Extra=15). The 'Extra' argument sets the percentage of Gaussian shape.

\texttt{{\textgreater}{\textgreater} x=[0:.005:1];y=humps(x);[FitResults,FitError]= peakfit([x' y'], 0.54,0.93,2,}\texttt{\textbf{13}}\texttt{,}\texttt{\textbf{15}}\texttt{,10,0,0,0)} 

 \texttt{FitResults =}

  \texttt{1 0.30078 190.41 0.19131 23.064}

  \texttt{2 0.89788 39.552 0.33448 6.1999}

 \texttt{FitError = 0.34502}

\textbf{Example 17.} Fit a slightly asymmetrical peak with a bifurcated Gaussian (shape 14). The 'Extra' argument (=45) controls the peak asymmetry (50 is symmetrical). 

\texttt{{\textgreater}{\textgreater} x=[0:.1:10];y=exp(-(x-4).\textasciicircum{}2)+.5*exp(-(x-5).\textasciicircum{}2)+.01*randn(size(x));}

\texttt{{\textgreater}{\textgreater} [FitResults,FitError]=peakfit([x' y'],0,0,1,}\texttt{\textbf{14}}\texttt{,}\texttt{\textbf{45}}\texttt{,10,0,0,0)} 

 \texttt{FitResults =}

  \texttt{1 4.2028 1.2315 4.077 2.6723}

 \texttt{FitError =0.84461}

\textbf{Example 18.} Returns output arguments only, without plotting or command window printing (11th input argument = 0, default is 1)

\texttt{{\textgreater}{\textgreater} x=[0:.1:10]';y=exp(-(x-5).\textasciicircum{}2);FitResults=peakfit([x y],0,0,1,1,0,0,0,0,0,}\texttt{\textbf{0}}\texttt{)}

\textbf{Example 19.} Same as example 4, but with \textit{fixed-position} Gaussian (shape 16), positions=[3 5]. 

\texttt{{\textgreater}{\textgreater} x=[0:.1:10];y=exp(-(x-5).\textasciicircum{}2)+.5*exp(-(x-3).\textasciicircum{}2)+.1*randn(size(x));}

\texttt{{\textgreater}{\textgreater} [FitResults,FitError]=peakfit([x' y'],0,0,2,}\texttt{\textbf{16}}\texttt{,0,0,0,0,}\texttt{\textbf{[3 5]}}\texttt{)}

 \texttt{\textcolor{color-4}{Peak number Position Height Width Peak area}}

  \texttt{1 3 0.49153 1.6492 0.86285}

  \texttt{2 5 1.0114 1.6589 1.786}

\texttt{FitError =8.2693}

\textbf{Example 20.} Exponentially modified Lorentzian (shape 18) with added noise. As for peak shape 5, peakfit.m recovers the original peak position (9), height (1), and width (1).

\texttt{{\textgreater}{\textgreater} x=[0:.01:20];} 

\texttt{{\textgreater}{\textgreater} L=lorentzian(x,9,1)+0.02.*randn(size(x));}

\texttt{{\textgreater}{\textgreater} L1=ExpBroaden(L',-100);}

\texttt{{\textgreater}{\textgreater} peakfit([x;L1'],0,0,1,}\texttt{\textbf{18}}\texttt{,100,1,0,0)}

\textbf{Example 21.} Fitting humps function with two unconstrained Voigt profiles  (version 9.5)

\texttt{{\textgreater}{\textgreater}disp('Peak Position Height Width Area Alpha')}

\texttt{{\textgreater}{\textgreater} [FitResults,FitError]=peakfit(humps(0:.01:2),60,120,2,30,1.7,5,0)}

\texttt{Peak Position Height Width Area Alpha}

\texttt{1 31.629 95.175 19.469 2404.5 2.1355}

\texttt{2 90.736 19.826 33.185 764.32 1.3188}

\texttt{GOF =}  \texttt{0.7618 0.9991}

\InsImageInline{0.5}{l}{peakfitdemob.png}\textbf{Example 22.} \href{https://terpconnect.umd.edu/~toh/spectrum/peakfitdemob.m}{peakfitdemob.m}. Illustrated on the right. Measurement of three weak Gaussian peaks at x=100, 250, 400, superimposed in a very strong curved baseline plus noise. The peakfit function fits \textit{four} peaks, treating the baseline as a 4th peak whose peak position is negative. (The true peaks heights are 1, 2, and 3, respectively). Because this results in so many adjustable variables (4 peaks x 2 variable/peak = 8 variables), you need to specify a "start" vector, like Example 7. You can test the reliability of this method by changing the peak parameters in lines 11, 12, and 13 and see if the peakfit function will successfully track the changes and give accurate results for the three peaks without having to change the start vector. See \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html\#examples}{Example 9 on iSignal.html }for other ways to handle this signal. \label{ref-0451}

\textbf{Example 23.} 12\textsuperscript{th} input argument (+/- mode) set to 1 (bipolar) to allow negative as well as positive peak heights. (Default is 0)

  \texttt{{\textgreater}{\textgreater} x=[0:.1:10];y=exp(-(x-5).\textasciicircum{}2)-.5*exp(-(x-3).\textasciicircum{}2)+.1*randn(size(x));}

  \texttt{{\textgreater}{\textgreater} peakfit([x' y'],0,0,2,1,0,1,0,0,0,1,}\texttt{\textbf{1}}\texttt{)}

  \texttt{FitResults =}

  \texttt{1 3.1636 -0.5433 1.62 -0.9369}

  \texttt{2 4.9487 0.96859 1.8456 1.9029}

  \texttt{FitError =8.2757}

\textbf{Example 24.} Version \textbf{5} or later. Fits humps function to a model consisting of one Lorentzian and one Gaussian peak.

 \texttt{{\textgreater}{\textgreater} x=[0:.005:1.2];y=humps(x);}

\texttt{[FitResults,FitError]=peakfit([x' y'],0,0,2,}\texttt{\textbf{[2 1]}}\texttt{,}\texttt{\textbf{[0 0]}}\texttt{)}

  \texttt{FitResults =}

  \texttt{1 0.30178 97.402 0.18862 25.116}

  \texttt{2 0.89615 18.787 0.33676 6.6213}

  \texttt{FitError = 1.0744}

\href{https://terpconnect.umd.edu/~toh/spectrum/MultipleShapesLarge.png}{\textbf{Example 25}}\textbf{.} Five peaks, five different shapes, all heights = 1, all widths = 3, "extra" vector included for peaks 4 and 5. 

  \texttt{x=0:.1:60;}

  \texttt{y=}\href{https://terpconnect.umd.edu/~toh/spectrum/modelpeaks2.m}{\texttt{modelpeaks2}}\texttt{(x,[1 2 3 4 5],[1 1 1 1 1],[10 20 30 40 50],[3 3 3 3 3],[0 0 0 2 -20])+.01*randn(size(x));}

 \texttt{peakfit([x' y'],0,0,5,}\texttt{\textbf{[1 2 3 4 5]}}\texttt{,}\texttt{\textbf{[0 0 0 2 -20]}}\texttt{)} 

You can also use this technique to create models with all the \textit{same} shapes but with different values of 'extra' using a vector of 'extra' values, or (in version 5.7) with different minimum width restrictions by using a vector of 'minwidth' values as input argument 13.

\textbf{Example 26.} Minimum width constraint (13th input argument)

 \texttt{{\textgreater}}\texttt{{\textgreater} x=1:30;y=gaussian(x,15,8)+.05*randn(size(x));}

  No constraint (minwidth=0):

  \texttt{peakfit([x;y],0,0,5,1,0,10,0,0,0,1,0,}\texttt{\textbf{0}}\texttt{);}

  Widths constrained to values 7 or above:

 \texttt{peakfit([x;y],0,0,5,1,0,10,0,0,0,1,0,}\texttt{\textbf{7}}\texttt{);}

\textbf{Example 27.} Noise test with very noisy peak signal: peak height and RMS noise both equal to 1. 

\texttt{{\textgreater}{\textgreater} x=[-10:.05:10];y=exp(-(x).\textasciicircum{}2)+randn(size(x));}

\InsImageInline{0.5}{l}{Example28b.png}\texttt{{\textgreater}{\textgreater} P=peakfit([x;y],0,0,1,1,0,10);}

\textbf{Example 28}: Weak Gaussian peak on sloped straight-line baseline, 2-peak fit with one Gaussian and one variable-slope straight line ('slope', shape 26, peakfit version 6 and later only).

\texttt{{\textgreater}{\textgreater} x=8:.05:12; y=x + exp(-(x-10).\textasciicircum{}2);} 

\texttt{[FitResults,FitError]= peakfit([x;y],0,0,2,[1 26],[1 1],1,0)}

  \texttt{FitResults =} 

  \texttt{1 10 1 1.6651 1.7642}

  \texttt{2 4.485 0.22297 0.05 40.045}

  \texttt{FitError =0.093}

To make a more difficult example, this one has \textit{two} weak Gaussian peaks on sloped straight-line baseline. In this case we use a 3-peak model with peakshape=[1 1 26], which is helped by adding rough first guesses ('start') using the 'polyfit' function to generate automatic first guesses for the sloping baseline. The third component (peak 3) is the baseline. 

\texttt{x=8:.05:12}

\texttt{y=x/2+exp(-(x-9).\textasciicircum{}2)+exp(-(x-11).\textasciicircum{}2)+.02.*randn(size(x));}

\texttt{start=[8 1 10 1 polyfit(x,y,1)];}

\texttt{peakfit([x;y],0,0,3,[1 1 26],[1 1 1],1,start)}

 See example 38 (page \pageref{ref-0453}) for a similar example with a \textit{curved} baseline. 

 \textbf{Example 29}: \textbf{Segmented linear fit (Shape 29)}. You specify the number of segments in the 4\textsuperscript{th} input argument ('NumPoints') and the program attempts to find the optimum \textit{x-axis positions} of the breakpoints that minimize the fitting error. The vertical dashed magenta lines mark the x-axis breakpoints. \href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedGaussian.png}{Another example with a single Gaussian band }.

 \texttt{{\textgreater}{\textgreater} x=[0.9:.005:1.7];y=humps(x);}

 \texttt{{\textgreater}{\textgreater} peakfit([x' y'],0,0,}\texttt{\textbf{9}}\texttt{,}\texttt{\textbf{29}}\texttt{,0,10,0,0,0,1,1)}

 

 \textbf{Example 30}: \textbf{Polynomial fit (Shape 28}). Specify the order of the polynomial (any positive integer) in the 6\textsuperscript{th} input argument ('extra'). (The 12th input argument, 'bipolar', is set to 1 to plot the entire y-axis range when y goes negative). 

 \texttt{{\textgreater}{\textgreater} x=[0.3:.005:1.7];y=humps(x);y=y + cumsum(y);}

 \texttt{{\textgreater}{\textgreater} peakfit([x' y'],0,0,1,}\texttt{\textbf{28}}\texttt{,}\texttt{\textbf{6}}\texttt{,10,0,0,0,1,1)}

\textbf{Example 31}: The Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/NumPeaksTest.m}{NumPeaksTest.m} uses peakfit.m to demonstrate one way to determine the minimum number of model peaks needed to fit a set of data, plotting the fitting error vs the number of model peaks, and looking for the point at which the fitting error reaches a minimum. This script creates a noisy computer-generated signal containing a user-selected 3, 4, 5 or 6 underlying peaks, fits to a series of models containing 1 to 10 model peaks, plots the fitting errors vs the number of model peaks and then determines the vertex of the best-fit parabola; the nearest integer is usually the correct number of peaks underlying peaks. Also requires that the \href{https://terpconnect.umd.edu/~toh/spectrum/plotit.m}{plotit.m} function be installed.

\textbf{Example 32}: Examples of \textit{unconstrained} variable shapes 30-33 and shape 39, all of which have \textit{three} iterated variables (position, width, and shape): 

a.~\textbf{Voigt} (shape 30). Returns Alphas (ratios of Lorentzian width to Gaussian width) as 6\textsuperscript{th} column.\label{mark-a.}

 \texttt{x=1:.1:30; y=modelpeaks2(x,[13 13],[1 1],[10 20],[3 3],[20 80]);} 

  \texttt{disp('Peak\# Position Height Width Area Alpha'}\index{Area}\texttt{)}

  \texttt{[FitResults,FitError] = peakfit([x;y],0,0,2,}\texttt{\textbf{30}}\texttt{,2,10)}.

 b. \textbf{Exponentially broadened Gaussian} (shape 31): 

 \texttt{load DataMatrix3;}

 \texttt{peakfit(DataMatrix3, 1860.5,364,2,31,3,5,[1810 60 30 1910 60 30])}

Version \textbf{8.4} also includes an alternative exponentially broadened Gaussian, shape 39, which is parameterized differently (see \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Example39}{Example 39} on the next page).

 c. \textbf{Pearson} (shape 32)  

 \texttt{x=1:.1:30;} 

 \texttt{y=modelpeaks2(x,[4 4],[1 1],[10 20],[5 5],[1 10]);} 

 \texttt{[FitResults,FitError] = peakfit([x;y],0,0,2,32,0,5)}

 d. \textbf{Gaussian/Lorentzian blend} (shape 33): 

 \texttt{x=1:.1:30; 0}

 \texttt{y=modelpeaks2(x,[13 13],[1 1],[10 20],[3 3],[20 80]);}

 \texttt{[FitResults,FitError]=peakfit([x;y],0,0,2,33,0,5)} 

\textbf{Example 34:} Using the built-in "sortrows" function to sort the FitResults table by peak position (column 2) or peak height (column 3).

 \texttt{{\textgreater}{\textgreater} x=[0:.005:1.2]; y=humps(x);} 

 \texttt{{\textgreater}{\textgreater} FitResults,FitError]=peakfit([x' y'],0,0,3,1)}

 \texttt{{\textgreater}{\textgreater} sortrows(FitResults,2)}

\texttt{ans =}

  \texttt{2 0.29898 56.463 0.14242 8.5601}

  \texttt{1 0.30935 39.216 0.36407 14.853}

  \texttt{3 0.88381 21.104 0.37227 8.1728}

\texttt{{\textgreater}{\textgreater} sortrows(FitResults,3)}

\texttt{ans =}

  \texttt{3 0.88381 21.104 0.37227 8.1728}

  \texttt{1 0.30935 39.216 0.36407 14.853}

  \texttt{2 0.29898 56.463 0.14242 8.5601}

\textbf{Example 35:} Version 7.6 or later. Using the fixed-width Gaussian/Lorentzian blend (shape 35).\label{ref-0452}

 \texttt{{\textgreater}{\textgreater} x=0:.1:10; y=GL(x,4,3,50)+.5*GL(x,6,3,50) + .1*randn(size(x));}

  \texttt{{\textgreater}{\textgreater} [FitResults,FitError]=peakfit([x;y],0,0,2,}\texttt{\textbf{35}}\texttt{,50,1,0,0,[3 3])}

  \texttt{\textcolor{color-4}{peak position height width area}}

      \texttt{1 3.9527 1.0048 3 3.5138}

  \texttt{2 6.1007 0.5008 3 1.7502}

   \texttt{GoodnessOfFit = 6.4783 0.95141}

Compared to variable-width fit (shape 13), the fitting error is larger but nevertheless results are more accurate (when true peak width is known, width = [3 3]).  

\texttt{{\textgreater}{\textgreater} [FitResults,GoodnessOfFit]= peakfit([x;y],0,0,2,}\texttt{\textbf{13}}\texttt{,50,1)}

  \texttt{1 4.0632 1.0545 3.2182 3.9242}

  \texttt{2 6.2736 0.41234 2.8114 1.3585}

  \texttt{GoodnessOfFit = 6.4311 0.95211}

\textbf{Note:} to display the FitResults table with column labels, call peakfit.m with output arguments [FitResults...] and type : 

\texttt{disp(' Peak number Position Height Width Peak area');disp(FitResults)}

\textbf{Example 36:} Variable exponent broadened Lorentzian function, shape 38. (Version 7.7 and above only). FitResults has an added 6th column for the measured time constant.

 \texttt{{\textgreater}{\textgreater} x=[1:100]';}

 \texttt{{\textgreater}{\textgreater} y=explorentzian(x',40,5,-10)+.01*randn(size(x));}

 \texttt{{\textgreater}{\textgreater} peakfit([x y],0,0,1,38,0,10)}

\textbf{Example 37:} 3-parameter logistic (Gompertz), shape 43. (Version 7.9 and above only). Parameters labeled Bo, Kh, and L. FitResults extended to 6 columns.

\texttt{{\textgreater}{\textgreater} t=0:.1:10;}

\texttt{{\textgreater}{\textgreater} Bo=6;Kh=3;L=4;}

\texttt{{\textgreater}{\textgreater} y=Bo*exp(-exp((Kh*exp(1)/Bo)*(L-t)+1))+.1.*randn(size(t));}

\texttt{{\textgreater}{\textgreater} [FitResults,GOF]=peakfit([t;y],0,0,1,43)}

\texttt{\InsImageInline{0.5}{l}{Example38.png}}

\textbf{Example 38:} Shape 46, 'quadslope'. Two overlapping Gaussians (position=9,11; heights=1; widths=1.666) on a curved baseline, using a 3-peak fit with peakshape=[1 1 46], default NumTrials and start. \label{ref-0453}

\texttt{{\textgreater}{\textgreater} x=7:.05:13;}

\texttt{{\textgreater}{\textgreater} y=x.\textasciicircum{}2/50+exp(-(x-9).\textasciicircum{}2)+exp(-(x-11).\textasciicircum{}2)+.01.*randn(size(x));}

\texttt{{\textgreater}{\textgreater} [FitResults,FitError]= peakfit([x;y],0,0,3,[1 1 46],[1 1 1])}

Screen image on the right\texttt{.} Note: if the baseline is much higher in amplitude that the peak amplitude, it will help to supply an approximate 'start' value and to use NumTrials {\textgreater} 1.

\textbf{Example 39:}  Comparison of alternative unconstrained exponentially broadened Gaussians, shapes 31 \InsImageInline{0.5}{l}{image367.png}and 39. Shape 31 (\href{https://terpconnect.umd.edu/~toh/spectrum/expgaussian.m}{expgaussian.m}) creates the shape by performing a Fourier convolution of a specified Gaussian by an exponential decay of specified time constant, whereas shape 39 (\href{https://terpconnect.umd.edu/~toh/spectrum/expgaussian2.m}{expgaussian2.m}) uses a mathematical expression for the final shape so produced. Both result in the \textit{same peak shape} but are parameterized differently. Shape 31 reports the peak height and position as that of the \textit{original} Gaussian before broadening, whereas shape 39 reports the peak height of the broadened \textit{result}. Shape 31 reports the width as the FWHM (full width at half maximum) and shape 39 reports the standard deviation (sigma) of the Gaussian. Shape 31 reports the exponential factor and the \textit{number of data points} and shape 39 reports the \textit{reciprocal of time constant} in time units. See the script \href{https://terpconnect.umd.edu/~toh/spectrum/GaussVsExpGauss.m}{GaussVsExpGauss.m} (on the left). See Matlab Figure windows \href{https://terpconnect.umd.edu/~toh/spectrum/GaussVsExpGaussFigure2.png}{\textbf{2}} and \href{https://terpconnect.umd.edu/~toh/spectrum/GaussVsExpGaussFigure3.png}{\textbf{3}}. For multiple-peak fits, both shapes usually require a reasonable first guess (``start'') vector for best results.\label{ref-0454}

\texttt{\textbf{Method Position Height Halfwidth Area}}\index{Area}  \texttt{\textbf{Exponential factor}}

\texttt{Shape 31 10 1 5 5.3223 20.0001}

\texttt{Shape 39 12.8282 0.35578 11.7731 5.3217 0.1}

See the script \href{https://terpconnect.umd.edu/~toh/spectrum/DemoExpgaussian.m}{DemoExpgaussian.m} for a more detailed explanation.

\textbf{Example 40}: Use of the "start" vector in 4-Gaussian fit to the "humps" function

  \texttt{x=[-.1:.005:1.2];y=humps(x);}

 First attempt with default start values gives poor fit:

 \texttt{[FitResults,GOF]=peakfit([x;y],0,0,4,1,0,10)}

 Second attempt specifying approximate start values in the 8\textsuperscript{th} input argument gives almost perfect fit:

 \texttt{start=[0.3 0.13 0.3 0.34 0.63 0.15 0.89 0.35];}

 \texttt{[FitResults,GOF]=peakfit([x;y],0,0,4,1,0,10,start)}

\textbf{Example 41:} Peakfit 9 and above. Use of peak shape 50 ("\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#peakfit9}{multilinear regression}") when the peak \textit{positions and widths} are known, and only the peak \textit{heights} are unknown. The peak shapes, positions, and widths are specified in the 10th input argument "fixedparameters", which must in this case be a \textit{matrix} listing the peak shape number (column 1), position (column 2), and width (column 3) of each peak, one row per peak. See the demonstration scripts \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit9demo.m}{peakfit9demo.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit9demoL.m}{peakfit9demoL.m}.



\textbf{Example 42:} \href{https://terpconnect.umd.edu/~toh/spectrum/RandPeaks.m}{RandPeaks.m} is a script that demonstrates the accuracy of iterative peak fitting when no customized "start" values are provided, that is, knowing only the peak shape and number of peaks. It generates any number of overlapping Gaussian peaks (NumPeaks in line 9) of random position, height, and width and calls the peakfit function. Calculates the average percent errors in position, height, and width. As you increase the number of peaks, accuracy degrades, even if R\textsuperscript{2 }remains close to 1.00. 

\subsection{How do you find the right input arguments for peakfit? \label{ref-0455}}

If you have no idea where to start, you can use the \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{Interactive Peak Fitter (ipf.m)} to quickly try different fitting regions, peak shapes, numbers of peaks, baseline correction modes, number of trials, etc. When you get a good fit, you can press the "\textbf{W}" key to print out the command line statement for peakfit.m that will perform that fit in a single line of code, with or without graphics. 

\subsection{Working with the fitting results matrix "FitResults". \label{ref-0456}}

Suppose you have performed a multi-peak curve fit to a set of data, but you are interested only in one or a few specific peaks. It is not always reliable to simply go by peak index number (the first column in the FitResults table); peaks sometimes change their position in the FitResults table arbitrarily, because the \textit{fitting error is independent of the peak order} (the sum of peaks 1+2+3 is exactly the same as 2+1+3 or 3+2+1, etc.). But you can sort this out by using the Matlab/Octave \href{https://www.mathworks.com/help/matlab/ref/sortrows.html}{"sortrows" command} to reorder the table in order of peak position or height. Also useful in such cases is my function \href{https://terpconnect.umd.edu/~toh/spectrum/val2ind.m}{val2ind}(v, val), which returns the index and the value of the element of vector 'v' that is closest to 'val' (download this function and place in the Matlab path). For example, suppose you want to extract the peak height (column \textbf{3} of FitResults) of the peak whose position (column \textbf{2} of FitResults) is closest to a particular value, call it "TargetPosition". There are three steps:

\texttt{VectorOfPositions=FitResults(:,}\texttt{\textbf{2}}\texttt{);}

\texttt{IndexOfClosestPeak=val2ind(VectorOfPositions, TargetPosition);}

\texttt{HeightOfClosestPeak=Fitresults(IndexOfClosestPeak,}\texttt{\textbf{3}}\texttt{);}

For an example of this use in a practical application, see \href{https://terpconnect.umd.edu/~toh/spectrum/RandomWalkBaseline.m}{RandomWalkBaseline.m}.\href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Top}{}

\subsection{Demonstration script for peakfit.m\label{ref-0457}\label{ref-0458}}

\href{https://terpconnect.umd.edu/~toh/spectrum/DemoPeakFit.m}{\textbf{DemoPeakFit.m}} is a demonstration script for peakfit.m. It generates an overlapping Gaussian peak signal, adds normally-distributed white noise, fits it with the \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m} function (in line 78), repeats this many times ("NumRepeats" in line 20), then compares the peak parameters (position, height, width, and area) of the measurements to their actual values and computes accuracy (percent error) and precision (percent relative standard deviation). You can change any of the initial values in lines 13-30. Here is a typical result for a two-peak signal with Gaussian peaks:

 \textbf{Percent errors of measured parameters:}

\texttt{\textbf{Position Height Width Area}}\index{Area}

 \texttt{0.048404 0.07906 0.12684 0.19799}

 \texttt{0.023986 0.38235 -0.36158 -0.067655}

{\textbar}\texttt{\textbf{Average Percent Parameter Error for all peaks:}} 

 \texttt{0.036195 0.2307 0.24421 0.13282}

In these results, you can see that the accuracy and precision (\%RSD) of the peak \textit{position} measurements are always the best, followed by peak \textit{height}, and then the peak \textit{width} and peak \textit{area}, which are usually the worst. 

\InsImageInline{0.5}{l}{DemoPeakFitTime.gif.png} \href{https://terpconnect.umd.edu/~toh/spectrum/DemoPeakFitTime.m}{\textbf{DemoPeakFitTime.m}} \href{https://terpconnect.umd.edu/~toh/spectrum/DemoPeakFitTime.gif}{}is a simple script that demonstrates how to apply multiple curve fits to a signal that is changing with time. The signal contains two noisy Gaussian peaks (like the illustration at the right) in which the peak position of the \textit{second} peak increases with time and the other parameters remain constant (except for the noise). The script creates a set of 100 noisy signals (on line 5) containing two Gaussian peaks where the position of the second peak changes with time (from x=6 to 8) and the first peak remains the same. Then it fits a 2-Gaussian model to each of those signals (on line 8), stores the FitResults in a 100 ${\times}$ 2 ${\times}$ 5 matrix, displays the signals and the fits graphically with time (\href{https://terpconnect.umd.edu/~toh/spectrum/DemoPeakFitTime.gif}{click to play animation}), then plots the measured peak position of the two peaks vs time on line 12. Here is a\href{https://terpconnect.umd.edu/~toh/spectrum/ChangingExponential.gif}{ real-data example with exponential pulse} that varies over time. For an example of automating the processing of multiple stored data files, see page \pageref{ref-0412}.

\subsection{Dealing with complex signals with lots of peaks\label{ref-0459}}

When a signal consists of lots of peaks on a highly variable background, the best approach is often to use peakfit is "center" and "window" arguments to break up the signal into segments containing smaller groups of overlapping peaks with their segments of background, isolating the peaks that do not overlap with other peaks. The reasons for this are several:

(a) peakfit.m works better if the number of variables for each fit is reduced;

(b) it is easier to compensate for the local background over those smaller segments;

(c) with smaller fits, you may not need to supply starting guesses for the peak position and widths;

(d) you can easily skip over peaks or data regions that you are not interested in;

(e) It is actually faster for the computer to execute a series of smaller peakfit() commands than a single one encompassing the entire data range in one go.

An easy way to do this is to use my interactive peak fitter \textbf{ipf.m} (described below) to explore various segments of the signal by panning and zooming and to try some trial fits and baseline correction settings, then press the "w" key to print out the peakfit syntax for that segment, with all its input arguments. Copy, paste, and edit the syntax for each segment as desired, then paste them into you code:

\texttt{[FitResults1, GOF1] = peakfit(datamatrix, center1, window1...}

\texttt{[FitResults2, GOF2] = peakfit(datamatrix, center2, window2...}

\subsection{Automatically finding and Fitting Peaks\label{ref-0460}}

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksfit.m}{\textbf{findpeaksfit.m}} is essentially a combination of \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaks}{findpeaks.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#command}{peakfit.m}. It uses the number of peaks found and the peak positions and widths determined by findpeaks as input for the peakfit.m function, which then fits the entire signal with the specified peak model. This combination function is more convenient that using findpeaks and peakfit separately. It yields better values than findpeaks, because peakfit fits the entire peak, not just the top part, and because it deals with non-Gaussian and overlapped peaks. However, it fits only those peaks that are found by findpeaks, so you will have to make sure that every peak that contributes to your signal is located by findpeaks. The syntax is 

\texttt{function [P,FitResults,LowestError,residuals,xi,yi] = findpeaksfit(x, y, SlopeThreshold, AmpThreshold, smoothwidth, peakgroup, smoothtype, peakshape, extra, NumTrials, BaselineMode, fixedparameters, plots)}

\InsImageInline{0.5}{l}{findpeaksfit.gif.png}

The first seven input arguments are exactly the same as for the findpeaks.... functions (page \pageref{ref-0300}); if you have been using findpeaks or \textit{iPeak} to find and measure peaks in your signals, you can use those same input argument values for findpeaksfit.m. The remaining six input arguments of findpeaksfit.m are for the peakfit function (page \pageref{ref-0448}); if you have been using peakfit.m or \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{ipf.m} to fit peaks in you signals, you can use those same input argument values for findpeaksfit.m. Type "help findpeaksfit" for more information. This function is included in the \href{https://terpconnect.umd.edu/~toh/spectrum/ipf13.zip}{ipf13.zip} distribution. 

The \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksfit.gif}{animation} on the right was created by the demo script \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksfitdemo.m}{findpeaksfitdemo.m}. It shows findpeaksfit finding and fitting the peaks in 150 signals in real-time. Each signal has from 1 to 3 noisy Lorentzian peaks in variable locations.

The script \href{https://terpconnect.umd.edu/~toh/spectrum/FindpeaksComparison.m}{FindpeaksComparison.m} compares the peak parameter accuracy of four peak detection functions: \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksG.m}{findpeaksG}/\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksL.m}{L}, \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksb.m}{findpeaksb}, \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksb3.m}{findpeaksb3}, and \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksfit.m}{findpeaksfit }applied to a computer-generated signal with multiple peaks plus variable types and amounts of baseline and random noise. The last three of these functions include iterative peak fitting equivalent to peakfit.m, in which the number of peaks and the "first guess" starting values are determined by findpeaksG/L. Typical result shown below.\label{ref-0461}\label{ref-0462}

\InsImageInline{0.5}{l}{image369.png}\texttt{Average absolute percent errors of all peaks}

           \texttt{Position error Height error Width error Elapsed time, sec}

\texttt{findpeaksG 0.365331 35.5778 11.6649} \texttt{0.005768}

\texttt{findpeaksb 0.28246 2.7755 3.4747} \texttt{0.069061}

\texttt{findpeaksb3 0.28693 2.2531 2.9951} \texttt{0.49538}

\texttt{findpeaksfit 0.341892 12.7095 18.3436} \texttt{0.273}

\texttt{5 peaks found.} \label{ref-0463}\label{ref-0464}

\textbf{Interactive Peak Fitter (ipf.m)}

\InsImageInline{0.5}{l}{ipf13.png}\href{https://terpconnect.umd.edu/~toh/spectrum/ipf.m}{\textbf{ipf.m}} (Version 13.3, September 2019) is a Matlab-only version of the peak fitter for x,y data that uses keyboard commands and the mouse cursor. It is a self-contained Matlab function, in a single m-file. The interactive keypress operation also works if you run \href{https://www.mathworks.com/products/matlab-online.html}{Matlab Online in a web browser}, but it does not work in \href{https://itunes.apple.com/us/app/matlab-mobile/id370976661?mt=8}{Matlab Mobile} on iPads and iPhones or in Octave. The flexible input syntax allows you to specify the data as separate x,y vectors or as a 2xn matrix, and to optionally define the initial focus by adding ``center'' and ``window'' values as additional input arguments, where 'center' is the desired x-value in the center of the upper window and ``window'' is the desired  width of that window. Examples:

 1 input argument:

  \texttt{ipf(y) or ipf([x;y]) or ipf([x;y]');}

  2 input arguments:

  \texttt{ipf(x,y) or ipf([x;y],center) or ipf([x;y]',center}\texttt{);}

  3 input arguments:

  \texttt{ipf(x,y,center) or ipf(y,center,window) or} 

  \texttt{ipf([x;y],center,window) or ipf([x;y]',center,window);}

  4 input arguments:

  \texttt{ipf(x,y,center,window);}

Like \textit{iPeak} and iSignal, ipf.m starts out by showing the entire signal in the lower panel and the selected region that will be fitted in the upper panel (adjusted by the same cursor controls keys as \textit{iPeak} and iSignal). After performing a fit (figure on the right), the upper panel shows the data as \textbf{\textcolor{color-4}{blue dots}}, the total model as a \textbf{\textcolor{color-19}{red line}} (ideally overlapping the blue dots), and the model components as \textbf{\textcolor{color-20}{green lines}}. The dotted \textbf{\textcolor{color-21}{magenta lines}} are the first-guess peak positions for the last fit. The lower panel shows the residuals (difference between the data and the model). \textbf{Important note}: Make sure you do not click on the ``Show Plot Tools'' button in the toolbar above the figure; that will disable normal program functioning. If you do; close the Matlab Figure window and start again. Animated instructions on its use are available online at \url{https://terpconnect.umd.edu/~toh/spectrum/ifpinstructions.html}.

\textbf{Recent version history.} Version 13.3: September 2019. Adds baseline corrections modes 4 and 5. Version 13.2 displays "Working..." while the fit is in progress; modified "d" key to save model data to disc as SavedModel.mat. Version 13 added new peak shapes, a total of 24 shapes is now selectable by single keystroke and 49 total selectable from the "-" menu. Version 12.4: Changed IQR in the bootstrap method to IQR/1.34896, which estimates the RSD without outliers for a normal distribution. Version 11.1 adds minimum peak width constraint (\textbf{Shift-W}); adds saturation maximum (\textbf{Shift-M}) to ignore points above saturation maximum (useful for fitting peaks whose peaks are flat because they have reached saturation). Version 11 adds polynomial fitting (\textbf{Shift-o} key). Version 10.7 corrects bugs in equal-width Lorentzians (shape 7) and in the bipolar (+ and -) mode. Version 10.5, August 2014 adds \textbf{Shift-Ctrl-S} and \textbf{Shift-Ctrl-P} keys to transfer the current signal to iSignal and \textit{iPeak}, respectively, if those functions are installed in your Matlab path; Version 10.4, June 2014. Moves fitting result text to bottom panel of graph. Log mode: (\textbf{M} key) toggles log mode on/off, fits log(model) to log(y). Replaces bifurcated Lorentzian with the Breit-Wigner-Fano resonance peak (\textbf{Shift-B} key); see \url{http://en.wikipedia.org/wiki/Fano_resonance}. \textbf{Ctrl-A} selects all; \textbf{Shift-N} negates signal. Version 10 adds \textit{multiple-shape models}; Version 9.9 adds '\textbf{+=}' key to flip between + (positive peaks only) and bipolar (+/- peaks) modes; Version 9.8 adds \textbf{Shift-C} to specify custom first guess ('start'). Version 9.7 adds Voigt profile shape. Version 9.6 added an additional BaselineMode that subtracts a flat baseline without requiring that the signal return to the baseline at both ends of the signal segment. Version 9.5 added exponentially broadened Lorentzian (peak shape 18) and alpha function (peak shape 19);Version 9.4: added bug fix for height of Gaussian/ Lorentzian blend shape; Version 9.3 added \textbf{Shift-S} to save the Matlab Figure window as a PNG graphic to the current directory. Version 9.2: bug fixes; Version 9.1 added fixed-position Gaussians (shape 16) and fixed-position Lorentzians (shape 17) and a peak shape selection menu ( activated by the '\textbf{\_-}' key). 

\href{https://terpconnect.umd.edu/~toh/spectrum/Demoipf.m}{Demoipf.m} is a demonstration script for ipf.m, with a built-in simulated signal generator. To download these m-files, right-click on these links, select \textbf{Save Link As...}, and click \textbf{Save}. You can also download a \href{https://terpconnect.umd.edu/~toh/spectrum/ipf13.zip}{ZIP file} containing ipf.m, Demoipf.m, and peakfit.m. 

\textbf{Example 1:} Test with pure Gaussian function, default settings of all input arguments.

\texttt{{\textgreater}{\textgreater} x=[0:.1:10];y=exp(-(x-5).\textasciicircum{}2);ipf(x,y)}

In this example, the fit is essentially perfect, no matter what are the pan and zoom settings or the initial first-guess (start) values. (However, the peak area, the last fit result reported, includes \textit{only the area within the upper window,} so it does vary). If there were noise in the data or if the model were imperfect, then \textit{all} the fit results will depend on the exact the pan and zoom settings.

\textbf{Example 2:} Test with "center" and "window" specified.

\texttt{{\textgreater}{\textgreater} x=[0:.005:1];y=humps(x).\textasciicircum{}3;}

\texttt{{\textgreater}{\textgreater} ipf(x,y,0.335,0.39)} focuses on first peak

\texttt{{\textgreater}{\textgreater} ipf(x,y,0.91,0.18)} focuses on second peak

\textbf{Example 3:} Isolates a  narrow segment toward the end of the signal.

\texttt{{\textgreater}{\textgreater} x=1:.1:1000;y=sin(x).\textasciicircum{}2;ipf(x,y,843.45,5)}

\textbf{Example 4:} Very noisy peak (SNR=1). 

\texttt{x=[0:.01:10];y=exp(-(x-5).\textasciicircum{}2)+randn(size(x));ipf(x,y,5,10)}

Press the \textbf{F} key to fit a Gaussian model to the data.

\texttt{Press the} \texttt{\textbf{N}} \texttt{key several times to see how much uncertainty in peak parameters is caused by the noise.}

\texttt{\textbf{Example 5:}} \texttt{1-4 Gaussian peaks, no noise, zero baseline, all peak parameters are integer numbers. Illustrates the use of the} \texttt{\textbf{X}} \texttt{key (best of 10 fits) as the number of peaks increases.}

\texttt{Height=[1 2 2 3 3 3 4 4 4 4];}

\texttt{Position=[10 30 35 50 55 60 80 85 90 95]; Width=[2 3 3 4 4 4 5 5 5 5];}

\texttt{x=[0:.01:100];y=modelpeaks(x,10,1,Height,Position,Width,0);}

\texttt{ipf(x,y);}

\subsection{\textit{ipf} keyboard controls (Version 13.2): Obtained by pressing the K key\label{ref-0465}\label{ref-0466}}

 \texttt{Pan signal left and right...Coarse:} \texttt{\textbf{{\textless}}} \texttt{and} \texttt{\textbf{{\textgreater}}}

  \texttt{Fine: left and right cursor arrow} 

  \texttt{Nudge:} \texttt{\textbf{[ ]}} 

 \texttt{Zoom in and out.............Coarse zoom: ?}\texttt{\textbf{/}} \texttt{and} \texttt{\textbf{"}}\texttt{'} 

  \texttt{Fine zoom: up and down arrow keys}

 \texttt{Select entire signal........}\texttt{\textbf{Crtl-A}} \texttt{(Zoom all the way out)}

 \texttt{Resets pan and zoom.........}\texttt{\textbf{ESC}}

 \texttt{Select \# of peaks...........Number keys} \texttt{\textbf{1}}\texttt{-}\texttt{\textbf{9}}\texttt{, or press} \texttt{\textbf{0}} \texttt{key to}

                            \texttt{enter number manually}

 \texttt{Peak shape from menu........}\texttt{\textbf{-}} \texttt{(minus or hyphen), then type}

                              \texttt{number or shape vector and Enter}

 \texttt{Select peak shape by key....}\texttt{\textbf{g}} \texttt{unconstrained Gaussian}

 \texttt{\textbf{h}} \texttt{equal-width Gaussians}

 \texttt{\textbf{Shift-G}} \texttt{fixed-width Gaussians}

 \texttt{\textbf{Shift-P}} \texttt{fixed-position Gaussians}

 \texttt{\textbf{Shift-H}} \texttt{bifurcated Gaussians}

                                \texttt{(equal shape,} \texttt{\textbf{a,z}} \texttt{adjust)}

 \texttt{\textbf{e}} \texttt{Exponential-broadened Gaussian}

                              \texttt{(equal shape,} \texttt{\textbf{a,z}} \texttt{adjust)}

 \texttt{\textbf{Shift-R}}  \texttt{ExpGaussian (var. tau)}

 \texttt{\textbf{j}} \texttt{exponential-broadened equal-width Gaussians}

 \texttt{(equal shape,} \texttt{\textbf{a}}\texttt{,}\texttt{\textbf{z}} \texttt{adjust)}

 \texttt{\textbf{l}} \texttt{unconstrained Lorentzian}

 \texttt{\textbf{:;}} \texttt{equal-width Lorentzians}

 \texttt{\textbf{Shift-[}} \texttt{fixed-position Lorentzians}

 \texttt{\textbf{Shift-E}} \texttt{Exponential-broadened Lorentzians} 

  \texttt{(equal shape, a,z adjust)}

 \texttt{\textbf{Shift-L}} \texttt{Fixed-width Lorentzians (a,z adjust)}

 \texttt{\textbf{o}} \texttt{LOgistic distribution (Use}

                              \texttt{Sigmoid for logistic} \texttt{\textit{function}}\texttt{)}

 \texttt{\textbf{p}} \texttt{Pearson (}\texttt{\textbf{a}}\texttt{,}\texttt{\textbf{z}} \texttt{keys adjust shape)}

 \texttt{\textbf{u}} \texttt{exponential pUlse}

                              \texttt{y=exp(-tau1.*x).*(1-exp(-tau2.*x))}

 \texttt{\textbf{Shift-U}} \texttt{Alpha: y=(x-tau2)./} 

  \texttt{tau1.*exp(1-(x-tau2)./tau1)}

 \texttt{\textbf{s}} \texttt{Up Sigmoid (logistic function):}

                              \texttt{y=.5+.5*erf((x-tau1)/sqrt(2*tau2))}

  \texttt{\textbf{Shift-D}} \texttt{Down Sigmoid}

                            \texttt{y=.5-.5*erf((x-tau1)/sqrt(2*tau2))}

 \texttt{\textbf{\textasciitilde{}}}\texttt{` Gauss/Lorentz blend (equal shape,}

 \texttt{\textbf{Shift}}\texttt{-V Voigt profile (}\texttt{\textbf{a}}\texttt{/}\texttt{\textbf{z}} \texttt{adjusts}

                                    \texttt{a,z adjust shape)}

 \texttt{\textbf{Shift-B}} \texttt{Breit-Wigner-Fano (equal}

                                 \texttt{shape} \texttt{\textbf{a,z}} \texttt{adjust)}

 \texttt{Fit.........................}\texttt{\textbf{f}}

 \texttt{Select BaselineMode ........}\texttt{\textbf{t}} \texttt{selects none, tilted, quadratic,} 

                               \texttt{flat, tilted mode(y), flat mode(y)}

 \texttt{+ or +/- peak mode..........}\texttt{\textbf{+=}}  \texttt{Flips between + peaks only and}

                                \texttt{+/- peak mode}

 \texttt{Invert (negate) signal......}\texttt{\textbf{Shift-N}}

 \texttt{Toggle log y mode OFF/ON....}\texttt{\textbf{m}}  \texttt{Log mode plots and fits}

                                \texttt{log(model) to log(y).}

 \texttt{2-point Baseline............}\texttt{\textbf{b}}\texttt{, then click left and right baseline}

 \texttt{Set manual baseline.........}\texttt{\textbf{Backspace}}\texttt{, then click baseline at}

                                        \texttt{multiple points}

 \texttt{Restore original baseline...}\texttt{\textbf{{\textbar}\textbackslash }}  \texttt{to cancel previous background subtraction}

 \texttt{Cursor start positions......}\texttt{\textbf{c}}\texttt{, then click on each peak position}

 \texttt{Type in start vector........}\texttt{\textbf{Shift-C}} \texttt{Type or Paste start vector}

                                    \texttt{[p1 w1 p2 w2 ...]}

 \texttt{Print current start vector..}\texttt{\textbf{Shift-Q}}

 \texttt{Enter value of 'Extra'......}\texttt{\textbf{Shift-x}}\texttt{, type value (or vector of values}

                               \texttt{in brackets), press Enter.}

 \texttt{Adjust 'Extra' up/down......}\texttt{\textbf{a}}\texttt{,}\texttt{\textbf{z}}\texttt{: 5\% change; upper case} \texttt{\textbf{A}}\texttt{,}\texttt{\textbf{Z}}\texttt{:0.5\% change}

 \texttt{Print parameters \& results..}\texttt{\textbf{q}}

 \texttt{Print fit results only......}\texttt{\textbf{r}}

 \texttt{Compute bootstrap stats.....}\texttt{\textbf{v}}  \texttt{Estimates standard deViations of parameters}

 \texttt{Fit single bootstrap........}\texttt{\textbf{n}}  \texttt{Extracts and Fits siNgle}

                                \texttt{bootstrap sub-sample.}

 \texttt{Plot signal in figure 2.....}\texttt{\textbf{y}}

 \texttt{save model to Disk..........}\texttt{\textbf{d}}  \texttt{Save model to} \texttt{\textbf{D}}\texttt{isk as SavedModel.mat.}

 \texttt{Refine fit..................}\texttt{\textbf{x}} \texttt{Takes best of 10 trial fits}

                              \texttt{(change number in line 227 of ipf.m)}

 \texttt{Print peakfit function......}\texttt{\textbf{w}}  \texttt{Print peakfit function with all}

                                \texttt{input arguments}

 \texttt{Save Figure as png file.....}\texttt{\textbf{Shift-S}}  \texttt{Saves Figure window as Figure1.png,}

                                      \texttt{Figure2.png, etc.}

 \texttt{Display current settings....}\texttt{\textbf{Shift-?}}  \texttt{displays list of current settings}

 \texttt{Fit polynomial to segment...}\texttt{\textbf{Shift-o}}  \texttt{Asks for polynomial order} 

 \texttt{Enter minimum width.........}\texttt{\textbf{Shift-W}} \texttt{Constrains peak widths to}

                                      \texttt{a specified minimum.}

 \texttt{Enter saturation maximum....}\texttt{\textbf{Shift-M}}  \texttt{Ignores points above a}

                                   \texttt{specified saturation maximum}.

 \texttt{Switch to iPeak.............}\texttt{\textbf{Shift-Ctrl-P}} \texttt{transfers current}

                                          \texttt{signal to iPeak.m}

 \texttt{Switch to iSignal...........}\texttt{\textbf{Shift-Ctrl-S}} \texttt{transfers current}

                                          \texttt{signal to iSignal.m}

(The function \href{https://terpconnect.umd.edu/~toh/spectrum/ShapeDemo.m}{ShapeDemo }demonstrates the basic peak shapes \href{https://terpconnect.umd.edu/~toh/spectrum/ShapeDemo.png}{graphically}, showing the variable-shape peaks as multiple lines; graphic on page \pageref{ref-0469})\href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Top}{}

\subsection{Practical examples with experimental data:\label{ref-0467}}

1.~\textbf{Fitting weak and noisy chromatographic peaks with exponentially modified Gaussians.}\label{mark-1.}

\InsImageInline{0.5}{l}{ipf.GIF.png}\begin{enumerate}[{a.}]


\item \textit{\InsImageInline{0.5}{l}{IPFkeyboard.GIF.png}In this example, pan and zoom controls are used to isolate a segment of a chromatogram that contains three very weak peaks near 5.8 minutes. The lower plot shows the whole chromatogram and the upper plot shows the segment. Only the peaks in that segment are subjected to the fit. Pan and zoom are adjusted so that the signal returns to the local baseline at the ends of the segment.}



\item \end{enumerate}
\textit{b. Pressing \textbf{T} selects BaselineMode 1, causing the program to subtract a linear baseline interpolated from these data points. Pressing \textbf{3}, \textbf{E} selects a 3-peak exponentially-broadened Gaussian model (a common peak shape in chromatography). Pressing \textbf{F} initiates the fit. The \textbf{A} and \textbf{Z} keys are then used to adjust the time constant ("Extra") to obtain the best fit. The residuals (bottom panel) are random and exhibit no obvious structure, indicating that the fit is as good as is possible at this noise level. A bootstrap error analysis (page \pageref{ref-0217}) indicates that the relative standard deviation of the measured peak heights is predicted to be less than 3\%.}



\textbf{2. Measuring the areas of peaks}. In this example, a sample of room air is analyzed by gas chromatography (\href{http://matlab.cheme.cmu.edu/2012/06/22/curve-fitting-to-get-overlapping-peak-areas/\#13}{data source}). The resulting chromatogram (next page, right) shows two slightly overlapping peaks, the first for \href{https://en.wikipedia.org/wiki/Oxygen}{oxygen }and the second for \href{https://en.wikipedia.org/wiki/Nitrogen}{nitrogen}. The area under each peak is presumed to be proportional to the gas composition. \href{https://terpconnect.umd.edu/~toh/spectrum/ChromAir.png}{}The \href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html\#pdrop}{perpendicular drop method} (page \pageref{ref-0184}) of measuring the areas gives peak areas in a ratio of 25\% and 75\%, compared to the actual 21\% and 78\% composition, which is not very accurate, possibly because the peaks are so asymmetric. However, an \InsImageInline{0.5}{l}{ChromAir.png}exponentially broadened Gaussian (which a commonly encountered peak shape in chromatography) gives a good fit to the data, using ipf.m and adjusting the exponential term with the A and Z keys to get the best fit. The results for a two-peak fit, shown in the ipf.m screen on the right and in the R-key report below, show that the peak areas are in a ratio of 23\% and 77\%, which is a bit better. With a \textit{3-peak fit,} modeling the nitrogen peak as the sum of \textit{two} closely overlapping peaks whose areas are added together, the \href{https://terpconnect.umd.edu/~toh/spectrum/ChromPeaks3b.png}{curve fit is much better} (less than 1\% fitting error), and indeed the result in that case is 21.1\% and 78.9\% - which is considerably closer the actual composition. 

\texttt{Percent Fitting Error =2.9318\% Elapsed time = 11 sec.}

  \texttt{\textcolor{color-4}{Peak\# Position Height Width Area}}\index{Area}

  \texttt{1 4.8385 17762 0.081094 1533.2}

  \texttt{2 5.1439 47142 0.10205 5119.2}

\InsImageInline{0.5}{l}{Sodium.png}\textbf{3. The accuracy of peak position measurement} can be good even if the fitting error is rather poor. In this example, an experimental high-resolution atomic emission spectrum is examined in the region of the \href{http://en.wikipedia.org/wiki/Sodium-vapor\_lamp}{well-known spectral lines of the element sodium}. Two lines are found there (figure on the right), and when an unconstrained Lorentzian \textit{or} Gaussian model is fit to the data, the peak wavelengths are determined to be 588.98 nm and 589.57 nm: 

\texttt{Percent Fitting Error 6.9922\%} 

  \texttt{\textcolor{color-4}{Peak\# Position Height Width Area}}\index{Area}

  \texttt{1 588.98 234.34 0.16079 56.473}

  \texttt{2 589.57 113.18 0.17509 29.63}

Compare this to the \href{http://www.astm.org/Standards/C1301.htm}{ASTM recommended wavelengths} for this element (588.995 and 589.59 nm) and you can see that the error is no greater than 0.02 nm, which is considerably \textit{less than the interval between the data points} (0.05 nm). And this is despite the fact that the fit is not particularly good, because the peaks shapes are rather oddly shaped (perhaps by \href{http://www.thespectroscopynet.eu/?Physical\_Background:Optics:Selfabsorption}{self-absorption}, because these particular atomic lines are strongly absorbing as well as strongly emitting). This high degree of absolute accuracy compared to a reliable exterior standard is a testament to the excellent wavelength calibration of the instrument on which these experimental data were obtained, but it also shows that peak position is by far the most precisely measurable parameter in peak fitting, even when the data are noisy and the curve fit is not particularly good. The \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Bootstrap}{bootstrap standard deviation estimates} (page \pageref{ref-0213}) calculated by ipf.m for both wavelengths is 0.015 nm (see \#17 in the next section), and using the 2 x standard deviation rule-of-thumb would have predicted a probable error within 0.03 nm. (An even lower \textit{fitting error} can be achieved by \href{https://terpconnect.umd.edu/~toh/spectrum/Na4peakfit.png}{fitting to 4 peaks,} but the \textit{position accuracy} of the larger peaks remains virtually unchanged). 

\textbf{4. How many peaks to model?} In the second and third examples above, the number of peaks in the model was suggested by the data and by the expectations of each of those experiments (\textit{two} major gasses in air; sodium has a well-known \textit{doublet} at that wavelength). In the first example, no \textit{a priori} expectation of number of peaks was available, but the data suggested three obvious peaks, and the residuals were random and unstructured with a 3-peak model, suggesting that no additional model peaks were needed. In many cases, however, the number of model peaks is not so clearly indicated. In a previously described example on page \pageref{ref-0273}~\ref{ref-0273}, the fitting error keeps getting lower as more peaks are added to the model, yet the residuals remain "wavy" and never become random. Without further knowledge of the experiment, it is impossible to know which the ``real peaks'' are and what is just "fitting the noise". 



\subsection{Operating instructions for ipf.m (version 13.2).\label{ref-0468}}

Animated instructions available at https://terpconnect.umd.edu/\textasciitilde{}toh/spectrum/ifpinstructions.html \begin{enumerate}[{1.}]


\item At the command line, type \texttt{\textbf{ipf(x,y)}}, (x = independent variable, y = dependent variable) or \texttt{\textbf{ipf(datamatrix)}} where "datamatrix" is a matrix that has x values in row or column 1 and y values in row or column 2. Or if you have only one signal vector y, type \texttt{\textbf{ipf(y)}}. You may optionally add to additional numerical arguments: \texttt{\textbf{ipf(x,y,center,window);}} where 'center' is the desired x-value in the center of the upper window and ``window'' is the width of that window.

\item \item Use the four \textbf{cursor arrow keys} on the keyboard to pan and zoom the signal to isolate the peak or group of peaks that you want to fit in the upper window. (Use the \textbf{{\textless}} and \textbf{{\textgreater}} and \textbf{?} and " keys for coarse pan and zoom and the square bracket keys \textbf{[} and \textbf{]} to nudge one point left and right). \textit{The curve fitting operation applies only to the segment of the signal shown in the top plot}. The bottom plot shows the entire signal. Try not to get any undesired peaks in the upper window or the program may try to fit them. To select the \textit{entire} signal, press \textbf{Ctrl-A}.

\item 

\item Press the number keys (\textbf{1}\textendash{} \textbf{9}) to choose the number of model peaks, that is, the minimum number of peaks that you think will suffice to fit this segment of the signal. For more than 9 peaks, press \textbf{0}, type the number, and press \textbf{Enter}. 

\item Select the desired model \textbf{peak shape}. In ipf.m version 13, there are 24 different peaks shapes are \InsImageInline{0.5}{l}{ShapeDemo.png}available by keystroke, e.g., \textbf{G}=Gaussian, \textbf{L}=Lorentzian, \textbf{U}=exponential pulse, \textbf{S}=sigmoid (logistic function), etc. Press \textbf{K} to see a list of all commands. You can also select the shape by number from an even larger menu of 49 shapes by pressing the - (minus) key and selecting the shape by number. If the peak widths of each group of peaks is expected to be the same or nearly so, select the "equal-width" shapes. If the peak widths or peak positions are known from previous experiments, select the "fixed-width" or "fixed position" \textcolor{color-22}{shapes.} \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Peak\_width\_constraints}{These more constrained fits} \textcolor{color-22}{are faster, easier, and much more stable than regular all-variable fits, especially if the number of model peaks is greater than 3 (because there are fewer variable parameters for the program to adjust - rather than an independent value for each peak).}\label{ref-0469}

\item \item A set of vertical dashed lines are shown on the plot, one for each model peak. Try to fine-tune the \textbf{Pan} and \textbf{Zoom} keys so that the signal goes to the baseline at both ends of the upper plot and so that the peaks (or bumps) in the signal \textit{roughly} line up with the vertical dashed lines. This does not have to be exact.

\item \item If you want to allow negative peaks as well as positive peaks, press the \textbf{+} key to flip to the +/- mode (indicated by the +/- sign in the y-axis label of the upper panel. Press it again to return to the + mode (positive peaks only). You can switch between these modes at any time. To negate the entire signal, press \textbf{Shift-N}.

\item \item Press \textbf{F} to initiate the curve-fitting calculation. In version 13.2, the center of the graph displays "Working..." while the fit is in progress. Each time you press \textbf{F}, another fit of the selected model to the data is performed with slightly different starting positions, so that you can judge the stability of the fit with respect to changes in starting first guesses. Keep your eye on the residuals plot and on the "Error \%" display. \href{https://terpconnect.umd.edu/~toh/spectrum/RefitAnimationX.gif}{Do this several times}, trying for the lowest error and the most unstructured random residuals plot. At any time, you can adjust the signal region to be fit (step 2), the baseline position (step 9 and 10), change the number of peaks (step 3), or peak shape (step 4), then press the \textbf{F} key again to compute another fit. If the fit seems unstable, try pressing the \textbf{X} key a few times (see \#14, below). 

\item \item The model parameters of the last fit are displayed in the upper window. For example, for a 3-peak fit: 

\item \texttt{Peak\# Position Height Width Area}\index{Area}

\item  \texttt{1 5.33329 14.8274 0.262253 4.13361}

\item  \texttt{2 5.80253 26.825 0.326065 9.31117}

\item  \texttt{3 6.27707 22.1461 0.249248 5.87425}

The column are, left to right: the peak number, peak position, peak height, peak width, and the peak area. (Note: for exponential pulse (\textbf{U}) and sigmoid (\textbf{S}) shapes, Position and Width are replaced by Tau1 and Tau2). Press \textbf{R} to print this table out in the command window. Peaks are numbered from left to right. (The area of each component peak within the upper window is computed using the trapezoidal method and displayed after the width). Pressing \textbf{Q} prints out a report of settings and results in the command window, like so:

\texttt{Peak Shape = Gaussian}

\texttt{Number of peaks = 3}

\texttt{Fitted range = 5 - 6.64}

\texttt{Percent Error = 7.4514 Elapsed time = 0.19 Sec.}

\texttt{Peak\# Position Height Width Area}\index{Area}

\texttt{1 5.33329 14.8274 0.262253 4.13361}

\texttt{2 5.80253 26.825 0.326065 9.31117}

\texttt{3 6.27707 22.1461 0.249248 5.87425}

\item To select the baseline correction mode, press the \textbf{T} key repeatedly; it cycles through \textit{6 baseline correction modes}: \textit{none}, \textit{linear tilted}, \textit{quadratic}, \textit{flat, tilted mode(y), flat mode(y)}. When baseline subtraction is linear, a straight-line baseline connecting the two ends of the signal segment in the upper panel will be automatically subtracted. When baseline subtraction is quadratic, a parabolic baseline connecting the two ends of the signal segment in the upper panel will be automatically subtracted. The \textit{mode(y)} method subtracts the \textit{most common y value} from all the points in the selected region. For peak-type signals where the peaks usually return to the baseline between peaks, this is usually the baseline even if the signal does not return to the baseline at the ends like modes 2 and 3 (\href{https://terpconnect.umd.edu/~toh/spectrum/Mode\%28y\%29example.png}{graphic example}). Use the quadratic baseline correction if the baseline is curved, as in these examples:

\end{enumerate}

\begin{table}
\begin{tabularx}{\textwidth}{|
p{\dimexpr 0.333\linewidth-2\tabcolsep-2\arrayrulewidth}|
p{\dimexpr 0.332\linewidth-2\tabcolsep-\arrayrulewidth}|
p{\dimexpr 0.335\linewidth-2\tabcolsep-\arrayrulewidth}|} \hline 
\InsImageInline{0.5}{l}{autozeroOFF.png} \par BaselineMode OFF & \InsImageInline{0.5}{l}{autozeroLinear.png} \par Linear BaselineMode & \InsImageInline{0.5}{l}{autozeroQuadratic.png} \par Quadratic BaselineMode \\\hline 
\end{tabularx}
\end{table}
\begin{enumerate}[{1.}]
\setcounter{enumi}{19}

\item If you prefer to set the baseline manually, press the \textbf{B} key, then click on the baseline to the LEFT the peak(s), then click on the baseline to the RIGHT the peak(s). The new baseline will be subtracted and the fit re-calculated. (The new baseline remains in effect until you use the pan or zoom controls). Alternatively, you may use the multipoint background correction for the entire signal: press the \textbf{Backspace} key, type in the desired number of background points and press the \textbf{Enter} key, then click on the baseline starting at the left of the lowest x-value and ending to the right of the highest x-value. Press the \textbf{\textbackslash } key to restore the previous background to start over.

\item \item In some cases, it will help to manually specify the first-guess peak positions: press \textbf{C}, then click on your estimates of the peak positions in the upper graph, once for each peak. A fit is automatically performed after the last click. Peaks are numbered in the order clicked. For the most difficult fits, you can type \textbf{Shift-C} and then type in or Paste in the \textit{entire start vector,} complete with square brackets, e.g., ``[pos1 wid1 pos2 wid2 ...]'' where "pos1" is the \textit{position} of peak 1, "wid1" is the \textit{width} of peak 1, and so on for each peak. The custom start values remain in effect until you change the number of peaks or use the pan or zoom controls. Hint: if you Copy the start vector and keep it in the Paste buffer, you can use the Shift-C key and Paste it back in after changing the pan or zoom controls. Note: It is possible to click beyond the x-axis range, to try to fit a peak whose maximum is outside the x-axis range displayed. This is useful when you want to fit a curved baseline by treating it as an additional peak whose peak position if off-scale (real data \href{https://terpconnect.umd.edu/~toh/spectrum/FiveLorentzianBackground.png}{graphic example}).

\item \item The \textbf{A} and \textbf{Z} keys control the "shape" parameter ('extra') that is used only if you are using the "equal shape" models such as the Voigt profile, Pearson, exponentially-broadened Gaussian (ExpGaussian), exponentially-broadened Lorentzian (ExpLorentzian), bifurcated Gaussian, \textcolor{color-22}{Breit-Wigner-Fano}, or Gaussian/Lorentzian blend. For these models, the shapes are variable with the \textbf{A}and \textbf{Z} keys but are the same for all peaks in the model. For the Voigt profile, the "shape" parameter controls \textit{alpha,} the ratio of the Lorentzian width to the Doppler width. For the Pearson shape, a value of 1.0 gives a Lorentzian shape, a value of 2.0 gives a shape roughly half-way between a Lorentzian and a Gaussian, and larger values give a nearly Gaussian shape. For the exponentially broadened Gaussian shapes, the "shape" parameter controls the exponential "time constant" (expressed as the number of points). For the Gaussian/Lorentzian blend and the bifurcated Gaussian shape, the "shape" parameter controls the peak asymmetry (a values of 50 gives a symmetrical peak). For the \textcolor{color-22}{Breit-Wigner-Fano, it controls the Fano factor.} You can enter an initial value of the "shape" parameter by pressing Shift-X , typing in a value, and pressing the \textbf{Enter} key. For multi-shape models, enter a vector of "extra" values, one for every peak, enclosed in square brackets. For single-shape models, you can adjust this value using the \textbf{A} and \textbf{Z} keys (hold down the \textbf{Shift} key to fine tune). Seek to minimize the Error \% or set it to a previously-determined value. Note: if fitting multiple overlapping variable-shape peaks, it is easier to fit a single peak first, to get a rough value for the "shape" parameter, then just fine-tune that parameter for the multipeak fit if necessary.

\item \item For situations where the shapes may be different for each peak and you want the computer to determine the best-fit shape for each peak separately, use the shapes with three unconstrained  iterated variables: 30=variable alpha Voigt, 31=variable time constant ExpGaussian (\textbf{Shift-R}), 32=variable shape Pearson, 33=variable percent Gaussian/Lorentzian blend. These models are more time-consuming and difficult, especially for multiple overlapping peaks.

\item \item \InsImageInline{0.5}{l}{ifp5demo1.png} For difficult fits, it may help to press \textbf{X,} which restarts the iterative fit 10 times \textit{with slightly different first guesses} and takes the one with the lowest fitting error. In version 13.2, the center of the graph displays "Working..." while the fits are in progress. This will take a little longer, obviously. (You can change the number of trials, "NumTrials", in or near line 227 - the default value is 10). \textit{The peak positions and widths resulting from this best-of-10 fit then become the starting points for subsequent fits}, so the fitting error should gradually get smaller and smaller is you press \textbf{X} again and again, until it settles down to a minimum. If none of the 10 trials gives a lower fitting error than the previous one, nothing is changed. Those starting values remain in effect until you change the number of peaks or use the pan or zoom controls. (Remember: \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Peak\_width\_constraints}{equal-width fits}, \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Peak\_width\_constraints}{fixed-width fits}, and fixed position shapes are both faster, easier, and much more stable than regular variable fits, so use equal-width fits whenever the peak widths are expected to be equal or nearly so, or fixed-width (or fixed position) fits when the peak widths or positions are known from previous experiments). 

\item \item Press \textbf{Y} to display the entire signal full screen without cursors, with the last fit displayed in green. The residual is displayed in red, on the same y-axis scale as the entire signal.

\item \item Press \textbf{M} to switch back and forth between log and linear modes. In log mode, the y-axis of the upper plot switches to semilog-y, and log(model) is fit to log(y), which may be useful if the peaks vary greatly in amplitude.

\item \item Press the \textbf{D} key to save the fitting data to disc as SavedModel.mat, containing two matrices: DataSegment (the raw data segment that is fit) and ModelMatrix (a matrix containing each component of the model interpolated to 600 points in that segment). To place these into the workspace, type load SavedModel. To plot saved DataSegment, type plot(DataSegment(:,1), DataSegment(:,2)). To plot SavedModel, type plot(ModelX,ModelMatrix); each component in the model will be plotted in a different color.

\item \item Press \textbf{W} to print out the peakfit.m function with all input arguments, including the last best-fit values of the first guess vector. You can copy and paste the peakfit.m function into your own code or into the command window, then replace "datamatrix" with you own x-y matrix variable. 

\item \item Both \href{https://terpconnect.umd.edu/~toh/spectrum/ipf.m}{ipf.m}, and \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m} are able to estimate the expected variability of the peak position, height, width, and area from the signal, by using the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Bootstrap}{bootstrap sampling method} (page \pageref{ref-0213}). This involves extracting 100 bootstrap samples from the signal, fitting each of those samples with the model, then computing the uncertainty of each peak: the standard deviation (RSD) and the relative percent standard deviation (\%RSD). Basically, this method calculates weighted fits of a single data set, using a different set of different weights for each sample. This process is computationally intensive can take several minutes to complete, especially if the number of peaks in the model and/or the number of points in the signal are high.

\item \item To activate this process in ipf.m, press the \textbf{V} key. It first asks you to type in the number of "best-of-x" trial fits per bootstrap sample (the default is 1, but you may need higher number here if the fits are occasionally unstable; try 5 or 10 here if the initial results give NaNs or wildly improbable numbers). (To activate this process in peakfit.m, you must use version 3.1 or later and include all six output arguments, e.g. [\texttt{FitResults, LowestError, residuals, xi, yi, BootstrapErrors]=peakfit...)}. In version 13.2, the center of the graph displays "Working..." while the fits are in progress. The program displays the results a table in the command window. For example, for a three-peak fit (to the same three peaks used by the Demoipf demonstration script described in the next section) and using 10 as the number or trials:

\texttt{Number of fit trials per bootstrap sample (0 to cancel): 10}

\texttt{Computing.... May take several minutes.}

  \texttt{Peak \#1 Position Height Width Area}\index{Area}

  \texttt{Mean: 800.5387 2.969539 31.0374 98.10405}

  \texttt{STD: 0.20336 0.02848 0.5061 1.2732}

 \texttt{STD(IQR): 0.21933 0.027387 0.5218 1.1555}

  \texttt{\% RSD: 0.025402 0.95908 1.6309 1.2978}

  \texttt{\% RSD (IQR): 0.027398 0.94226 1.6812 1.1778}

  \texttt{Peak \#2 Position Height Width Area}

  \texttt{Mean: 850.0491 1.961368 36.4809 75.9868}

  \texttt{STD: 6.4458 0.14622 3.0657 4.7596}

 \texttt{STD(IQR): 0.52358 0.058845 1.4303 3.9161}

  \texttt{\% RSD: 0.73828 7.2549 8.2035 6.2637}

 \texttt{\% RSD (IQR): 0.71594 7.08002 7.9205 6.1537} 

 \texttt{etc ....}

\item If the RSD and the RSD (IQR) are roughly the same (as in the example above), then the distribution of bootstrap fitting results is close to normal and the fit is stable. If the RSD is substantially greater than RSD (IQR), then the RSD is biased high by "outliers" (obviously erroneous fits that fall far from the norm), and in that case, you should use the RSD (IQR) rather than the RSD because the IQR is much less influenced by outliers. Alternatively, you could use another model or a different data set to see if that gives more stable fits.

\item \item Notice that the RSD of the peak \textit{position is best} (lowest), followed by height and width and area. This is a typical pattern, seen before. Also, be aware that the reliability of the computed variability depends on the assumption that the noise in the signal is representative of the average noise in repeated measurements. If the number of data points in the signal is small, these estimates can be very approximate.

\item \item A likely pitfall with the bootstrap method, when applied to iterative fits, is the possibility that one (or more) of the bootstrap fits will go astray, that is, will result in peak parameters that are wildly different from the norm, causing the estimated variability of the parameters to be too high. For that reason, in ipf 12.3, \textit{two measures} of uncertainty are calculated: (a) the regular \textit{standard deviation} (STD) and (b) the standard deviation estimated by dividing the \href{http://stattrek.com/statistics/dictionary.aspx?definition=Interquartile\%20range}{\textit{interquartile range}} (IQR) by 1.34896. The IQR is more robust to outliers. For a \textit{normal} distribution, the interquartile range is on average equal to 1.34896 times the standard deviation. If one or more of the bootstrap sample fits fails, resulting in a distribution of peak parameters with large outliers, the regular STD will be much greater than the IQR. In that case, a more realistic estimate of variability is IRQ/1.34896. It is best to try to increase the fit stability by choosing a better model (e.g. using an \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Peak\_width\_constraints}{equal-width or fixed-width model}, or a fixed-position shape, if appropriate), adjusting the fitted range (pan and zoom keys), the background subtraction (\textbf{T} or \textbf{B} keys), or the start positions (\textbf{C} key), and/or selecting a higher number of fit trials per bootstrap (which will increase the computation time). As a quick preliminary test of bootstrap fit stability, pressing the \textbf{N} key will perform a single iterative fit to a random bootstrap sub-sample and plot the result; do that several times to see whether the bootstrap fits are stable enough to be worth computing a 100-sample bootstrap. Note: it is normal for the stability of the bootstrap sample fits (\href{https://terpconnect.umd.edu/~toh/spectrum/RefitAnimationN.gif}{\textbf{N} key: click here for animation}) to be poorer than the full-sample fits (\href{https://terpconnect.umd.edu/~toh/spectrum/RefitAnimationX.gif}{\textbf{F} key; click here for animation}), because the latter includes only the variability caused by changing the starting positions for one set of data and noise, whereas the \textbf{N} and \textbf{V} keys aim to include the variability caused by the random noise in the sample by fitting bootstrap sub-samples. Moreover, the best estimates of the measured peak parameters are those obtained by the normal fits of the full signal (\textbf{F} and \textbf{X} keys), \textit{not} the means reported for the bootstrap samples (\textbf{V} and \textbf{N} keys), because there are more independent data points in the full fits and because the bootstrap averages are influenced by the outliers that occur more commonly in the bootstrap fits. The bootstrap results are useful only for estimating the variability of the peak parameters, not for estimating their mean values. The \textbf{N} and \textbf{V keys} are also very useful ways to determine if you are using too many peaks in your model; \textit{superfluous peaks will be very unstable when} \textbf{\textit{N}} \textit{is press repeatedly} and will have in much higher standard deviation of its peak height when the \textbf{V key} is used.

\end{enumerate}
19\textbf{.} \textbf{Shift-o} fits a simple polynomial (linear, quadratic, cubic, etc.) to the segment of the signal dis-played in the upper panel and displays the polynomial coefficients (in descending powers) and the R\textsuperscript{2}.

20. If some peaks are saturated and have a flat top (clipped at maximum height), you can make the program ignore the saturated points by pressing \textbf{Shift-M} and entering the maximum Y values to keep. Y values above this limit will simply be ignored; peaks below this limit will be fit as usual.



21.~To constrain the model to peaks above a certain width, press \textbf{Shift-W} and enter the minimum peak width allowed.\label{mark-21.}

\subsection{\href{https://terpconnect.umd.edu/~toh/spectrum/Demoipf.m}{\label{ref-0470}Demoipf.m}\href{https://terpconnect.umd.edu/~toh/spectrum/ifp5demo1.png}{}}

\InsImageInline{0.5}{l}{ifp5demo1.png}\href{https://terpconnect.umd.edu/~toh/spectrum/Demoipf.m}{Demoipf.m} is a demonstration script for ipf.m, with a built-in simulated signal generator. The true values of the simulated peak positions, heights, and widths are displayed in the Matlab command window, for comparison to the FitResults obtained by peak fitting. The default simulated signal contains six independent groups of peaks that you can use for practice: a triplet near x = 150, a singlet at 400, a doublet near 600, a triplet near 850, and two broad single peaks at 1200 and 1700. Run this demo and see how close to the actual true peak parameters you can get. The useful thing about a simulation like this is that you can get a feel for the accuracy of peak parameter measurements, that is, the difference between the true and measured values of peak parameters. To download these m-files, right-click on the links, select \textbf{Save Link As...}, and click \textbf{Save}. To run it, place both \href{https://terpconnect.umd.edu/~toh/spectrum/ipf.m}{ipf.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/Demoipf.m}{Demoipf} in the Matlab path, then type \textbf{Demoipf} at the Matlab command prompt.

An example of the use of this script is shown in the figure. Here we focus on the 3 fused peaks located near x=850. The true peak parameters (before the addition of the random noise) are:

               \texttt{\textbf{Position Height Width Area}}\index{Area}

    \texttt{800 3 30 95.808}

    \texttt{850 2 40 85.163}

    \texttt{900 1 50 53.227}

When these peaks are isolated in the upper window and fitted with three Gaussians, the results are:

           \texttt{\textbf{Position Height Width Area}} 

      \texttt{800.04 3.0628 29.315 95.583}

      \texttt{850.15 1.9881 41.014 86.804}

      \texttt{901.3 0.9699 46.861 48.376}

So you can see that the accuracy of the measurements is excellent for peak position, good for peak height, and least good for peak width and area. It is no surprise that the least accurate measurements are for the smallest peak with the poorest signal-to-noise ratio. Note: the predicted standard deviation of these peak parameters can be determined by the bootstrap sampling method, as described in the previous section. We would expect that the measured values of the peak parameters (comparing the true to the measured values) would be within about 2 standard deviations of the true values listed above).

\href{https://terpconnect.umd.edu/~toh/spectrum/Demoipf2.m}{Demoipf2.m} is identical, except that the peaks are superimposed on a strongly curved baseline, so you can test the accuracy of the baseline correction methods (\# 9 and 10, above). \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Top}{}

\section{Execution time of peak fitting and other signal processing tasks\label{ref-0471}\label{ref-0472}}

By execution time, I mean the time it takes for one operation to be performed, exclusive of plotting or printing the results, when Matlab is running on a standard Windows PC. For iterative peak fitting, the biggest factors that determine the execution time are (a) the speed of the computer, (b) the number of peaks, and (c) the peak shape:

a) The execution time can vary over a factor of 4 or 5 or more between different computers, (e.g., comparing a small laptop, with 1.6 GHz, dual-core Athlon CPU with 4 Gbytes RAM, to a desktop with a 3.4 GHz i7 CPU with 16 Gbytes RAM). Run the Matlab "bench.m" benchmark test to see how your computer stacks up. 

b) The execution time increases with the product of the number of peaks in the model times the number of iterated variables per peak. (See \href{https://terpconnect.umd.edu/~toh/spectrum/PeakfitTimeTest.m}{PeakfitTimeTest.m}).

c) The execution time varies greatly (sometimes by a factor of 100 or more) with the peak shape, with the exponentially-broadened shapes being the slowest and the fixed-width shapes being the fastest. See \href{https://terpconnect.umd.edu/~toh/spectrum/PeakfitTimeTest2.m}{PeakfitTimeTest2.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/PeakfitTimeTest2a.m}{PeakfitTimeTest2a.m}. The equal-width and fixed-width shape variations are always faster. 

d) The execution time increases directly with NumTrials in peakfit.m. The "Best of 10 trials" function (\textbf{X} key in ipf.m) takes about 10 times longer than a single fit. 

Other factors that are less important are the number of data points in the fitted region (but only if the number of data points is very large; for example see \href{https://terpconnect.umd.edu/~toh/spectrum/PeakfitTimeTest3.m}{PeakfitTimeTest3.m}) and the starting values (good starting values can reduce execution time slightly; \href{https://terpconnect.umd.edu/~toh/spectrum/PeakfitTimeTest2.m}{PeakfitTimeTest2.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/PeakfitTimeTest2a.m}{PeakfitTimeTest2a.m} have examples of that). Note: some of these scripts need \href{https://terpconnect.umd.edu/~toh/spectrum/DataMatrix2.mat}{DataMatrix2} and \href{https://terpconnect.umd.edu/~toh/spectrum/DataMatrix3.mat}{DataMatrix3}, which you can download from \url{http://tinyurl.com/cey8rwh}. \label{ref-0473}

\href{https://terpconnect.umd.edu/~toh/spectrum/TimeTrial.txt}{TimeTrial.txt} is a text file comparing the speed of \textit{18 different signal processing tasks} running on 5 different systems: (1) Windows 10, 64-bit, 3.6 GHz core i7, with 16 GBytes RAM, using Matlab 9.9 (R2020b) Update 3 academic, (2) Matlab 2017b Home, and (3) Matlab Online R2018b in Chrome, both running on older desktop PCs, (4) Matlab Mobile on an iPad, and (5) Octave 6.2.0. The Matlab/Octave code that generated this is \href{https://terpconnect.umd.edu/~toh/spectrum/TimeTrial.m}{TimeTrial.m}, which runs all the tasks one after the other and prints out the elapsed times for your particular machine, in addition to the times previously recorded for each task on each of the five software systems. \href{https://terpconnect.umd.edu/~toh/spectrum/TimeTrial.xlsx}{TimeTrial.xlsx} summarizes the comparison of Matlab to Octave.

\section{Curve Fitting Hints and Tips\label{ref-0474}}

\begin{enumerate}[{1.}]


\item If the fit fails completely, returning all zeros, the data may be formatted incorrectly. The independent variable ("x") and the dependent variable ("y") must be separate vectors or columns of a 2xn matrix, with x in the first row or column. Or it may be that first guesses ("start") need to be provided for that fit. 

\item It is best \textit{not} to smooth your data prior to curve fitting. Smoothing can distort the signal shape and the noise distribution, making it harder to evaluate the fit by visual inspection of the residuals plot. Smoothing your data beforehand makes it impossible to achieve the goal of obtaining a random unstructured residuals plot and it increases the chance that you will "fit the noise" rather than the actual signal. The bootstrap error estimates are invalid if the data are smoothed.

\item The most important factor in non-linear iterative curve fitting is selecting the \textit{underlying model peak function}, for example, Gaussian, Equal-width Gaussians, Lorentzian, etc. (see page \pageref{ref-0270}). It is worth spending some time finding and verifying a suitable function for your data. If the peak widths of each group of peaks are expected to be the same or nearly so, select the "equal-width" shapes; equal-width fits (available for the Gaussian and Lorentzian shapes) are faster, easier, and much more stable than regular variable-width fits. But it is important to understand that a good fit is \textit{not by itself proof} that the shape function you have chosen it the correct one; in some cases, the wrong function can give a fit that looks perfect. For example, consider a 5-peak Gaussian model that has a low percent fitting error and for which the residuals look random - usually an indicator of a good fit (\href{https://terpconnect.umd.edu/~toh/spectrum/GoodFitWrongModel.png}{Click for a graphic} if you are reading online). But in fact, in this case, the model is \textit{wrong}; that data came from an experimental domain where the underlying shape is fundamentally non-Gaussian but in some cases can look very like a Gaussian. It is important to get the model right for the data and not depend solely on the goodness of fit.

\item You should always use the \textit{minimum} number of peaks that adequately fit your data. (page \pageref{ref-0271}). Using too many peaks will result in an unstable fit - the green lines in the upper plot, representing the individual component peaks, will bounce around wildly for each repeated fit, without significantly reducing the Error. A very useful way to determine if you are using too many peaks in your model is to use the \textbf{N key} (see \#10, {\hyperref[ref-0475]{below}}) to perform a single fit to a bootstrap sub-sample of points; \textit{superfluous peaks will be very unstable when} \textbf{\textit{N}} \textit{is press repeatedly}. (You can get better statistics for this test, at the expense of time, by using the \textbf{V key} to compute the standard deviation of 100 bootstrap sub-samples).

\item If the peaks are superimposed on a background or baseline, then that must be accounted for \textit{before} fitting, otherwise, the peak parameters (especially height, width and area) will be inaccurate. Either subtract the baseline from the \textit{entire} signal using the \textbf{Backspace} key (\#10 in \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#ipf\_instructions}{Operating Instructions}, above) or use the \textbf{T} key to select one of the automatic baseline correction modes (\# 9 in \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#ipf\_instructions}{Operating Instructions}, above).

\item This program uses an iterative non-linear search function (\textit{"modified Simplex"}) to determine the peak positions and widths that best match the data. This requires first guesses for the peak positions and widths. (The peak \textit{heights} do not require first guesses, because they are linear parameters; the program determines them by linear regression). The default first guesses for the peak positions are made by the computer based on the pan and zoom settings and are indicated by the magenta vertical dashed lines. The first guesses for the peak widths are computed from the Zoom setting, so the best results will be obtained if you zoom in so that the group of peaks is isolated and spread out as suggested by the peak position markers (vertical dashed lines). 

\item If the peak components are very unevenly spaced, you might be better off entering the first-guess peak positions yourself by pressing the \textbf{C} key and then clicking on the top graph where you think the peaks might be. None of this must be exact - they're just first guesses, but if they are too far off it can throw the search algorithm off. You can also type in the first guesses for position \textit{and} width manually by pressing \textbf{Shift-C}).

\item Each time you perform another iterative fit (e.g., pressing the \textbf{F} key), the program adds small random deviations to the first guesses, to determine whether an improved fit might be obtained with slightly different first guesses. This is useful for determining the robustness or stability of the fit \textit{with respect to starting values}. If the error and the values of the peak parameters vary slightly in a tight region, this means that you have a robust fit (for that number of peaks). If the error and the values of the peak parameters bounce around wildly, it means the fit is not robust (try changing the number of peaks, peak shape, and the pan and zoom settings), or it may simply be that the data are not good enough to be fit with that model. Try pressing the \textbf{X} key, which takes the best of 10 iterative fits and uses those best-fit values as the \textit{starting first guesses} for subsequent fits. So, each time you press \textbf{X}, if any of those fits yield a fitting error less than the previous best, that one is taken as the start for the next fit. As a result, the fits tend to get better and better gradually as the \textbf{X} key is pressed repeatedly. Often, even if the first fit is terrible, subsequent \textbf{X}-key fits will improve considerably.

\item The variability in the peak parameters from fit to fit using the \textbf{X} or \textbf{F} keys is only an estimate of the uncertainty caused by the curve fitting procedure (but \textit{not} of the uncertainty caused by the noise in the data, because this is only for one sample of the data and noise; for that you need the \textbf{N} key fits). 

\item To examine the robustness or stability of the fit \textit{with respect to random noise in the data}, press the \textbf{N} key. \href{https://terpconnect.umd.edu/~toh/spectrum/RefitAnimationN.gif}{Each time you press \textbf{N}}, it will perform an iterative fit on a different subset of data points in the selected region (called a "bootstrap sample"; see page \pageref{ref-0212}). \href{https://terpconnect.umd.edu/~toh/spectrum/RefitAnimationN.gif}{Click for animation}. If that gives reasonable-looking fits, then you can go on to compute the peak error statistics by pressing the \textbf{V} key. If on the other hand the \textbf{N} key gives wildly different fits, with highly variable fitting errors and peak parameters, then the fit is not stable, and you might try the \textbf{X} key to take the best of 10 fits and reset the starting guesses, then press \textbf{N} again. In difficult cases, it may be necessary to increase the number of trials when asked (but that will increase the time it takes to complete), or if that does not help, use another model or a better data set. (The \textbf{N} and \textbf{V} keys are a good way to evaluate a multi-peak model for the possibility of superfluous peaks, see \# 4 above).\label{ref-0475}

\item If you do not find the peak shape you need in this program, look at the next section to learn how to add your own new ones, or write me at toh@umd.edu and I'll see what I can do.

\item If you try to fit a very small independent variable (x-axis) segment of a very large signal, say, a region that is only 1000\textsuperscript{th} or less of the entire x-axis range, you might encounter a problem with unstable fits. If that happens, try subtracting a constant from x, then perform the fit, then add in the subtracted amount to the measured x positions. 

\item If there are very few data points on the peak, it might be necessary to reduce the minimum width (set by minwidth in peakfit.m or Shift-W in ipf.m) to zero or to something smaller than the default minimum (which defaults to the x-axis spacing between adjacent points).

\item Difference between the \textbf{F}, \textbf{X}, \textbf{N}, and \textbf{V} keys in ipf.m:\begin{enumerate}[{a}]
\setcounter{enumii}{14}

\item \textbf{F key}: Slightly varies the starting values and performs a single iterative fit using all the data points in the selected region.

\setcounter{enumii}{14}
\item \textbf{X key}: Performs 10 iterative trial fits using all the data points in the selected region, slightly varying the starting values before each trial, then takes the one with the lowest fitting error. Press it again to refine the fit. Takes about 10 times longer than the \textbf{F} key.

\setcounter{enumii}{14}
\item \textbf{N ke}y: Slightly varies the starting values and performs an iterative single fit using a random subset of the data points in the selected region. Use to visualize the stability of the fit with respect to random noise. Takes the same time as the \textbf{F} key.

\setcounter{enumii}{14}
\item \textbf{V key}: Asks for several trial fits, then performs 100 iterative fits each on a separate random subset of the data points in the selected region, each fit using the specified number of trials and taking the best one, then calculates the mean and standard deviation of the peak parameters of all 100 best-fit results. Use to quantify the stability of peak parameters with respect to random noise. Takes about 100 times longer than the \textbf{X} key.

\end{enumerate}
\end{enumerate}
\section{}

\section{Extracting the equations for the best-fit models\label{ref-0476}}

The equations for the peak shapes are in the peak shape menu in ipf.m, isignal.m, and ipeak.m. Here are the expressions for the shapes that are expressed mathematically rather than algorithmically:

\texttt{Gaussian: y =exp(-((x-pos)/(0.60056120439323*width)) \textasciicircum{}2)}

\texttt{Lorentzian: y =1/(1+((x-pos)/(0.5*width))\textasciicircum{}2)}

\texttt{Logistic: y =exp(-((x-pos)/(.477*wid))\textasciicircum{}2); y=(2*n)/(1+n)}

\texttt{Lognormal: y = exp(-(log(x/pos)/(0.01*wid)) \textasciicircum{}2)}

\texttt{Pearson: y =1/(1+((x-pos)/((0.5\textasciicircum{}(2/m))*wid))\textasciicircum{}2)\textasciicircum{}m}

\texttt{Breit-Wigner-Fano: y =((m*wid/2+x-pos)\textasciicircum{}2)/(((wid/2)\textasciicircum{}2)+(x-pos)\textasciicircum{}2)}

\texttt{Alpha function: y =(x-spoint)/pos*exp(1-(x-spoint)/pos)}

\texttt{Up Sigmoid: y =.5+.5*erf((x-tau1)/sqrt(2*tau2))}

\texttt{Down Sigmoid y =.5-.5*erf((x-tau1)/sqrt(2*tau2))}

\texttt{Gompertz: y =Bo*exp(-exp((Kh*exp(1)/Bo)*(L-t) +1))}

\texttt{FourPL: y = 1+(miny-1)/(1+(x/ip)\textasciicircum{}slope)}

\texttt{OneMinusExp: y = 1-exp(-wid*(x-pos))}

\texttt{EMG (shape 39) y = s*lambda*sqrt(pi/2)*exp(0.5*(s*lambda)\textasciicircum{}2-lambda*(t-mu))*erfc((1/sqrt(2))*(s*lambda-((t-mu)/s)))}

The peak parameters (height, position, width, tau, lambda, etc.) that are returned by the FitResults displayed on the graph and in the command window. For example, if you fit a set of data to a single Gaussian and get{\ldots}

 \texttt{\textbf{Peak\# Position Height Width Area}}\index{Area}

  \texttt{1} \texttt{\textbf{0.30284 87.67 0.23732}}   \texttt{22.035}

...then the equation would be:

\texttt{y =} \texttt{\textbf{87.67}}\texttt{*exp(-((x-}\texttt{\textbf{0.30284}}\texttt{)/(0.60056120439323*}\texttt{\textbf{0.23732}}\texttt{)).\textasciicircum{}2)}

If you specify a model of more than one peak, then the equation is the \textit{sum} of each peak in the model. For example, fitting the built-in Matlab "humps" function using a model of 2 Lorentzians

\texttt{{\textgreater}{\textgreater} x=[0:.005:2];y=humps(x);[FitResults,GOF]=peakfit([x' y'],0,0,2,2)}

  \texttt{Peak\# Position Height Width Area} 

  \texttt{1 0.3012 96.9405 0.1843 24.9270}

  \texttt{2 0.8931 21.1237 0.2488 7.5968}

The equation would be: 

\texttt{y = 96.9405*(1/(1+((x-0.3012)/(0.5*0.1843))\textasciicircum{}2)) + 21.1237*(1//(1+((x-0.8931)/(0.5*0.2488)).\textasciicircum{}2))}

It is also possible to use multiple shapes in one fit, by specifying the peak shape parameter as a vector. For example, you could fit the first peak of the "humps" function with a Lorentzian and the second peak with a Gaussian by using [2 1] as the shape argument.

\texttt{{\textgreater}{\textgreater} x=[0:.005:2];y=humps(x);peakfit([x' y'],0,0,2,[2 1])}

  \texttt{Peak\# Position Height Width Area} 

  \texttt{1 0.3018 97.5771 0.1876 25.4902}

  \texttt{2 0.8953 18.8877 0.3341 6.7180}

In that case the expression would be y = peak 1 (Lorentzian) + peak 2 (Gaussian):

\texttt{y = 97.5771*(1/(1+((x-0.3018)/(0.5*0.18}

\texttt{76))\textasciicircum{}2)) + 18.8877*exp(-((x-0.8953)/(0.60056120439323*0.3341))\textasciicircum{}2)}

\textbf{Note}: To obtain the \textit{digitally sampled} model data, use peakfit is 6\textsuperscript{th} and 7\textsuperscript{th} \textit{output} parameters, xi and yi, which return a 600-point interpolated model as a vector of x values and a matrix of y values with one row for each component. Type plot(xi,yi) to plot each model peak separately in a different color or plot(xi,yi(1,:)) to plot just peak 1.



\section{How to add a new peak shape to peakfit.m, ipf.m, iPeak, or iSignal\label{ref-0477}\label{ref-0478}}

  It is easier than you think to add your own custom peak shape to peakfit.m (or to those interactive functions that use peakfit.m internally, such as ipf.m, iSignal, or iPeak), if you have a mathematical expression for your shape. The easiest way is to \textit{modify an existing peak shape} that you do not plan to use, replacing it with your new function. You must pick a shape to sacrifice that has the \textit{same number of variables and constraints} are your new shape. For example, if your shape has \textit{two} iterated parameters (e.g., variable position and width), you could modify the Gaussian, Lorentzian, or triangular shape (number 1, 2 or 21, respectively). If your shape has \textit{three} iterated variables, use shapes like 31, 32, 33, or 34. If your shape has \textit{four} iterated variables, use shape 49 (``double gaussian'', Shift-K). If your shape has an 'extra' parameter, like the equal-shape Voigt, Pearson, BWF, or blended Gaussian/ Lorentzian, use one of those. If you need an exponentially modified shape, use the exponentially modified Gaussian (5 or 31) or Lorentzian (18). If you need equal widths or fixed widths, etc., use one of those shapes. \textbf{\textit{This is important}}; you \textit{must} have the \textit{same number of variables and constraints}, because the structure of the code is different for each class of shapes.

There are just two required steps to the process:\begin{enumerate}[{1.}]


\item  Let us say that your shape has \textit{two} iterated parameters and you are going to sacrifice the triangular shape, number 21. Open peakfit.m or ipf.m in the Matlab editor and re-write the \textit{old} shape function ("triangular", located near line 3672 in ipf.m - there is one of those functions for each shape - by changing \textit{name} of the function and the \textit{mathematics} of the assignment statement (e.g., \texttt{g = 1-(1./wid).*abs(x-pos);}). You can use the same variables ('x' for the independent variable, 'pos' for peak position, 'wid' for peak width, etc.) Scale your function to have a peak height of 1.0 (e.g., after computing y as a function of x, divide by \texttt{max(y)}).

\item Use the search function in Matlab to find all instances of the name of the old function and replace it with the new name, \textit{checking "Wrap around" but leaving "Match case" and "Whole word" unchecked} in the Search box. If you do it right, for example, all instances of "triangular" and all instances of "fittriangular' will be modified with your new name replacing "triangular". \textbf{Save} the result (or \textbf{Save as...} with a modified file name).

\end{enumerate}
That is it! Your new shape will now be shape 21 (or whatever was the shape number of the old shape you sacrificed). In ipf.m, it will be activated by the same keystroke used by the old shape (e.g. Shift-T for the triangular, key number 84), and in iSignal and in iPeak, the menu of peak shapes will have been modified by the search and replace in step 2.

If you wish, you can change the keystroke assignment in ipf.m; first, find a key or Shift-Key that is not yet assigned (and which gives an "UnassignedKey" error statement when you press it with ipf.m running). Then change the old key number 84 to that unassigned one in the big "\texttt{switch double(key)}\texttt{,}" case statement near the beginning of the code. But it is simpler just to use the old key. 

\section{Which to use? \textit{peakfit}, \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#3.\_Interactive\_keypress-operated\_}{\textit{ipf}}, \textit{findpeaks{\ldots}}, \href{https://terpconnect.umd.edu/~toh/spectrum/ipeak.html}{\textit{iPeak}}, or \href{https://terpconnect.umd.edu/~toh/spectrum/isignal.html}{\textit{iSignal}}?\label{ref-0479}\label{ref-0480}}

I designed \href{https://terpconnect.umd.edu/~toh/spectrum/ipeak.html}{\textit{iPeak}} (page \pageref{ref-0461}),  \href{https://terpconnect.umd.edu/~toh/spectrum/isignal.html}{\textit{iSignal}} (page \pageref{ref-0432}), \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{\textit{peakfit}} (page \pageref{ref-0448}), and \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#3.\_Interactive\_keypress-operated\_}{\textit{ipf}} (page \pageref{ref-0462}) each with a different emphasis, although there is some overlap in their functions. Briefly, iSignal combines several basic functions, including smoothing, differentiation, spectrum analysis, etc.; findpeaks{\ldots} and iPeak focus on finding multiple peaks in large signals; and peakfit and ipf focus on iterative peak fitting. But there is some overlap; iPeak and iSignal can also perform least-squares iterative peak fitting, and iSignal can perform peak finding. In addition, iSignal, iPeak, and ipf are \textit{interactive}, and work only in \textit{Matlab} or in \textit{Matlab Online} in a web browser, whereas their \textit{command-line} versions ProcessSignal.m, the many variation of findpeaks.m, and peakfit.m are functions and they also work in \textit{Octave}. The interactive functions are better for exploration and trying out different setting directly, whereas the command-line functions are better for automatic hand-off processing of masses of data.

\textbf{Common features}. The interactive programs iSignal, iPeak, and ipf all have several features in common. 

(a) The \textbf{K} key displays the keyboard controls for each program. 

(b) The pan and zoom keys are the four cursor keys \textendash{} left and right for pan, up and down for zoom.

(c) \textbf{Ctrl-Shift-A} selects the entire signal (that is, zooms out all the way). 

(d)  The \textbf{W} key. To facilitate transfer of settings from one of these functions to another or to a command-line version, all these functions use the \textbf{W} key to print out the syntax of other related functions, with the pan and zoom settings and other numerical input arguments specified, ready for you to Copy and Paste into your own scripts or back into the command window. For example, you can convert a curve fit from \textbf{\textit{ipf}} into the command-line \textbf{\textit{peakfit}} function; or you can convert a peak finding operation from \textbf{\textit{ipeak}} into the command line \textbf{\textit{findpeaksG}} or \textbf{\textit{findpeaksb}} or \textbf{\textit{findpeaksb3}} functions. This provides a way to deal with signals that require different signal processing in different regions of their x-axis ranges, by allowing you to create a series of command-line functions for each local region that, when executed in sequence, quickly process each segment of the signal appropriately and can be repeated easily for any number of other examples of that same type of signal.

(e) All these programs use the \textbf{Shift-Ctrl-S}, \textbf{Shift-Ctrl-F}, and \textbf{Shift-Ctrl-P} keys to transfer the current signal, as a \href{https://www.mathworks.com/help/matlab/ref/global.html}{global variable}s X and Y, to i\textbf{S}ignal.m, ip\textbf{f}.m, and i\textbf{P}eak.m, respectively.

\textbf{\textit{First time here?}} Check out these \textit{animated Web demos} of \href{https://terpconnect.umd.edu/~toh/spectrum/ipeak.html}{ipeak.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/ifpinstructions.html}{ipf.m}. Or download these Matlab demo functions that compare ipeak.m with peakfit.m for signals with a \href{https://terpconnect.umd.edu/~toh/spectrum/idemo1.m}{few peaks} and signals with \href{https://terpconnect.umd.edu/~toh/spectrum/idemo2.m}{many peaks} and that shows how to adjust ipeak to detect \href{https://terpconnect.umd.edu/~toh/spectrum/idemo.m}{broad or narrow peaks}. These self-contained demos include all required Matlab functions. Just place them in your path and click \textbf{Run} or type their name at the command prompt. Or you can download all these demos together in \href{https://terpconnect.umd.edu/~toh/spectrum/idemos.zip}{idemos.zip}. \href{https://terpconnect.umd.edu/~toh/spectrum/peakfitVSfindpeaks.m}{peakfitVSfindpeaks.m} performs a direct comparison of the peak parameter accuracy of findpeaks vs peakfit. 

\textbf{Note 1:} Click on the Matlab Figure window to activate the interactive keyboard tools. Make sure you do not click on the ``Show Plot Tools'' button in the toolbar above the figure; that will disable normal program functioning. If you do, just close the Figure window and start again.

\textbf{Note 2:} The interactive keypress-operated iPeak, iSignal, iFilter, and ipf functions also work when you run \href{https://www.mathworks.com/products/matlab-online.html}{Matlab in a web browser} (just click on the figure window first), but they do not currently work on \href{https://itunes.apple.com/us/app/matlab-mobile/id370976661?mt=8}{Matlab Mobile} or in Octave.

\section{S.P.E.C.T.R.U.M.: Simple freeware signal processing program for Macintosh OS 8.1\label{ref-0481}\label{ref-0482}\label{ref-0483}}

Some of the figures in this book are screen images from a now-obsolete programming project called S.P.E.C.T.R.U.M. (\textbf{S}ignal \textbf{P}rocessing for \textbf{E}xperimental \textbf{C}hemistry \textbf{T}eaching and \textbf{R}esearch/ \textbf{U}niversity of \textbf{M}aryland), a Macintosh program that I developed in 1989 for teaching signal processing to chemistry students. It runs only in Macintosh OS 8.1 and earlier (and on Windows 7 PCs and various specific \href{http://en.wikipedia.org/wiki/Linux}{Linux} distributions using the \href{http://en.wikipedia.org/wiki/Executor\_\%28software\%29}{Executor emulator).}


\begin{center}
\InsImageInline{0.5}{l}{ColorScreenShot.GIF.png}  
\end{center}


SPECTRUM is designed for post-run (rather than real-time) processing of "spectral" or time- series data (y values at equally-spaced x intervals), such as spectra, chromatograms, electrochemical signals, etc. The program enhances the information content of instrument signals, for example by reducing noise, improving resolution, compensating for instrumental artifacts, testing hypotheses, and decomposing a complex signal into its component parts.

\textit{SPECTRUM was the winner of} \textbf{\textit{two}} \href{http://h-net.msu.edu/cgi-bin/logbrowse.pl?trx=vx&list=edtech&month=9008&week=&msg=t2EsV9QVXNoMf8HBYVnKYQ&user=&pw=}{\textit{EDUCOM/NCRIPTAL}} \textit{national software awards in 1990, for} \textbf{\textit{Best Chemistry}} \textit{software and for} \textbf{\textit{Best Design}}.

\subsection{Features\label{ref-0484}}


\begin{itemize}
\item Reads one- or two- column (y-only or x-y) text data tables with either tab or space separators.

\item Displays fast, labeled plots in standard resizable windows with full x- and y-axis scale expansion and a mouse- controlled measurement cursor.

\item Addition, subtraction, multiplication, and division of two signals.

\item Two kinds of smoothing.

\item Three kinds of differentiation.

\item Integration.

\item Resolution enhancement (peak sharpening).

\item Interpolation

\item Fused peak area measurement by perpendicular drop or tangent skim methods, with mouse-controlled setting of start and stop points (page \pageref{ref-0184}).

\item Fourier transformation

\item Power spectra

\item Fourier filtering

\item Convolution and deconvolution

\item Cross- and auto-correlation

\item Built-in signal simulator with Gaussian and Lorentzian bands, sine wave and normally-distributed random noise.

\item A host of other useful functions, including the following: inspect and edit points, normalize, histogram, interpolate, zero fill, group points by 2s, bridge segment, superimpose, extract subset of points, concatenate, reverse X-axis, rotate, set X-axis values, reciprocal, log, ln, antilog, anti natural log, standard deviation, absolute value\index{\textcolor{color-3}{absolute value}}, square root.


\end{itemize}
SPECTRUM can be used both as a research tool and as an instructional aid in teaching signal processing techniques. The program and its associated tutorial was originally developed for students of analytical chemistry, but the program could be used in any field in which instrumental measurements are used: e.g., chemistry, biochemistry, physics, engineering, medical research, clinical psychology, biology, environmental and earth sciences, agricultural sciences, or materials testing.

\subsection{Machine Requirements\label{ref-0485}}

 SPECTRUM runs only on older Macintosh models running OS 7 or 8, minimum 1 MByte RAM, any standard printer. Color screen desirable. SPECTRUM has been tested on most Macintosh models and on all versions of the operating system through OS 8.1. No PC version or more recent Mac version is available or planned, but if you have some older model Macs laying around, you might find this program useful. SPECTRUM was written in Borland's Turbo Pascal in 1989. That firm has long been out of business, neither the Turbo Pascal compiler nor the executable code generated by that compiler runs on current Macs, and therefore there is no way for me to update SPECTRUM without completely re-writing it in another language.

SPECTRUM also runs on Windows 7 PCs using the \href{http://en.wikipedia.org/wiki/Executor\_\%28software\%29}{Executor emulator}, which since 2008 has been made available as \href{http://en.wikipedia.org/wiki/Open\_source}{open-source} software.

\subsection{Download links\label{ref-0486}}

\textbf{The full version of SPECTRUM 1.1 is available as freeware and} can be downloaded from \href{http://www.wam.umd.edu/~toh/spectrum/SPECTRUM.html}{http://terpconnect.umd.edu/\textasciitilde{}toh/spectrum/}. There are two versions:

\textbf{SPECTRUM 1.1e}: Signals are stored internally as \textit{extended-precision} real variables and there is a limit of 1024 points per signal. This version performs all its calculations in extended precision and thus has the best dynamic range and the smallest numeric round-off errors. The download address of this version in HQX format is \href{http://www.wam.umd.edu/~toh/spectrum/SPECTRUM11e.hqx}{http://terpconnect.umd.edu/\textasciitilde{}toh/spectrum/SPECTRUM11e.hqx}.

\textbf{SPECTRUM 1.1b}: Signals are stored internally as \textit{single-precision} real variables and there is a limit of 4000 points per signal. This version is less precise in its calculations (has more numerical round-off error) than the other version but allows signals with data more points. The download address of this version in HQX format is \href{http://www.wam.umd.edu/~toh/spectrum/SPECTRUM11b.hqx}{http://terpconnect.umd.edu/\textasciitilde{}toh/spectrum/SPECTRUM11b.hqx}.

The two versions are otherwise identical.

There is also a documentation package \href{http://www.wam.umd.edu/~toh/spectrum/SPECTRUMdemo.hqx}{(located at http://terpconnect.umd.edu/\textasciitilde{}toh/spectrum/SPECTRUMdemo.hqx}) consisting of:

\textbf{1. Reference manual.} MacWrite format (Can be opened from within MacWrite, Microsoft Word, ClarisWorks, WriteNow, and most other full-featured Macintosh word processors). Explains each menu selection and describes the algorithms and mathematical formulae for each operation. The SPECTRUM Reference Manual is also available separately in \href{http://terpconnect.umd.edu/~toh/spectrum/SPECTRUMReferenceManual.html}{HTML} and \href{http://terpconnect.umd.edu/~toh/spectrum/SPECTRUMReferenceManual.pdf}{PDF} format.

\textbf{2. Signal processing tutorial.} MacWrite format (Can be opened from within MacWrite, Microsoft Word, ClarisWorks, WriteNow, and most other full-featured Macintosh word processors). Self-guided tutorial on the applications of signal processing in analytical chemistry. This tutorial is also available on the Web at \href{http://www.wam.umd.edu/~toh/Chem498C/SignalProcessing.html}{(http://terpconnect.umd.edu/\textasciitilde{}toh/Chem498C/SignalProcessing.html)}

\textbf{3. Tutorial signals:} A library of prerecorded data files for use with the signal processing tutorial. These are plain decimal ASCII (tab-delimited) data files.

These files are ``binhex'' encoded: use \textit{Stuffit Expander} to decode and decompress as usual. If you are downloading on a Macintosh, all this should happen completely automatically. If you are downloading on a Windows PC, shift-click on the download links above to begin the download. If you are using the ARDI Executor Mac simulator, download the "HQX" files to your C drive, launch Executor, then open the downloaded HQX files with Stuffit Expander, which is pre-loaded into the Executor Macintosh environment. Stuffit Expander will automatically decode and decompress the downloaded files. Note: Because it was developed for academic teaching application where the most modern and powerful models of computers may not be available, SPECTRUM was designed to be "lean and mean" - that is, it has a \textit{simple} Macintosh-type user interface and very small memory and disk space requirements. It will work quite well on Macintosh models as old as the Macintosh II and will even run on older monochrome models (with some cramping of screen space). It does not even require a math co-processor.

\textbf{What SPECTRUM does not do}: this program does \textit{not} have a \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#ipeak}{peak detector}, \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html}{multiple linear regression}, or an iterative \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Matlab}{non-linear curve fitter}.

\subsection{Worksheets for Analytical Calibration Curves\label{ref-0487}\label{ref-0488}}

These are fill-in-the-blanks spreadsheet templates for performing the calibration curve fitting and concentration calculations for analytical methods using the calibration curve method (page \pageref{ref-0401}). All you must do is to type in (or paste in) the concentrations of the standard solutions and their instrument readings (e.g., absorbances, or whatever method you are using) and the instrument readings of the unknowns. The spreadsheet automatically plots and fits the data to a straight line, quadratic or cubic curve (page \pageref{ref-0197}), then uses the equation of that curve to convert the readings of the unknown samples into concentration. You can add and delete calibration points at will, to correct errors or to remove outliers; the sheet re-plots and recalculates automatically. 

\subsection{Background\label{ref-0489}}

In analytical chemistry, the accurate quantitative measurement of the composition of samples, for example by various types of spectroscopy, usually requires that the method be \href{http://en.wikipedia.org/wiki/Calibration}{calibrated} using standard samples of known composition. This is most commonly, but not necessarily, done with solution samples and standards dissolved in a suitable solvent, because of the ease of preparing and diluting accurate and homogeneous mixtures of samples and standards in solution form. In the calibration curve method, a series of external standard solutions with different concentrations is prepared and measured. A line or curve is fit to the data and the resulting equation is used to convert readings of the unknown samples into concentration. The advantages of this method are that (a) the random errors in preparing and reading the standard solutions are averaged over several standards, and (b) non-linearity in the calibration curve can be detected and can be avoided (by diluting into the linear range) or compensated (by using non-linear curve fitting methods). 

\subsection{Fill-in-the-blanks worksheets for several different calibration methods\label{ref-0490}\label{ref-0491}}

\InsImageInline{0.5}{l}{image381.png}\textbf{A first-order (straight line) fit} of measured signal \textbf{A} (y-axis) vs concentration \textbf{C} (x-axis). The model equation is \textbf{A} = \textit{slope} * \textbf{C} + \textit{intercept}. This is the most common and straightforward method, and it is the one to use if you \textit{know} that your instrument response is linear. This fit uses the equations described and listed on page \pageref{ref-0231}. You need a minimum of \textit{two} points on the calibration curve. The concentration of unknown samples is given by \textit{(}\textbf{A} \textit{- intercept) / slope} where \textbf{A} is the measured signal and \textit{slope} and \textit{intercept} from the first-order fit. If you would like to use this method of calibration for your own data, download in \href{https://terpconnect.umd.edu/~toh/models/CalibrationLinear2.xls}{Excel} or OpenOffice \href{https://terpconnect.umd.edu/~toh/models/CalibrationLinear2.ods}{Calc} format. View equations for \href{https://terpconnect.umd.edu/~toh/models/CurveFitting.html\#MathDetails}{linear} least-squares. 

\InsImageInline{0.5}{l}{image382.png}\textbf{Linear interpolation calibration}. In the linear interpolation method, sometimes called the bracket method, the spreadsheet performs a \href{http://en.wikipedia.org/wiki/Linear\_interpolation}{linear interpolation} between the two standards that are just above and just below each unknown sample, rather than doing a least-squares fit over then entire calibration set. The concentration of the sample Cx is calculated by C1s+(C2s-C1s)*(Sx-S1s)/ (S2s-S1s), where S1x and S2s are the signal readings given by the two standards that are just above and just below the unknown sample, C1s and C2s are the concentrations of those two standard solutions, and Sx is the signal given by the sample solution. This method may be useful if none of the least-squares methods can fit the entire calibration range adequately (for instance, if it contains two linear segments with different slopes). It works well enough if the standards are spaced closely enough so that the actual signal response does not deviate significantly from linearity between the standards. However, this method does not deal well with random scatter in the calibration data due to random noise, because it does not compute a ``best-fit'' through multiple calibration points as the least-squares methods do. Download a template in \href{https://terpconnect.umd.edu/~toh/models/CalibrationLinearBracket.xls}{Excel (.xls)} format. 

\InsImageInline{0.5}{l}{image383.png}\textbf{A quadratic fit} of measured signal \textbf{A} (y-axis) vs concentration \textbf{C} (x-axis). The model equation is \textbf{A} = \textit{a}\textbf{C}\textsuperscript{2} \textit{+ b}\textbf{C} \textit{+ c.} This method can compensate for non-linearity in the instrument response to concentration. This fit uses the equations described and listed on page \pageref{ref-0231}. You need a minimum of \textit{three} points on the calibration curve. The concentration of unknown samples is calculated by solving this equation for \textbf{C} using the classical "quadratic formula", namely \textbf{C} = (-\textit{b}+SQRT(\textit{b}\textsuperscript{2}-4*\textit{a}*(\textit{c}-\textbf{A})))/(2*\textit{a}), where \textbf{A} = measured signal, and \textit{a}, \textit{b}, and \textit{c} are the three coefficients from the quadratic fit. If you would like to use this method of calibration for your own data, download in \href{https://terpconnect.umd.edu/~toh/models/CalibrationQuadratic.xls}{Excel} or OpenOffice \href{https://terpconnect.umd.edu/~toh/models/CalibrationQuadratic.ods}{Calc} format. View equations for q\href{https://terpconnect.umd.edu/~toh/models/CalibrationQuadraticEquations.txt}{uadratic }least-squares. The alternative version \href{https://terpconnect.umd.edu/~toh/models/CalibrationQuadraticB.xlsx}{CalibrationQuadraticB.xlsx} computes the concentration standard deviation (column \textbf{L}) and percent relative standard deviation (column \textbf{M}) using the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Bootstrap}{bootstrap method}. You need at least 5 standards for the error calculation to work. If you get a "\#NUM!" or \#DIV/0" in the columns \textbf{L} or \textbf{M}, just press the \textbf{F9} key to re-calculate the spreadsheet. There is also a \textit{reversed} quadratic \href{https://terpconnect.umd.edu/~toh/models/CalibrationReverseQuadratic.xls}{template }and \href{https://terpconnect.umd.edu/~toh/models/CalibrationReverseQuadraticExample.xls}{example}, which is analogous to the reversed cubic (\#5 below).

\InsImageInline{0.5}{l}{image384.png}\textbf{Weighted fits}. A weighted curve fit applies more weight (emphasis) to some points than others, which is especially useful when the calibration curve spans a very large range of concentrations. There are weighted versions of the linear (\href{https://terpconnect.umd.edu/~toh/models/CalibrationLinearWeighted.xls}{CalibrationLinearWeighted.xls}) and quadratic (\href{https://terpconnect.umd.edu/~toh/models/CalibrationQuadraticWeighted.xls}{CalibrationQuadraticWeighted.xls}) templates. There is also a weighted version of the drift-corrected calibration template (\href{https://terpconnect.umd.edu/~toh/models/CalibrationDriftingQuadraticWeighted.xls}{CalibrationDriftingQuadraticWeighted.xls}); see \#7 below. A weight (usually between zero and 1) for each point must be entered in Column A. There are pre-calculated weights for 1/X, 1/X\textsuperscript{2}, 1/Y, and 1/Y\textsuperscript{2} weighting in columns \textbf{Z} to \textbf{AC} (in the linear template) or \textbf{AK} to \textbf{AN} (in the quadratic template); you can either Copy and Paste (numbers only) these into column \textbf{A}, or you can enter =\textbf{Z6} or =\textbf{AK6} into cell \textbf{A6}, then "drag copy down" that cell to the last data points in column \textbf{A}. (Alternatively, you can enter equations into column A that calculate weights in any way you wish). If you want to disregard (ignore) one or more data points, make their weights zero. To make the calibration \textit{unweighted}, simply make all the weights 1.0. 

\InsImageInline{0.5}{l}{CalibrationCubic.GIF.png}\textbf{A reversed cubic fit} of concentration \textbf{C} (y-axis) vs measured signal \textbf{A} (x-axis). The model equation is \textbf{C} = \textit{a}\textbf{A}\textsuperscript{3} \textit{+ b}\textbf{A}\textsuperscript{2} \textit{+ c}\textbf{A} \textit{+ d}. This method can compensate for more complex non-linearity than the quadratic fit. A "reversed fit" flips the usual order of axes, by fitting concentration as a function of measured signal. The aim is to avoid the need to \href{http://mathworld.wolfram.com/CubicFormula.html}{solve a cubic equation }when the calibration equation is solved for \textbf{C} and used to convert the measured signals of the unknowns into concentration. This coordinate transformation is a short-cut, commonly done in least-squares curve fitting, at least by non-statisticians, to avoid mathematical messiness when the fitting equation is solved for concentration and used to convert the instrument readings into concentration values. However, this reversed method is theoretically not optimum, as demonstrated for the quadratic case \href{https://terpconnect.umd.edu/~toh/models/Bracket.html\#ReversedAxis}{Monte-Carlo simulation} in the spreadsheet \href{https://terpconnect.umd.edu/~toh/models/NormalVsReversedQuadFit2.ods}{NormalVsReversedQuadFit2.ods} (\href{https://terpconnect.umd.edu/~toh/models/NormalVsReversedComparison.jpg}{Screenshot}), and should be used only if the experimental calibration curve is so non-linear that it cannot be fit by other simpler means. The reversed cubic fit is performed using the \href{http://wiki.services.openoffice.org/wiki/Documentation/How\_Tos/Calc:\_LINEST\_function}{LINEST} function on Sheet3. You need a minimum of \textit{four} points on the calibration curve. The concentration of unknown samples is calculated directly by \textit{a}\textbf{A}\textsuperscript{3}\textit{+b}\textbf{A}\textsuperscript{2}\textit{+c}*\textbf{A}\textit{+d}, where \textbf{A} is the measured signal, and \textit{a}, \textit{b}, \textit{c}, and \textit{d} are the four coefficients from the cubic fit. The math is shown and explained better in the template \href{https://terpconnect.umd.edu/~toh/models/CalibrationCubic5Points.xls}{CalibrationCubic5Points.xls} (\href{https://terpconnect.umd.edu/~toh/models/CalibrationCubic5Points.png}{screen image}), which is set up for a 5-point calibration, with sample data already entered. To expand this template to a greater number of calibration points, follow these steps exactly: select \textbf{row 9} (click on the "9" row label), right-click and select \textbf{Insert}, and repeat for each additional calibration point required. Then select \textbf{row 8} columns \textbf{D} through \textbf{K} and drag-copy them down to fill in the newly created rows. That will create all the required equations and will modify the LINEST function in O16-R20. There is also another template, \href{https://terpconnect.umd.edu/~toh/models/CalibrationCubic.xls}{CalibrationCubic.xls}, which uses some spreadsheet "tricks" to \textit{sense the number of calibration points automatically} that you enter and adjust the calculations accordingly; download in \href{https://terpconnect.umd.edu/~toh/models/CalibrationCubic.xls}{Excel} or OpenOffice \href{https://terpconnect.umd.edu/~toh/models/CalibrationCubic.ods}{Calc} format. 

\InsImageInline{0.5}{l}{image386.png}\textbf{Log-log Calibration}. In log-log calibration, the logarithm of the measured signal \textbf{A} (y-axis) is plotted against the logarithm of concentration \textbf{C} (x-axis) and the calibration data are fit to a linear or quadratic model, as in \#1 and \#2 above. The concentration of unknown samples is obtained by taking the logarithm of the instrument readings, computing the corresponding logarithms of the concentrations from the calibration equation, then taking the anti-log to obtain the concentration. (These additional steps do not introduce any additional error, because the log and anti-log conversions can be made quickly and without significant error by the computer). Log-log calibration is well suited for data with a very large range of values because it distributes the relative fitting error more evenly among the calibration points, preventing the larger calibration points to dominate and cause excessive errors in the low points. (In that sense, it is like a weighted fit). In some cases (e.g., \href{http://en.wikipedia.org/wiki/Power\_law}{Power Law relationships}) a nonlinear relationship between signal and concentration can be completely linearized by a log-log transformation. (Some official government laboratories operate under rules that \href{https://terpconnect.umd.edu/~toh/spectrum/RegulatedLabRules.txt}{do not allow the use of nonlinear least-squares fits to calibration data}, so the use of log-log transformation might help in such cases). \textit{However}, because of the use of logarithms, the data set can not \textit{contain any zero or negative values}. To use this method of calibration for your own data, download the templates for log-log linear (\href{https://terpconnect.umd.edu/~toh/models/LogLogCalibrationLinear.xls}{Excel} or \href{https://terpconnect.umd.edu/~toh/models/LogLogCalibrationLinear.ods}{Calc}) or log-log quadratic (\href{https://terpconnect.umd.edu/~toh/models/LogLogCalibrationQuadratic.xls}{Excel} or \href{https://terpconnect.umd.edu/~toh/models/LogLogCalibrationQuadratic.ods}{Calc}).\label{ref-0492}

\InsImageInline{0.5}{l}{DriftingCalibration.gif.png}\textbf{Drift-corrected calibration}. All the above methods assume that the calibration of the instrument is stable with time and that the calibration (usually performed before the samples are measured) remains valid while the unknown samples are measured. In some cases, however, instruments and sensors can \textit{drift}, that is, the \textit{slope} and/or \textit{intercept} of their calibration curves, and even their \textit{linearity}, can gradually change with time after the initial calibration. You can test for this drift by measuring the standards again \textit{after} the samples are run, to determine how different the second calibration curve is from the first. If the difference is not too large, it is reasonable to assume that the drift is approximately linear with time, that is, that the calibration curve parameters (intercept, slope, and curvature) have changed linearly as a function of time between the two calibration runs. It is then possible to correct for the drift if you record the \textit{time} when each calibration is run and when each unknown sample is measured. The drift-correction spreadsheet (CalibrationDriftingQuadratic) does the calculations: it computes a quadratic fit for the pre- and post-calibration curves, then it uses linear interpolation to estimate the calibration curve parameters for each separate sample based on the time it was measured. The method works perfectly only if the drift is linear with time (a reasonable assumption if the amount of drift is not too large), but in any case, it is \textit{better than simply assuming that there is no drift at all}. If you would like to use this method of calibration for your own data, download in \href{https://terpconnect.umd.edu/~toh/models/CalibrationDriftingQuadratic.xls}{Excel} or OpenOffice \href{https://terpconnect.umd.edu/~toh/models/CalibrationDriftingQuadratic.ods}{Calc} format. (See instructions, page \pageref{ref-0495}.) 

\textbf{Error calculations}. In many cases, it is important to calculate the likely error in the computed concentration values (column \textbf{K}) caused by imperfect calibration. This is discussed on page \pageref{ref-0204}, "\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Reliability}{Reliability of curve fitting results}". The linear calibration spreadsheet (download in \href{https://terpconnect.umd.edu/~toh/models/CalibrationLinear.xls}{Excel} or OpenOffice \href{https://terpconnect.umd.edu/~toh/models/CalibrationLinear.ods}{Calc} format) performs a classical algebraic error-propagation calculation (page \pageref{ref-0207}) on the equation that calculates the concentration from the unknown signal and the slope and intercept of the calibration curve. The quadratic calibration spreadsheet (Download in \href{https://terpconnect.umd.edu/~toh/models/CalibrationQuadraticB.xlsx}{Excel} or OpenOffice \href{https://terpconnect.umd.edu/~toh/models/CalibrationQuadraticB.ods}{Calc} format) performs a \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Bootstrap}{bootstrap }calculation (page \pageref{ref-0214}). You must have a least 5 calibration points for these error calculations to be even minimally reliable; the more the better. That is because these methods need a representative sample of deviations from the ideal calibration line. If the calibration line fits the points exactly, then the computed error will be zero.

\subsection{Comparison of calibration methods\label{ref-0493}}

\InsImageInline{0.2}{l}{image388.png}To compare these various different methods of calibration, I will take one set of real data and subject it to five different calibration curve-fitting methods. The data set, shown on the left, has 10 data points covering a wide (1000-fold) range of concentrations. Over that range, the instrument readings are not linearly proportional to concentration. These data are used to construct a calibration curve, which is then fit using five different models, using the spreadsheet templates described above, and then the equations of the fits, solved for concentration, are used to calculate the concentration of each standard according to that calibration equation. For each method, I compute the relative percent difference between the actual concentration of each standard and the concentrations calculated from the calibration curves. Then I calculated the average of those errors for each method. The objective of this exercise is to determine which method gives the lowest average error for all 10 standards \textit{in this data set}. 

\InsImageInline{0.3}{l}{image389.png}The five methods used are (1) \href{https://terpconnect.umd.edu/~toh/models/ExampleLinearUnweighted.xls}{linear unweighted}; (2) \href{https://terpconnect.umd.edu/~toh/models/ExampleLinearWeighted.xls}{linear weighted}; (1/x weighting); (3) \href{https://terpconnect.umd.edu/~toh/models/ExampleQuadraticUnweighted.xls}{quadratic unweighted}; (4) \href{https://terpconnect.umd.edu/~toh/models/ExampleQuadraticWeighted.xls}{quadratic weighted} (1/x weighting); and (5) \href{https://terpconnect.umd.edu/~toh/models/ExampleLogLogLinear.xls}{log-log linear}. In the PDF version of this book, each of these is hot-linked to the corresponding spreadsheet. \href{https://terpconnect.umd.edu/~toh/models/ComparisonOfCalibrations.xlsx}{ComparisonOfCalibrations.xlsx} summarizes the results. For this data set, the best method is the 1/x weighted quadratic, but that does not mean that this method will be the best in every situation. These calibration data are non-linear \textit{and} they cover a very wide range of x-values (concentrations), which is a challenge for most calibration methods. 

\subsection{Instructions for using the calibration templates\label{ref-0494}}

1. Download and open the desired calibration worksheet from among those \href{https://terpconnect.umd.edu/~toh/models/CalibrationCurve.html\#worksheets}{listed above} (page \pageref{ref-0490}).

2. Enter the concentrations of the standards and their instrument readings (e.g., absorbance) into the blue table on the left. Leave the rest of the table blank. You must have at least two points on the calibration curve (three points for the quadratic method or four points for the cubic method), including the blank (zero concentration standard). If you have multiple instrument readings for one standard, it is better to enter each as a separate standard with the same concentration, rather than entering the average. The spreadsheet automatically gives more weight to standards that have more than one reading. 

3. Enter the instrument readings (e.g., absorbance) of the unknowns into the yellow table on the right. You can have any number of unknowns up to 20. (If you have multiple instrument readings for one unknown, it is better to enter each as a separate unknown, rather than averaging them, so you can see how much variation in calculated concentration is produced by the variation in instrument reading).

4. The concentrations of the unknowns are automatically calculated and displayed column K. If you edit the calibration curve, by deleting, changing, or adding more calibration standards, the concentrations are automatically recalculated. 

For the linear fit (CalibrationLinear.xls), if you have three or more calibration points, the estimated standard deviation of the slope and intercept will be calculated and displayed in cells \textbf{G36} and \textbf{G37}, and the resulting standard deviation (SD) of each concentration will be displayed in rows \textbf{L} (absolute SD) and \textbf{M} (percent relative SD). These standard deviation calculations are estimates of the variability of slopes and intercepts you are likely to get if you repeated the calibration over and over multiple times under the same conditions, assuming that the deviations from the straight line are due to \textit{random variability} and not a systematic error caused by non-linearity. If the deviations are random, they will be slightly different from time to time, causing the slope and intercept to vary from measurement to measurement. However, if the deviations are caused by systematic non-linearity, they will be the same from measurement to measurement, in which case these predictions of standard deviation will not be relevant, and you would be better off using a polynomial fit such as a quadratic or cubic. The reliability of these standard deviation estimates also depends on the number of data points in the curve fit; they improve with the square root of the number of points.

5. You can remove any point from the curve fit by deleting the corresponding X and Y values in the table. To delete a value; right-click on the cell and click "Delete Contents" or "Clear Contents". The spreadsheet automatically re-calculates and the graph re-draws; if it does not, press F9 to recalculate. (Note: the cubic calibration spreadsheet must have contiguous calibration points with no blank or empty cells in the calibration range). 

6. The linear calibration spreadsheet also calculates the coefficient of determination, R\textsuperscript{2}, which is an indicator of the "goodness of fit", in cell \textbf{C37}. R\textsuperscript{2} is 1.0000 when the fit is perfect but less than that when the fit is imperfect. The closer to 1.0000 the better.

7. A "residuals plot" is displayed just below the calibration graph (except for the interpolation method). This shows the difference between the best-fit calibration curve and the actual readings of the standards. The smaller these errors, the more closely the curve fits the calibration standards. (The standard deviation of those errors is also calculated and displayed below the residuals plot; the lower this standard deviation, the better). 

You can tell a lot by looking at the shape of the residual plot: if the points are scattered randomly above and below zero, it means that the curve fit is \textit{as good as it can be,} given the random noise in the data. But if the residual plot has a smooth shape, say, a U-shaped curve, then it means that there is a mismatch between the curve fit and the actual shape of the calibration curve; suggesting that another curve fitting technique might be tried (say, a quadratic or cubic fit rather than a linear one) or that the experimental conditions be modified to produce a less complex experimental calibration curve shape.

8. \textbf{Drift-corrected calibration.} If you are using the spreadsheet for \textit{drift-corrected calibration}, you must measure \textit{two} calibration curves, one \textit{before} and one \textit{after} you run the samples, and you must record the date and time each calibration curve is measured. Enter the concentrations of the standards into column \textbf{B}. Enter the instrument readings for the first (pre-) calibration into column \textbf{C} and the date/time of that calibration into cell \textbf{C5}; enter the instrument readings for the post-calibration into column \textbf{D} and the date/time of that calibration into cell \textbf{D5}. The format for the date/time entry is \textbf{Month-Day-Year Hours:Minutes:Seconds}, for example, 6-2-2011 13:30:00 for June 2, 2011, 1:30 PM (13:30 on the 24-hour clock). Note: if you run both calibrations on the same day, you can leave off the date and just enter the time. In the graph, the pre-calibration curve is in \textbf{\textcolor{color-20}{green}} and the post-calibration curve is in \textbf{\textcolor{color-12}{red}}\textbf{.} Then, for each unknown sample measured, enter the date/time (in the same format) into column \textbf{K} and the instrument reading for that unknown into column \textbf{L}. The spreadsheet computes the drift-corrected sample concentrations in column \textbf{M}. Note: Version 2.1 of this spreadsheet (July 2011) allows different sets of concentrations for the pre- and post-calibrations. Just list all the concentrations used in the "Concentration of standards" column (\textbf{B}) and put the corresponding instrument readings in columns \textbf{C} \textit{or} \textbf{D}, or \textit{both}. If you do not use a particular concentration for one of the calibrations, just leave that instrument reading blank. \label{ref-0495}


\begin{center}
\InsImageInline{0.5}{l}{DriftingCalibration.gif.png}
\end{center}


This figure shows an application of the \href{https://terpconnect.umd.edu/~toh/models/ReversedQuadraticVsCubic.ods}{drift-corrected quadratic} calibration spreadsheet to a remote sensing experiment. In this demonstration, the calibrations and measurements were made over a period of several days. The pre-calibration (column \textbf{C}) was performed with six standards (column \textbf{B}) on 01/25/2011 at 1:00 PM. Eight unknown samples were measured over the following five days (columns \textbf{L} and \textbf{M}), and the post-calibration (column \textbf{D}) was performed after then last measurement on 01/30/2011 at 2:45 PM. The graph in the center shows the pre-calibration curve in green and the post-calibration curve in red. As you can see, the sensor (or the instrument) had drifted over that time period, the sensitivity (slope of the calibration curve) becoming 28\% smaller and the curvature becoming noticeably more non-linear (concave down). This may have been caused in this case by the accumulation of dirt and algal growth on the sensor over time. Whatever the cause, both the pre- and post-calibration curves fit the quadratic calibration equations very well, as indicated by the residuals plot and the ``3 nines'' coefficients of determination (R\textsuperscript{2}) listed below the graphs. The eight "unknown" samples that were measured for this test (yellow table) were the same sample measured repeatedly - a standard of concentration 1.00 units - but you can see that the sample gave lower instrument readings (column \textbf{L}) each time it was measured (column \textbf{K}), due to the drift. Finally, the drift-corrected concentrations calculated by the spreadsheet (column \textbf{M} on the right) are all very close to 1.00, with a standard deviation of 0.6\%, showing that the drift correction works well, within the limits of the random noise in the instrument readings and subject to the assumption that the drift in the calibration curve parameters is linear with time between the pre- and post-calibrations. 

\subsection{Frequently Asked Questions (taken from emails and search engine queries)\label{ref-0496}}

\textbf{1. Question:} \textit{What is the purpose of the calibration curve?}

\textbf{Answer}: Most analytical instruments generate an electrical output signal such as a current or a voltage. A calibration curve establishes the relationship between the signal generated by a measurement instrument and the concentration of the substance being measured. Different chemical compounds and elements give different signals. When an unknown sample is measured, the signal from the unknown is converted into concentration using the calibration curve.

\textbf{2}. \textbf{Question:} \textit{How do you make a calibration curve?}

\textbf{Answer}: You prepare a series of "standard solutions" of the substance that you intend to measure, measure the signal (e.g., absorbance, if you are doing absorption spectrophotometry), and plot the concentration on the x-axis and the measured signal for each standard on the y-axis. Draw a straight line as close as possible to the points on the calibration curve (or a smooth curve if a straight line will not fit), so that as many points as possible are right on or close to the curve. 

\textbf{3}. \textbf{Question:} \textit{How do you use a calibration curve to predict the concentration of an unknown sample? How do you determine concentration from a non-linear calibration plot?}

\textbf{Answer}: You can do that in two ways, graphically and mathematically. Graphically, draw a horizontal line from the signal of the unknown on the \textit{y} axis over to the calibration curve and then straight down to the concentration (\textit{x}) axis to the concentration of the unknown. Mathematically, fit an equation to the calibration data, and solve the equation for concentration as a function of signal. Then, for each unknown, just plug its signal into this equation and calculate the concentration. For example, for a linear equation, the curve fit equation is \textbf{Signal} = \textit{slope} * \textbf{Concentration} + \textit{intercept}, where \textit{slope} and \textit{intercept} are determined by a linear (first-order) \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html}{least-squares curve fit} to the calibration data. Solving this equation for \textbf{Concentration} yields \textbf{Concentration} = (\textbf{Signal} \textit{- intercept) / slope}, where \textbf{Signal} is the signal reading (e.g., absorbance) of the unknown solution. (\href{http://terpconnect.umd.edu/~toh/models/CalibrationLinear.odt}{Click here} for a fill-in-the-blank OpenOffice spreadsheet that does this for you. \href{http://terpconnect.umd.edu/~toh/models/CalibrationLinear.GIF}{View screen shot}).

\textbf{4}. \textbf{Question:} \textit{How do I know when to use a straight-line curve fit and when to use a curved line fit like a quadratic or cubic?} 

\textbf{Answer}: Fit a straight line to the calibration data and look at a plot of the "residuals" (the differences between the \textit{y} values in the original data and the \textit{y} values computed by the fit equation). Deviations from linearity will be much more evident in the residuals plot than in the calibration curve plot. (\href{http://terpconnect.umd.edu/~toh/models/CalibrationLinear.odt}{Click here} for a fill-in-the-blank OpenOffice spreadsheet that does this for you. \href{http://terpconnect.umd.edu/~toh/models/CalibrationLinear.GIF}{View screen shot}). If the residuals are randomly scattered all along the best-fit line, then it means that the deviations are caused by random errors such as instrument noise or by random volumetric or procedural errors; in that case you can use a straight line (linear) fit. If the residuals have a smooth shape, like a "U" shape, this means that the calibration curve is curved, and you should use a non-linear curve fit, such as a \href{https://terpconnect.umd.edu/~toh/models/CalibrationCurve.html}{quadratic or cubic fit}. If the residual plot has a "S" shape, you should probably use a cubic fit. (If you are doing absorption spectrophotometry, see \href{https://terpconnect.umd.edu/~toh/models/BeersLawCurveFit.html}{Comparison of Curve Fitting Methods in Absorption Spectroscopy}).

\textbf{5}. \textbf{Question:} \textit{What if my calibration curve is linear at low concentrations but curves off at the highest concentrations?}

\textbf{Answer}: You can't use a linear curve fit in that case, but if the curvature is not too severe, you might be able to get a good fit with a \href{https://terpconnect.umd.edu/~toh/models/CalibrationCurve.html}{quadratic or cubic fit}. If not, you could break the concentration range into two regions and fit a linear curve to the lower linear region and a quadratic or cubic curve to the higher non-linear region.

\textbf{6}. \textbf{Question:} \textit{What is the difference between a calibration curve and a line of best fit? What is the difference between a linear fit and a calibration curve?}

\textbf{Answer}: The calibration curve is an experimentally measured relationship between concentration and signal. You do not ever really know the \textit{true} calibration curve; you can only \textit{estimate} it at a few points by measuring a series of standard solutions. Then draw a line or a smooth curve that goes as much as possible through the points, with some points being a little higher than the line and some points a little lower than the line. That is what we mean by that is a "best fit" to the data points. The actual calibration curve might not be perfectly linear, so a linear fit is not always the best. A quadratic or cubic fit might be better if the calibration curve shows a gradual smooth curvature.

\textbf{7}. \textbf{Question:} \textit{Why the slope line does not go through all points on a graph?}

\textbf{Answer}: That will only happen if you (1) are a perfect experimenter, (2) have a perfect instrument, and (3) choose the perfect curve-fit equation for your data. That is not going to happen. There are \textit{always} little errors. The least-squares curve-fitting method yields a \textit{best} fit, not a \textit{perfect} fit, to the calibration data for a given curve shape (linear, quadratic, or cubic). Points that fall off the curve are assumed to do so because of random errors or because the actual calibration curve shape does not match the curve-fit equation. 

There is one artificial way you can make the curve go through all the points, and that is to use \textit{too few calibration standards}: for example, if you use only \textit{two} points for a straight-line fit, then the best-fit line will go right through those two points \textit{no matter what}. Similarly, if you use only \textit{three} points for a quadratic fit, then the quadratic best-fit curve will go right through those three points, and if you use only \textit{four} points for a cubic fit, then the cubic best-fit curve will go right through those four points. But that is not really recommended, because if one of your calibration points is off by a huge error, the curve fit \textit{will} \textit{still look perfect}, and you will have \textit{no clue} that something is wrong. \textit{You really must use more standards so that you will know when something has gone wrong.}

\textbf{8}. \textbf{Question:} \textit{What happens when the absorbance reading is higher than any of the standard solutions?}

\textbf{Answer}: If you are using a curve-fit equation, you will still get a value of concentration calculated for \textit{any} signal reading you put in, even above the highest standard. However, it is risky to do that, because you really do not know for sure what the shape of the calibration curve is above the highest standard. It could continue straight, or it could curve off in some unexpected way - how would you know for sure? It is best to add another standard at the high end of the calibration curve.

\textbf{9}. \textbf{Question:} \textit{What's the difference between using a single standard and multiple standards?}

\textbf{Answer}: The single standard method is the simplest and quickest method, but it is accurate only if the calibration curve is known to be linear. Using multiple standards has the advantage that any non-linearity in the calibration curve can be detected and avoided (by diluting into the linear range) or compensated (by using non-linear curve fitting methods). Also, the random errors in preparing and reading the standard solutions are averaged over several standards, which is better than "putting all your eggs in one basket" with a single standard. On the other hand, an obvious disadvantage of the multiple standard method is that it requires much more time and uses more standard material than the single standard method.

\textbf{10}. \textbf{Question:} \textit{What's the relationship between sensitivity in analysis and the slope of standard curve}?

\textbf{Answer}: Sensitivity is \href{http://goldbook.iupac.org/S05606.html}{defined} as the slope of the standard (calibration) curve.

\textbf{11}. \textbf{Question:} \textit{How do you make a calibration curve in Excel or in OpenOffice?}

\textbf{Answer}: Put the concentration of the standards in one column and their signals (e.g., absorbances) in another column. Then make an XY \href{http://www.ncsu.edu/labwrite/res/gt/graphtut-home.html}{scatter graph}, putting concentration on the X (horizontal) axis and signal on the Y (vertical) axis. Plot the data points with symbols only, not lines between the points. To compute a least-squares curve fit, you can either put in the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#MathDetails}{least-squares equations} into your spreadsheet, or you can use the built-in LINEST function in both \href{http://www.google.com/url?sa=t&ct=res&cd=1&url=http\%3A\%2F\%2Fwww.colby.edu\%2Fchemistry\%2FPChem\%2Fnotes\%2Flinest.pdf&ei=DTl\_SMnUDIye8gTV-vjTCw&usg=AFQjCNGNAoinemLK1XD9VxkN0SlQvlseFg&sig2=cxJZu6DsHOSjcBWjaSbsiw}{Excel} and \href{http://wiki.services.openoffice.org/wiki/Documentation/How\_Tos/Calc:\_LINEST\_function}{OpenOffice Calc} to compute polynomial and other curvilinear least-squares fits. For examples of OpenOffice spreadsheets that graphs and fits calibration curves, see \href{https://terpconnect.umd.edu/~toh/models/CalibrationCurve.html}{Worksheets for Analytical Calibration Curves}.

\textbf{12}. \textbf{Question:} \textit{What's the difference in using a calibration curve in absorption spectrometry vs other analytical methods such a fluorescence or emission spectroscopy?}

\textbf{Answer}: The only difference is the units of the signal. In absorption spectroscopy you use \textit{absorbance} (because it is the most nearly linear with concentration) and in fluorescence (or emission) spectroscopy you use the \textit{fluorescence (or emission) intensity}, which is usually linear with concentration (except sometimes at high concentrations). The methods of curve fitting and calculating the concentration are basically the same.

\textbf{13}. \textbf{Question:} \textit{If the solution obeys Beer's Law, is it better to use a calibration curve rather than a single standard?}

\textbf{Answer}: It might not make much difference either way. If the solution is known from previous measurements to obey Beer's Law exactly on the same spectrophotometer and under the conditions in use, then a single standard can be used (although it is best if that standard gives a signal close to the maximum expected sample signal or to whatever signal gives the best signal-to-noise ratio - an absorbance near 1.0 in absorption spectroscopy). The only real advantage of multiple standards in this case is that the random errors in preparing and reading the standard solutions are averaged over several standards, but the same effect can be achieved more simply by making up multiple copies of the same single standard (to average out the random volumetric errors) and reading each separately (to average out the random signal reading errors). And if the signal reading errors are much smaller than the volumetric errors, then a \textit{single} standard solution can be measured repeatedly to average out the random measurement errors.

\textbf{14}. \textbf{Question:} \textit{What is the effect on concentration measurement if the monochromator is not perfect?}

\textbf{Answer}: If the wavelength calibration if off a little bit, it will have no significant effect if the monochromator setting is left untouched between measurement of standards and unknown sample; the slope of the calibration curve will be different, but the calculated concentrations will be OK. (But if anything changes the wavelength between the time you measure the standards and the time you measure the samples, an error will result). If the wavelength has a poor stray light rating or if the resolution is poor (spectral bandpass is too big), the calibration curve may be affected adversely. In absorption spectroscopy, stray light and poor resolution may result in non-linearity, which requires a non-linear curve fitting method. In emission spectroscopy, stray light and poor resolution may result in a spectral interference which can result in significant analytical errors.

\textbf{15}. \textbf{Question:} \textit{What does it mean if the intercept of my calibration curve fit is not zero?}

\textbf{Answer}: Ideally, the y-axis intercept of the calibration curve (the signal at zero concentration) should be zero, but there are several reasons why this might not be so. (1) If there is substantial random scatter in the calibration points above and below the best-fit line, then it is likely that the non-zero intercept is just due to random error. If you prepared another separate set of standards, that standard curve would have different intercept, either positive or negative. There is nothing that you can do about this, unless you can reduce the random error of the standards and samples. (2) If the shape of the calibration curve does not match the shape of the curve fit, then it is very likely that you will get a non-zero intercept every time. For example, if the calibration curve bends down as concentration increases, and you use a straight-line (linear) curve fit, the intercept will be positive (that is, the curve fit line will have a positive y-axis intercept, even if the actual calibration curve goes through zero). This is an artifact of the poor curve fit selection; if you see that happen, try a different curve shape (quadratic or cubic). (3) If the instrument is not "zeroed" correctly, in other words, if the instrument gives a non-zero reading when the \href{http://en.wikipedia.org/wiki/Blank\_\%28solution\%29}{blank solution} is measured. In that case you have three choices: you can zero the instrument (if that is possible); you can subtract the blank signal from all the standard and sample readings; or you can just let the curve fit subtract the intercept for you (if your curve fit procedure calculates the intercept and you keep it in the solution to that equation, e.g., Concentration \textit{=} (Signal \textit{- intercept}) \textit{/ slope}). 

\textbf{16}. \textbf{Question:} \textit{How can I reduce the random scatter of calibration points above and below the best-fit line?}

\textbf{Answer}: Random errors like this could be due either to random volumetric errors (small errors in volumes used to prepare the standard solution by diluting from the stack solution or in adding reagents) or they may be due to random signal reading errors of the instrument, or to both. To reduce the volumetric error, use more precise volumetric equipment and practice your technique to perfect it (for example, use your technique to deliver pure water and weigh it on a precise analytical balance). To reduce the signal reading error, adjust the instrument conditions (e.g., wavelength, path length, slit width, etc.) for best signal-to-noise ratio and average several readings of each sample or standard. 

\textbf{17}. \textbf{Question:} \textit{What are interferences? What effect do interferences have on the calibration curve and on the accuracy of concentration measurement?}

\textbf{Answer}: When an analytical method is applied to complex real-world samples, for example the determination of drugs in blood serum, measurement error can occur due to \textit{interferences}. Interferences are measurement errors caused by chemical components in the samples that influence the measured signal, for example by contributing their own signals or by reducing or increasing the signal from the analyte. Even if the method is well calibrated and is capable of measuring solutions of pure analyte accurately, interference errors may occur when the method is applied to complex real-world samples. One way to correct for interferences is to use "matched-matrix standards", standard solution that are prepared to contain \textit{everything that the real samples contain}, except that they have known concentrations of analyte. But this is very difficult and expensive to do exactly, so every effort is made to reduce or compensate for interferences in other ways. For more information on types of interferences and methods to compensate for them, see \href{https://terpconnect.umd.edu/~toh/models/Bracket.html}{Comparison of Analytical Calibration Methods}.

\textbf{18}. \textbf{Question:} \textit{What are the sources of error in preparing a calibration curve?}

\textbf{Answer}: A calibration curve is a plot of analytical signal (e.g., absorbance, in absorption spectrophotometry) vs concentration of the standard solutions. Therefore, the main sources of error are the errors in the standard concentrations and the errors in their measured signals. Concentration errors depend mainly on the accuracy of the volumetric glassware (volumetric flasks, pipettes, solution delivery devices) and on the precision of their use by the persons preparing the solutions. In general, the accuracy and precision of handling large volumes above 10 mL is greater than that at lower volumes below 1 mL. Volumetric glassware can be calibrated by weighing water on a precise analytical balance (you can look up the density of water at various temperatures and thus calculate the exact volume of water from its measured weight); this would allow you to label each of the flasks, etc., with their actual volume. But precision may still be a problem, especially at lower volumes, and it is very much operator-dependent. It takes practice to get good at handling small volumes. Signal measurement error depends hugely on the instrumental method used and on the concentration of the analyte; it can vary from near 0.1\% under ideal conditions to 30\% near the detection limit of the method. Averaging repeat measurements can improve the precision with respect to random noise. To improve the signal-to-noise ratio at low concentrations, you may consider modifying the conditions, such as changing the slit width or the path length or using another instrumental method (such as a graphite furnace atomizer rather than flame atomic absorption).

\textbf{19}. \textbf{Question:} \textit{How can I find the error in a specific quantity using least square fitting method? How can I estimate the error in the calculated slope and intercept?}

\textbf{Answer}: When using a simple straight-line (first-order) least-squares fit, the best fit line is specified by only two quantities: the \textit{slope} and the \textit{intercept}. The \textit{random error} in the slope and intercept (specifically, their \href{http://en.wikipedia.org/wiki/Standard\_deviation}{\textit{standard deviation}}) can be estimated mathematically from the extent to which the calibration points deviate from the best-fit line. The equations for doing this are given \href{https://terpconnect.umd.edu/~toh/models/ErrorPropagation.pdf}{here} and are implemented in the \href{https://terpconnect.umd.edu/~toh/models/CalibrationLinear2.ods}{"spreadsheet for }\href{http://terpconnect.umd.edu/~toh/models/CalibrationLinear2.ods}{linear} calibration with error calculation". \textit{It is important to realize that these error computations are only estimates}, because they assume that the calibration data set is representative of all the calibration sets that would be obtained if you repeated the calibration a large number of times - in other words, the assumption is that the random errors (volumetric and signal measurement errors) in your particular data set are typical. If your random errors happen to be small when you run your calibration curve, you will get a deceptively \textit{good}-looking calibration curve, but your estimates of the random error in the slope and intercept will be too \textit{low}. If your random errors happen to be large, you will get a deceptively \textit{bad}-looking calibration curve, and your estimates of the random error in the slope and intercept will be too \textit{high}. These error estimates can be particularly poor when the number of points in a calibration curve is small; the accuracy of the estimates increases if the number of data points increases, but of course preparing many standard solutions is time consuming and expensive. The bottom line is that you can only expect these error predictions from a single calibration curve to be very rough; they could easily be off by a factor of two or more, as demonstrated by the simulation "Error propagation in the Linear Calibration Curve Method" \href{https://terpconnect.umd.edu/~toh/models/CalCurveOOError.ods}{(download OpenOffice version).} 

\textbf{20.} \textit{How can I estimate the error in the calculated concentrations of the unknowns?}

\textbf{Answer}: You can use the slope and intercept from the least-squares fit to calculate the concentration of an unknown solution by measuring its signal and computing (\textbf{Signal} \textit{- intercept}) \textit{/ slope}, where \textbf{Signal} is the signal reading (e.g., absorbance) of the unknown solution. The errors in this calculated concentration can then be estimated by the usual rules for the propagation of error: first, the error in (\textbf{Signal} \textit{- intercept}) is computed by the rule for addition and subtraction; second, the error in (\textbf{Signal} \textit{- intercept}) \textit{/ slope} is computed by the rule for multiplication and division. The equations for doing this are given \href{https://terpconnect.umd.edu/~toh/models/ErrorPropagation.pdf}{here} and are implemented in the "\href{https://terpconnect.umd.edu/~toh/models/CalibrationLinear2.ods}{spreadsheet for }\href{http://terpconnect.umd.edu/~toh/models/CalibrationLinear2.ods}{linear} calibration with error calculation". \textit{It is important to realize that these error computations are only estimates}, for the reason given in Question \#19 above, especially if the number of points in a calibration curve is small, as demonstrated by the simulation "Error propagation in the Linear Calibration Curve Method" \href{https://terpconnect.umd.edu/~toh/models/CalCurveOOError.ods}{(download OpenOffice version).} 

\textbf{21.} \textit{What is the minimum} \textit{acceptable value of the coefficient of determination (R}\textsuperscript{\textit{2}}\textit{)?}

\textbf{Answer}: It depends on the accuracy required. As a rough rule of thumb, if you need an accuracy of about 0.5\%, you need an R\textsuperscript{2} of 0.9998; if a 1\% error is good enough, an R\textsuperscript{2} of 0.997 will do; and if a 5\% error is acceptable, an R\textsuperscript{2} of 0.97 will do. The bottom line is that the R\textsuperscript{2 }must be very close to 1.0 for quantitative results in analytical chemistry.

\label{ref-0497}\label{ref-0498}

\label{ref-0499}

\chapter{Catalog of signal processing functions, scripts, and spreadsheet templates\label{ref-0500}}

This is a complete list of all the functions, scripts, data files, and spreadsheets used in this book, collected according to topic, with brief descriptions. If you are reading this book online, on an Internet connected computer, you can \textbf{Ctrl-Click} on any of links and select "Save link as..." to download them to your computer. There are approximately 200 Matlab/Octave m-files (functions and demonstration scripts); place these into the Matlab or Octave "path" so you can use them just like any other built-in feature. (\href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#ScriptsVsFunctions}{Difference between scripts and functions}). To display the built-in help for these functions and script, type "help {\textless}name{\textgreater}" at the command prompt (where "{\textless}name{\textgreater}" is the name of the script or function). 

Many of the figures in this book are screen images of my software in action. Often the screen shots display a version number that is older than the current version, betraying the date when they were first made; there is almost certainly a newer version that includes addition functions. If you are unsure whether you have all the latest versions, the simplest way to update all my functions, scripts, tools, spreadsheets and documentation files is to download the latest \href{https://terpconnect.umd.edu/~toh/spectrum/SPECTRUM.zip}{site archive ZIP file} (approx. 400 MBytes), then right-click on the zip file and click "Extract all". Then list the contents of the extracted folder \textit{by} \textit{date} and then drag and drop any \textit{new or newly updated files} into a folder in your Matlab/Octave path. The ZIP files contains \textit{all} the files used by this web site \textit{in one directory}, so you can search for them by file name or sort them by date to determine which ones that have changed since the last time you downloaded them. 

\textit{If you try to run one of my scripts or functions and it gives you a "missing function" error, look for the missing item here, download it into your path, and try again.} The script \href{https://terpconnect.umd.edu/~toh/testallfunctions.m}{testallfunctions.m} is intended to test for the existence of all/most of the functions in this collection. If it comes to a function that is not installed on your system, or if one of them does not run, it will stop with an error, alerting you of the problem. It takes about 5 minutes to run in Matlab on a contemporary PC (slower in Octave).

Some of these functions have been requested by users, suggested by Google search terms, or corrected and expanded based on extensive user feedback; you could almost consider this an international "crowd-sourced" software project. \textit{I wish to express my thanks and appreciation for all those who have made useful suggestions, corrected errors, and especially those who have sent me data from their work to test my programs on. These contributions have really helped to correct bugs and to expand the capabilities of my programs.}

\subsection{Peak shape functions (for Matlab and \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html}{Octave}) \label{ref-0501}\label{ref-0502}}


Most of these shape functions take \textit{three} required input arguments: the independent variable ("x") vector, the peak position, "pos", and the peak width, "wid", usually the full width at half maximum. The functions marked '\textit{variable shape}' require an additional fourth input argument that determines the exact peak shape. The sigmoidal and exponential shapes (alpha function, exponential pulse, up-sigmoid, down-sigmoid, Gompertz, FourPL, and OneMinusExp) have different variables names.

\href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{Gaussian} \texttt{y = gaussian(x,pos,wid)}\href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{ }

\href{https://terpconnect.umd.edu/~toh/spectrum/expgaussian.m}{exponentially-broadened Gaussian} (variable shape) 

\href{https://terpconnect.umd.edu/~toh/spectrum/GaussTriangle.m}{Triangle-broadened Gaussian} (variable shape)

\href{https://terpconnect.umd.edu/~toh/spectrum/BiGaussian.m}{bifurcated Gaussian} (variable shape)

\href{https://terpconnect.umd.edu/~toh/spectrum/flattenedgaussian.m}{Flattened Gaussian} (variable shape)

\href{https://terpconnect.umd.edu/~toh/spectrum/clippedgaussian.m}{Clipped Gaussian} (variable shape)

\href{https://terpconnect.umd.edu/~toh/spectrum/lorentzian.m}{Lorentzian} (aka 'Cauchy') \texttt{y = lorentzian(x,pos,wid)}

\href{https://terpconnect.umd.edu/~toh/spectrum/explorentzian.m}{exponentially-broadened Lorentzian} (variable shape)

\href{https://terpconnect.umd.edu/~toh/spectrum/clippedlorentzian.m}{Clipped Lorentzian} (variable shape)

\href{https://terpconnect.umd.edu/~toh/spectrum/GL.m}{Gaussian/Lorentzian blend} (variable shape)

\href{https://terpconnect.umd.edu/~toh/spectrum/voigt.m}{Voigt }profile (variable shape)

\href{https://terpconnect.umd.edu/~toh/spectrum/lognormal.m}{lognormal}

\href{https://terpconnect.umd.edu/~toh/spectrum/logistic.m}{logistic }\href{https://terpconnect.umd.edu/~toh/spectrum/logistic.m}{\textit{distribution}} (for logistic \textit{function}, see \href{https://terpconnect.umd.edu/~toh/spectrum/upsigmoid.m}{up-sigmoid})

\href{https://terpconnect.umd.edu/~toh/spectrum/pearson.m}{Pearson 5} (variable shape)

\href{https://terpconnect.umd.edu/~toh/spectrum/alphafunction.m}{alpha function}

\href{https://terpconnect.umd.edu/~toh/spectrum/exppulse.m}{exponential pulse}

\href{https://terpconnect.umd.edu/~toh/spectrum/plateau.m}{plateau }(variable shape, symmetrical product of sigmoid and down sigmoid, similar to \href{https://terpconnect.umd.edu/~toh/spectrum/flattenedgaussian.m}{Flattened Gaussian})

\href{https://terpconnect.umd.edu/~toh/spectrum/BWF.m}{Breit-Wigner-Fano resonance (BWF)} (variable shape)

\href{https://terpconnect.umd.edu/~toh/spectrum/triangle.m}{triangle}

\href{https://terpconnect.umd.edu/~toh/spectrum/exptriangle.m}{exponentially-broadened triangle} (variable shape)\href{https://terpconnect.umd.edu/~toh/spectrum/GT.m}{}

\href{https://terpconnect.umd.edu/~toh/spectrum/GT.m}{Gaussian/Triangle blend} (variable shape)

\href{https://terpconnect.umd.edu/~toh/spectrum/rectanglepulse}{rectanglepulse}

\href{https://terpconnect.umd.edu/~toh/spectrum/tsallis.m}{tsallis distribution} (variable shape, similar to Pearson 5)

\href{https://terpconnect.umd.edu/~toh/spectrum/upsigmoid.m}{up-sigmoid} (logistic \textit{function} or "S-shaped"). Simple upward going sigmoid. 

\href{https://terpconnect.umd.edu/~toh/spectrum/downsigmoid.m}{down-sigmoid} ("Z-shaped") Simple downward going sigmoid.

\href{https://terpconnect.umd.edu/~toh/spectrum/gompertz.m}{Gompertz, }3-parameter logistic, a variable-shape sigmoidal: 

\texttt{y=Bo*exp(-exp((Kh*exp(1)/Bo)*(L-t) +1))}

\href{https://terpconnect.umd.edu/~toh/spectrum/FourPL.m}{FourPL}, 4-parameter logistic, \texttt{y = maxy+(miny-maxy)./(1+(x./ip).\textasciicircum{}slope)}

\href{https://terpconnect.umd.edu/~toh/spectrum/OneMinusExp.m}{OneMinusExp}, Asymptotic rise to flat plateau: \texttt{g = 1-exp(-wid.*(x-pos))}

\href{https://terpconnect.umd.edu/~toh/spectrum/peakfunction.m}{peakfunction.m}, a function that generates many different peak types specified by number.

\href{https://terpconnect.umd.edu/~toh/spectrum/modelpeaks.m}{modelpeaks}, a function that simulates multi-peak time-series signal data consisting of any number of peaks of the same shape. Syntax is model= modelpeaks(x, NumPeaks, peakshape, Heights, Positions, Widths, extra), where 'x' is the independent variable vector, 'NumPeaks' is the number of peaks, 'peakshape' is the peak shape number, 'Heights' is the vector of peak heights, 'Positions' is the vector of peak positions, 'Widths' is the vector of peak widths, and 'extra' is the additional shape parameter required by the exponentially broadened, Pearson, Gaussian/Lorentzian blend, BiGaussian and BiLorentzian shapes. Type 'help modelpeaks'. To create noisy peaks, use one of the following noise functions to create some random noise to add to the modelpeaks array. 

\href{https://terpconnect.umd.edu/~toh/spectrum/modelpeaks2.m}{modelpeaks2}, a function that simulates multi-peak time-series signal data consisting of any number of peaks of different shapes. Syntax is y=modelpeaks2(t, Shape, Height, Position, Width, extra) where 'shape' is a vector of peak type numbers and the other input arguments are the same as for modelpeaks.m. Type 'help modelpeaks2'.

\href{https://terpconnect.umd.edu/~toh/spectrum/ShapeDemo.m}{ShapeDemo} demonstrates 16 basic peak shapes graphically, showing the variable-shape peaks as multiple lines. (graphic on page \pageref{ref-0469})

\href{https://terpconnect.umd.edu/~toh/spectrum/SignalGenerator.m}{SignalGenerator.m} is a script that uses the modelpeaks.m function above to create and plot realistic computer-generated signal consisting of multiple peaks on a variable baseline plus variable random noise. You may change the lines marked by ``{\textless}{\textless}{\textless}'' to modify the character of the signal peaks, baseline, and noise.

\subsection{Signal Arithmetic\label{ref-0503}}

\href{https://terpconnect.umd.edu/~toh/spectrum/stdev.m}{stdev.m} Octave and Matlab compatible standard deviation function (because the regular built-in std.m function behaves differently in Matlab and in Octave). \href{https://terpconnect.umd.edu/~toh/spectrum/rsd.m}{rsd.m} is the relative standard deviation (the standard deviation divided by the mean).

\href{https://terpconnect.umd.edu/~toh/spectrum/PercentDifference.m}{PercentDifference.m} A simple function that calculates the percent difference between two numbers or vectors, i.e., \texttt{100.*(b-a)./a,} where a and b can be scalar or vector.

\href{https://terpconnect.umd.edu/~toh/spectrum/halfwidth.m}{halfwidth} \uline{\textcolor{color-18}{and}} \href{https://terpconnect.umd.edu/~toh/spectrum/tenthwidth.m}{tenthwidth}\uline{\textcolor{color-18}{:}} [FWHM,slope1,slope2,hwhm1,hwhm2] = halfwidth(x,y,xo) uses linear interpolation between points to compute the approximate FWHM (full width at half maximum) of any smooth peak whose maximum is at x=xo, has a zero baseline, and falls to below one-half of the maximum height on both sides. Not accurate if the peak is noisy or sparsely sampled. If the additional output arguments are supplied, it also returns the leading and trailing edge slopes, slope1 and slope2, and the leading and trailing edge half widths at half maximum, hwhm1 and hwhm2, respectively. If x0 is omitted, it determines the halfwidth of the largest peak. Example: xo=500; width=100; x=1:1000; y=exp(-1.*((x-xo)/(0.60056120439323.*width)).\textasciicircum{}2); halfwidth(x,y,xo). The analogous function [twidth,slope1,slope2,hwhm1,hwhm2] = tenthwidth(x,y,xo) computes the full width at 1/10 maximum, and just for the heck of it, \href{https://terpconnect.umd.edu/~toh/spectrum/hundredthwidth.m}{hundredthwidth}, [hwidth,slope1,slope2] = hundredthwidth(x,y,xo), computes the full width at 1/100 maximum. 

\href{https://terpconnect.umd.edu/~toh/spectrum/MeasuringWidth.m}{MeasuringWidth.m} is a script that compares two methods of measuring the full width at half maximum of a peak: Gaussian fitting (using \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit.m}) and direct interpolation (using halfwidth.m). The two methods agree exactly for a finely-sampled noiseless Gaussian on a zero baseline but give slightly different answers if any of these conditions are not met. The halfwidth function works well for any finely-sampled smooth peak shape on a zero baseline, but the peakfit function is better at resisting random noise and it can correct for some types of baseline and it has a wide selection of peak shapes to use as a model. See the help file.

\href{https://terpconnect.umd.edu/~toh/spectrum/IQrange.m}{IQrange.m}, estimates the standard deviation of a set of numbers by dividing its ``\href{https://en.wikipedia.org/wiki/Interquartile\_range}{interquartile range}'' (IQR) by 1.34896, an alternative to the usual standard deviation calculation that works better for computing the dispersion (spread) of a data set that contains outliers. Essentially it is the standard deviation with outliers removed. Syntax is b = IQrange(a)\uline{\textcolor{color-18}{.}}

\href{https://terpconnect.umd.edu/~toh/spectrum/rmnan.m}{rmnan(a)}\textcolor{color-3}{,} which stands for "\textbf{R}e\textbf{M}ove \textbf{N}ot \textbf{A N}umber", removes NaNs ("\textbf{N}ot \textbf{a} \textbf{N}umber") and Infs ("Infinite") from vectors, replacing with nearest real numbers and printing out the number of changes (if any are made). Use this to prevent subsequent operations from stopping on an error.

\href{https://terpconnect.umd.edu/~toh/spectrum/rmz\%28a\%29}{rmz(a)} \textbf{R}e\textbf{M}oves \textbf{Z}eros from vectors, replacing with nearest non-zero numbers and printing out the number of changes (if any are made). Use this to remove zeros from vectors that will subsequently be used as the denominator of a division.

[a,changes]=\href{https://terpconnect.umd.edu/~toh/spectrum/nht.m}{nht(a,b);} "no higher than" replaces any numbers in vector a that are above the scalar b with b. Optionally "changes" returns the number of changes. The similar function [a,changes]=\href{https://terpconnect.umd.edu/~toh/spectrum/nlt.m}{nlt(a,b)}, "no lower than", replaces any numbers in vector a that are lower than the scalar b with b. Optionally "changes" returns the number of changes.

\href{https://terpconnect.umd.edu/~toh/spectrum/makeodd.m}{makeodd(a)}: Makes the elements of vector "a" the next higher odd integers. This can be useful in computing smooth widths to ensure that the smooth will not shift the maximum of peaks. For example, \texttt{makeodd([1.1 2 3 4.8 5 6 7.7 8 9]) = [1 3 3 5 5 7 9 9 9]}

\href{https://terpconnect.umd.edu/~toh/spectrum/condense.m}{condense}\uline{\textcolor{color-18}{(y,n)}}, function to reduce the length of vector y by replacing each group of n successive values by their average. The similar function \href{https://terpconnect.umd.edu/~toh/spectrum/condensem.m}{condensem.m} works for matrices. Use to re-sample an oversampled signal. Mentioned on Smoothing (page \pageref{ref-0045}) and iSignal (page \pageref{ref-0439})

\href{https://terpconnect.umd.edu/~toh/spectrum/val2ind.m}{val2ind(x,val)}, returns the index and the value of the element of vector x that is closest to val. Example: if x=[1 2 4 3 5 9 6 4 5 3 1], then val2ind(x,6)=7 and val2ind(x,5.1)=[5 9]. This is useful for accessing subsets of x, y data sets; for example,  the code sequence \texttt{x1=7;x2=8; irange = val2ind(x,x1):val2ind(x,x2); xx=x(irange); yy=y(irange); plot(xx,yy}) will isolate the subset xx, yy and plot it only over the range of x values from 7 to 8. For some other examples of how this can be used, see \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#UsingP}{page} \pageref{ref-0314}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/testcondense.m}{testcondense.m} is a script that demonstrates of the effect of boxcar averaging using the condense.m function to reduce noise without changing the noise color. Shows that it reduces the measured noise, removing the high frequency components, resulting in a faster fitting execution time and a lower fitting error, but no more accurate measurement of peak parameters. 

\href{https://terpconnect.umd.edu/~toh/spectrum/NumAT.m}{NumAT(m,threshold)}: "\textbf{Num}bers \textbf{A}bove \textbf{T}hreshold": Counts the number of adjacent elements in the vector 'm' that are greater than or equal to the scalar value 'threshold'. It returns a matrix listing each group of adjacent values, their starting index, the number of elements in that group, and the sum of that group, and the mean. Type "help NumAT" and try the example.

\href{https://terpconnect.umd.edu/~toh/spectrum/isOctave.m}{isOctave.m}  Returns 'true' if this code is being executed by Octave. It returns 'false' if this code is being executed by MATLAB, or any other MATLAB variant. Useful in those few cases where there is a small difference between the syntax or operation of Matlab and Octave functions, as for example \href{https://terpconnect.umd.edu/~toh/spectrum/trypoly.m}{trypoly(x,y)}, \href{https://terpconnect.umd.edu/~toh/spectrum/tablestats.m}{tablestats.m}, and \href{https://terpconnect.umd.edu/~toh/spectrum/trydatatrans.m}{trydatatrans}.m.

\textbf{Data plotting}. The Matlab/Octave scripts \href{https://terpconnect.umd.edu/~toh/spectrum/plotting.m}{plotting.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/plotting2.m}{plotting2.m} show how to plot multiple signals using matrices and subplots (multiple small plots in a single Figure window). The scripts \href{https://terpconnect.umd.edu/~toh/spectrum/realtimeplotautoscale.m}{realtimeplotautoscale.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/realtimeplotautoscale2.zip}{realtimeplotautoscale2.m} demonstrate plotting in real time (Click for \href{https://terpconnect.umd.edu/~toh/spectrum/realtimeplotautoscale2.gif}{animated graphic}).

\href{https://terpconnect.umd.edu/~toh/spectrum/plotit.m}{plotit}, version 2, is an easy-to-use function for plotting x,y data in matrices or in separate vectors. Syntax: [coef,RSquared,StdDevs,BootResults]=plotit(xi,yi,polyorder,datastyle,fitstyle). It can also fit polynomials to the data and compute the errors. \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Matlab}{Click here} or type "help plotit" at the Matlab/Octave prompt for some examples.

\href{https://terpconnect.umd.edu/~toh/spectrum/plotxrange.m}{plotxrange} extracts and plots values of vectors x,y only for x values between x1 and x2. Returns extracted values in vectors xx,yy and the range of index values in irange. Ignores values of x1 and x2 outside the range of x.

\textbf{segplot}, syntax [s,xx,yy]=\href{https://terpconnect.umd.edu/~toh/spectrum/segplot.m}{segplot}(x,y,NumSegs,seg), divides y into "NumSegs" equal-length segments and plots the x, y data with segments marked by vertical lines, each labeled with a small segment number at the bottom. Returns a vector   's' of segment indexes, and the subset xx,yy, of values in the segment number 'seg'. If the 4th input argument, 'seg', is included, it plots this segment only.\label{ref-0504}

\subsection{Signals and Noise\label{ref-0505}}

\href{https://terpconnect.umd.edu/~toh/spectrum/whitenoise.m}{whitenoise}, \href{https://terpconnect.umd.edu/~toh/spectrum/pinknoise.m}{pinknoise}, \href{https://terpconnect.umd.edu/~toh/spectrum/bluenoise.m}{bluenoise} \href{https://terpconnect.umd.edu/~toh/spectrum/propnoise.m}{propnoise}, \href{https://terpconnect.umd.edu/~toh/spectrum/sqrtnoise.m}{sqrtnoise}, \href{https://terpconnect.umd.edu/~toh/spectrum/bimodal.m}{bimodal}: different types of random noise that might be encountered in physical measurements. Type "help whitenoise", etc., for help and examples.

\href{https://terpconnect.umd.edu/~toh/spectrum/noisetest.m}{noisetest.m} is a self-contained Matlab/Octave function for demonstrating different noise types. It plots Gaussian peaks with four different types of added noise with the same standard deviation: constant white noise; constant pink (1/f\index{1/f}) noise; proportional white noise; and square-root white noise, then fits a Gaussian model to each noisy data set and computes the average and the standard deviation of the peak height, position, width and area for each noise type. See page \pageref{ref-0021}. See also \href{https://terpconnect.umd.edu/~toh/spectrum/NoiseColorTest.m}{NoiseColorTest.m}.

\href{https://terpconnect.umd.edu/~toh/spectrum/SubtractTwoMeasurements.m}{SubtractTwoMeasurements.m} is a Matlab/Octave script demonstration of measuring the noise and signal-to-noise ratio of a stable waveform by subtracting two measurements of the signal waveform, m1 and m2 and computing the standard deviation of the difference. The signal must be stable between measurements (except for the random noise). The standard deviation of the measured noise is given by \texttt{sqrt((std(m1-m2).\textasciicircum{}2)/2)}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/NoiseColorTest.m}{NoiseColorTest.m}, a function that demonstrates the effect of smoothing white, pink, and blue noise. It displays a graphic of five noise color types both \href{https://terpconnect.umd.edu/~toh/spectrum/NoiseColorTest1.png}{before }and \href{https://terpconnect.umd.edu/~toh/spectrum/NoiseColorTest2.png}{after }smoothing, as well as their \href{https://terpconnect.umd.edu/~toh/spectrum/NoiseColorTest3.png}{frequency spectra}. All noise samples have a standard deviation of 1.0 before smoothing. You can change the smooth width and type in lines 6 and 7.

\href{https://terpconnect.umd.edu/~toh/spectrum/CurvefitNoiseColorTest.m}{CurvefitNoiseColorTest.m}, a function that demonstrates the effect of white, pink, and blue noise on curve fitting a single Gaussian peak.

\href{https://terpconnect.umd.edu/~toh/spectrum/RANDtoRANDN.m}{RANDtoRANDN.m} is a script that demonstrates how the expression \texttt{1.73*(RAND() - RAND() + RAND() - RAND())} approximates normally-distributed random numbers with zero mean and a standard deviation of 1. See page \pageref{ref-0021}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/RoundingError.m}{RoundingError.m}. A script that demonstrates digitization (rounding) noise and shows that adding noise and then ensemble averaging multiple signals can reduce the overall noise in the signal. This is a rare example where adding noise is beneficial. See page \pageref{ref-0367}.

\href{https://terpconnect.umd.edu/~toh/spectrum/DigitizedSpeech.m}{DigitizedSpeech.m}, an audible/graphic demonstration of rounding error on digitized speech. It starts with an audio recording of the spoken phrase "Testing, one, two, three", previously recorded at 44000 Hz and saved in WAV format (\href{https://terpconnect.umd.edu/~toh/spectrum/TestingOneTwoThree.wav}{download link}), rounds off the amplitude data progressively to 8 bits (256 steps), 4 bits (16 steps), and 1 bit (2 steps), and then the same with random white noise added before the rounding (2 steps + noise), plots the waveforms and plays the resulting sounds, demonstrating both the degrading effect of rounding and the remarkable improvement caused by adding noise. See page \pageref{ref-0367}.

\href{https://terpconnect.umd.edu/~toh/spectrum/CentralLimitDemo.m}{CentralLimitDemo.m}, script that demonstrates that the more independent uniform random variables are combined, the probability distribution becomes closer and closer to normal (Gaussian). See \textcolor{color-3}{page} \pageref{ref-0021}

\href{https://terpconnect.umd.edu/~toh/spectrum/EnsembleAverageDemo.m}{EnsembleAverageDemo.m} is a Matlab/Octave script that demonstrates ensemble averaging to improve the signal-to-noise ratio of a very noisy signal. \href{https://terpconnect.umd.edu/~toh/spectrum/EnsembleAverageDemo.png}{Click for graphic.} The script requires the "\href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian}.m" function to be downloaded and placed in the Matlab/Octave path, or you can use any other \href{https://terpconnect.umd.edu/~toh/spectrum/Peak\_shape\_functions}{peak shape function}, such as \href{https://terpconnect.umd.edu/~toh/spectrum/lorentzian.m}{lorentzian.m} or \href{https://terpconnect.umd.edu/~toh/spectrum/rectanglepulse.m}{rectanglepulse.m}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/EnsembleAverageDemo2.m}{EnsembleAverageDemo2.m} is a Matlab/Octave script that demonstrates the effect of \textit{amplitude noise}, \textit{frequency noise}, and \textit{phase noise} on the ensemble averaging of a sine waveform. 

\href{https://terpconnect.umd.edu/~toh/spectrum/EnsembleAverageFFT.m}{EnsembleAverageFFT.m} is a Matlab/Octave script that demonstration of the effect of \textit{amplitude noise}, \textit{frequency noise}, and \textit{phase noise} on the ensemble averaging of a sine waveform signal. Shows that: (a) ensemble averaging reduces the white noise in the signal but not the frequency or phase noise, (b) ensemble averaging the Fourier transform has the same effect as ensemble averaging the signal itself, and (c) the effect of phase noise is reduced if the power spectra are ensemble averaged.\href{https://terpconnect.umd.edu/~toh/spectrum/EnsembleAverageFFTGaussian.m}{ EnsembleAverageFFTGaussian.m} does the same for a Gaussian peak signal, where variation in peak width is frequency noise and variation in peak position is phase noise.

\href{https://terpconnect.umd.edu/~toh/spectrum/iPeakEnsembleAverageDemo.m}{iPeakEnsembleAverageDemo.m} is a self-contained demonstration of the iPeak function. In this example, the signal contains a repeated pattern of two overlapping Gaussian peaks of width 12, with a 2:1 height ratio. These patterns occur at random intervals, and the noise level is about 10\% of the average peak height. Using iPeak's ensemble average function (\textbf{Shift-E}), the patterns can be averaged and the signal-to-noise ratio significantly improved. See \textcolor{color-3}{page} \pageref{ref-0026}\textcolor{color-3}{.}

\href{https://terpconnect.umd.edu/~toh/spectrum/PeriodicSignalSNR.m}{PeriodicSignalSNR.m} is a Matlab/Octave script demonstrating the estimation of the peak-to-peak and root-mean-square signal amplitude and the signal-to-noise ratio of a periodic waveform, estimating the noise by looking at the time periods where its envelope drops below a threshold. See \textcolor{color-3}{page} \pageref{ref-0021}\textcolor{color-3}{.}

\href{https://terpconnect.umd.edu/~toh/spectrum/iPeakEnsembleAverageDemo.m}{iPeakEnsembleAverageDemo.m} is a demonstration of iPeak's ensemble average function. In this example, the signal contains a repeated pattern of two overlapping Gaussian peaks, 12 points apart, both of width 12, with a 2:1 height ratio. These patterns occur at random intervals throughout the recorded signal, and the random noise level is about 10\% of the average peak height. Using iPeak's ensemble average function (\textbf{Shift-E}), the patterns can be averaged and the signal-to-noise ratio significantly improved.

\href{https://terpconnect.umd.edu/~toh/spectrum/LowSNRdemo.m}{LowSNRdemo.m} is a script that compares several different methods of peak measurement with very low signal-to-noise ratios. It creates a single peak, with adjustable shape, height, position, and width, adds constant white random noise so the signal-to-noise ratio varies from 0 to 2, then measures the peak height and position by each method and computes the average error. Four methods are compared: (1) the peak-to-peak measure of the smoothed signal and background; (2) a peak finding method based on \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{findpeakG}; (3) \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{unconstrained iterative least-squares fitting} (INLS) based on the \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m} function; and (4) \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html}{constrained classical least-squares fitting}(CLS) based on the \href{https://terpconnect.umd.edu/~toh/spectrum/cls2.m}{cls2.m} function. See \textcolor{color-3}{the appendix: How Low can you Go? Performance with very low signal-to-noise ratios.}

\href{https://terpconnect.umd.edu/~toh/spectrum/RandomWalkBaseline.m}{RandomWalkBaseline.m} simulates a Gaussian peak with randomly variable position and width superimposed on a drifting "random walk" baseline. Compare to \href{https://terpconnect.umd.edu/~toh/spectrum/WhiteNoiseBaseline.m}{WhiteNoiseBaseline.m}. See \textcolor{color-3}{page} \pageref{ref-0380}\textcolor{color-3}{.}

\href{https://terpconnect.umd.edu/~toh/spectrum/AmplitudeModulation.m}{AmplitudeModulation.m} is a Matlab/Octave script simulation of modulation and synchronous detection, demonstrating the noise reduction capability. See page \pageref{ref-0382}\textcolor{color-3}{.}

\href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeNumericalPrecisionDemo.m}{DerivativeNumericalPrecisionDemo.m}. Self-contained function that demonstrates how the \textit{numerical precision limits} of the computer affects the first through fourth derivatives of a smooth ("noiseless") Gaussian band, showing both the waveforms (in Figure window 1) and their frequency spectra (in Figure window 2). The numerical precision limit of the computer creates random noise at very high frequencies, which is emphasized by differentiation, and by the fourth derivative that noise overwhelms the signal frequencies at lower frequencies. Most of the noise can be removed by smoothing with a p-spline (three passes of a sliding-average) with a smooth ratio of 0.2. With real experimental data, even the tiniest amounts of noise in the original data would be much greater than this. See page \pageref{ref-0405}\textcolor{color-3}{.}

\href{https://terpconnect.umd.edu/~toh/spectrum/RegressionNumericalPrecisionTest.m}{RegressionNumericalPrecisionTest.m} is a Matlab/Octave script that demonstrates how the \textit{numerical precision limits} of the computer effects the Classical Least-squares (multilinear regression) of two very closely-spaced "noiseless" overlapping Gaussian peaks. This uses three different mathematical formulation of the least-squares calculation that give different results when the numerical precision limits of the computer are reached. But practically, the difference between these methods is unlikely to be seen; even the tiniest bit of added random noise (line 15) or signal instability produces a far greater error. Used in page \pageref{ref-0405}\textcolor{color-3}{.}

\href{https://terpconnect.umd.edu/~toh/spectrum/RegressionADCbitsTest.m}{RegressionADCbitsTest.m}. Demonstration of the effect of analog-to-digital converter resolution (defined by the number of bits in line 9) on Classical Least-squares (multilinear regression) of two closely-spaced overlapping Gaussian peaks. Normally, the random noise (line 10) produces a greater error than the ADC resolution. Used on page \pageref{ref-0406}.

\subsection{Smoothing\label{ref-0506}}

\href{https://terpconnect.umd.edu/~toh/spectrum/fastsmooth.m}{fastsmooth}, versatile function for fast data smoothing. The syntax is \texttt{SmoothY=fastsmooth(Y,w, type, ends)}. See page \pageref{ref-0046}. Note: \href{http://uk.mathworks.com/matlabcentral/profile/authors/1859625-greg-pittam}{Greg Pittam} has published a modification of the fastsmooth function that tolerates NaNs (Not a Number) in the data file \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#Matlab}{(}\href{http://uk.mathworks.com/matlabcentral/fileexchange/52688-nan-tolerant-fast-smooth}{nanfastsmooth(Y,w,type,tol)}) and a version for smoothing angle data \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#Matlab}{(}\href{http://uk.mathworks.com/matlabcentral/fileexchange/52689-angular-fast-smooth-nan-tolerant}{nanfastsmoothAngle(Y,w,type,tol)}). \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothWidthTest.gif}{Click for animated example.}\label{ref-0507}

\href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSmooth.m}{SegmentedSmooth.m}, segmented\index{\textcolor{color-3}{segmented}} multiple-width data smoothing function based on the fastsmooth algorithm. The syntax is \texttt{SmoothY = SegmentedSmooth(Y,smoothwidths,type,ends)}. This function divides Y into several equal-length segments according to the length of the vector 'smoothwidths', then smooths each segment with a smooth of width defined by the sequential elements of vector 'smoothwidths' and smooth type 'type'. Type "help SegmentedSmooth" for examples. \href{https://terpconnect.umd.edu/~toh/spectrum/DemoSegmentedSmooth.m}{DemoSegmentedSmooth.m} demonstrates the operation (\href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSmoothDemo.png}{click for graphic}). See page \pageref{ref-0047}.

\href{https://terpconnect.umd.edu/~toh/spectrum/medianfilter.m}{medianfilter} , median-based filter function for eliminating narrow spike artifacts. The syntax is mY=medianfilter(y, Width), where "Width" is the number of points in the spikes that you wish to eliminate. Type "help medianfilter" at the command prompt.

\href{https://terpconnect.umd.edu/~toh/spectrum/killspikes.m}{killspikes.m} is a threshold-based filter function for eliminating narrow spike artifacts. The syntax is \texttt{fy= killspikes(x, y, threshold, width}\texttt{).} Each time it finds a positive or negative jump in the data between y(n) and y(n+1) that exceeds "threshold", it replaces the next "width" points of data with a \textit{linearly interpolated segment} spanning x(n) to x(n+width+1), See \href{https://terpconnect.umd.edu/~toh/spectrum/killspikesdemo.m}{\texttt{killspikesdemo}}\texttt{.} Type "\texttt{help killspikes}" at the command prompt.

\href{https://terpconnect.umd.edu/~toh/SPECTRUM}{testcondense.m} is a script that demonstrates of the effect of boxcar averaging using the \href{https://terpconnect.umd.edu/~toh/spectrum/condense.m}{condense.m} function, which performs a non-overlapping boxcar averaging function, to reduce noise without changing the noise color. Shows that it \href{https://terpconnect.umd.edu/~toh/spectrum/testcondense.png}{reduces the measured noise}, \href{https://terpconnect.umd.edu/~toh/spectrum/testcondense2.png}{removing the high frequency components}, resulting in a faster fitting execution time and a lower fitting error, but unfortunately \textit{no more accurate measurement of peak parameters}.

\href{https://terpconnect.umd.edu/~toh/spectrum/SmoothWidthTest.m}{SmoothWidthTest.m} is a Matlab/Octave script that demonstrates the effect of smoothing on the peak height, random white noise, and signal-to-noise ratio of a noisy peak signal. Produces an animation showing the effect of progressively wider smooth widths, then draws a graph of peak height, noise, and signal-to-noise ratio vs smooth ratio. \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothWidthTest.gif}{Click to see gif animation}. You can change the peak \textit{shape} and \textit{width} in line 8 and the smooth \textit{type} in line 9: 1=rectangle; 2=triangle; 3=p-spline. The script requires the "\href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian}.m" function to be downloaded and placed in the Matlab/Octave path, or you can use any other \href{https://terpconnect.umd.edu/~toh/spectrum/Peak\_shape\_functions}{peak shape function}, such as \href{https://terpconnect.umd.edu/~toh/spectrum/lorentzian.m}{lorentzian.m} or \href{https://terpconnect.umd.edu/~toh/spectrum/rectanglepulse.m}{rectanglepulse.m}, etc.

\href{https://terpconnect.umd.edu/~toh/spectrum/SmoothExperiment.m}{SmoothExperiment.m}, very simple script that demonstrates the effect of smoothing on the position, width, and height of a single Gaussian peak. Requires that the \href{https://terpconnect.umd.edu/~toh/spectrum/fastsmooth.m}{fastsmooth.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m} functions be present in the path. See page \pageref{ref-0073}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/smoothdemo.m}{smoothdemo.m}, self-contained function that compares the performance and speed of four types of \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smooth operations}: (1) sliding-average, (2) triangular, (3) p-spline (equivalent to three passes of a sliding-average), and (4) Savitzky-Golay. These smooth operations are applied to a single noisy Gaussian peak. The peak height of the smoothed peak, the standard deviation of the smoothed noise, and the signal-to-noise ratio are all measured as a function of smooth width. See page \pageref{ref-0073}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/SmoothOptimization.m}{SmoothOptimization.m}, script that shows why you do not need to smooth data prior to least-squares curve fitting; it compares the effect of smoothing on the signal-to-noise ratio of peak height of a noisy Gaussian peak, using three different measurement methods. Requires that the \href{https://terpconnect.umd.edu/~toh/spectrum/fitgauss2.m}{fitgauss2.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/gaussfit.m}{gaussfit.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian.m}, and fminsearch.m functions be present in the path. See page \pageref{ref-0285}.

\href{https://terpconnect.umd.edu/~toh/spectrum/SmoothVsCurvefit.m}{SmoothVsCurvefit.m}, comparison of peak height measurement by taking the maximum of the smoothed signal and by curve fitting the original unsmoothed data. Requires peakfit.m and gaussian.m in the path.

\href{https://terpconnect.umd.edu/~toh/spectrum/DemoSegmentedSmooth.m}{DemoSegmentedSmooth.m} demonstrates the operation of \href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSmooth.m}{SegmentedSmooth.m} with a signal consisting of noisy variable-width peaks that get progressively wider. Requires SegmentedSmooth.m and \href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian.m} in the path.

\href{https://terpconnect.umd.edu/~toh/spectrum/DeltaTest.m}{DeltaTest.m}. A simple Matlab/Octave script that demonstrates the shape of any smoothing algorithm can be determined by applying that smooth to a \textit{delta function,} a signal consisting of all zeros except for one point. The result is called the \textit{impulse response function}.

\href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{iSignal} (page \pageref{ref-0439}) performs several different kinds of smoothing, segmented\index{\textcolor{color-3}{segmented}} smoothing, median filtering, and spike removal (as well as differentiation, peak sharpening, least-squares measurements of peak position, height, width, and area, signal and noise amplitudes, frequency spectra in selected regions of the signal, and signal-to-noise ratio of peaks). m-file link: \href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{isignal.m}. \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal8.zip}{Click here to download the ZIP file "iSignal8.zip"}. \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothAnimation.gif}{Click for animated example}.

The script \href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeSmoothTest.zip}{RealTimeSmoothTest.m} demonstrates real-time smoothing, plotting the raw unsmoothed data as a black line and the smoothed data in red. In this case the script pre-calculates simulated data in line 28 and then accesses the data point-by-point in the processing loop (lines 30-51). The total number of data points is controlled by 'maxx' in line 17 (initially set to 1000) and the smooth width (in points) is controlled by 'SmoothWidth' in line 20. \href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeSmooth.gif}{Animated graphic}.

\subsection{Differentiation and peak sharpening\label{ref-0508}}

\href{https://terpconnect.umd.edu/~toh/spectrum/deriv.m}{deriv}, \href{https://terpconnect.umd.edu/~toh/spectrum/deriv2.m}{deriv2}, \href{https://terpconnect.umd.edu/~toh/spectrum/deriv3.m}{deriv3}, \href{https://terpconnect.umd.edu/~toh/spectrum/deriv4.m}{deriv4}, \href{https://terpconnect.umd.edu/~toh/spectrum/derivxy.m}{derivxy }and \href{https://terpconnect.umd.edu/~toh/spectrum/secderivxy.m}{secderivxy}, simple functions for computing the derivatives of time-series data without smoothing. See page \pageref{ref-0098}\textcolor{color-3}{.}

\href{https://terpconnect.umd.edu/~toh/spectrum/SmoothDerivative.m}{SmoothDerivative.m} combines differentiation and smoothing. The syntax is SmoothedDeriv = SmoothedDerivative(x, y, DerivativeOrder, w, type, ends) where 'DerivativeOrder' determines the derivative order (0 through 5), 'w' is the smooth width, 'type' determines the smooth mode, and 'ends' controls how the "ends" of the signal (the first w/2 points and the last w/2 points) are handled.

\href{https://terpconnect.umd.edu/~toh/spectrum/SlopeAnimation.m}{SlopeAnimation.m} is an \href{https://terpconnect.umd.edu/~toh/spectrum/SlopeAnimation.gif}{animated }Matlab/Octave demonstration that shows that the first derivative of a signal is the slope of the tangent to the signal at each point.

\href{https://terpconnect.umd.edu/~toh/spectrum/sharpen.m}{sharpen}, peak sharpening by the even-derivative method. Syntax is \texttt{SharpenedSignal = sharpen(signal, factor1, factor2, SmoothWidth)}. See page \pageref{ref-0114}. Related demos: \href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSharpen.m}{SegmentedSharpen.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/DemoSegmentedSharpen.m}{DemoSegmentedSharpen.m} (\href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSharpenDemo.png}{graphic}), \href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedGaussianDemo.m}{SharpenedGaussianDemo.m} (\href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedGaussianDemo.png}{graphic}), \href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedGaussianDemo4terms.m}{SharpenedGaussianDemo4terms.m} (\href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedGaussianDemo4terms.png}{graphic}), \href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedLorentzianDemo.m}{SharpenedLorentzianDemo.m} (\href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedLorentzianDemo.png}{graphic}), \href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedLorentzianDemo4terms.m}{SharpenedLorentzianDemo4terms.m}. 

 \href{https://terpconnect.umd.edu/~toh/spectrum/symmetrize.m}{symmetrize}.m converts exponentially broadened peaks into symmetrical peaks by the \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html\#Asymmetrical}{weighted addition or subtraction of the first derivative}. The syntax is  ySym =\href{https://terpconnect.umd.edu/~toh/spectrum/symmetrize.m}{ symmetrize}(t, y, factor, smoothwidth, type, ends), where t, y are the raw data vectors, 'factor' is the derivative weighting factor, and 'smoothwidth', 'type', 'ends' are the \href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSmooth.m}{SegmentedSmooth arguments} for smoothing the derivative. To perform a \textit{segmented} symmetrization, "factor" and "smoothwidth" can be vectors. (In version 2, only the derivative is smoothed internally, not the entire symmetrized signal). \href{https://terpconnect.umd.edu/~toh/spectrum/SymmetrizeDemo.m}{SymmetrizeDemo.m} runs all five examples in the symmetrize.m help file, each in a different figure window.

First derivative symmetrization can be followed by an application of even derivative sharpening for further peak sharpening, as demonstrated for a single exponentially modified Gaussian (EMG) by the self-contained Matlab/Octave demo function \href{https://terpconnect.umd.edu/~toh/spectrum/EMGplusfirstderivative.m}{EMGplusfirstderivative.m} and for an exponentially modified Lorentzian (EML) by \href{https://terpconnect.umd.edu/~toh/spectrum/EMLGplusfirstderivative.m}{EMLplusfirstderivative.m}. In both of these demos, Figure 1 shows the \href{https://terpconnect.umd.edu/~toh/spectrum/FirstDerivativeSymmetricalization.png}{symmetrization} and Figure 2 shows that the symmetrized peak can be further narrowed by \href{https://terpconnect.umd.edu/~toh/spectrum/AdditionalSharpening.png}{additional 2\textsuperscript{nd} and 4\textsuperscript{th} derivative sharpening}. \href{https://terpconnect.umd.edu/~toh/spectrum/SymmetizedOverlapDemo.m}{SymmetizedOverlapDemo.m} demonstrates the optimization of the first derivative symmetrization for the measurement of the areas of two overlapping exponentially broadened Gaussians. \textit{Double} exponential symmetrization is performed by the function \href{https://terpconnect.umd.edu/~toh/spectrum/DEMSymm.m}{DEMSymm.m}. It is demonstrated by the script \href{https://terpconnect.umd.edu/~toh/spectrum/DemoDEMSymm.m}{DemoDEMSymm.m} and its two variations (1, 2), which creates two overlapping double exponential peaks from Gaussian originals, then calls the function DEMSymm.m to perform the symmetrization, using a three-level plus-and-minus bracketing technique to help you to determine the best values of the two weighting factors by trial and error. The interactive function \textit{iSignal} (page \pageref{ref-0440}) can perform first derivative symmetrization interactively, with keystrokes to increase and decrease the ``factor'' while watching the effect on the signal. The script \href{https://terpconnect.umd.edu/~toh/spectrum/AsymmetricalOverlappingPeaks.m}{AsymmetricalOverlappingPeaks.m} demonstrates the use of first-derivative symmetrization and curve fitting to analyze a complex ``mystery'' peak. See page \pageref{ref-0429}).

\href{https://terpconnect.umd.edu/~toh/spectrum/ProcessSignal.m}{ProcessSignal}, a Matlab/Octave command-line multi-purpose function that includes smoothing, differentiation, peak sharpening, and median filtering on the time-series data set x,y (column or row vectors). Like \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{iSignal}, without the plotting and interactive keystroke controls. Type "help ProcessSignal". It returns the processed signal as a vector that has the same shape as x, regardless of the shape of y. The syntax is \texttt{Processed= Processed=ProcessSignal(x, y, DerivativeMode, w, type, ends, Sharpen, factor1, factor2, Symize, Symfactor, SlewRate, MedianWidth).}

\href{https://terpconnect.umd.edu/~toh/spectrum/derivdemo1.m}{derivdemo1.m}, a function that demonstrates the basic shapes of derivatives. See page \pageref{ref-0083}.

\href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeShapeDemo.m}{DerivativeShapeDemo}.m is a function that demonstrates the first derivatives of 16 different peak shapes. (\href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeShapeDemo.png}{graphic})

\href{https://terpconnect.umd.edu/~toh/spectrum/derivdemo2.m}{derivdemo2.m}, a function that demonstrates the effect of peak width on the amplitude of derivatives. See page \pageref{ref-0083}.

\href{https://terpconnect.umd.edu/~toh/spectrum/derivdemo3.m}{derivdemo3.m}, a function that demonstrates the effect of smoothing on the \textit{first} derivative of a noisy signal. See page \pageref{ref-0083}.

\href{https://terpconnect.umd.edu/~toh/spectrum/derivdemo4.m}{derivdemo4.m}, a function that demonstrates the effect of smoothing on the \textit{second} derivative of a noisy signal. See page \pageref{ref-0083}.

\href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeDemo.m}{DerivativeDemo.m} is a self-contained Matlab/Octave demo function that uses \href{https://terpconnect.umd.edu/~toh/spectrum/ProcessSignal.m}{ProcessSignal.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/plotfit.m}{plotit.m} to demonstrate an application of differentiation to the quantitative analysis of a peak buried in an unstable background (e.g. as in various forms of spectroscopy). The object is to derive a measure of peak amplitude that varies linearly with the actual peak amplitude and is minimally affected by the background and the noise. To run it, just type DerivativeDemo at the command prompt. You can change several of the internal variables (e.g., Noise, BackgroundAmplitude) to make the problem harder or easier. Note that, even though the magnitude of the derivative is numerically smaller than the original signal (because it has different units), the signal-to-noise ratio of the derivative is better, and the derivative signal is linearly proportional to the actual peak height, despite the interference of large background variations and random noise. See page \pageref{ref-0098}.

\href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{\textbf{iSignal}} (page \pageref{ref-0439}) \href{https://terpconnect.umd.edu/~toh/spectrum/functions.html\#Top}{}is an interactive function for Matlab that includes \textit{differentiation and smoothing} for time-series signals, up to the 5\textsuperscript{th} derivative, automatically including the required type of smoothing. Simple keystrokes allow you to adjust the smoothing parameters (smooth type, width, and ends treatment) while observing the effect on your signal dynamically. It can also perform \textit{interactive} \textit{symmetrization} and \textit{sharpening} of exponentially broadened peaks by the first-derivative addition technique (page \pageref{ref-0106}). \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal8.zip}{Click here to download the ZIP file "iSignal8.zip"}. \href{https://terpconnect.umd.edu/~toh/spectrum/DerivAnimation.gif}{Click for animated example}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/demoisignal.m}{demoisignal.m} for Matlab is a self-running script that demonstrates several of the features of \href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{iSignal }(and requires that the latest version of iSignal, and version 6 of \href{https://terpconnect.umd.edu/~toh/spectrum/plotit.m}{plotit.m}, be present in your Matlab path). Demonstrates panning and zooming, smoothing, differentiation, frequency spectrum, peak measurement, and derivative spectroscopy calibration (in conjunction with \href{https://terpconnect.umd.edu/~toh/spectrum/plotit.m}{plotit.m} version 6).

\href{https://terpconnect.umd.edu/~toh/spectrum/iSignalDeltaTest.m}{iSignalDeltaTest} is a Matlab/Octave script that demonstrates the frequency response (power spectrum) of the smoothing and differentiation functions of \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{iSignal }by a applying them to a \href{https://en.wikipedia.org/wiki/Dirac\_delta\_function}{delta function}. Change the smooth type, smooth width, and derivative order and see how the power spectrum changes.

 The script \href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeSmoothFirstDerivative.zip}{RealTimeSmoothFirstDerivative.m} demonstrates real-time smoothed differentiation, using a simple adjacent-difference algorithm (line 47) and plotting the raw data as a black line and the first derivative data in red. The script \href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeSmoothSecondDerivative.zip}{RealTimeSmoothSecondDerivative.m} computes the smoothed \textit{second} derivative by using a central difference algorithm (line 47). Both scripts pre-calculate the simulated data in line 28 and then accesses the data point-by-point in the processing loop (lines 31-52). In both cases the maximum number of points is set in line 17 and the smooth width is set in line 20.

 The script \href{https://terpconnect.umd.edu/~toh/spectrum/RealTimePeakSharpening.zip}{RealTimePeakSharpening.m} demonstrates real-time peak sharpening using the second derivative technique. It uses pre-calculated simulated data in line 30 and then accesses the data point-by-point in the processing loop (lines 33-55). In both cases the maximum number of points is set in line 17 and the smooth width is set in line 20 and the weighting factor (K1) is set in line 21. In this example the smooth width is 101 points, which accounts for the delay in the sharpened peak compared to the original.

\subsection{Harmonic Analysis\label{ref-0509}}

\href{https://terpconnect.umd.edu/~toh/spectrum/FrequencySpectrum.m}{FrequencySpectrum.m} (syntax \texttt{fs=FrequencySpectrum(x,y}\texttt{)}) returns real part of the Fourier power spectrum of x,y as a matrix.

\href{http://terpconnect.umd.edu/~toh/spectrum/PlotFrequencySpectrum.m}{PlotFrequencySpectrum.m} plots the frequency spectrum or periodogram of the signal x,y on linear or log coordinates. The syntax is \texttt{PowerSpectrum= PlotFrequencySpectrum(x, y, plotmode, XMODE, LabelPeaks).}Type "help PlotFrequencySpectrum" for details. Try this example: 

\texttt{x= [0:.01:2*pi]'; y=sin(200*x)+randn(size(x));} 

\texttt{subplot(2,1,1); plot(x,y); subplot(2,1,2);} 

\texttt{PowerSpectrum=PlotFrequencySpectrum(x,y,1,0,1);}

\href{https://terpconnect.umd.edu/~toh/spectrum/CompareFrequencySpectrum.m}{CompareFrequencySpectrum.m.} A script that compares two signals (upper panel) and their frequency spectra (lower panel) with the original signal shown in blue and the modified signal in green. plotmode: =1:linear, =2:semilog X, =3:semilog Y; =4: log-log). XMODE: =0 for frequency Spectrum (x is frequency); =1 for periodogram (x is time). Define the signal modification in line 15. You can load a signal stored in .mat format or create a simulated signal for testing\texttt{.} You must have \href{http://terpconnect.umd.edu/~toh/spectrum/PlotFrequencySpectrum.m}{PlotFrequencySpectrum.m} in the path.

\href{https://terpconnect.umd.edu/~toh/spectrum/PlotSegFreqSpect.m}{PlotSegFreqSpect.m} is a segmented Fourier spectrum (syntax PSM=(x,y, NumSegments, MaxHarmonic, LogMode)) breaks y into 'NumSegments' equal length segments, multiplies each by an apodizing Hanning window, computes the power spectrum of each segment, returns the power spectrum matrix (PSM), and plots the result of the first 'MaxHarmonic' Fourier components as a contour plot. See page \pageref{ref-0131} for an example of its application to a signal that is completely buried in an excess of noise and interfering signals.

\href{https://terpconnect.umd.edu/~toh/spectrum/iSignalDeltaTest.m}{iSignalDeltaTest} is a Matlab/Octave script that demonstrates the \textit{frequency response} (power spectrum) of the smoothing and differentiation functions of \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{iSignal }by a applying them to a \href{https://en.wikipedia.org/wiki/Dirac\_delta\_function}{delta function}. Change the smooth type, smooth width, and derivative order and see how the power spectrum changes.

\href{https://terpconnect.umd.edu/~toh/spectrum/SineToDelta.m}{SineToDelta.m}. A demonstration animation (\href{https://terpconnect.umd.edu/~toh/spectrum/SineToDelta.gif}{animated graphic}) showing the waveform and the power spectrum of a rectangular pulsed sine wave of variable duration (whose power spectrum is a "sinc" function) changing continuously from a pure sine wave at one extreme (where its power spectrum is a delta function) to a single-point pulse at the other extreme (where its power spectrum is a flat line). \href{https://terpconnect.umd.edu/~toh/spectrum/GaussianSineToDelta.m}{GaussianSineToDelta.m} is similar, except that it shows a \textit{Gaussian} pulsed sine wave, whose power spectrum is a Gaussian function, but which is the same at the two extremes of pulse duration (\href{https://terpconnect.umd.edu/~toh/spectrum/GaussianSineToDelta.gif}{animated graphic}).

\href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{iSignal} (page \pageref{ref-0439}) is a multi-purpose interactive signal processing tool (for Matlab only) that includes a \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html\#Spectrum}{\textbf{Frequency Spectrum mode}}, toggled on and off by the \textbf{Shift-S} key; it computes frequency spectrum of the segment of the signal displayed in the upper window and displays it in the lower window (in red). You can use the pan and zoom keys to adjust the region of the signal to be viewed or press \textbf{Ctrl-A} to select the entire signal. Press \textbf{Shift-S} again to return to the normal mode. See page \pageref{ref-0117} for a relevant example. \href{https://terpconnect.umd.edu/~toh/spectrum/iSignalSpectrumMode.gif}{Click for animated example.}

\href{https://terpconnect.umd.edu/~toh/spectrum/HarmonicAnalysis.html\#ipower}{iPower}, a keyboard-controlled interactive power spectrum demonstrator, useful for teaching and learning about the power spectra of different types of signals and the effect of signal duration and sampling rate. Single keystrokes allow you to select the type of signal (12 different signals included), the total duration of the signal, the sampling rate, and the global variables f1 and f2 which are used in different ways in the different signals. When the \textbf{Enter} key is pressed, the signal (y) is sent to the Windows WAVE audio device. Press K to see a list of all the keyboard commands. (m-file link: \href{https://terpconnect.umd.edu/~toh/spectrum/ipower.m}{ipower.m}). \href{https://terpconnect.umd.edu/~toh/spectrum/iPowerAnimated.gif}{Slideshow of examples}.

 The script \href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeFrequencySpectrumWindow.zip}{RealTimeFrequencySpectrumWindow.m} computes and plots the Fourier frequency spectrum of a signal. It loads the simulated real-time data from a ``.mat file'' (in line 31) and then accesses that data point-by-point in the processing 'for' loop. A critical variable in this case is ``WindowWidth'' (line 37), the number of data points taken to compute each frequency spectrum. If the data stream is an audio signal, it is also possible to play the sound through the computer's sound system synchronized with the display of the frequency spectra (set "PlaySound" to 1).

\subsection{Fourier convolution and deconvolution\label{ref-0510}}

\href{https://terpconnect.umd.edu/~toh/spectrum/ExpBroaden.m}{E}\href{https://terpconnect.umd.edu/~toh/spectrum/ExpBroaden.m}{xpBroaden}, exponential broadening function. Syntax is \texttt{yb = ExpBroaden(y,t)}. Convolutes the vector \textit{y} with an exponential decay of time constant \textit{t}. Mentioned on pages \pageref{ref-0040} and \pageref{ref-0461}.

\href{https://terpconnect.umd.edu/~toh/spectrum/GaussConvDemo.m}{GaussConvDemo.m}, a script that demonstrates that a Gaussian of unit height, Fourier convoluted with a zero-centered Gaussian of the same width is a Gaussian with a height of 1/sqrt(2) and a width of sqrt(2) and of equal area to the original Gaussian. When you run this script, the top panel shows the convolution and the bottom panel shows how to recover the original y from the convoluted result (\href{https://terpconnect.umd.edu/~toh/spectrum/deconvgauss.png}{graphic}). You can optionally add noise in line 9 to show how convolution smooths the noise and how Fourier deconvolution restores it. Requires gaussian.m in the path.

\href{https://terpconnect.umd.edu/~toh/spectrum/CombinedDerivativesAndSmooths.txt}{CombinedDerivativesAndSmooths.txt}. Convolution coefficients for computing the first through fourth derivatives, with rectangular, triangular, and P-spline smooths.

\href{https://terpconnect.umd.edu/~toh/spectrum/Convolution.txt}{Convolution.txt}, simple examples of whole-number convolution vectors for smoothing and differentiation.

\href{https://terpconnect.umd.edu/~toh/spectrum/deconvolutionexample.m}{deconvolutionexample.m}, a simple example script that demonstrates the use of the Matlab Fourier deconvolution 'deconv' function. See page \pageref{ref-0154}.

\href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo.m}{DeconvDemo.m}, a Fourier deconvolution demo script with a signal containing four Gaussians broadened by an exponential function (\href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo.png}{graphic}). \href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo2.m}{DeconvDemo2.m} is a similar script for a single Gaussian (\href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo2.png}{graphic}).\href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo3.m}{DeconvDemo3.m} demonstrates deconvolution of a \textit{Gaussian} convolution function from a rectangular pulse (\href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo3.gif}{animated graphic}). \href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo4.m}{DeconvDemo4.m} (\href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo4.gif}{animated graphic}) demonstrates "self-deconvolution" applied a signal consisting of a Gaussian peak that is broadened by the measuring instrument, and an attempt to recover the original peak width.\href{https://terpconnect.umd.edu/~toh/SPECTRUM}{ }\href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo5.m}{DeconvDemo5.m} (\href{https://terpconnect.umd.edu/~toh/spectrum/GaussianDeconvolution5.png}{graphic}) shows an attempt to resolve \textit{two} closely-spaced underlying peaks that are \textit{completely unresolved} in the observed signal. See page \pageref{ref-0362}. Variation of this include versions with \href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo6.m}{Lorentzian peaks} and one with a \href{https://terpconnect.umd.edu/~toh/spectrum/DeconvDemo7.m}{triangular convolution function}.

\href{https://terpconnect.umd.edu/~toh/spectrum/deconvgauss.m}{deconvgauss.m}. ydc=deconvgauss(x,y,w) deconvolutes a Gaussian function of width 'w' from vector y, returning the deconvoluted result.

\href{https://terpconnect.umd.edu/~toh/spectrum/LorentzianSelfDeconvDemo.m}{LorentzianSelfDeconvDemo.m}. Demonstration of Lorentzian self-deconvolution. Requires lorentzian, halfwidth, and fastsmooth functions.

\href{https://terpconnect.umd.edu/~toh/spectrum/deconvexp.m}{deconvexp.m}. ydc=deconvexp(y,tc) deconvolutes an exponential function of time constant 'tc' from vector y, returning the deconvoluted result.

\href{https://terpconnect.umd.edu/~toh/spectrum/SegExpDeconv.m}{SegExpDeconv(x,y,tc)} is a segmented\index{\textcolor{color-3}{segmented}} version of \href{https://terpconnect.umd.edu/~toh/spectrum/deconvexp.m}{deconvexp.m}; it divides x,y into a number of equal-length segments defined by the length of the vector ‘tc’, then each segment is deconvoluted with an exponential decay of the form exp(-x./t) where ‘t’ is the corresponding element of the vector ‘tc’. \textit{Any number and sequence of t values can be used.} Useful when the peak width and/or exponential tailing of peaks varies across the signal duration. \href{https://terpconnect.umd.edu/~toh/spectrum/SegExpDeconvPlot.m}{SegExpDeconvPlot.m} is the same except that it plots the original and deconvoluted signals and \textit{shows the divisions between the segments by vertical magenta lines}. \href{https://terpconnect.umd.edu/~toh/spectrum/SegGaussDeconv.m}{SegGaussDeconv.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/SegGaussDeconvPlot.m}{SegGaussDeconvPlot.m} are the same except that they perform a symmetrical (zero-centered) Gaussian deconvolution. \href{https://terpconnect.umd.edu/~toh/spectrum/SegDoubleExpDeconv.m}{SegDoubleExpDeconv.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/SegDoubleExpDeconvPlot.m}{SegDoubleExpDeconvPlot.m} perform a symmetrical (zero-centered) exponential deconvolution.

\href{https://terpconnect.umd.edu/~toh/spectrum/convdeconv.m}{P=convdeconv(x,y,vmode,smode,vwidth,DAdd)}, for Matlab or Octave, performs Gaussian, Lorentzian, or exponential convolution and deconvolution of the signal in x,y. See page \pageref{ref-0160}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{ \textbf{iSignal} }\textcolor{color-3}{8.3} (page \pageref{ref-0439}) has a \textbf{Shift-V} keypress that displays the menu of Fourier convolution and deconvolution operations that allow you to convolute a Gaussian or exponential function with the signal, or to deconvolute a Gaussian or exponential function from the signal and allows you to adjust the width interactively. \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal8.zip}{Click here to download the ZIP file "iSignal8.zip"}

\subsection{Fourier Filter\label{ref-0511}}

\href{https://terpconnect.umd.edu/~toh/spectrum/FouFilter.m}{FouFilter}, Fourier filter function, with variable band-pass, low-pass, high-pass, or notch (band reject). The syntax is [ry,fy,ffilter,ffy] =FouFilter(y, samplingtime, centerfrequency, frequencywidth, shape, mode. Version 2, March 2019. See page \pageref{ref-0165}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedFouFilter.m}{SegmentedFouFilter.m} is a segmented version of FouFilter.m that applies different center frequencies and widths to different segments of the signal. The syntax is the same as FouFilter.m except that the two input arguments ``centerFrequency'' and ``FilterWidth'' must be vectors with the values of centerFrequency of filterWidth for each segment. The signal is divided equally into several segments determined by the length of centerFrequency and filterWidth, which must be equal in length. Type ``help SegmentedFouFilter'' for help and examples. See page \pageref{ref-0165}.

\href{https://terpconnect.umd.edu/~toh/spectrum/InteractiveFourierFilter.htm}{iFilter}, interactive Fourier filter. (m-file link: \href{https://terpconnect.umd.edu/~toh/spectrum/ifilter.m}{ifilter.m}), which uses the pan and zoom keys to control the center frequency and the filter width. \href{https://terpconnect.umd.edu/~toh/spectrum/iFilterAnimation.gif}{Click here for animated example}. Select from low-pass, high-pass, band-pass, band-reject, harmonic comb-pass, or harmonic comb-reject filters. \href{https://terpconnect.umd.edu/~toh/spectrum/MorseCode.mp4}{Click here to watch or download an mp4 video} of iFilter filtering a noisy Morse code signal, with sound (watch the title of the figure as the video plays).

\href{https://terpconnect.umd.edu/~toh/spectrum/MorseCode.m}{MorseCode.m} is a script that uses iFilter to demonstrate the abilities and limitations of Fourier filtering. It creates a pulsed fixed frequency sine wave that spells out ``SOS'' in Morse code (dit-dit-dit/dah-dah-dah/dit-dit-dit), adds random white noise so that the SNR is very poor (about 0.1 in this example), then uses a Fourier bandpass filter tuned to the signal frequency, to isolate the signal from the noise. As the bandwidth is reduced, the signal-to-noise ratio begins to improve and the signal emerges from the noise until it becomes clear, but if the bandwidth is too narrow, the step response time is too slow to give distinct ``dits'' and ``dahs''. Use the ``?'' and `` " '' keys to adjust the bandwidth. (The step response time is inversely proportional to the bandwidth). Press 'P' or the Spacebar to hear the sound. You must install \href{https://terpconnect.umd.edu/~toh/spectrum/ifilter.m}{iFilter.m} in the Matlab path. Watch on YouTube at \url{https://youtu.be/agjs1-mNkmY}. (look at the explanation in the title of the figure as the video plays).

\href{https://terpconnect.umd.edu/~toh/spectrum/TestingOneTwoThree.wav}{TestingOneTwoThree.wav} is a 1.58 sec duration audio recording of the spoken phrase "Testing, one, two, three", recorded at a sampling rate of 44000 Hz and saved in WAV format. When loaded into \href{https://terpconnect.umd.edu/~toh/spectrum/ifilter.m}{iFilter}(\texttt{v=wavread}\texttt{('}\href{https://terpconnect.umd.edu/~toh/spectrum/TestingOneTwoThree.wav}{TestingOneTwoThree.wav}\texttt{');)}, set to bandpass mode, and tuned to a narrow segment that is well above the frequency range of most of the signal, it might seem as if though this passband would miss most of the frequency components in the signal, yet even in this case the speech is intelligible, demonstrating the remarkable ability of the ear-brain system to make do with a highly compromised signal. Press P or space to hear the filter's output. Different filter settings will change the \href{https://en.wikipedia.org/wiki/Timbre}{timbre} of the sound. See page \pageref{ref-0441}. Click for \href{https://terpconnect.umd.edu/~toh/spectrum/TestingOneTwoThreeiFilter.png}{graphic}.

The script \href{https://terpconnect.umd.edu/~toh/spectrum/real-time\%20Fourier\%20bandpass\%20filter.zip}{RealTimeFourierFilter.m} is a demonstration of a real-time \href{https://terpconnect.umd.edu/~toh/spectrum/FourierFilter.html}{Fourier filter}. Like the \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html\#realtime}{other real-time signal processing scripts}, this one pre-computes a simulated signal starting in line 38, then access the data point-by-point (lines 56, 57), and divides up the data stream into segments to compute each filtered section. In this demonstration, a \href{https://en.wikipedia.org/wiki/Band-pass\_filter}{bandpass} filter is used to detect a 500 Hz ('f' in line 28) sine wave that occurs in the middle third of a very noisy signal (line 32), from about 0.7 sec to 1.3 sec. The filter center frequency (CenterFrequency) and width (FilterWidth) are set in lines 46 and 47. 

\subsection{Wavelets and wavelet denoising\label{ref-0512}}

 \href{https://terpconnect.umd.edu/~toh/spectrum/Morelet.m}{Morelet.m} demonstrates the application of the wavelet transform to unravel the components of a complicated signal. Code written by Michael X. Cohen, in ``A better way to define and describe Morlet wavelets for time-frequency analysis'', \textit{NeuroImage}, Volume 199, 1 October 2019, Pages 81-86. 

\href{https://terpconnect.umd.edu/~toh/spectrum/MorletExample2.m}{MorletExample2.m} creates and analyzes the ``buried peaks'' signal consisting of three components: a pair of weak Gaussian peaks which are the desirable signal components, a strong interference by a variable-frequency sine wave, and an excess of random white noise. The Gaussian peaks are invisible in the raw signal.

\subsection{Peak area measurement\label{ref-0513}\label{ref-0514}}

\textcolor{color-3}{PerpDropAreas.m} [AreaVector]=PerpDropAreas(x,y,startx,endx,MaxVector) measures he peak areas of the peaks in x, y, starting an x value of startX and ending at endX, with specified peak positions in the vector MaxVector, which can be of any length. Uses the halfwaypoint method. Returns the areas in the vector PDMeasAreas and the midpoint indices in the optional second output argument. 

\href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndArea.m}{HeightAndArea}.m is a demonstration script that uses \href{https://terpconnect.umd.edu/~toh/spectrum/measurepeaks.m}{measurepeaks.m} to measure the peaks in computer-generated \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest.png}{signals} consisting of a series of Gaussian peaks with gradually increasing widths that are superimposed in a curved baseline plus random white noise. It \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest.png}{plots the signal} and the \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest2.png}{individual peaks} and compares the actual peak position, heights, and areas of each peak to those measured by \href{https://terpconnect.umd.edu/~toh/spectrum/measurepeaks.m}{measurepeaks.m} using the absolute peak height, peak-valley difference, perpendicular drop, and tangent skim methods. Prints out a \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaErrors.txt}{table }of the relative percent difference between the actual and measured values for each peak and the average error for all peaks.

\href{https://terpconnect.umd.edu/~toh/spectrum/measurepeaks.m}{measurepeaks.m} automatically detects peaks in a signal, similar to findpeaksSG. It returns a \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest.txt}{table }of peak number, position, absolute peak height, peak-valley difference, perpendicular drop area, and tangent skim area of each peak. It can \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest.png}{plot the signal} and the \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest2.png}{individual peaks} if the last (7\textsuperscript{th}) input argument is 1. Type ``help measurepeaks'' and try the seven examples there or run \uline{HeightAndArea.m} to run a test of the accuracy of peak height and area measurement with signals that have multiple peaks with noise, background, and some peak overlap. The script \href{https://terpconnect.umd.edu/~toh/spectrum/testmeasurepeaks.m}{testmeasurepeaks.m} will run all of the examples with a 1-second pause between each (requires measurepeaks.m and gaussian.m in the path).

The script \href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedOverlapDemo.m}{SharpenedOverlapDemo.m} (\href{https://terpconnect.umd.edu/~toh/spectrum/SharpenedOverlapDemo.png}{graphic}) demonstrates the effect of sharpening on \href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html\#pdrop}{perpendicular drop area measurements} of two overlapping Gaussians peaks with adjustable height, separation, and width, calculating the percent different between the area measured on the overlapping peak signal compared to the true areas of the isolated peaks.

\href{https://terpconnect.umd.edu/~toh/spectrum/functions.html}{SharpenedOverlapCalibrationCurve.m} is a script that simulates quantitative measurement of mixtures of \textit{three} overlapping Gaussian peaks. Even-derivative sharpening (the red line in the signal plots) is used to improve the resolution of the peaks to allow perpendicular drop area measurement. A straight line is fit to the calibration curve and the R\textsuperscript{2 }is calculated, to demonstrate (1) the linearity of the response, and (2) the independence of the overlapping adjacent peaks. Must have gaussian.m, derivxy.m, autopeaks.m, val2ind.m, halfwidth.m, fastsmooth.m, and plotit.m in the path.

\href{https://terpconnect.umd.edu/~toh/spectrum/ComparePDAreas.m}{ComparePDAreas.m} compares the effect of digital processing on the areas of a set of peaks measured by the perpendicular drop method. Syntax is \texttt{[P1,P2,coef,R2] = ComparePDAreas(x, orig, processed, PeakSensitivity)}, where x=independent variable (e.g., time); orig = original signal y values; processed = processed signal y values; P1 = peak table of original signal; P2 = peak table of processed signal; PeakSensitivity = approximate number of peaks that would fit into the entire x-axis range (larger numbers {\textgreater} more peak detected). Displays a scatter plot of original areas vs processed areas for each peak and returns the peak tables, P1 and P2 respectively, and the slope, intercept, and R\textsuperscript{2 }values, which should ideally be 1,0, and 1, if the processing has no effect at all on peak area.

\href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{iSignal} (page \pageref{ref-0439}) is my downloadable Matlab function that performs various signal processing functions described in this tutorial, including one-at-a-time manual measurement of peak area using Simpson\index{Simpson}'s Rule and the perpendicular drop method. Click to view or right-click {\textgreater} Save link as... \href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{here}, or you can download the \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal7.zip}{ZIP file} with sample data for testing. The animated GIF iSignalAreaAnimation.gif (\href{https://terpconnect.umd.edu/~toh/spectrum/iSignalAreaAnimation.gif}{click to view}) shows iSignal applying the perpendicular drop method to a series of four peaks of equal area. (Look at the bottom panel to see how the measurement intervals, marked by the vertical dotted magenta lines, are positioned at the valley minimum on either side of each of the four peaks). It also has a built-in peak fitter, activated by the \textbf{Shift-F} key, based on \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m}, that measures the areas of overlapping peak of known shape. There is also an \textit{automatic} peak finding function based on the \href{https://terpconnect.umd.edu/~toh/spectrum/autopeaks.m}{autopeaks }function, activated by the \textbf{J} or \textbf{Shift-J} keys, which displays a \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest.txt}{table }listing the peak number, position, absolute peak height, peak-valley difference, perpendicular drop area, and tangent skim area of each peak in the signal.

\href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit}, a command-line function for multiple peak fitting by iterative non-linear least-squares. It measures the peak position, height, width, and area of overlapping peaks, and it has several ways to \href{https://terpconnect.umd.edu/~toh/spectrum/Integration.html\#baseline}{correct for non-zero baselines.} For best results, it requires that the peak shape of your peaks be among those \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#PeakShapes}{listed here}.

\href{https://terpconnect.umd.edu/~toh/spectrum/PeakCalibrationCurve.m}{PeakCalibrationCurve.m} is a Matlab/Octave simulation of the calibration of a flow injection or chromatography system that produces signal peaks that are related to an underlying concentration or amplitude ('amp'). The \href{https://terpconnect.umd.edu/~toh/spectrum/measurepeaks.m}{measurepeaks.m} function is used to determine the absolute peak height, peak-valley difference, perpendicular drop area, and tangent skim area. The Matlab/Octave script \href{https://terpconnect.umd.edu/~toh/spectrum/PeakShapeAnalyticalCurve.m}{PeakShapeAnalyticalCurve.m} shows that, for a single isolated peak whose shape is constant and independent of concentration, if the wrong model shape is used, the peak heights measured by curve fitting will be inaccurate, but that error will be exactly the same for the unknown samples and the known calibration standards, so the error will ``cancel out'' and the measured concentrations will be accurate, provided you use the same inaccurate model for both the known standards and the unknown samples. See page \pageref{ref-0401}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/PowerTransformTest.m}{PowerTransformTest.m} is a simple script that demonstrates the \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html\#power}{power method} of peak sharpening to aid in reducing in peak overlap. The scripts \href{https://terpconnect.umd.edu/~toh/spectrum/PowerMethodGaussian.m}{PowerMethodGaussian.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/PowerMethodLorentzian.m}{PowerMethodLorentzian.m} compare the power methods to deconvolution, for Gaussian and Lorentzian peak, respectively. \href{https://terpconnect.umd.edu/~toh/spectrum/PowerMethodCalibrationCurve.m}{PowerMethodCalibrationCurve} is a variant of \href{https://terpconnect.umd.edu/~toh/spectrum/PeakCalibrationCurve.m}{PeakCalibrationCurve.m} that evaluates the \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html\#power}{power method} in the context of a flow injection or chromatography measurement. The self-contained function \href{https://terpconnect.umd.edu/~toh/spectrum/PowerMethodDemo.m}{PowerMethodDemo.m} demonstrates the power method for measuring the area of small shouldering peak that is partly overlapped by a much stronger interfering peak (\href{https://terpconnect.umd.edu/~toh/spectrum/PowerMethod2.png}{Graphic}). It also demonstrates the effect of random noise, smoothing, and any uncorrected background under the peaks.

\href{https://terpconnect.umd.edu/~toh/spectrum/AsymmetricalAreaTest.m}{AsymmetricalAreaTest.m}. Test of accuracy of peak area measurement methods for an asymmetrical peak, comparing (A) Gaussian estimation, (B) triangulation, (C) perpendicular drop method, and curve fitting by (D) exponentially broadened Gaussian, and (E) two overlapping Gaussians. Must have the following functions in the Matlab/Octave path: gaussian.m, expgaussian.m, findpeaksplot.m, findpeaksTplot.m, autopeaks.m, and peakfit.m. Related script \href{https://terpconnect.umd.edu/~toh/spectrum/AsymmetricalAreaTest2.m}{AsymmetricalAreaTest2.m} compares the standard deviations of those same methods with randomized noise samples. 

\href{https://terpconnect.umd.edu/~toh/spectrum/functions.html}{ SumOfAreas.m}. Demonstrates that even drastically non-Gaussian peaks can be fit with up to five overlapping Gaussian components, and that the total area of the components approaches the area under the non-Gaussian peak as the number of components increases (\href{https://terpconnect.umd.edu/~toh/spectrum/SumOfAreas.png}{graphic}). In most cases only a few components are necessary to obtain a good estimate of the peak area. 

\subsection{Linear Least-squares\label{ref-0515}}

\href{https://terpconnect.umd.edu/~toh/spectrum/TestLinearFit\%20effect\%20of\%20number\%20of\%20points.txt}{TestLinearFit effect of number of points.txt}. Effect of sample size on least-square error estimates by Monte Carlo Simulation, Algebraic propagation-of-errors, and the bootstrap method, using the Matlab script \href{https://terpconnect.umd.edu/~toh/spectrum/TestLinearFit.m}{TestLinearFit.m}.

\href{https://terpconnect.umd.edu/~toh/spectrum/LeastSquaresCode.txt}{LeastSquaresCode.txt}. Simple pseudocode for calculating the first-order least-square fit of y vs x, including the Slope and Intercept and the predicted standard deviation of the slope (SDslope) and intercept (SDintercept).

\href{https://terpconnect.umd.edu/~toh/spectrum/CalibrationQuadraticEquations.txt}{CalibrationQuadraticEquations.txt}. Simple pseudocode for calculating the second order least-square fit of y vs x, including the constant, x, and x2 terms.

\href{https://terpconnect.umd.edu/~toh/spectrum/plotit.m}{plotit}, version 2, (previously named 'plotfit'), is a function for plotting x,y data in matrices or in separate vectors. It optionally fits the data with a polynomial of order \textit{n} if \textit{n} is included as the third input argument. In \textbf{version 6} the syntax is \texttt{[coef, RSquared, StdDevs] =} \texttt{plotit(x,y)} \texttt{or plotit(x,y,}\texttt{n}\texttt{)} or optionally \texttt{plotit(x, y,} \texttt{n, datastyle, fitstyle}\texttt{)}, where \texttt{data}\texttt{style} and \texttt{fitstyle} are optional strings specifying the line and symbol style and color, in standard Matlab convention. For example, \texttt{plotit(x,y,3,'or','-g')} plots the data as red circles and the fit as a green solid line (the default is red dots and a blue line, respectively). Plotit returns the best-fit coefficients '\texttt{coeff}', in decreasing powers of x, the standard deviations of those coefficients '\texttt{StdDevs'} in the same order, and the R-squared value. Type "help plotit" at the command prompt for syntax options. See page \pageref{ref-0238}. This function works in Matlab or Octave and has a built-in bootstrap routine that computes coefficient error estimates (STD and \% RSD of each coefficient) by the bootstrap method and returns the results in the matrix "BootResults" (of size 5 x polyorder+1). The calculation is triggered by including a 4\textsuperscript{th} output argument, e.g. \texttt{[coef, RSquared, StdDevs, BootResults]= plotit(x,y,polyorder)}. This works for any positive integer polynomial order. The variation \href{https://terpconnect.umd.edu/~toh/spectrum/plotfita.m}{plotfita} animates the bootstrap process for instructional purposes. The variation \href{https://terpconnect.umd.edu/~toh/spectrum/logplotfit.m}{logplotfit} plots and fits log(x) vs log(y), for data that follows a \href{http://en.wikipedia.org/wiki/Power\_law}{power law relationship} or that covers a very wide numerical range.

\href{https://terpconnect.umd.edu/~toh/spectrum/RSquared.m}{RSquared.m} Computes the R\textsuperscript{2} (Rsquared or correlation coefficient) in both Matlab and Octave. Syntax \texttt{RS=RSquared(polycoeff, x,y)}.

\href{https://terpconnect.umd.edu/~toh/spectrum/trypoly.m}{trypoly(x,y)} fits the data in x,y with a series of polynomials of degree 1 through length(x)-1 and returns the coefficients of determination (R\textsuperscript{2}) of each fit as a vector, allowing you to evaluate how polynomials of various orders fit your data. To plot as a bar graph, write \texttt{b}\texttt{ar(trypoly(x,y)); xlabel('Polynomial Order'); ylabel('Coefficient of Determination (R2)')}. \href{https://terpconnect.umd.edu/~toh/spectrum/trypoly.png}{Click for an example}. See related function \href{https://terpconnect.umd.edu/~toh/spectrum/testnumpeaks.m}{testnumpeaks.m}.

\href{https://terpconnect.umd.edu/~toh/spectrum/trydatatrans.m}{trydatatrans(x,y,polyorder)} tries 8 different simple data transformations on the data x,y, fits the transformed data to a polynomial of order 'polyorder', displays results\href{https://terpconnect.umd.edu/~toh/spectrum/trydatatrans.png}{ graphically in 3 x 3 array of small plots} and returns all the R\textsuperscript{2} values in a vector.

\href{https://terpconnect.umd.edu/~toh/spectrum/LinearFiMC.m}{LinearFiMC.m}, a script that compares standard deviation of slope and intercept for a first order least-squares fit computed by random-number simulation of 1000 repeats to predictions made by closed-form algebraic equations. See page \pageref{ref-0205}.

\href{https://terpconnect.umd.edu/~toh/spectrum/TestLinearFit.m}{TestLinearFit.m}, a script that compares standard deviation of slope and intercept for a first-order least-squares fit computed by random-number simulation of 1000 repeats to predictions made by closed-form algebraic equations and to the bootstrap sampling method. Several different noise models can be selected by commenting/uncommenting the code in lines 20-26. See page \pageref{ref-0205}.

\href{https://terpconnect.umd.edu/~toh/spectrum/GaussFitMC.m}{GaussFitMC.m}, a function that demonstrates Monte Carlo simulation of the measurement of the peak height, position, and width of a noisy x,y Gaussian peak. See page \pageref{ref-0229}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/GaussFitMC2.m}{GaussFitMC2.m}, a function that demonstrates measurement of the peak height, position, and width of a noisy x,y Gaussian peak, comparing the gaussfit parabolic fit to the fitgaussian iterative fit. See page \pageref{ref-0229}.

\href{https://terpconnect.umd.edu/~toh/spectrum/SandPfrom1950.mat}{SandPfrom1950.mat} is a MAT file containing the daily value of the \href{http://us.spindices.com/indices/equity/sp-500}{S\&P 500 stock market index} vs time from 1950 through September of 2016. These data are used by \href{https://terpconnect.umd.edu/~toh/spectrum/FitSandP.m}{FitSandP.m} a Matlab/Octave script that performs a least-squares fit of the \href{http://www.investopedia.com/terms/e/exponential-growth.asp}{compound interest equation} to the daily value, V, of the \href{http://us.spindices.com/indices/equity/sp-500}{S\&P 500 stock market index} vs time, T, from 1950 through September of 2016, by two methods: (1) the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{iterative curve fitting method,} and (2) by taking the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#Transforming}{logarithm of the values} and fitting those to a straight line. \href{https://terpconnect.umd.edu/~toh/spectrum/SnPsimulation.m}{SnPsimulation.m}. Matlab/Octave script that simulates the S\&P 500 stock market index by adding proportional random noise to data calculated by the \href{http://www.investopedia.com/terms/e/exponential-growth.asp}{compound interest equation} with a known annual percent return, then fits the equation to that noisy synthetic data by the two methods above. See page \pageref{ref-0390}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/gaussfit.m}{gaussfit.m} function \texttt{[Height, Position, Width]=gaussfit(x,y)}. Takes the natural log of y, fits a parabola (quadratic) to the (x, ln(y)) data, then calculates the position, width, and height of the Gaussian from the three coefficients of the quadratic fit.

\href{https://terpconnect.umd.edu/~toh/spectrum/lorentzfit.m}{lorentzfit.m} function \texttt{[Height, Position, Width]=lorentzfit(x,y)}. Takes the reciprocal of y, fits a parabola (quadratic) to the (x,1/y) data, then calculates the position, width, and height of the Lorentzian from the three coefficients of the quadratic fit.

\href{https://terpconnect.umd.edu/~toh/spectrum/OverlappingPeaks.m}{OverlappingPeaks.m} is a demo script that shows how to use gaussfit.m as a quick way to measure \href{https://terpconnect.umd.edu/~toh/spectrum/OverlappingPeaks.png}{two overlapping partially Gaussian peaks}. It requires careful selection of the optimum data regions around the top of each peak (lines 15 and 16). Try changing the relative position and height of the second peak or adding noise (line 3) and see how it effects the accuracy. This function needs the gaussian.m and gaussfit.m functions in the path. \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{Iterative methods} work much better in such cases, but they are slower.\href{https://terpconnect.umd.edu/~toh/spectrum/functions.html\#Top}{}

\subsection{Peak Finding and Measurement\label{ref-0516}}

\href{https://terpconnect.umd.edu/~toh/spectrum/allpeaks.m}{allpeaks.m}. \texttt{allpeaks(x,y)} A super-simple peak detector for x,y, data sets that lists every \textit{y} value that has lower \textit{y} values on both sides; \href{https://terpconnect.umd.edu/~toh/spectrum/allvalleys.m}{allvalleys.m} is the same for valleys, lists every \textit{y} value that has \textit{higher y} values on both sides. 

\href{https://terpconnect.umd.edu/~toh/spectrum/peaksat.m}{peaksat.m}. (\textbf{peaks} \textbf{a}bove \textbf{t}hreshold) lists every y value that (a) has lower y values on both sides and (b) is above the specified threshold. Returns a 2 by \textit{n} matrix P with the x and y values of each peak, where \textit{n} is the number of detected peaks.

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksx.m}{findpeaksx.m}, \texttt{P=findpeaksx(x,y, SlopeThreshold, AmpThreshold, SmoothWidth, FitWidth, smoothtype)} is a simple command-line function to locate and count the positive peaks in noisy data sets. It is an alternative to the findpeaks function in the Signal Processing Toolkit. It detects peaks by looking for downward zero-crossings in the smoothed first derivative that exceed SlopeThreshold and peak amplitudes that exceed AmpThreshold and returns a list (in matrix P) containing the peak number and the position and height of each peak. It can find and count over 10,000 peaks per second in very large signals. Type "help findpeaksx.m". See \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{PeakFindingandMeasurement.htm}. The variant \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksxw.m}{findpeaksxw.m} additionally measures the \href{https://en.wikipedia.org/wiki/Full\_width\_at\_half\_maximum}{width }of the peaks. See the demonstration script \href{https://terpconnect.umd.edu/~toh/spectrum/demofindpeaksxw.m}{demofindpeaksxw.m}. \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{}



\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksG.m}{findpeaksG.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/findvalleys.m}{findvalleys}.m automatically find the peaks or valleys in a signal and measure their position, height, width, and area by curve fitting. The syntax is \texttt{P= findpeaksG(x, y, SlopeThreshold, AmpThreshold, SmoothWidth, FitWidth, smoothtype)}. It returns a matrix containing the peak parameters for each detected peak. For peak of Lorentzian shape, use \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksL.m}{findpeaksL.m} instead. See page \pageref{ref-0292}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksplot.m}{findpeaksplot.m} is a simple variant of findpeaksG.m that also plots the x,y data and numbers the peaks on the graph (if any are found). Syntax: \texttt{findpeaksplot(x, y, SlopeThreshold, AmpThreshold, SmoothWidth, FitWidth, smoothtype)}

\href{https://terpconnect.umd.edu/~toh/spectrum/OnePeakOrTwo.m}{OnePeakOrTwo.m} is a demo script that creates a signal that might be interpreted as either one peak at x=3 on a curved baseline or as two peaks at x=.5 and x=3, depending on context. In this demo, the findpeaksG.m function was called twice, with two different values of SlopeThreshold to demonstrate.

\href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{iPeak} (page \pageref{ref-0319}) automatically finds and measures multiple peaks in a signal. (m-file link: \href{https://terpconnect.umd.edu/~toh/spectrum/ipeak.m}{ipeak.m}). Check out the \href{https://terpconnect.umd.edu/~toh/spectrum/ipeak.html}{Animated step-by-step instructions}. The ZIP file \href{https://terpconnect.umd.edu/~toh/spectrum/ipeak8.zip}{ipeak8.zip} contains several demo scripts (ipeakdemo.m, ipeakdemo1.m, etc.) that illustrate various aspects of the iPeak function and how it can be used effectively. \href{https://terpconnect.umd.edu/~toh/spectrum/testipeak.m}{testipeak.m} is a script that tests for the proper installation and operation of iPeak by running quickly through \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#ipeak}{all eight examples and six demos} for iPeak. Assumes that \href{https://terpconnect.umd.edu/~toh/spectrum/ipeakdata.mat}{ipeakdata.mat} has been loaded into the Matlab workspace. \href{https://terpconnect.umd.edu/~toh/spectrum/animationlarger.gif}{Click for slideshow of examples}. The syntax is \texttt{P=ipeak(DataMatrix, PeakD, AmpT, SlopeT, SmoothW, FitW, xcenter, xrange, MaxError, positions, names)}

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSG.m}{findpeaksSG.m} is a \textit{segmented}\index{\textit{\textcolor{color-3}{segmented}}} variant of \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksG.m}{findpeaksG} with the same syntax, except that the peak detection parameters can be \textit{vectors}, dividing up the signal into regions optimized for peaks of different widths. The syntax is \texttt{P = findpeaksSG(x, y, SlopeThreshold, AmpThreshold, smoothwidth, peakgroup, smoothtype).} This works better than findpeaksG when the peak widths vary greatly over the duration of the signal. The script \href{https://terpconnect.umd.edu/~toh/spectrum/TestPrecisionFindpeaksSG.m}{TestPrecisionFindpeaksSG.m} demonstrates the application. \href{https://terpconnect.umd.edu/~toh/spectrum/TestPrecisionFindpeaskSG.png}{Graphic}. See page \pageref{ref-0397}.

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSGw.m}{\textbf{findpeaksSGw.m}}  is like the above except that is uses \textit{wavelet denoising} (page \pageref{ref-0177}) instead of smoothing. It takes the wavelet level rather than the smooth width as an input argument. The script \href{https://terpconnect.umd.edu/~toh/spectrum/TestPrecisionFindpeaksSGvsW.m}{TestPrecisionFindpeaksSGvsW.m} compares the precision and accuracy for peak position and height measurement.

\href{https://terpconnect.umd.edu/~toh/spectrum/autofindpeaks.m}{autofindpeaks.m} (and \href{https://terpconnect.umd.edu/~toh/spectrum/autofindpeaksplot.m}{autofindpeaksplot.m}) are similar to findpeaksSG.m except that you can \textit{leave out the peak detection parameters} and just write ``autofindpeaks(x,y)'' or autofindpeaks(x,y,\textit{n}) where \textit{n} is the peak capacity, roughly the number of peaks that would fit into that signal record (greater \textit{n} looks for many narrow peaks; smaller \textit{n} looks for fewer wider peaks). It also prints out the input argument list for use with any of the findpeaks... functions. In version 1.1, you can call autofindpeaks with the output arguments [P,A] and it returns the calculated peak detection parameters as a 4-element row vector A, which you can then pass on to other functions such as measurepeaks, effectively giving that function the ability to calculate the peak detection parameters from a single number n . For example:

\texttt{x=[0:.1:50];}

\texttt{y=5+5.*sin(x)+randn(size(x));} 

\texttt{[P,A]=autofindpeaks(x,y,3);} 

\texttt{P=measurepeaks(x,y,A(1),A(2),A(3),A(4),1);}

Type "help autofindpeaks" and run the examples. The script \href{https://terpconnect.umd.edu/~toh/spectrum/testautofindpeaks.m}{testautofindpeaks.m} runs all the examples in the help file, additionally plotting the data and numbering the peaks (like autofindpeaksplot.m). \href{https://terpconnect.umd.edu/~toh/spectrum/testautofindpeaks.gif}{Graphic animation}.

\href{https://terpconnect.umd.edu/~toh/spectrum/autopeaks.m}{[M,A]=autopeaks.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/autopeaksplot.m}{autopeaksplot.m}. Peak detection and height and area measurement for peaks of arbitrary shape in x,y time series data. The syntax is \texttt{[P, DetectionParameters] =} 

\texttt{autofindpeaks(x, y, SlopeThreshold, AmpThreshold, smoothwidth, peakgroup, smoothtype),} but like autofindpeaks.m, the peak detection parameters SlopeThreshold, AmpThreshold, smoothwidth peakgroup, and smoothtype can be omitted and the function will calculate estimated initial values. Uses the measurepeaks.m algorithm for measurement, returning a \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest.txt}{table }in the matrix M containing the peak number, position, absolute peak height, peak-valley difference, perpendicular drop area, and tangent skim area of each peak. Optionally returns the peak detection parameters that it calculates in the vector A. Using the simple syntax M=autopeaks(x,y) works well in some cases, but if not try M=autopeaks(x,y,\textit{n}), using different values of \textit{n} (roughly the number of peaks that would fit into the signal record) until it detects the peaks that you want to measure. For the most precise control over peak detection, you can specify all the peak detection parameters by typing M=autopeaks(x,y, SlopeThreshold, AmpThreshold, smoothwidth, peakgroup). \href{https://terpconnect.umd.edu/~toh/spectrum/autopeaksplot.m}{autopeaksplo}\href{https://terpconnect.umd.edu/~toh/spectrum/autopeaksplot.m}{t.m} is the same but it also \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest.png}{plots the signal} and the \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest2.png}{individual peaks} (in blue) with the maximum (red circles), valley points (magenta), and tangent lines (cyan) marked. The script \href{https://terpconnect.umd.edu/~toh/spectrum/testautopeaks.m}{testautopeaks.m} runs all the examples in the autopeaks help file, with a 1-second pause between each one, printing out results in the command window and additionally plotting and numbering the peaks (Figure window 1) and each individual peak (Figure window 2); it requires \href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/fastsmooth.m}{fastsmooth.m} in the path. \href{https://terpconnect.umd.edu/~toh/spectrum/iSignal.html}{iSignal} (page \pageref{ref-0439}) has a peak finding function based on the \href{https://terpconnect.umd.edu/~toh/spectrum/autopeaks.m}{autopeaks }function, activated by the \textbf{J} or \textbf{Shift-J} keys, which displays a \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest.txt}{table }of peak number, position, absolute peak height, peak-valley difference, perpendicular drop area, and tangent skim area of each peak in the signal.

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksG2d.m}{findpeaksG2d.m} is a variant of findpeaksSG that can be used to locate the positive peaks \textit{and shoulders} in a noisy x-y time series data set. Detects peaks in the negative of the \textit{second} derivative of the signal, by looking for downward slopes in the \textit{third} derivative that exceed SlopeThreshold. See \href{https://terpconnect.umd.edu/~toh/spectrum/TestFindpeaksG2d.m}{TestFindpeaksG2d.m}. Syntax: \texttt{P = findpeaksG2d(x, y, SlopeThreshold, AmpThreshold, smoothwidth, peakgroup, smoothtype)}

\href{https://terpconnect.umd.edu/~toh/spectrum/measurepeaks.m}{measurepeaks.m} automatically detects peaks in a signal, like \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSG.m}{findpeaksSG}. \texttt{M = measurepeaks(x, y, SlopeThreshold, AmpThreshold, SmoothWidth, FitWidth, plots)}. It returns a \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest.txt}{table M} of peak number, position, absolute peak height, peak-valley difference, perpendicular drop area, and tangent skim area of each peak. It can \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest.png}{plot the signal} and the \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndAreaTest2.png}{individual peaks} if the last (7\textsuperscript{th}) input argument is 1. Type ``help measurepeaks'' and try the seven examples there or run \href{https://terpconnect.umd.edu/~toh/spectrum/HeightAndArea.m}{HeightAndArea.m} to run a test of the accuracy of peak height and area measurement with signals that have multiple peaks with noise, background, and some peak overlap. Generally, its values for perpendicular drop area are best for peaks that have no background, even if they are slightly overlapped, whereas its values for tangential skim area are better for isolated peaks on a straight or slightly curved background. Note: this function uses \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html}{smoothing }(specified by the SmoothWidth input argument) only for peak \textit{detection}; it performs measurements on the \textit{raw unsmoothed} y data. In some cases, it may be beneficial to smooth the y data yourself before calling measurepeaks.m, using any smooth function of your choice. The script \href{https://terpconnect.umd.edu/~toh/spectrum/testmeasurepeaks.m}{testmeasurepeaks.m} will run all the examples in the measurepeaks help file with a 1-second pause between each (requires measurepeaks.m and gaussian.m in the path). \href{https://terpconnect.umd.edu/~toh/spectrum/testmeasurepeaks.gif}{Graphic animation}. The related functions \href{https://terpconnect.umd.edu/~toh/spectrum/wmeasurepeaks.m}{wmeasurepeaks.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/testwmeasurepeaks.m}{testwmeasurepeaks.m} utilize \textit{wavelet denoising} (page \pageref{ref-0178}) rather than smoothing.

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksT.m}{findpeaksT.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksTplot.m}{findpeaksTplot.m} are variants of findpeaks that measure the peak parameters by constructing a triangle around each peak with sides tangent to the sides of the peak. \href{https://terpconnect.umd.edu/~toh/spectrum/triangulation.png}{Graphic example}.

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksb.m}{findpeaksb.m} is a variant of findpeaksG.m that more accurately measures peak parameters by using iterative least-square curve fitting based on \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m}. This yields better peak parameter values than findpeaks alone, because it fits the entire peak, not just the top part, and because it has provision for 33 different \href{https://terpconnect.umd.edu/~toh/spectrum/Peak\_shape\_functions}{peak shapes} and for background subtraction (linear or quadratic). Works best with isolated peaks that do not overlap. Syntax is \texttt{P = findpeaksb(x,y, SlopeThreshold, AmpThreshold, smoothwidth, peakgroup, smoothtype, window, PeakShape, extra, BASELINEMODE)}. The first seven input arguments are exactly the same as for the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaks}{findpeaksG.m} function; if you have been using findpeaks or iPeak (page \pageref{ref-0319}) to find and measure peaks in your signals, you can use those same input argument values for findpeaksb.m. The demonstration script \href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindPeaksb.m}{DemoFindPeaksb.m} shows how findpeaksb3 works with multiple overlapping peaks. Type "help \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksb.m}{findpeaksb"} at the command prompt. See \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{PeakFindingandMeasurement.htm}. Compare this to the related findpeaksfit.m and findpeaksb3, next. \href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindPeaksb.gif}{Click for slideshow of examples.}

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSb.m}{findpeaksSb.m} is a segmented\index{\textcolor{color-3}{segmented}} variant of findpeaksb.m. It has the same syntax as findpeaksb.m, \texttt{P = findpeaksb(x, y, SlopeThreshold, AmpThreshold, smoothwidth, peakgroup, smoothtype, window, PeakShape, extra, NumTrials, BASELINEMODE)}, except that the input arguments SlopeThreshold, AmpThreshold, smoothwidth, peakgroup, window, width, PeakShape, extra, NumTrials, BaselineMode, and fixedparameters, can all optionally be scalars or vectors with one entry for each segment, in the same manner as \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksSG.m}{findpeaksSG.m}. It returns a matrix P listing the peak number, position, height, width, area, percent fitting error and "R2" of each detected peak. \href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindPeaksSb.m}{DemoFindPeaksSb.m} demonstrates this function by creating a series of Gaussian peaks whose widths increase by a factor of 25-fold and that are superimposed in a curved baseline with random white noise that increases gradually; four segments are used, changing the peak detection and curve fitting values so that all the peaks are measured accurately. \href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindPeaksSbLarge.png}{Graphic}. \href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindpeaksSb.txt}{Printout}. See page \pageref{ref-0397}.

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksb3.m}{findpeaksb3.m} is a variant of findpeaksb.m that fits each detected peak \textit{together with the previous and following peaks} found by findpeaksG.m. It deals better with overlapping peaks than findpeaksb.m does, and it handles larger numbers of peaks better than findpeaksfit.m, but \textit{it fits only those peaks that are found} by findpeaks. The syntax is \texttt{P=findpeaksb3(x,y, SlopeThreshold, AmpThreshold, smoothwidth, peakgroup, smoothtype, PeakShape, extra, NumTrials, BASELINEMODE, ShowPlots).} The first seven input arguments are exactly the same as for the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaks}{findpeaksG.m} function; if you have been using findpeaks or iPeak (page \pageref{ref-0319}) to find and measure peaks in your signals, you can use those same input argument values for findpeaksb3.m. The demonstration script \href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindPeaksb3.m}{DemoFindPeaksb3.m} shows how findpeaksb3 works with multiple overlapping peaks.

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksfit.m}{findpeaksfit.m} is essentially a serial combination of \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaks}{findpeaksG.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit.m}. It uses the number of peaks found by findpeaks and their peak positions and widths as input for the peakfit.m function, which then fits the entire signal with the specified peak model. This deals with non-Gaussian and overlapped peaks better than findpeaks alone. However, it fits only those peaks that are found by findpeaks. The syntax is \texttt{[P, FitResults, LowestError, BestStart, xi, yi] = findpeaksfit(x, y, SlopeThreshold, AmpThreshold, smoothwidth, peakgroup, smoothtype, peakshape, extra, NumTrials, BaselineMode, fixedparameters, plots).}The first seven input arguments are exactly the same as for the \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaks}{findpeaksG.m} function; if you have been using findpeaks or iPeak (page \pageref{ref-0319}) to find and measure peaks in your signals, you can use those same input argument values for findpeaksfit.m. The remaining six input arguments of findpeaksfit.m are for the \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit }function; if you have been using peakfit.m or \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm\#Keypress\_operated\_version:\_ipf.m}{ipf.m} (page \pageref{ref-0461}) to fit peaks in your signals, you can use those same input argument values for findpeaksfit.m. Type "help findpeaksfit" for more information. See page \pageref{ref-0292}. \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksfit.gif}{Click for animated example.}



\href{https://terpconnect.umd.edu/~toh/spectrum/peakstats.m}{peakstats.m} uses the same algorithm as findpeaksG.m, but it computes and returns a table of summary statistics of the peak intervals (the x-axis interval between adjacent detected peaks), heights, widths, and areas, listing the maximum, minimum, average, and percent standard deviation of each, and optionally displaying the x, t data plot with numbered peaks in Figure window 1, the table of peak statistics in the command window, and the histograms of the peak intervals, heights, widths, and areas in \href{https://terpconnect.umd.edu/~toh/spectrum/histograms.png}{Figure window 2}. Type "help peakstats". See page \pageref{ref-0292}. Version 2, March 2016, adds median and mode.

\href{https://terpconnect.umd.edu/~toh/spectrum/tablestats.m}{tablestats.m} (\texttt{PS=tablestats(P, displayit}\texttt{)}) is similar to peakstats.m except that it accepts as input a peak table P such as generated by findpeaksG.m, findvalleys.m, findpeaksL.m, findpeaksb.m, findpeaksplot.m, findpeaksnr.m, findpeaksGSS.m, findpeaksLSS.m, or findpeaksfit.m, any function that return a table of peaks with at least 4 columns listing peak number, height, width, and area. Computes the peak intervals (the x-axis interval between adjacent detected peaks) and the maximum, minimum, average, and percent standard deviation of each, and optionally displaying the histograms of the peak intervals, heights, widths, and areas in \href{https://terpconnect.umd.edu/~toh/spectrum/histograms.png}{Figure window 2}. The optional last argument displayit = 1 if the histograms are to be displayed, otherwise not.

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksnr.m}{findpeaksnr.m} is a variant of findpeaksG.m that additionally computes the \href{https://terpconnect.umd.edu/~toh/spectrum/SignalsAndNoise.html\#SNR}{signal-to-noise ratio} (SNR) of each peak and returns it in the 5\textsuperscript{th} column of the peak table. The SNR is computed as the ratio of the peak height to the root-mean-square residual (difference between the actual data and the least-squares fit over the top part of the peak). See \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{PeakFindingandMeasurement.htm}\textcolor{color-3}{.}

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksE.m}{findpeaksE.m} is a variant of findpeaksG.m that additionally estimates the percent relative fitting error of each peak (assuming a Gaussian peak shape) and returns it in the 6\textsuperscript{th} column of the peak table.

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksGSS.m}{findpeaksGSS.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksLSS.m}{findpeaksLSS.m}, for Gaussian and Lorentzian peaks respectively, are variants of findpeaksG.m and findpeaksL.m that additionally compute the 1\% start and end positions return them in the 6\textsuperscript{th} and 7\textsuperscript{th}columns of the peak table. See \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm}{PeakFindingandMeasurement.htm}\textcolor{color-3}{.}

\href{https://terpconnect.umd.edu/~toh/spectrum/findsquarepulse.m}{findsquarepulse.m} (syntax \texttt{S=findsquarepulse(t, y, threshold}) locates the rectangular pulses in the signal t, y that exceed a y-value of "threshold" and determines their start time, average height (relative to the adjacent baseline) and width. \href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindsquare.m}{DemoFindsquare.m} creates a test signal and calls findsquarepulse.m to demonstrate.

\href{https://terpconnect.umd.edu/~toh/spectrum/findsteps.m}{findsteps.m} \texttt{P= findsteps(x, y, SlopeThreshold, AmpThreshold, SmoothWidth, peakgroup)} locates positive transient steps in noisy x-y time series data, by computing the first derivative of y that exceed SlopeThreshold, computes the step height as the difference between the maximum and minimum y values over a number of data point equal to "Peakgroup". It returns list (P) with step number, x and y positions of the bottom and top of each step, and the step height of each step detected; "SlopeThreshold" and "AmpThreshold" control step sensitivity; higher values will neglect smaller features. Increasing "SmoothWidth" ignores small sharp false steps caused by random noise or by "glitches" in the data acquisition. See \href{http://terpconnect.umd.edu/~toh/spectrum/findsteps.png}{findsteps.png} for a real example. \href{https://terpconnect.umd.edu/~toh/spectrum/findstepsplot.m}{findstepsplot.m} plots the signal and numbers the peaks.

\href{https://terpconnect.umd.edu/~toh/spectrum/idpeaks.m}{idpeaks}, peak identification function. The syntax is \texttt{[IdentifiedPeaks, AllPeaks] = idpeaks(DataMatrix, AmpT, SlopeT, sw, fw, maxerror, Positions, Names)}. Locates and identifies peaks in \texttt{DataMatrix} that match the position of peaks in the array "\texttt{Positions}" with matching names "\texttt{Names}". Type "\texttt{help idpeaks}" for more information. Download and extract \href{https://terpconnect.umd.edu/~toh/spectrum/idpeaks.zip}{idpeaks.zip} for a working example or see \textbf{Example 8} on page \pageref{ref-0317}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/idpeaktable.m}{idpeaktable.m} \texttt{[IdentifiedPeaks]=idpeaktable(P, maxerror, Positions, Names)}. Compares the found peak positions in peak table "P" to a database of known peaks, in the form of a cell array of known peak maximum positions ("Positions") and matching cell array of names ("Names"). If the position of a found peak in the signal is closer to one of the known peaks by less than the specified maximum error ("maxerror"), that peak is considered a match and its peak position, name, error, and amplitude are entered into the output cell array "IdentifiedPeaks". The peak table may be one returned by any of my peak finder or peak fitting functions, having one row for each peak and columns for peak number, position, and height as the first three columns.

\href{https://terpconnect.umd.edu/~toh/spectrum/demoipeak.m}{demoipeak.m} is a simple demo script that generates a noisy signal with peaks, calls iPeak, and then prints out a table of the actual peak parameters and a list of the peaks detected and measured by iPeak for comparison. Before running this demo, \href{https://terpconnect.umd.edu/~toh/spectrum/ipeak.m}{ipeak.m} (page \pageref{ref-0319}) must be downloaded and placed in the Matlab path.

\href{https://terpconnect.umd.edu/~toh/spectrum/DemoFindPeak.m}{DemoFindPeak.m}, a demonstration script using the findpeaksG function on noisy synthetic data. Numbers the peaks and prints out the peak parameters in the command window. Requires that \href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaks.m}{findpeaksG.m} be present in the path. See page \pageref{ref-0292}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/TestFindpeaksG2d.m}{TestFindpeaksG2d.m}. Demonstration script for findpeaks2d.m, which shows that this function can locate peaks resulting in 'shoulders' that do not produce a distinct maximum in the original signal. Detects peaks in the negative of the smoothed second derivative of the signal (shown as the dotted line in the figure). Requires gaussian.m, findpeaksG.m, findpeaksG2d.m, fastsmooth.m, and peakfit.m in the path. \href{https://terpconnect.umd.edu/~toh/spectrum/TestFindpeaksG2d.png}{Graphic.} Also uses the\href{https://terpconnect.umd.edu/~toh/spectrum/TestFindpeaksG2d.m}{TestFindpeaksG2d} results as the "start" value for iterative peak fitting using \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit.m}, which takes longer to compute but gives more accurate results, especially for width and area:\href{https://terpconnect.umd.edu/~toh/spectrum/TestFindpeaksG2d.png}{}



\href{https://terpconnect.umd.edu/~toh/spectrum/TestFindpeaksG2d.png}{DemoFindPeakSNR} is a variant of DemoFindPeak.m that uses \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksnr.m}{findpeaksnr.m} to compute the signal-to-noise ratio (SNR) of each peak and returns it in the 5\textsuperscript{th} column. 

\href{https://terpconnect.umd.edu/~toh/spectrum/triangulationdemo.m}{triangulationdemo.m} is a demo function (\href{https://terpconnect.umd.edu/~toh/spectrum/TriangulationComparison.png}{screen graphic}) that compares \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksG.m}{findpeaksG} (which determines peak parameters by curve-fitting a Gaussian to the center of each peak) to \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksT.m}{findpeaksT}, which determines peak parameters by the triangle construction method (drawing a triangle around each peak with sides that are tangent to the sides of the peak). Performs the comparison with 4 different peak shapes: plain Gaussian, bifurcated Gaussian, exponential modified Gaussian, and Breit-Wigner-Fano). In some cases, the triangle construction method can be more accurate than the Gaussian method if the peak shape is asymmetric. 

\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksfitdemo.m}{findpeaksfitdemo.m}, a demonstration script of findpeaksfit automatically finding and fitting the peaks in a set of 150 signals, each of which may have 1 to 3 noisy Lorentzian peaks in variable locations. Requires the findpeaksfit.m and lorentzian.m functions installed. This script was used to generate the GIF animation \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksfit.gif}{findpeaksfit.gif}.

\href{https://terpconnect.umd.edu/~toh/spectrum/FindpeaksComparison.m}{FindpeaksComparison.m}. Which to use: findpeaksG, findpeaksb, findpeaksb3, or findpeaksfit? This script compares all four functions applied to a computer-generated signal with multiple peaks with variable types and amounts of baseline and random noise. (Requires all these functions, plus modelpeaks.m, findpeaksG, and findpeaksL.m, in the Matlab/Octave path. Type "help FindpeaksComparison" for details). \href{https://terpconnect.umd.edu/~toh/spectrum/findpeakscomparison2large.png}{Results are displayed graphically} in Figure windows 1, 2, and 3 and printed out in a \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#FindpeaksComparison}{\textit{table of parameter accuracy and elapsed time for each method}}. You may change the lines in the script marked by {\textless}{\textless}{\textless} to modify the number and character and amplitude of the signal peaks, baseline, and noise. (Adjust the parameters to make the simulated signal like your experimental signal to discover which method works best for your type of signal). The best method depends mainly on the shape and amplitude of the baseline and on the extent of peak overlap.

\href{https://terpconnect.umd.edu/~toh/spectrum/iPeakEnsembleAverageDemo.m}{iPeakEnsembleAverageDemo.m} is a demonstration script for iPeak's ensemble average function. In this example, the signal contains a repeated pattern of two overlapping Gaussian peaks, 12 points apart, both of width 12, with a 2:1 height ratio. These patterns occur at random intervals throughout the recorded signal, and the random noise level is about 10\% of the average peak height. Using iPeak's ensemble average function (\textbf{Shift-E}), the patterns can be averaged and the signal-to-noise ratio significantly improved.

\href{https://terpconnect.umd.edu/~toh/spectrum/ipeakdata.mat}{ipeakdata.mat}, data set for demonstrating idpeaks.m or the peak identification function of iPeak; includes a high-resolution atomic spectrum and a table of known atomic emission wavelenghs. See page \pageref{ref-0292}. 

\textbf{\textit{Which to use: iPeak or Peakfit?}} Try these Matlab demo functions that compare iPeak.m (page \pageref{ref-0319}) with peakfit.m (page \pageref{ref-0291}) for signals with a \href{https://terpconnect.umd.edu/~toh/spectrum/idemo1.m}{few peaks} and signals with \href{https://terpconnect.umd.edu/~toh/spectrum/idemo2.m}{many peaks} and that shows how to adjust iPeak to detect \href{https://terpconnect.umd.edu/~toh/spectrum/idemo.m}{broad or narrow peaks}. These are self-contained demos that include all required sub-functions. Just place them in your path and type their name at the command prompt. You can download all these demos together in \href{https://terpconnect.umd.edu/~toh/spectrum/idemos.zip}{idemos.zip}. They require no input or output arguments.

\href{https://terpconnect.umd.edu/~toh/spectrum/SpikeDemo1.m}{SpikeDemo1.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/SpikeDemo2.m}{SpikeDemo2.m} are Matlab/Octave scripts that demonstrate how two measure spikes (very narrow peaks) in the presence of serious interfering signals. See page \pageref{ref-0359}.

\href{https://terpconnect.umd.edu/~toh/spectrum/PowerTransformTest.m}{PowerTransformTest.m} is a simple script that demonstrates the \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html\#power}{power method} of peak sharpening to aid in reducing in peak overlap. \href{https://terpconnect.umd.edu/~toh/spectrum/PowerMethodCalibrationCurve.m}{PowerMethodCalibrationCurve} is a variant of \href{https://terpconnect.umd.edu/~toh/spectrum/PeakCalibrationCurve.m}{PeakCalibrationCurve.m} that evaluates the power method in the context of a flow injection or chromatography measurement. \href{https://terpconnect.umd.edu/~toh/spectrum/powertest2.m}{powertest2} is a self-contained function that demonstrates the power method for measuring the area of small shouldering peak (\href{https://terpconnect.umd.edu/~toh/spectrum/PowerMethod2.png}{Graphic}). 

The script \href{https://terpconnect.umd.edu/~toh/spectrum/realtimepeak.m}{realtimepeak.m} demonstrates simple real-time peak detection based on derivative zero-crossing, using mouse clicks to simulate data. Each time your mouse clicks form a peak (that is, go up and then down again), the program will register and label the peak on the graph (as illustrated on the right) and print out its \textit{x} and \textit{y} values. In this case, a peak is defined as any data point that has lower amplitude points adjacent to it on both sides, which is determined by the nested 'for' loops in lines 31-36. The more sophisticated script \href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeSmoothedPeakDetectionGauss.zip}{RealTimeSmoothedPeakDetectionGauss.m} uses the technique described on \href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingandMeasurement.htm\#findpeaks}{page} \pageref{ref-0293} that locates the positive peaks in a noisy data set that rise above a set amplitude threshold, performs a least-squares curve-fit of a Gaussian function to the top part of the raw data peak, computes the position, height, and width (FWHM) of each peak from that least-squares fit and prints out each peak found in the command window. (\href{https://terpconnect.umd.edu/~toh/spectrum/RealTimeSmoothedPeakDetectionGauss.gif}{Animated graphic}).

\subsection{Multicomponent Spectroscopy\label{ref-0517}}

\href{https://terpconnect.umd.edu/~toh/spectrum/cls.m}{cls.m} is a classical least-squares function that you can use to fit a computer-generated model, consisting of any number of peaks of known shape, width, and position, but of unknown height, to a noisy x,y signal. The syntax is \texttt{heights= cls(x,y, NumPeaks, PeakShape, Positions, Widths, extra)} where x and y are the vectors of measured signal (e.g. x might be wavelength and y might be the absorbance at each wavelength), 'NumPeaks' is the number of peaks, 'PeakShape' is the peak shape number (1=Gaussian, 2=Lorentzian, 3=logistic, 4=Pearson, 5=exponentially broadened Gaussian; 6=equal-width Gaussians; 7=Equal-width Lorentzians; 8=exponentially broadened equal-width Gaussian, 9=exponential pulse, 10=sigmoid, 11=Fixed-width Gaussian, 12=Fixed-width Lorentzian; 13=Gaussian/Lorentzian blend; 14=BiGaussian, 15=BiLorentzian), 'Positions' is the vector of peak positions on the x axis (one entry per peak), 'Widths' is the vector of peak widths in x units (one entry per peak), and 'extra' is the additional shape parameter required by the exponentially broadened, Pearson, Gaussian/Lorentzian blend, BiGaussian and BiLorentzian shapes. cls.m returns a vector of measured peak heights for each peak. See \href{https://terpconnect.umd.edu/~toh/spectrum/clsdemo.m}{clsdemo.m}. (Note: this method is now included in the non-linear iterative peak fitter \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m} (page \pageref{ref-0291}) as peak shape 50\textit{.} See the demonstration script \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit9demo.m}{\textit{peakfit9demo.m}}\textit{)}

The \href{https://terpconnect.umd.edu/~toh/spectrum/cls2.m}{cls2.m} function is similar to \href{https://terpconnect.umd.edu/~toh/spectrum/cls.m}{cls.m}, except that it also measures the baseline (assumed to be flat) and returns a vector containing the background B and measured peak heights H for each peak , e.g. [B H1 H2 H3...].

\href{https://terpconnect.umd.edu/~toh/spectrum/RegressionDemo.m}{RegressionDemo.m}, script that demonstrates the classical least-squares procedure for a simulated absorption spectrum\index{absorption spectrum} of a 5-component mixture at 100 wavelengths. Requires that \href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian.m} be present in the path. See page \pageref{ref-0242}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/clsdemo.m}{clsdemo.m} is a demonstration script that creates a noisy signal, fits it using the Classical Least-squares method with \href{https://terpconnect.umd.edu/~toh/spectrum/cls.m}{cls.m}, computes the accuracy of the measured heights, then repeats the calculation using \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html}{iterative least-squares} using peakfit.m (page \pageref{ref-0291}) for comparison. (This script requires \href{https://terpconnect.umd.edu/~toh/spectrum/cls.m}{cls.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/modelpeaks.m}{modelpeaks.m}, and \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m} in the Matlab/Octave path).

\href{https://terpconnect.umd.edu/~toh/spectrum/CLSvsINLS.m}{CLSvsINLS.m} is a script that compares the classical least-squares (CLS) method with three different variations of the iterative method (INLS) method for measuring the peak heights of three Gaussian peaks in a noisy test signal, demonstrating that the fewer the number of unknown parameters, the faster and more accurate is the peak height calculation.

\subsection{Non-linear iterative curve fitting and peak fitting\label{ref-0518}}

\href{https://terpconnect.umd.edu/~toh/spectrum/gaussfit.m}{gaussfit}, function that performs least-squares fit of a single Gaussian function to an x,y data set, returning the height, position, and width of the best-fit Gaussian. Syntax is \texttt{[Height, Position, Width] = gaussfit(x,y).}The similar function \href{https://terpconnect.umd.edu/~toh/spectrum/lorentzfit.m}{lorentzfit.m} performs the calculation for a Lorentzian peak shape. See page \pageref{ref-0226}. The similar function \href{https://terpconnect.umd.edu/~toh/spectrum/plotgaussfit.m}{plotgaussfit} does the same thing as gaussfit.m but also plots the data and the fit. The data set cannot contain any zero or negative values.

\href{https://terpconnect.umd.edu/~toh/spectrum/bootgaussfit.m}{bootgaussfit} is an expanded version of \href{https://terpconnect.umd.edu/~toh/spectrum/gaussfit.m}{gaussfit} that provides optional plotting and error estimation. The syntax is \texttt{[Height, Position, Width, BootResults] = bootgaussfit(x, y, plots)}. If plots=1, plots the raw data as red dots and the best-fit Gaussian as a line. If the 4\textsuperscript{th} output argument (\texttt{BootResults}) is supplied, computes peak parameter error estimates by the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#bootstrap}{bootstrap} method.

\href{https://terpconnect.umd.edu/~toh/spectrum/fitshape2.m}{fitshape2.m}, syntax \texttt{[Positions, Heights, Widths, FittingError] = fitshape2(x, y, start)}, is a simplified general-purpose Matlab/Octave \textit{function} for fitting multiple overlapping model shapes to the data contained in the vector variables x and y. The model is linear combination of any number of basic functions that are defined mathematically as a function of x, with two variables that the program will independently determine for each peak, positions and widths, in addition to the peak heights (i.e., the weights of the weighted sum). You must provide the first guess starting vector 'start', in the form [position1 width1 position2 width2 ...etc.], which specifies the first-guess position and width of each component (one pair of position and width for each peak in the model). The function returns the parameters of the best-fit model in the vectors \texttt{Positions, Heights, Widths}, and computes the percent error between the data and the model in \texttt{FittingError}. It also plots the data as dots and the fitted model as a line. The interesting thing about this function is that \textit{the only part that defines the shape of the model is the last line}. In fitshape2.m, that line contains the expression for a \textit{Gaussian peak of unit height}, but you could change that to \textit{any other expression or algorithm} that computes \textit{g} as a function of \textit{x} with two unknown parameters 'pos' and 'wid' (position and width, respectively, for peak-type shapes, but they could represent anything for other function types, such as the exponential pulse, sigmoidal, etc.); everything else in the \uline{fitshape.m} function can remain the same. This makes fitshape a good platform for experimenting with different mathematical expression as proposed models to fit data. There are also two other variations of this function for models with \textit{one} iterated variable plus peak height (\href{https://terpconnect.umd.edu/~toh/spectrum/fitshape1.m}{fitshape1.m}) and \textit{three} iterated variables plus peak height (\href{https://terpconnect.umd.edu/~toh/spectrum/fitshape3.m}{fitshape3.m}). Each has illustrative examples contained in the built-in help file (type ``help {\textless}filename{\textgreater}'').

\href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit} (page \pageref{ref-0291}) a versatile command-line function for multiple peak fitting by iterative non-linear least-squares. A Matlab File Exchange "\href{http://blogs.mathworks.com/pick/2016/09/09/most-activeinteractive-file-exchange-entry/}{Pick of the Week}". The full syntax is \texttt{[FitResults, GOF, baseline, coeff, BestStart, xi, yi, BootResults] = peakfit(signal, center, window, NumPeaks, peakshape, extra, NumTrials, start, BASELINEMODE, fixedwidth, plots, bipolar, minwidth)}. Type "help peakfit". See page \pageref{ref-0444}. Compared to the fitshape.m function described previously, \href{https://terpconnect.umd.edu/~toh/spectrum/InteractivePeakFitter.htm}{peakfit.m} has a large number of \textit{built-in} peak shapes selected by number, it does not require (although it can be given) the first-guess position and width of each component, and it has features for background correction and other useful features to improve the quality and estimate the reliability of fits. Test the installation on your computer by running the \href{https://terpconnect.umd.edu/~toh/spectrum/autotestpeakfit.m}{auto}\href{https://terpconnect.umd.edu/~toh/spectrum/autotestpeakfit.m}{testpeakfit.m} script, which runs through the whole gauntlet of fitting tests without pause, printing out what it is doing and the results, checking to see if the fitting error is greater than expected and printing out a WARNING if it is. This takes 17 seconds to run in Matlab 9.9 2020b on a 3.5Ghx i7 windows 10 machine. See the \href{https://terpconnect.umd.edu/~toh/spectrum/peakfitVersionHistory.txt}{version history}, page \pageref{ref-0445}, for a brief description of the new features of each version of peakfit.m from 3.7 to the present. 

\href{https://terpconnect.umd.edu/~toh/spectrum/testnumpeaks.m}{testnumpeaks}(x,y,peakshape,extra,NumTrials,MaxPeaks). Simple test to estimate the number of model peaks required to fit an x,y data set. Fits data x,y, with shape "peakshape", with optional extra shape factor "extra", with NumTrials repeat per fit, up to a maximum of "MaxPeaks" model peaks, displays each fit and graphs fitting error vs number of model peaks. If two or more numbers of peaks give about the same error, its best to take the smaller number. 

\href{https://terpconnect.umd.edu/~toh/spectrum/SmoothVsFit.m}{SmoothVsFit.m} is a demonstration script that compares iterative least-square fitting to two simpler methods for the measurement of the peak height of a single peak of uncertain width and position and with a very poor signal-to-noise ratio of 1. The accuracy and precision of the methods are compared. \href{https://terpconnect.umd.edu/~toh/spectrum/SmoothVsFitArea.m}{SmoothVsFitArea.m} does the same thing for the measurement of peak area. See page \pageref{ref-0229}.

\href{https://terpconnect.umd.edu/~toh/spectrum/ipf.m}{ipf.m} (page \pageref{ref-0461}) is an interactive multiple peak fitter (m-file link: \href{https://terpconnect.umd.edu/~toh/spectrum/ipf.m}{ipf.m}). It uses iterative nonlinear least-squares to fit any number of overlapping peaks of the same or different peak shapes to x-y data sets. \href{https://terpconnect.umd.edu/~toh/spectrum/Demoipf.m}{Demoipf.m} is a demonstration script for ipf.m, with a built-in simulated signal generator. The true values of the simulated peak positions, heights, and widths are displayed in the Matlab command window, for comparison to the FitResults obtained by peak fitting. C\href{https://terpconnect.umd.edu/~toh/spectrum/ifpinstructions.html}{lick for animated step-by-step instructions}. You can also download a \href{https://terpconnect.umd.edu/~toh/spectrum/ipf13.zip}{ZIP file} containing ipf.m plus some examples and demos. \href{https://terpconnect.umd.edu/~toh/spectrum/ipfE.gif}{Click for animated example}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/SmallPeak.m}{SmallPeak.m} is a demonstration of several curve-fitting techniques applied to the challenging problem of measuring the height of a small peak that is closely overlapped with and completely obscured by a much larger peak. It compares iterative fits by unconstrained, equal-width, and fixed-position models (using \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m}, page \pageref{ref-0291}) to a classical least-squares fit in which \textit{only} the peak heights are unknown (using \href{https://terpconnect.umd.edu/~toh/spectrum/cls.m}{cls.m}). Spread out the four Figure windows so you can observe the dramatic difference in stability of the different methods. A final table of relative percent peak height errors shows that the more the constraints, the better the results (but \textit{only if the constraints are justified}). See page \pageref{ref-0387}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/BlackbodyDataFit.m}{BlackbodyDataFit.m}, a script that demonstrates iterative least-squares fitting of the \textit{blackbody equation} to a measured spectrum of an incandescent body, for the purpose of estimating its color temperature. See page \pageref{ref-0265}.

\href{https://terpconnect.umd.edu/~toh/spectrum/Demofitgauss.m}{Demofitgauss.m} a script that demonstrates iterative fitting a \textit{single} Gaussian function to a set of data, using the fminsearch function. Requires that \href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian.m} and fmsearch.m (in the "Optim 1.2.1" package) be installed. \href{https://terpconnect.umd.edu/~toh/spectrum/Demofitgaussb.m}{Demofitgaussb.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/fitgauss2b.m}{fitgauss2b.m} illustrate a modification of this technique to accommodate shifting baseline (\href{https://terpconnect.umd.edu/~toh/spectrum/Demofitlorentzianb.m}{Demofitlorentzianb.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/fitlorentzianb.m}{fitlorentzianb.m} for Lorentzian peaks). This modification is now incorporated to peakfit.m (version 4.2 and later), ipf.m (version 9.7 and later), findpeaksb.m (version 3 and later), and findpeaksfit, (version 3 and later). See page \pageref{ref-0265}.

\href{https://terpconnect.umd.edu/~toh/spectrum/Demofitgauss2.m}{Demofitgauss2.m} a script that demonstrates iterative fitting of \textit{two} overlapping Gaussian functions to a set of data, using the fminsearch function. Requires that \href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian.m} and fmsearch.m (in the "Optim 1.2.1" package) be installed. Demofitgauss2b.m is the baseline-corrected extension. See page \pageref{ref-0265}.

\href{https://terpconnect.umd.edu/~toh/spectrum/VoigtFixedAlpha.m}{VoigtFixedAlpha.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/VoigtVariableAlpha.m}{VoigtVariableAlpha.m} demonstrate two different ways to fit peaks with \textit{variable shapes}, such as the Voigt profile, Pearson, Gauss-Lorentz blend, and the bifurcated and exponentially-broadened shapes, which are defined not only by a peak position, height, and width, but also by an additional "shape" parameter that fine-tunes the shape of the peak. If that parameter is \textit{equal} for all peaks in a group, it can be passed as an additional input argument to the shape function, as shown in \href{https://terpconnect.umd.edu/~toh/spectrum/VoigtFixedAlpha.m}{VoigtFixedAlpha.m}. If the shape parameter is allowed to be \textit{different} for each peak in the group and is to be determined by iteration (just as is position and width), then the routine must be modified to accommodate \textit{three}, rather than \textit{two}, iterated variables, as shown in \href{https://terpconnect.umd.edu/~toh/spectrum/VoigtVariableAlpha.m}{VoigtVariableAlpha.m}. Although the \textit{fitting error is lower} with variable alphas, the execution time is longer and the \textit{alphas values so determined are not very stable}, with respect to noise in the data and the starting guess values, especially for multiple peaks. See page \pageref{ref-0263}. The script \href{https://terpconnect.umd.edu/\%7Etoh/spectrum/VoigtShapeFittingDemonstration.m}{VoigtShapeFittingDemonstration.m} uses peakfit.m version 9.5 to fit a single Voigt profile and to calculate the Gaussian width component, Lorentzian width component, and alpha. It computes the theoretical Voigt profile and adds random noise for realism. \href{https://terpconnect.umd.edu/\%7Etoh/spectrum/VoigtShapeFittingDemonstration2.m}{VoigtShapeFittingDemonstration2.m} does the same for two overlapping Voigt profiles, using both fixed alpha and variable alpha models (shape numbers 20 and 30). (Requires voigt.m, halfwidth.m, and peakfit.m in the path).

\href{https://terpconnect.umd.edu/~toh/spectrum/Demofitmultiple.m}{Demofitmultiple.m}. Demonstrates an iterative fit to sets of computer-generated noisy peaks of different types, knowing only the shape types and variable shape parameters of each peak. Iterated parameters are shape, height, position, and width of all peaks. Requires the \href{https://terpconnect.umd.edu/~toh/spectrum/fitmultiple.m}{fitmultiple.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/peakfunction.m}{peakfunction.m} functions. \href{https://terpconnect.umd.edu/~toh/spectrum/Demofitmultiple.png}{View screen shot}. See page \pageref{ref-0263}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/BootstrapIterativeFit.m}{BootstrapIterativeFit.m}, a function that demonstrates bootstrap estimation of the variability of an iterative least-squares fit to a single noisy Gaussian peak. The syntax is: \texttt{BootstrapIterativeFit(TrueHeight, TruePosition, TrueWidth, NumPoints, Noise, NumTrials)}. See page \pageref{ref-0215}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/BootstrapIterativeFit2.m}{BootstrapIterativeFit2.m}, a function that demonstrates bootstrap estimation of the variability of an iterative least-squares fit to two noisy Gaussian peaks. The syntax is: \texttt{BootstrapIterativeFit2(TrueHeight1, TruePosition1, TrueWidth1, TrueHeight2, TruePosition2, TrueWidth2, NumPoints, Noise, NumTrials)}. See page \pageref{ref-0215}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/DemoPeakfitBootstrap.m}{DemoPeakfitBootstrap.m}. Self-contained demonstration function for peakfit.m (page \pageref{ref-0291}), with built-in signal generator. Demonstrates bootstrap error estimation. See page \pageref{ref-0215}.

\href{https://terpconnect.umd.edu/~toh/spectrum/DemoPeakfit.m}{DemoPeakfit.m}, Demonstration script (for peakfit.m) that generates an overlapping peak signal, adds noise, fits it with peakfit.m, then computes the accuracy and precision of peak parameter measurements. Requires that \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit}\href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{.m} be present in the path. See page \pageref{ref-0457}.

\href{https://terpconnect.umd.edu/~toh/spectrum/peakfit9demo.m}{peakfit9}\href{https://terpconnect.umd.edu/~toh/spectrum/peakfit9demo.m}{demo}. Demonstrates multilinear regression (shape 50) available in peakfit.m version 9 (Requires modelpeaks.m and peakfit.m in the Matlab path). Creates a noisy model signal of three peaks of known shapes, positions, and widths, but unknown heights. Compares multilinear regression in Figure window 1 with unconstrained iterative non-linear least-squares in Figure window 2. For shape 50, the 10th input argument fixedparameters must be a \textit{matrix} listing the peak shape (column 1), position (column 2), and width (column 3) of each peak, one row per peak. \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit9demoL.m}{peakfit9demoL} is similar but uses Lorentzian peaks (specified in the fixedparameters matrix and in the PeakShape vector).

\href{https://terpconnect.umd.edu/~toh/spectrum/DemoPeakFitTime.m}{DemoPeakFitTime.m} is a simple script that demonstrates how to use peakfit.m to apply \textit{multiple curve fits to a signal that is changing with time}. The signal contains two noisy Gaussian peaks in which the peak position of the \textit{second} peak increases with time and the other parameters remain constant (except for the noise). The script creates a set of 100 noisy signals (on line 5) containing two Gaussian peaks where the position of the \textit{second} peak changes with time (from x=6 to 8) and the \textit{first} peak remains the same. Then it fits a 2-Gaussian model to each of those signals (on line 8), displays the signals and the fits graphically with time as a kind of animation (\href{https://terpconnect.umd.edu/~toh/spectrum/DemoPeakFitTime.gif}{click to play animation}), then plots the measured peak position of the two peaks vs time on line 12.

\href{https://terpconnect.umd.edu/~toh/spectrum/isignal.m}{isignal} (page \pageref{ref-0439}) can be used as a command-line function in Octave, but its \textit{interactive features currently work only in Matlab}. The syntax is \texttt{isignal(DataMatrix, xcenter, xrange, SmoothMode, SmoothWidth, ends, DerivativeMode, Sharpen, Sharp1, Sharp2, SlewRate, MedianWidth).} 

\href{https://terpconnect.umd.edu/~toh/spectrum/testpeakfit.m}{testpeakfit.m}, a test script that demonstrates 36 different examples on page \pageref{ref-0461}. Use for testing that peakfit and related functions are present in the path. Updated for \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit 6}. \href{https://terpconnect.umd.edu/~toh/spectrum/autotestpeakfit.m}{autotestpeakfit.m} does the same without pausing between functions and waiting for a keypress (takes about 17 seconds to run).

\textbf{Multiple peak fits with different profiles}. \href{https://terpconnect.umd.edu/~toh/spectrum/ShapeTestS.m}{ShapeTestS.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/ShapeTestA.m}{ShapeTestA.m} tests the data in its input arguments x,y, assumed to be a single isolated peak, fits it with \textit{different candidate model peak shapes} using peakfit.m, plots each fit in a separate figure window, and prints out a table of fitting errors in the command window. \href{https://terpconnect.umd.edu/~toh/spectrum/ShapeTestS.m}{ShapeTestS.m} tries seven different candidate symmetrical model peaks, and \href{https://terpconnect.umd.edu/~toh/spectrum/ShapeTestS.m}{ShapeTestA.m} tries six different candidate asymmetrical model peaks. The one with the lowest fitting error (and R\textsuperscript{2 }closest to 1.000) is presumably the best candidate. \textit{Try the examples in their help files}. But beware: if there is too much noise in your data, the results can be misleading. For example, a multiple Gaussians model is likely to fit best because it has more degrees of freedom and can "fit the noise", even if the \textit{actual} peak shape is something other than a Gaussian. (The function peakfit.m has many more built-in shapes to choose from, but still it is a \textit{finite} list and there is always the possibility that the actual underlying peak shape is not available in the software you are using or that it is simply not describable by a single mathematical function).

\href{https://terpconnect.umd.edu/~toh/spectrum/widthtest.m}{WidthTest.m} is a script that demonstrates that constraining some of the peak parameters of a fitting model to fixed values, \textit{if} those values are accurately known, improves that accuracy of measurement of the \textit{other} parameters, even though it \textit{increases} the fitting error. Requires installation of the \href{https://terpconnect.umd.edu/~toh/spectrum/GL.m}{GL.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m} functions (version 7.6 or later) in the Matlab/Octave path. 

The script \href{https://terpconnect.umd.edu/~toh/spectrum/NumPeaksDemo.m}{NumPeaksDemo.m} demonstrates one way to attempt to estimate the minimum number of model peaks needed to fit a set of data, plotting the fitting error vs the number of model peaks, and looking for the point at which the fitting error reaches a minimum. This script creates a noisy computer-generated signal containing a user-selected 3, 4, 5 or 6 underlying Lorentzian peaks and uses peakfit.m to fit the data to a series of models containing 1 to 10 model peaks. The correct number of underlying peaks is either the fit with the lowest fitting error, or, if two or more fits have about the same fitting error, the fit with the least number of peaks, as in \href{https://terpconnect.umd.edu/~toh/spectrum/NumPeaksTest.png}{this example}, which actually has 4 underlying peaks. If the data are very noisy, however, the determination becomes unreliable. (To make this demo closer to your type of data, you could change Lorentzian to Gaussian or any other model shape, or change the peak width, number of data points, or the noise level). This script requires that \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m} and the \href{https://terpconnect.umd.edu/~toh/spectrum/functions.html\#Peak\_shape\_functions}{appropriate shape functions} (gaussian.m, lorentzian.m, etc.) be present in the path. The function \href{https://terpconnect.umd.edu/~toh/spectrum/testnumpeaks.m}{testnumpeaks.m} does this for your own x,y data.

\textbf{Peakfit Time Tests}. These are a series of scripts that demonstrate how the execution time of the \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m} function varies with the peak shape (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakfitTimeTest2.m}{PeakfitTimeTest2.m} and \href{https://terpconnect.umd.edu/~toh/spectrum/PeakfitTimeTest2a.m}{PeakfitTimeTest2a.m}, with number of peaks in the model (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakfitTimeTest.m}{PeakfitTimeTest.m}), and with the number of data points in the fitted region (\href{https://terpconnect.umd.edu/~toh/spectrum/FitSandP.m}{PeakfitTimeTest3.m}). This issue is discussed on page \pageref{ref-0471}.

\href{https://terpconnect.umd.edu/~toh/spectrum/TwoPeaks.m}{TwoPeaks.m} is a simple 8-line script that compares findpeaksG.m and peakfit.m with a signal consisting to two noisy peaks. findpeaksG.m and peakfit.m must be in the Matlab/Octave path.

\href{https://terpconnect.umd.edu/~toh/spectrum/peakfitVSfindpeaks.m}{peakfitVSfindpeaks.m} performs a direct comparison of the accuracy of findpeaksG vs peakfit. This script generates \href{https://terpconnect.umd.edu/~toh/spectrum/peakfitVSfindpeaks.png}{four very noisy peaks} of different heights and widths, then applies findpeaksG.m and peakfit.m to measure the peaks and compares the results. The peaks detected by findpeaks are labeled "Peak 1", "Peak 2", etc. If you run this script several times, you'll find that both methods work well most of the time, with peakfit giving smaller errors in most cases, but occasionally findpeaks will miss the first (lowest) peak and rarely it will detect an extra peak that is not there if the signal is very noisy.

\href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudyC.m}{CaseStudyC.m }is a self-contained Matlab/Octave demo function that demonstrates the application of several techniques described on this site to the quantitative measurement of a peak buried in an unstable background, a situation that can occur in the quantitative analysis applications of various forms of spectroscopy and remote sensing. See \href{https://terpconnect.umd.edu/~toh/spectrum/CaseStudies.html}{Case Studies C}.

\href{https://terpconnect.umd.edu/~toh/spectrum/GaussVsExpGauss.m}{GaussVsExpGauss.m} Comparison of alternative models for the unconstrained exponentially broadened Gaussians, shapes 31 and 39. Shape 31 (\href{https://terpconnect.umd.edu/~toh/spectrum/expgaussian.m}{expgaussian.m}) creates the shape by performing a Fourier convolution of a specified Gaussian by an exponential decay of specified time constant, whereas shape 39 (\href{https://terpconnect.umd.edu/~toh/spectrum/expgaussian2.m}{expgaussian2.m}) uses a mathematical expression for the final shape so produced. Both result in the \textit{same shape} but are parameterized differently. Shape 31 reports the peak height and position as that of the original Gaussian before broadening, whereas shape 39 reports the peak height of the broadened result. Shape 31 reports the width as the FWHM of the original Gaussian and shape 39 reports the standard deviation (sigma) of that Gaussian. Shape 31 reports the exponential factor on the \textit{number of data points} and shape 39 reports the \textit{reciprocal of time constant} in time units. See Figure windows \href{https://terpconnect.umd.edu/~toh/spectrum/GaussVsExpGaussFigure2.png}{\textbf{2}} and \href{https://terpconnect.umd.edu/~toh/spectrum/GaussVsExpGaussFigure3.png}{\textbf{3}}. You must have \href{https://terpconnect.umd.edu/~toh/spectrum/peakfit.m}{peakfit.m} (version 8.4) \href{https://terpconnect.umd.edu/~toh/spectrum/gaussian.m}{gaussian.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/expgaussian.m}{expgaussian.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/expgaussian2.m}{expgaussian2.m}, \href{https://terpconnect.umd.edu/~toh/spectrum/findpeaksG.m}{findpeaksG.m}, and \href{https://terpconnect.umd.edu/~toh/spectrum/halfwidth.m}{halfwidth.m} in the Matlab/Octave path. \href{https://terpconnect.umd.edu/~toh/spectrum/DemoExpgaussian.m}{DemoExpgaussian.m} is a script that gives another, more detailed, exploration of the effect of exponential broadening on a Gaussian peak (requires gaussian.m, expgaussian.m, halfwidth.m, val2ind.m, and peakfit.m in the Matlab/Octave path).

\href{https://terpconnect.umd.edu/~toh/spectrum/AsymmetricalOverlappingPeaks.m}{AsymmetricalOverlappingPeaks.m} is a multi-step script that demonstrates the use of a combination of first-derivative symmetrization before curve fitting to analyze a complex mystery peak. See page \pageref{ref-0429}).

\subsection{Keystroke-operated \textit{interactive} functions (for \textit{Matlab} or \textit{Matlab Online}) \label{ref-0519}}

The three interactive functions described above, \textbf{iPeak}, \textbf{iSignal}, and \textbf{ipf}, all have several keystroke commands in common: all share the same set of \textit{pan and zoom keys} (cursor arrow keys, ‘\textbf{{\textless}}’ and ‘\textbf{{\textgreater}’} keys, ‘[‘ and ‘]’ keys, etc.) to adjust the portion of the signal that is displayed in the upper panel. All use the \textbf{K} key to display the list of keystroke commands. All use the \textbf{T} key to cycle through the baseline correction modes. All use the \textbf{Shift-Ctrl-S}, \textbf{Shift-Ctrl-F}, and \textbf{Shift-Ctrl-P} keys to transfer the current signal between \textbf{iSignal}, \textbf{ipf}, and \textbf{iPeak}, respectively. To make it easier to transfer settings from one of these functions to other related functions, all use the \textbf{W} key to print out the syntax of other related functions, with the pan and zoom settings and other numerical input arguments specified, ready for you to Copy, Paste and edit into your own scripts or back into the command window. For example, you can convert a curve fitting operation performed in \textbf{ipf.m} into the command-line \textbf{peakfit.m} function; or you can convert a peak finding operation performed in ipeak.m into a command-line findpeaksG.m or findpeaksb.m function. The \textbf{W} key is useful with signals that require different signal processing in different regions of their x-axis ranges, by allowing you to create a series of command-line functions for each local region that, when executed in sequence, quickly processes each segment of the signal appropriately and can be repeated easily for any number of other examples of that same type of signal. To adjust continuously variable parameters, these programs use \textit{pairs of adjacent keys} to increase or decrease each parameter in steps, often with the \href{https://terpconnect.umd.edu/~toh/spectrum/functions.html\#Top}{shift-key} controlling the step size.

\subsection{Hyperlinear Quantitative Absorption Spectrophotometry\label{ref-0520}}

\href{https://terpconnect.umd.edu/~toh/spectrum/tfit.m}{\textbf{tfit.m}}, a self-contained command-line Matlab/Octave function that demonstrates a \href{https://terpconnect.umd.edu/~toh/spectrum/TFit.html}{computational method} for quantitative analysis by multiwavelength absorption spectroscopy which uses convolution and iterative curve fitting to correct for spectroscopic non-linearity. The syntax is tfit(TrueAbsorbance). \href{https://terpconnect.umd.edu/~toh/spectrum/TFitStats.m}{TFitStats.m} is a script that demonstrates the reproducibility of the method. \href{https://terpconnect.umd.edu/~toh/spectrum/TFitCalCurve.m}{TFitCalCurve.m} compares the calibration curves for single-wavelength, simple regression, weighted regression, and TFit methods. \href{https://terpconnect.umd.edu/~toh/spectrum/TFit3.m}{TFit3.m} is a demo function for a mixture of 3 absorbing components; the syntax is TFit3(TrueAbsorbanceVector), e.g., TFit3([3 .2 5]). Download all these as a \href{https://terpconnect.umd.edu/~toh/spectrum/TFit.zip}{ZIP} file. \href{https://terpconnect.umd.edu/~toh/spectrum/TFitAnimated.gif}{Click for animated example}.

\href{https://terpconnect.umd.edu/~toh/spectrum/TFitDemo.m}{TFitDemo.m} is a keypress-operated \textit{interactive} explorer for the Tfit method (for Matlab only), applied to the measurement of a single component with a Lorentzian (or Gaussian) absorption peak, with controls that allow you to adjust the true absorbance (``Peak A''), spectral width of the absorption peak (``AbsWidth''), spectral width of the instrument function (``InstWidth''), stray light, and the noise level (``Noise'') continuously while observing the effects graphically and numerically. See page \pageref{ref-0330}. \href{https://terpconnect.umd.edu/~toh/spectrum/TFitAnimated.gif}{Click for animated example}.

\subsection{MAT files (for Matlab and \href{https://terpconnect.umd.edu/~toh/spectrum/SignalArithmetic.html}{Octave}) and Text files (.txt)\label{ref-0521}}

\href{https://terpconnect.umd.edu/~toh/spectrum/DataMatrix2.mat}{DataMatrix2} is a computer-generated test signal consisting of 16 symmetrical Gaussian peaks with random white noise added. Can be used to test the peakfit.m function. See page \pageref{ref-0281}.

\href{https://terpconnect.umd.edu/~toh/spectrum/DataMatrix3.mat}{DataMatrix3} is a computer-generated test signal consisting of 16 Gaussian peaks with random white noise that have been exponentially broadened with a time constant of 33 x-axis units. See page \pageref{ref-0281}.

\href{https://terpconnect.umd.edu/~toh/spectrum/udx.txt}{udx.txt}: a text file containing the 2 x 1091 matrix that consists of two Gaussian peaks with different sampling intervals. It is used as an example in \href{https://terpconnect.umd.edu/~toh/spectrum/Smoothing.html\#Examples}{Smoothing} and in \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Noise}{Curve Fitting}. 

\href{https://terpconnect.umd.edu/~toh/spectrum/TimeTrial.txt}{TimeTrial.txt,} a text file comparing the speed of several different signal processing tasks, using the following different software configurations:

(a) Matlab 2020b on Windows 10, 64-bit, 3.6 GHz, core i7, 16 GBytes RAM

(b) Matlab 2009a, on older Windows machine

(c) Matlab 2017b Home, on older Windows machine

(d) Matlab Online, R2018b, in Google Chrome

(e) Matlab Mobile (on recent iPad)

(f) Octave 6.2.0 on Windows 10, 64-bit, 3.6 GHz, core i7, 16 GBytes RAM

The Matlab/Octave code that generated this is \href{https://terpconnect.umd.edu/~toh/spectrum/TimeTrial.m}{TimeTrial.m}, which runs all of the tasks one after the other and prints out the elapsed times for your machine plus the times previously recorded for each tasks on each of the five software systems. \href{https://terpconnect.umd.edu/~toh/spectrum/TimeTrial.xlsx}{TimeTrial.xlsx} summarizes the comparison of Matlab to Octave.

\href{https://terpconnect.umd.edu/~toh/spectrum/Readability.txt}{Readability.txt}. Report on the English language readability analysis of \href{https://terpconnect.umd.edu/~toh/spectrum/IntroToSignalProcessing.pdf}{IntroToSignalProcessing.pdf} performed by \url{http://www.online-utility.org/english/readability_test_and_improve.jsp}

\subsection{Spreadsheets (for Excel or OpenOffice Calc)\label{ref-0522}\label{ref-0523}}

\textbf{\textit{Notes. These spreadsheets are self-contained and so not rely on external files. You may transfer your data to them by using the Data tab and/or Copy and Paste.}}

If you see a yellow bar at the top of the spreadsheet window, click the "Enable Editing" button.

If your browser changes the file extension of these spreadsheets to .zip when they are downloaded, rename the files to their original file extensions (.ods, .xls, or .xlsx) before running them.

These spreadsheets have no protected cells, so there is nothing stopping you from changing the formulas accidentally. This means you can modify any aspect of these spreadsheets for your own purposes, which you are invited to do. If you mess up, just use the Undo function (\textbf{Ctrl-Z}) or you can download another copy. 

\textbf{Random numbers and noise} (page \pageref{ref-0022}). The spreadsheets \href{https://terpconnect.umd.edu/~toh/spectrum/RandomNumbers.xls}{RandomNumbers.xls} (for Excel) and \href{https://terpconnect.umd.edu/~toh/spectrum/RandomNumbers.ods}{RandomNumbers.ods} (for OpenOffice) demonstrate how to create a column of normally-distributed random numbers (like white noise) in a spreadsheet that has only a uniformly-distributed random number function. Also shows how to compute the interquartile range and the peak-to-peak value and how they compare to the standard deviation. See page \pageref{ref-0021}. The same technique is used in the spreadsheet \href{https://terpconnect.umd.edu/~toh/spectrum/SimulatedSignal6Gaussian.xlsx}{SimulatedSignal6Gaussian.xlsx}, which computes and plots a simulated signal consisting of up to 6 overlapping Gaussian bands plus random white noise. 

\textbf{Smoothing} (page \pageref{ref-0048}). The spreadsheets \href{https://terpconnect.umd.edu/~toh/spectrum/smoothing.ods}{smoothing.ods} (for Open office Calc) and \href{https://terpconnect.umd.edu/~toh/spectrum/smoothing.xls}{smoothing.xls} (for Microsoft Excel) demonstrate a 7-point rectangular (sliding average) in column C and a 7-point triangular smooth in column E, applied to the data in column A. You can type in (or Copy and Paste) any data you like into column A. You can extend the spreadsheet to longer columns of data by dragging the last row of columns A, C, and E down as needed. You can change the smooth width by changing the equations in columns C or E. The spreadsheet \href{https://terpconnect.umd.edu/~toh/spectrum/MultipleSmoothing.xls}{MultipleSmoothing.xls} for Excel or Calc demonstrates a more flexible method that allows you to define various types of smooths by typing a few integer numbers. The spreadsheets \href{https://terpconnect.umd.edu/~toh/spectrum/UnitGainSmooths.xls}{UnitGainSmooths.xls} and \href{https://terpconnect.umd.edu/~toh/spectrum/UnitGainSmooths.ods}{UnitGainSmooths.ods} contain a collection of unit-gain convolution coefficients for rectangular, triangular, and P-spline smooths of width 3 to 29 in both vertical (column) and horizontal (row) format. You can Copy and Paste these into your own spreadsheets. \href{https://terpconnect.umd.edu/~toh/spectrum/Convolution.txt}{Convolution.txt} lists some simple whole-number coefficient sets for performing single and multi-pass smoothing. \href{https://terpconnect.umd.edu/~toh/spectrum/VariableSmooth.xlsx}{VariableSmooth.xlsx} demonstrates an even more powerful and flexible technique, especially for very large and variable smooth widths, that uses the spreadsheet AVERAGE and INDIRECT functions (page \pageref{ref-0420}). It allows you to change the smooth width simply by changing the value of a single cell. See page \pageref{ref-0069} for details. \href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSmoothTemplate.xlsx}{SegmentedSmoothTemplate.xlsx} is a segmented\index{\textcolor{color-3}{segmented}} multiple-width data smoothing spreadsheet template, which can apply individually specified different smooth widths to different regions of the signal, especially useful if the widths of the peaks or the noise level varies substantially across the signal. In this version there are 20 segments. \href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSmoothExample.xlsx}{SegmentedSmoothExample.xlsx} is an example with data (\href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedSmoothExample.png}{graphic}). A related sheet \href{https://terpconnect.umd.edu/~toh/spectrum/GradientSmoothTemplate.xlsx}{GradientSmoothTemplate.xlsx} (\href{https://terpconnect.umd.edu/~toh/spectrum/GradientSmoothExample.png}{graphic}) performs a linearly increasing (or decreasing) smooth width across the entire signal, given only the start and end values, automatically generating as many segments are necessary. 

\textbf{Differentiation} (page \pageref{ref-0081}). \href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeSmoothingOO.ods}{DerivativeSmoothingOO.ods} (for OpenOffice Calc) and \href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeSmoothing.xls}{DerivativeSmoothing.xls} (for Excel) demonstrate the application of differentiation for measuring the amplitude of a peak that is buried in a broad curved background. Differentiation and smoothing are both performed together. Higher order derivatives are computed by taking the derivatives of previously computed derivatives. \href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeSmoothingWithNoise.xlsx}{DerivativeSmoothingWithNoise.xlsx} is a related spreadsheet that demonstrates the dramatic effect of smoothing on the signal-to-noise ratio of derivatives on a noisy signal. It uses the same signal as \href{https://terpconnect.umd.edu/~toh/spectrum/DerivativeSmoothing.xls}{DerivativeSmoothing.xls}, but adds simulated white noise to the Y data. You can control the amount of added noise. \href{https://terpconnect.umd.edu/~toh/spectrum/SecondDerivativeXY2.xlsx}{SecondDerivativeXY2.xlsx}, demonstrates locating and measuring changes in the second derivative (a measure of curvature or acceleration) of a time-changing signal, showing the apparent increase in noise caused by differentiation and the extent to which the noise can be reduced by smoothing (in this case by two passes of a 5-point triangular smooth). The smoothed second derivative shows a large peak at the point where the acceleration changes and plateaus on either side showing the magnitude of the acceleration before and after the change (2 and 4, respectively). \href{https://terpconnect.umd.edu/~toh/spectrum/Convolution.txt}{Convolution.txt} lists simple whole-number coefficient sets for performing differentiation and smoothing. \href{https://terpconnect.umd.edu/~toh/spectrum/CombinedDerivativesAndSmooths.txt}{CombinedDerivativesAndSmooths.txt} lists the sets of unit-gain coefficients that perform 1\textsuperscript{st} through 4\textsuperscript{th} derivatives with various degrees of smoothing. See page \pageref{ref-0081}.

\textbf{Peak} \textbf{sharpening} (page \pageref{ref-0099}). The derivative sharpening method with two derivative terms (2\textsuperscript{nd} and 4\textsuperscript{th}) is available in the form of an empty template (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningDeriv.xlsx}{PeakSharpeningDeriv.xlsx} and \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningDeriv.ods}{PeakSharpeningDeriv.ods}) or with example data entered (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningDerivWithData.xlsx}{PeakSharpeningDerivWithData.xlsx} and \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningDerivWithData.ods}{PeakSharpeningDerivWithData.ods}). You can either type in the values of the derivative weighting factors K1 and K2 directly into cells J3 and J4, or you can enter the estimated peak width (FWHM in number of data points) in cell H4 and the spreadsheet will calculate K1 and K2. There is a demo version with adjustable simulated peaks (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningDemo.xlsx}{PeakSharpeningDemo.xlsx} and \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningDemo.ods}{PeakSharpeningDemo.ods}), as well as a \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningDemo.xlsm}{version with clickable buttons} for convenient interactive adjustment of the K1 and K2 factors by 1\% or by 10\% for each click. There is also a 20-segment version where the sharpening constants can be specified for each of 20 signal segments (\href{https://terpconnect.umd.edu/~toh/spectrum/SegmentedPeakSharpeningDeriv.xlsx}{SegmentedPeakSharpeningDeriv.xlsx}). For applications where the peak widths gradually increase (or decrease) with time, there is also a gradient peak sharpening template (\href{https://terpconnect.umd.edu/~toh/spectrum/GradientPeakSharpeningDeriv.xlsx}{GradientPeakSharpeningDeriv.xlsx}) and an example with data (\href{https://terpconnect.umd.edu/~toh/spectrum/GradientPeakSharpeningDerivExample.xlsx}{GradientPeakSharpeningDerivExample.xlsx}); you need only set the starting and ending peak widths and the spreadsheet will apply the required sharpening factors K1 and K2.  \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSymmetricalizationDemo.xlsm}{PeakSymmetricalizationDemo.xlsm} (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningAreaMeasurementDemoEMG3.png}{graphic}) demonstrates the symmetrization of exponentially modified Gaussians (EMG) by the \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html\#Asymmetrical}{weighted addition of the first derivative} (and also allows further second derivative sharpening of the resulting symmetrized peak). There is also an empty template \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSymmetricalizationTemplate.xlsm}{PeakSymmetricalizationTemplate.xlsm} (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSymmetricalizationExample.png}{graphic}) and an example application with sample data already typed in: \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSymmetricalizationExample.xlsm}{PeakSymmetricalizationExample.xlsm}. \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDoubleSymmetrizationExample.xlsm}{PeakDoubleSymmetrizationExample.xlsm} performs the symmetrization of a \textit{doubly} exponential broadened peak. It has buttons to interactively adjust the two stages of first-derivative weighting. Two variations (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakDoubleSymmetrizationExample1.xlsm}{1}, \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDoubleSymmetrizationExample2.xlsm}{2}) include example data for two overlapping peaks, for which the areas after symmetrization are measured by perpendicular drop. \href{https://terpconnect.umd.edu/~toh/spectrum/ComparisonOfPerpendicularDropAreaMeasurements.xlsx}{ComparisonOfPerpendicularDropAreaMeasurements.xlsx} (\href{https://terpconnect.umd.edu/~toh/spectrum/ComparisonOfPerpendicularDropAreaMeasurements.png}{graphic}) demonstrates the effect of the \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html\#power}{\textit{power sharpening method}} on perpendicular drop area measurements of Gaussian and exponentially broadened Gaussian peaks, including the effect of resolution, relative peak height, random noise, smoothing, and non-zero baseline has on the normal and power sharpening method. \href{https://terpconnect.umd.edu/~toh/spectrum/PowerSharpeningTemplate.xlsx}{PowerSharpeningTemplate.xlsx} is an empty template that preforms this method and \href{https://terpconnect.umd.edu/~toh/spectrum/PowerSharpeningExample.xlsx}{PowerSharpeningExample.xlsx} is the same with example data.

\textbf{Convolution (page} \pageref{ref-0137}\textbf{).} Spreadsheets can be used to perform "shift-and-multiply" convolution for small data sets (for example, \href{https://terpconnect.umd.edu/~toh/spectrum/MultipleSmoothing.xls}{MultipleConvolution.xls} or \href{https://terpconnect.umd.edu/~toh/spectrum/MultipleConvolution.xlsx}{MultipleConvolution.xls}\href{http://terpconnect.umd.edu/~toh/spectrum/MultipleConvolution.xlsx}{x} for Excel and \href{https://terpconnect.umd.edu/~toh/spectrum/MultipleConvolutionOO.ods}{MultipleConvolutionOO.ods} for Calc), which is essentially the same technique as the above spreadsheets for smoothing and differentiation. Use this spreadsheet to investigate convolution, smoothing, differentiation, and the effect of those operations on noise and signal-to-noise ratio. (For larger data sets the performance is slower than Fourier convolution, which is much easier done in Matlab or Octave than in spreadsheets). \href{https://terpconnect.umd.edu/~toh/spectrum/Convolution.txt}{Convolution.txt} lists simple whole-number coefficient sets for performing differentiation and smoothing.

\textbf{Peak Area}\index{Area} \textbf{Measurement (page} \pageref{ref-0173}\textbf{)}. \href{https://terpconnect.umd.edu/~toh/spectrum/EffectOfDx.xlsx}{EffectOfDx.xlsx} demonstrates that the simple equation sum(y)*dx accurately measures the peak area of an isolated Gaussian peak if there are at least 4 or 5 points visibly above the baseline. \href{https://terpconnect.umd.edu/~toh/spectrum/EffectOfNoiseAndBaseline.xlsx}{EffectOfNoiseAndBaseline.xlsx} demonstrates the effect of random noise and non-zero baseline, showing that the area is more sensitive to non-zero baseline that the same amount of random noise. \href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningAreaMeasurementDemo.xlsm}{PeakSharpeningAreaMeasurementDemo.xlsm} (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakSharpeningAreaMeasurementDemo.png}{screen image}) demonstrates the effect of \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html}{derivative peak sharpening} on perpendicular drop area measurements of two overlapping Gaussian peaks. Sharpening the peaks reduces the degree of overlap and can greatly reduce the peak area measurement error errors made by the \textit{perpendicular drop} method (page \pageref{ref-0184}). The spreadsheets listed under ``Peak Sharpening'' on the previous page include peak area measurement.

\textbf{Curve Fitting (page} \pageref{ref-0199}\textbf{).} \href{https://terpconnect.umd.edu/~toh/spectrum/LeastSquares.xls}{LeastSquares.xls} and \href{https://terpconnect.umd.edu/~toh/spectrum/LeastSquares.odt}{LeastSquares.odt} perform polynomial least-squares fits to a straight-line model and \href{https://terpconnect.umd.edu/~toh/spectrum/QuadraticLeastSquares.xls}{QuadraticLeastSquares.xls} and \href{https://terpconnect.umd.edu/~toh/spectrum/QuadraticLeastSquares.ods}{QuadraticLeastSquares.ods} does the same for a quadratic (parabolic) model. There are specific versions of these spreadsheets that also calculate the concentrations of the unknowns (download complete set as \href{http://terpconnect.umd.edu/~toh/models/CalibrationSpreadsheets.zip}{CalibrationSpreadsheets.zip}). 

\textbf{Multi-component spectroscopy (page} \pageref{ref-0243}\textbf{).} \href{https://terpconnect.umd.edu/~toh/spectrum/RegressionTemplate.xls}{RegressionTemplate.xls} and \href{https://terpconnect.umd.edu/~toh/spectrum/RegressionTemplate.ods}{RegressionTemplate.ods} (\href{https://terpconnect.umd.edu/~toh/spectrum/RegressionTemplate.png}{graphic with example data}) perform multicomponent analysis using the \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingB.html}{matrix method} for a \textit{fixed} 5-component, 100 wavelength data set. \href{https://terpconnect.umd.edu/~toh/spectrum/RegressionTemplate2.xls}{RegressionTemplate2.xls} uses a more advanced spreadsheet technique (page \pageref{ref-0420}) that allows the template to \textit{automatically adjust} to different numbers of components and wavelengths. Two examples show the \textit{same} template with data entered for a mixture of 5 components measured at 100 wavelengths (\href{https://terpconnect.umd.edu/~toh/spectrum/RegressionTemplate2Example.xls}{RegressionTemplate2Example.xls}) and for 2 components at 59 wavelengths (\href{https://terpconnect.umd.edu/~toh/spectrum/RegressionTemplate3Example.xls}{RegressionTemplate3Example.xls}). 

\textbf{Peak fitting (page} \pageref{ref-0229}\textbf{).} A set of spreadsheets using the \href{https://www.solver.com/solver-tutorial-using-solver?gclid=CjwKCAjwur7YBRA\_EiwASXqIHMFct5zaxGyiACQoUf1tmQ1B0lidPPfxfwgIDsVombZgc-BgNtvH1hoCO\_oQAvD\_BwE}{Solver }function to perform \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Spreadsheets}{iterative non-linear peak fitting} for multiple overlapping peak models is described \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFittingC.html\#Spreadsheets}{here}. There are versions for Gaussian and for Lorentzian peak shapes, with and without baseline, for 2-6 peak models and 100 wavelengths (with instructions for modification). All of these have file names beginning with "\href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitterSpreadsheets.png}{CurveFitter...}".

\textbf{Peak detection and measurement (page} \pageref{ref-0294}\textbf{).} The spreadsheet \href{https://terpconnect.umd.edu/~toh/spectrum/PeakAndValleyDetectionTemplate.xlsx}{PeakAndValleyDetectionTemplate.xlsx} (or \href{https://terpconnect.umd.edu/~toh/spectrum/PeakAndValleyDetectionExample.xlsx}{PeakAndValleyDetectionExample.xlsx} with sample data), is a simple peak and valley detector that defines a peak as any point with lower points on both sides and a valley as any point with higher points on both sides (see page \pageref{ref-0513}). The spreadsheet \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetection.xls}{PeakDetection.xls} implements as more selective derivative zero-crossing peak detection method described on page \pageref{ref-0305}. In both cases, the input x,y data are contained in Sheet1, columns \textbf{A} and \textbf{B}, starting in row 9. (You can paste your own data there). See \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionExample.xlsx}{PeakDetectionExample.xlsx}/\href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionExample.xls}{.xls}) for an example with data already pasted in. \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionDemo2.xls}{ PeakDetectionDemo2.xls}/\href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionDemo2.xlsx}{xlsx} is a demonstration with a user-controlled computer-generated series of peaks. \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionSineExample.xls}{PeakDetectionSineExample.xls} is a demo that generates a sinusoid with an adjustable number of peaks.

An extension of that method is made in \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionAndMeasurement.xlsx}{PeakDetectionAndMeasurement.xlsx} (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakFindingAndMeasurement.png}{screen image}), which makes the assumption that the peaks are \textit{Gaussian} and measures their height, position, and width on the \textit{unsmoothed} data using a \textit{least-squares technique}, just like "\href{https://terpconnect.umd.edu/~toh/spectrum/findpeaks.m}{findpeaksG.m}". The advantage of this technique is that it eliminates the peak distortion that might results from smoothing the data to prevent false peaks arising from random noise. For the first 10 peaks found, the x,y original unsmoothed data are copied to Sheets 2 through 11, respectively, where that segment of data is subjected to a Gaussian least-squares fit, using the same technique as \href{https://terpconnect.umd.edu/~toh/spectrum/CurveFitting.html\#GaussFit}{GaussianLeastSquares.xls}. The best-fit Gaussian parameter results are copied back to Sheet1, in the table in columns \textbf{AH-AK}. (In its present form. the spreadsheet is limited to measuring 10 peaks, although it can detect any number of peaks. Also, it is limited in Smooth Width and Fit Width by the 17-point convolution coefficients). The spreadsheet is available in OpenOffice (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionAndMeasurement.ods}{.ods}) and in Excel (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionAndMeasurement.xls}{.xls}) and (\href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionAndMeasurement.xlsx}{.xlsx}) formats. They are functionally equivalent and differ only in minor cosmetic aspects. An \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionAndMeasurementExample.ods}{example} spreadsheet, with data, is available. A \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionExample2.xlsx}{demo version}, with a calculated noisy waveform that you can modify, is also available. See page \pageref{ref-0326}. If the peaks in the data are too much overlapped, they may not make sufficiently distinct maxima to be detected reliably. If the noise level is low, the peaks can be artificially sharped by the \href{https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html}{derivative sharpening technique described previously.} This is implemented by \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionAndMeasurementPS.xlsx}{PeakDetectionAndMeasurementPS.xlsx} and its demo version \href{https://terpconnect.umd.edu/~toh/spectrum/PeakDetectionAndMeasurementDemoPS.xlsx}{PeakDetectionAndMeasurementDemoPS.xlsx}. 

\textbf{Spreadsheets for the TFit Method (page} \pageref{ref-0331}\textbf{):} Hyperlinear Quantitative Absorption Spectrophotometry. \href{https://terpconnect.umd.edu/~toh/spectrum/TransmissionFittingTemplate.xls}{TransmissionFittingTemplate.xls} (\href{https://terpconnect.umd.edu/~toh/spectrum/TFitSpreadsheetTemplate.png}{screen image}) is an empty template for a single isolated peak;\href{https://terpconnect.umd.edu/~toh/spectrum/TransmissionFittingTemplateExample.xls}{TransmissionFittingTemplateExample.xls} (\href{https://terpconnect.umd.edu/~toh/spectrum/TFitSpreadsheetTemplateExample.png}{screen image}) is the same template with example data entered. \href{https://terpconnect.umd.edu/~toh/spectrum/TransmissionFittingDemoGaussian.xls}{TransmissionFittingDemoGaussian.xls} (\href{https://terpconnect.umd.edu/~toh/spectrum/TFitSpreadsheetDemoGaussian.png}{screen image}) is a demonstration with a simulated Gaussian absorption peak with variable peak position, width, and height, plus added stray light, photon noise, and detector noise, as viewed by a spectrometer with a triangular slit function. You can vary all the parameters and compare the best-fit absorbance to the true peak height and to the conventional log(1/T) absorbance.

\href{https://terpconnect.umd.edu/~toh/spectrum/TransmissionFittingCalibrationCurve.xls}{TransmissionFittingCalibrationCurve.xls} (\href{https://terpconnect.umd.edu/~toh/spectrum/TransmissionFittingCalibrationCurve.png}{screen image}) includes an Excel \href{https://terpconnect.umd.edu/~toh/spectrum/macro2.txt}{macro} (page \pageref{ref-0376}) that automatically constructs a calibration curve comparing the TFit and conventional log(1/T) methods, for a series of 9 standard concentrations that you can specify. To create a calibration curve, enter the standard concentrations in AF10 - AF18 (or just use the ones already there, which cover a 10,000-fold concentration range), enable macros, then press \textbf{Ctrl-f} (or click \textbf{Developer} tab, click \textbf{Macros}, select \textbf{macro2}, and click \textbf{Run}). This macro constructs and plots the calibration curve for both the TFit (blue dots) and conventional (red dots) methods and computes the R\textsuperscript{2 }value for the TFit calibration curve, in the upper right corner of graph. (Note: you can also use this spreadsheet to compare the precision and reproducibility of the two methods by entering the \textit{same} concentration 9 times in AF10 - AF18. The result should be a straight flat line with zero slope).

\textbf{Advanced spreadsheet techniques} (page \pageref{ref-0420}): ``\href{https://terpconnect.umd.edu/~toh/spectrum/SpecialFunctions.xlsx}{SpecialFunctions.xlsx}'' (\href{https://terpconnect.umd.edu/~toh/spectrum/SpecialFunctions.png}{Graphic}) demonstrates the applications of the MATCH, INDIRECT, COUNT, IF, and AND functions when dealing with data arrays of variable size. ``\href{https://terpconnect.umd.edu/~toh/spectrum/IndirectLINEST.xls}{IndirectLINEST.xls}'' (\href{https://terpconnect.umd.edu/~toh/spectrum/IndirectLINEST.png}{Graphic} link) demonstrates the particular benefit of using the INDIRECT function in conjunction with \textit{array} functions such as INV and LINEST. 

\chapter{Afterword\label{ref-0524}}

\section{How this book came to be. \label{ref-0525}}

During my career at the University of Maryland in the Department of Chemistry and Biochemistry, I did \href{https://terpconnect.umd.edu/~toh/OHaverCV.html\#books}{research in analytical chemistry} and developed and taught several courses including an upper-division undergraduate lab course in ``\href{https://terpconnect.umd.edu/~toh/Chem498C/}{Electronics for Chemists}'', which by the 1980s included a laboratory computer component and one experiment in digital data acquisition and processing dealing with the use of mathematical and numerical techniques used in the processing of experimental data from scientific instruments. When the Web became available to the academic community in the early 90s, like many instructors, I put up a syllabus, experiments, and other reading material for this and for my other courses online for students to access.

When I retired from the University in 1999, after 30 years of service, I noticed that I was getting a lot of pageviews on that course site that came from outside the University, especially directed to the lab experiment in digital data processing that I had developed in the 80's, when computers were relatively new in chemistry laboratories. I started getting an increasing number of emails with questions, suggestions, and comments from people in widely varied scientific fields. So, I began to broaden this beyond chemistry and my specific course. Ultimately, I decided to make this a long-term retirement project. My aim is to help science workers learn and apply computer-based mathematical data processing techniques, by producing free tutorial materials that explains things intuitively rather than in mathematical formality, with coding examples, practical software, and guidance/consulting on specific projects. Analytical chemists like myself are basically tool builders. In the early days of our profession, the tools where mainly chemical (e.g., color reagents), but in later years included instruments (e.g., spectroscopy and chromatography), and by the late 20\textsuperscript{th} century included software tools.

\section{Who needs this software? \label{ref-0526}}

Isn't software already included in every scientific instrument hardware purchase? This is true, especially for those who are using conventional instruments in standard ways. But many scientists are working in new research areas for which there are no commercial instruments, or they are building completely new types of instruments, or they are using modifications of existing systems for which there is no software. In some cases, the software provided with commercial instruments is inflexible, inadequately documented, or hard to use. Not every researcher or science worker likes programming, or has time for it, or is good at it. Hired programmers typically do not understand the science and in any case sooner or later move on and no longer maintain their code. Well-documented code is more important than ever. I enjoy writing and coding, so this seemed to be a niche I could fit into.

\section{Organization. \label{ref-0527}}

My project has five parts:
\begin{itemize}
\item A book, entitled "A Pragmatic Introduction to Signal Processing", available in both \href{https://www.amazon.com/Pragmatic-Introduction-Signal-Processing-applications/dp/153335491X/ref=sr\_1\_1?s=books&ie=UTF8&qid=1467637283&sr=1-1&keywords=A+Pragmatic+Introduction+to+Signal+Processing}{paper}, Kindle, and in DOCX and PDF printable online formats;

\item A \href{https://terpconnect.umd.edu/~toh/spectrum/TOC.html}{Web site} (.edu domain), with essentially the same material as the book. No sign-in or registration is required.

\item Downloadable free software in several different forms, listed on the \href{https://terpconnect.umd.edu/~toh/spectrum/functions.html}{web site} and page \pageref{ref-0499}.

\item Help and consulting via email (optionally with data attachments).

\item A \href{https://www.facebook.com/groups/237474013116361/}{Facebook group} and the \href{https://www.mathworks.com/matlabcentral/fileexchange/?term=authorid\%3A24576}{Matlab File Exchange} for announcements and public discussion.


\end{itemize}
Although the complete book is available freely in DOCX and PDF format, several readers have found it too long to print themselves and have requested a pre-printed version, which is now \href{https://www.amazon.com/Introduction-Processing-applications-scientific-measurement/dp/B084NZRZM3}{sold through Amazon} (\href{https://www.amazon.com/dp/1792916590}{ISBN} \textcolor{color-23}{979-8589799453}). The on-line materials, software, help, and consulting are all free. Open-source software alternatives are available, namely Octave and OpenOffice/LibreOffice. 

\section{Methodology\label{ref-0528}}

My policy is that contact with users ("clients") is initiated only from the clients and is strictly in written form, in English, mostly by email or Facebook group message - not phone or \textit{Skype}. Requests for direct real-time voice or video communication are politely deflected. This is done to allow extended conversations between time zones, to preserve communications in written form, and to avoid language problems and my own age-related hearing difficulties (readers have come from at least 162 different countries). Written communications via email also allow the use of machine translation apps such as Google Translate. Moreover, clients can send examples of their data via email attachment or via Google drive.

Information about the affiliation of the client and the nature of the project is not solicited and is strictly at the discretion of the client. Client information and data are kept confidential. In many cases, I know nothing about the origin of their data and must treat it as abstract numbers. I usually do not know the age, gender, race, country of origin, level of education, experience, or employment of clients unless they tell me. I must look for clues in their writing to gauge their level of knowledge and experience and to avoid insulting them on the one hand or confusing them on the other. Everyone is welcome.

I have attempted to minimize the use of fancy formatting and special effects on my web site, to make it compatible with older operating systems and browsers. No account or registration is needed. I allow no advertising on my web pages. I minimize the use of video, but I do use simple GIF animations where it would be useful, as these can be viewed right on the web page without downloading any addition plug-ins or software. I test my formatting to make sure it viewable on mobile devices (tablet, smartphone). 

\section{Influence of the Internet\label{ref-0529}}

The Internet spans traditional boundaries. There are many different countries, states, universities, departments, specialties, and journals, but only one global Internet. Most, but not all, of it is accessible to anyone with an internet connection and a computer, tablet, or smartphone. Google (or any search engine) looks at (almost) the entire internet, irrespective of the academic specialization, leading to the possibility that a solution arising in one corner of scholarship will be discovered by a need in another corner. Why, for example, would a neuroscientist, or a cancer researcher, or a linguist, or a music scholar, know anything about my work? They would surely not, if I published only in the scientific journals of my specialty. But in fact, all those types of researchers, and hundreds more from other diverse fields, have found my work by "stumbling across it" in a search engine query, rather than by reading scholarly publications, and have found it useful enough to cite in their publications. In my own academic career, I published research only in analytical chemistry journals, which are read mostly by other analytical chemists. In contrast, my Web hits, emails, and citations have come from a much wider range of scientists, engineers, researchers, instructors, and students working in academia, industry, environmental fields, medical, engineering, earth science, space, military, financial, agriculture, communications, and even language and musicology.

\section{Writing\label{ref-0530}}

I intended my writing to be instructional, not especially scholarly, or rigorous. It is unashamedly \textit{pragmatic}, meaning ``Relating to matters of fact or practical affairs, often to the exclusion of intellectual or artistic matters; practical as opposed to idealistic.'' For many people, abstract mathematics can be a barrier to understanding. I make only basic assumptions about prior knowledge beyond the usual college science major level: minimal math background and an 11th grade (USA high school) reading level, according to several \href{https://terpconnect.umd.edu/~toh/spectrum/Readability.txt}{automated readability indexes} (Gunning Fog index; Coleman-Liau index; Flesch-Kincaid Grade level; ARI; SMOG; Flesch Reading Ease; ATOS Level). I have tried to minimize slang and obscure idioms and figures of speech that might confuse translators (machine and human), and I try to minimize the use of the passive voice. I often explain the same concept more than once in different contexts because I believe that can help to make some ideas ``stick'' better. An important part of my writing process is \textit{feedback from users}, by email, social media, search engine terms, questions, corrections, etc. Moreover, I also regularly re-read older sections with ``fresh eyes'', correcting errors, and making improvements in phrasing. Questions from readers, and even search terms in Google searches, suggest areas where improvements are possible.

To make access easier, I make my writing available in multiple formats: Web (Simple HTML, with graphics and silent self-running GIF animations, and a site-specific search); DOCX (editable Microsoft Word), the latest version of which display the GIF animations; PDF (Portable Document Format) for printing, and paperback and Kindle versions, through Amazon's \href{https://kdp.amazon.com/}{Kindle Direct Publishing} program. All except for the web version have a detailed table of contents. All except the paperback and Kindle versions are free.

A paper book is usually read starting from the beginning: the table of contents and the introduction. But web site access, especially via search engines (Google, Bing, etc.), is not related to the order of pages. This is evident in the data for web page accesses: the table of contents and introduction are \textit{not} the most accessed; in fact, on most days there are \textit{no visits at all} to the table of contents or to the introduction pages. This can cause a problem with sequencing the topics, which is only partially reduced by including, on each Web page, hot links to the table of contents and to related previous and following material. (The print version has an average of three internal page references per page, plus a table of contents with over 200 entries). Also, to facilitate communication, I have added a "mail-to" link to each page in the Web version that includes my email address and the title of the page as the subject line (so I can tell from the email's subject line what page they were on when they clicked the mail-to link). 

\section{Software platform selection criteria\label{ref-0531}}

As for software platforms, I chose two types: spreadsheets (page \pageref{ref-0009}) and Matlab (page \pageref{ref-0013}) and its clone Octave (page \pageref{ref-0016}). Both have the advantage of being multi-platform; they run on PC, Mac, Unix, even on mobile devices (tablets/iPad) and on miniature deployable devices (e.g., Octave on Raspberry Pi, page \pageref{ref-0409}). Both are popular development environments that have large user communities with multiple contributors, and both are widely used in science applications. Both have a degree of backward compatibility that allows for interoperability with older legacy versions. Companies, organizations, and college campuses often have site licenses for these products. These platforms also have the advantage that they avoid secret algorithms, that is, their algorithms can be viewed in detail by any user. Their code is distributed in "open source" and "open document" formats that are either in plain text format (such as Matlab ".m" files) or in a format that could be opened and inspected using even free software (e.g., Microsoft Excel .xls and .xlsx spreadsheets can be opened with OpenOffice or LibreOffice). For those who cannot afford expensive software, OpenOffice Calc (page \pageref{ref-0010}) and Octave (page \pageref{ref-0016}) can be downloaded without cost.

Most of my Matlab/Octave programs are ``functions'', which are essentially modular bits of code that fit together in different ways, much like high-tech Lego bricks, rather than self-contained stand-alone programs with elaborate graphical user interfaces, like commercial programs. Functions can be useful on their own but can also be used as components to construct something bigger. You can write your own functions or, if you wish, you can download and use \href{https://www.mathworks.com/matlabcentral/fileexchange/}{functions written by others}.  When using functions, you could simply ignore the internal code and use the well-defined standard inputs and outputs. This is analogous to assembling customized electronics systems using standard AC power adaptors, USB and HDMI ports and cables, or Bluetooth connections between smartphone/tablets/computers and printers/earphones/speakers, etc., without worrying about the internal design of each component. I chose Matlab/Octave because of its high performance, very wide popularity, and its similarity to other languages that have often been used by  scientists, such as Fortran, Basic, and Pascal. Even so, there are other languages that have their champions and would have been valid alternatives, such as R, Python, Mathematica, Julia, and Scilab. In the interests of time and sanity, I have limited myself, for the time being, to Matlab/Octave.

I have tried to strike a balance between cost, speed, ease of use, and learning curve. I have attempted to make my software usable even to those who do not read all the documentation, by providing lots and lots of examples and demos, including animated GIFs that will play on any web browser. Every script or function has \textit{built-in} help that is internal to the software. In Matlab/Octave; you can display this built-in help simply by typing ``help \_\_'', where \_\_ is the name of the script or function. These help files contain not only instructions but also simple \textit{examples of use} and in many cases include references to other similar functions. You can always look at (and even modify) my code if you wish, by opening it in the Matlab/Octave editor, but it is not necessary if the existing action and inputs and outputs provide what you need. The spreadsheet templates and their examples and demos also have built-in instructions, and most of the spreadsheet have pop-up ``cell comments'' on certain cells (marked by a red dot) that pop up when the mouse pointer is hovered over them, providing an explanation for the function of that cell. 

\section{Outcomes\label{ref-0532}}

My website has received over 2 million page views and over 100,000 downloads of my software programs (currently several hundred per month), from either my \href{https://terpconnect.umd.edu/~toh/spectrum/TOC.html}{web site} or from the \href{https://www.mathworks.com/matlabcentral/fileexchange/?term=authorid\%3A24576}{Matlab File Exchange}. I have received thousands of emails with comments, suggestions, corrections, questions, offers to translate, etc. Comments from readers have been overwhelmingly positive, even enthusiastic, as indicated by these verbatim excerpts from emails \href{https://terpconnect.umd.edu/~toh/spectrum/index.html\#comments}{about the website} and about \href{https://terpconnect.umd.edu/~toh/spectrum/SignalProcessingTools.html\#comments}{my software}. In fact, many of these comments are so ``over the top'' that one wonders\textit{: why such enthusiasm for such a nerdy topic?} After all, most people do not take the time to write to the authors of web sites, especially to compliment them. One factor is that the number of users of the global Internet is so huge that even highly specialized topics can gather a substantial audience. As they say, "A wide net catches even the rarest fish". According to the National Science Foundation, there are 11,000,000 graduate students in science worldwide, and UNESCO estimates that there are 7,000,000 full-time equivalents in research and development worldwide, and that makes it likely that among them will be a substantial number who will find my work useful. But I also believe that part of the reason for the enthusiastic response is that software documentation is often poorly written and hard to understand, so more effort is needed in better explaining software and how it works and where it cannot be expected to work. I try to be responsive, answering each email and acting on their suggestions and corrections. The growth in social media is also a contributing factor; for a specific example of that, from the \href{https://www.mathworks.com/matlabcentral/fileexchange/?term=authorid\%3A24576}{Matlab File Exchange}, see \href{https://blogs.mathworks.com/pick/2016/09/09/most-activeinteractive-file-exchange-entry/https:/blogs.mathworks.com/pick/2016/09/09/most-activeinteractive-file-exchange-entry/}{https://blogs.mathworks.com/pick/2016/09/09/most-activeinteractive-file-exchange-entry/}. 

\textbf{Impact.} Positive comments and lots of downloads are nice, but not everyone who downloads something tries in their work, and not everyone who does try it finds it valuable enough to cite it in their publications. Most gratifyingly, as of December 2020, \textit{over 515 publications had cited my website and programs}, based on \textit{Google Scholar} searches (as listed beginning on page \pageref{ref-0537}), covering an extraordinarily wide range of topics in industry, environment, medical, engineering, earth science, space, military, financial, agriculture, communications, and even occasionally language and musicology.

\chapter{References \label{ref-0533}\label{ref-0534}}

\input{../../IntroToSignalProcessing_Tail.tex}

\end{document}
